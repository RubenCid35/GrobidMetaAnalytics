<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A brief survey of visualization methods for deep learning models from the perspective of Explainable AI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-09">January 9, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,252.77,170.76,106.45,10.48"><forename type="first">Ioannis</forename><surname>Chalkiadakis</surname></persName>
						</author>
						<title level="a" type="main">A brief survey of visualization methods for deep learning models from the perspective of Explainable AI</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-09">January 9, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">A04788FB6605748DE7488CB0BE1FF0E8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,96.21,239.87,88.82,12.62;1,72.00,266.26,231.58,10.52">Introduction 1.1 Motivation and expected outcome</head><p><s coords="1,72.00,287.60,468.00,9.57;1,72.00,301.15,468.00,9.57;1,72.00,314.70,60.06,9.57">The fairly recent success of neural networks in large scale tasks ranging from speech recognition to localization and mapping of autonomous systems, has made them prevalent in state-of-the-art AI applications.</s><s coords="1,136.78,314.70,403.22,9.57;1,72.00,328.25,468.00,9.57">Although the success is in part due to algorithmic advances in the training procedure of such networks, the intrinsics of the learning process and the learned content remain largely vague.</s><s coords="1,72.00,341.80,468.00,9.57;1,72.00,355.35,468.00,9.57;1,72.00,368.90,61.55,9.57">However, since such AI systems will be used even more frequently to deal with safety critical tasks, for instance, autonomous driving or social robotic applications, the issue of trusting them arises imperatively.</s><s coords="1,140.50,368.90,399.51,9.57;1,72.00,382.45,468.00,9.57;1,72.00,396.00,227.78,9.57">How can a car manufacturing company persuade clients to let an AI system drive their car when the manufacturers themselves cannot fully explain how that system works or the conditions under which it successfully operates?</s><s coords="1,304.57,396.00,235.44,9.57;1,72.00,409.55,468.00,9.57;1,72.00,423.10,66.85,9.57">How can a doctor employ a neural network-based system to infer about a patient's treatment if the doctor has no way to know how that inference will be made?</s><s coords="1,144.87,423.10,395.13,9.57;1,72.00,436.65,71.05,9.57">Is it possible that a robotic mechanism supposed to aid an elderly person will fail in the process?</s></p><p><s coords="1,88.94,450.19,451.06,9.57;1,72.00,463.74,468.00,9.57;1,72.00,477.29,468.00,9.57;1,72.00,490.84,23.06,9.57">Although research into rigorous mathematical explanation of neural networks is ongoing, such questions raise the issue of how to build neural network models that will be interpretable not only to experts in the task field but also to users without prior knowledge or expertise in the area.</s><s coords="1,101.29,490.84,226.72,9.57">Research such as that presented by Nguyen et.</s><s coords="1,332.11,490.84,207.88,9.57;1,72.00,504.39,468.00,9.57;1,72.00,517.94,468.00,9.57;1,72.00,531.49,467.99,9.57;1,72.00,545.04,98.58,9.57">al., 2015 [1], where they manage to 'fool' a convolutional neural network into making an erroneous classification decision with high confidence is an indication that despite huge successes in test tasks, we cannot fully trust machine learning systems and, in particular, neural networks; we need to have a way of understanding what makes them fail or succeed.</s></p><p><s coords="1,88.94,558.59,451.06,9.57;1,72.00,572.14,468.00,9.57;1,72.00,585.69,468.01,9.57">Moreover, from a human rights and fiscal point of view, the European Union's General Data Protection Regulation (GDPR, EU 2016/679) which will take effect as a law in 2018, highlights the importance of being able to explain decision-making systems (Goodman and Flaxman, 2016 [2]).</s><s coords="1,72.00,599.24,468.00,9.57;1,72.00,612.79,167.05,9.57">The regulation stipulates the right of citizens to be able to understand how an algorithmic decision that concerned them was reached.</s><s coords="1,247.27,612.79,292.74,9.57;1,72.00,626.33,468.00,9.57;1,72.00,639.88,468.00,9.57;1,72.00,653.43,261.41,9.57">The importance of this regulation is paramount considering that many of the state-of-the-art algorithms that social media, recommender and credit assessment systems employ are difficult to understand, even for experts in the area, or totally inexplicable if they are based, for instance, on deep neural networks.</s><s coords="1,339.60,653.43,200.40,9.57;1,72.00,666.98,468.01,9.57;1,72.00,680.53,411.85,9.57">Furthermore, according to the regulation, given that many AI systems are used for profiling purposes, one should be able to explain that the algorithm that made a particular decision was impartial and non-discriminatory.</s><s coords="1,491.15,680.53,48.85,9.57;1,72.00,694.08,468.00,9.57;1,72.00,707.63,403.45,9.57">Therefore, algorithm designers and machine learning experts should be able to explain to non-experts in an understandable way how their system inferred its final decision from input features.</s><s coords="1,481.53,707.63,58.47,9.57">In case they</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="2,72.00,75.51,468.00,9.57;2,72.00,89.06,245.43,9.57">fail to do so, under the aforementioned directive their employers (possibly tech giants like Facebook and Google) could face fines up to billions of euros.</s></p><p><s coords="2,88.94,102.61,168.44,9.57">Inspired by the work of Nguyen et.</s><s coords="2,261.24,102.61,278.76,9.57;2,72.00,116.16,468.00,9.57;2,72.00,129.71,26.12,9.57">al. we aim to study a special neural network architecture, namely the Recurrent Neural Network (RNN), and in particular its behavior under critical conditions.</s><s coords="2,104.75,129.71,435.25,9.57;2,72.00,143.25,468.00,9.57;2,72.00,156.80,85.20,9.57">We are confident, that given their similarity to convolutional neural networks (see section 3), they will also exhibit the same brittle behavior, which we will try to explain in a visual, easily interpretable way.</s><s coords="2,162.04,156.80,377.96,9.57;2,72.00,170.35,447.23,9.57">Our work will contribute to the interpretability of one of the most popular and successful models of neural networks, and thus further spread their application to more areas.</s><s coords="2,524.06,170.35,15.94,9.57;2,72.00,183.90,468.00,9.57;2,72.00,197.45,352.43,9.57">For the time being we aim to focus on a certain application area of RNN, possibly language generation due to the fact that it is a well-researched and visualization-suitable field.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2" coords="2,72.00,225.91,122.90,10.52">Scope of review</head><p><s coords="2,72.00,247.26,343.61,9.57">Our future work can be tentatively divided into the following sub-tasks:</s></p><p><s coords="2,88.36,272.76,302.26,9.57">• Generation of stimuli that 'fool' a Recurrent Neural Network</s></p><p><s coords="2,88.36,295.28,224.78,9.57">• Identification of network's inherent weakness</s></p><p><s coords="2,88.36,317.79,451.64,9.57;2,99.27,331.34,440.72,9.57;2,88.94,356.85,451.06,9.57;2,72.00,370.40,368.71,9.57">• Provision of an interactive, visual explanation of the network's failure, that will allow a network designer to understand how the design decisions affected the robustness of the network Because of the vast amount of literature that refers to deep neural networks and attempts at visualizing them, we set certain criteria that the review material should meet.</s><s coords="2,445.44,370.40,94.55,9.57;2,72.00,383.95,383.88,9.57">The following list of criteria was consulted when assembling research papers to include in the review:</s></p><p><s coords="2,88.36,409.45,451.63,9.57;2,99.27,423.00,154.06,9.57">• Reference to Recurrent Neural Networks (if relevant literature is sparse, look for literature in Convolutional Neural Networks)</s></p><p><s coords="2,88.36,445.52,333.64,9.57">• Popular network architecture (Long-Short Term Memory networks)</s></p><p><s coords="2,88.36,468.03,304.67,9.57">• Domain of RNN application -is it amenable to visualization?</s></p><p><s coords="2,88.36,490.55,197.28,9.57">• Availability/Popularity of dataset used</s></p><p><s coords="2,88.36,513.06,178.67,9.57">• Easily reproducible baseline results</s></p><p><s coords="2,88.36,535.58,401.18,9.57">• Reference to construction of stimuli that break the network ('adversarial' stimuli)</s></p><p><s coords="2,88.36,558.09,320.36,9.57">• Provision of visualization or understanding of intricacies of RNN</s></p><p><s coords="2,88.36,580.61,284.88,9.57">• Degree of human interpretability of the proposed method</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,88.36,603.12,327.51,9.57">• Reference to quantitative evaluation of the visualization technique</head><p><s coords="2,88.94,628.63,451.07,9.57;2,72.00,642.18,400.87,9.57">Having identified relevant papers to our research, we further refined the collection to key papers that we will base our project on, and which will comprise the basis of the review.</s><s coords="2,480.14,642.18,59.86,9.57;2,72.00,655.73,379.91,9.57">In those key papers we looked for the following information that will be helpful to our work:</s></p><p><s coords="2,88.36,681.23,338.15,9.57;2,88.36,703.75,350.91,9.57">• Methods of constructing and/or visualizing an RNN/LSTM network • Specificity of the architecture and general applicability to various tasks</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="3,88.36,75.51,389.24,9.57">• Domain and type (sequence generation or classification) of the application task</head><p><s coords="3,88.36,98.02,451.63,9.57;3,99.27,111.57,440.73,9.57;3,99.27,125.12,179.24,9.57">• Popularity and availability of the dataset and codebase, as well as reproducibility of the presented results (familiarity with development framework, published parameters and the training procedure that was followed)</s></p><p><s coords="3,88.36,147.64,236.51,9.57">• Reference to construction of adversarial stimuli</s></p><p><s coords="3,88.36,170.15,427.03,9.57;3,88.36,192.67,451.64,9.57;3,99.27,206.22,440.73,9.57">• Attempt at explaining (visually or otherwise) the failure of the network to such stimuli • In the case of a visual explanation, reference to the exact method employed (interactivity, what is visualized, degree of attractiveness and interpretability to non-experts, target group)</s></p><p><s coords="3,88.36,228.73,422.40,9.57">• Provision of (quantitative) evaluation of the research results and visualization method</s></p><p><s coords="3,88.36,251.25,265.51,9.57">• Reference to future work that aligns with our project</s></p><p><s coords="3,88.94,276.75,451.06,9.57;3,72.00,290.30,407.71,9.57">The above lists, proved to be of utmost importance during writing the review as there are ample papers that refer to recurrent networks but very few are actually relevant to our goals.</s><s coords="3,484.40,290.30,55.60,9.57;3,72.00,303.85,468.00,9.57;3,72.00,317.40,468.00,9.57;3,72.00,330.95,297.09,9.57">In addition, there is rich literature on visualizing convolutional networks, as their parameters are amenable to visualization as images, but scarce literature on visualization methods for recurrent networks, and in particular non-expert interpretable visualization techniques.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3" coords="3,72.00,359.41,235.45,10.52">Outline and organization of review</head><p><s coords="3,72.00,380.76,468.00,9.57;3,72.00,394.31,162.06,9.57">We attempted to organize the review in a way that allows the reader to coherently follow and relate the various aspects of the project.</s></p><p><s coords="3,88.94,407.86,451.06,9.57;3,72.00,421.41,143.76,9.57">We begin the report with section 2 where we set the framework on trust and the necessity for interpretability of AI systems.</s><s coords="3,220.79,421.41,319.21,9.57;3,72.00,434.95,320.60,9.57">In section 3 we present the Recurrent Neural Network architecture and a certain variant, the LSTM, which will be our main focus.</s><s coords="3,401.98,434.95,138.02,9.57;3,72.00,448.50,468.00,9.57;3,72.00,462.05,408.31,9.57">In the same section we also identify similarities and differences between the RNN and the CNN architectures on which we base our expectation that they exhibit similar behavior under critical operating conditions.</s><s coords="3,485.07,462.05,54.93,9.57;3,72.00,475.60,468.00,9.57;3,72.00,489.15,219.72,9.57">In section 4 we review work on constructing adversarial stimuli, i.e. inputs to the network that will cause it to work under critical conditions, close to failure.</s><s coords="3,296.44,489.15,243.56,9.57;3,72.00,502.70,379.72,9.57">Next, in section 5 we refer to visualization methods in RNN and also to selected literature on deep networks' and CNN visualization.</s><s coords="3,456.36,502.70,83.64,9.57;3,72.00,516.25,468.00,9.57;3,72.00,529.80,288.88,9.57">Finally, in section 6 we present work on evaluating visual representations in order to be able to formally decide which visualizations are indeed easy to understand by non-experts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="3,72.00,561.78,8.07,12.62;3,96.21,561.78,140.57,12.62">Trust in AI systems</head><p><s coords="3,72.00,588.52,468.00,9.57;3,72.00,602.07,468.01,9.57">Due to the rise in popularity and effectiveness of machine learning systems, and especially neural networks, a discussion has been initiated about the consequences of such technology to society.</s><s coords="3,72.00,615.62,223.70,9.57">How safe and accident-proof are such systems?</s></p><p><s coords="3,88.94,629.17,451.06,9.57;3,72.00,642.72,468.00,9.57;3,72.00,656.27,122.50,9.57">Usually researchers focus on improving the effectiveness of AI systems in specific tasks but they tend to overlook their behavior when faced with 'outliers', i.e. data at the edge of the system's successful operation area.</s><s coords="3,200.92,656.27,339.07,9.57;3,72.00,669.82,136.54,9.57">The issue of accidents involving machine learning systems is the focus of <ref type="bibr" coords="3,86.24,669.82,118.66,9.57">Amodei et. al., 2016 [3]</ref>.</s><s coords="3,218.86,669.82,321.14,9.57;3,72.00,683.37,468.00,9.57;3,72.00,696.92,468.00,9.57;3,72.00,710.47,114.55,9.57">They define accidents as 'unintended and harmful behavior that may emerge from poor design of real-world AI-systems'; however, we add that one should also consider the possibility that such events could be incited not only inadvertently but also by malicious attackers to the system.</s><s coords="3,191.81,710.47,348.19,9.57;4,72.00,75.51,468.01,9.57;4,72.00,89.06,468.00,9.57;4,72.00,102.61,37.43,9.57">In their work, they categorize safety-related research problems according to their fundamental cause: a wrong training criterion of the network, a training criterion that is too expensive to evaluate frequently, or potential unpredictable behavior during the training process.</s></p><p><s coords="4,88.94,116.16,451.07,9.57;4,72.00,129.71,468.00,9.57;4,72.00,143.25,308.98,9.57">Defining a wrong training criterion can, according to the writers, be expressed as not taking into consideration important environmental variables while designing the objective function, or, implement an objective function that could easily be 'cheated'.</s><s coords="4,388.95,143.25,151.04,9.57;4,72.00,156.80,468.00,9.57">Furthermore, even if the training/rewarding criterion is correct, it is pointed out that it could be too expensive to often evaluate.</s><s coords="4,72.00,170.35,468.00,9.57;4,72.00,183.90,131.71,9.57">This means that the system will have to use a faster but less accurate criterion and resort to the exact only at specific times.</s><s coords="4,208.49,183.90,331.50,9.57;4,72.00,197.45,94.33,9.57">Lastly, they emphasize the failures that could occur due to badly prepared training data.</s><s coords="4,170.92,197.45,369.08,9.57;4,72.00,211.00,468.00,9.57;4,72.00,224.55,131.94,9.57">In addition to this categorization, they present a comprehensive list of possible accidents that each of the aforementioned problems could induce, as well as potential solutions and experiments to verify them.</s></p><p><s coords="4,88.94,238.10,451.06,9.57;4,72.00,251.65,349.67,9.57">However, their work mainly revolves around (deep) reinforcement learning and refers to issues that could arise from the learning agent's behavior and its environment.</s><s coords="4,428.05,251.65,111.96,9.57;4,72.00,265.20,468.00,9.57;4,72.00,278.75,468.01,9.57;4,72.00,292.30,275.78,9.57">The most relevant part of their work to ours is what they refer to as 'robustness to distributional shift', namely how to ensure that the system will perform well in an environment other than that of the training, and it will be able to identify when it has failed to perform well.</s><s coords="4,352.63,292.30,187.37,9.57;4,72.00,305.84,468.00,9.57;4,72.00,319.39,313.64,9.57">Furthermore, we agree with the writers that such failures should not be just detectable, but the designers should also have a rigorous method ('statistical assurances') as to how often they will occur.</s><s coords="4,392.17,319.39,147.83,9.57;4,72.00,332.94,468.00,9.57;4,72.00,346.49,148.66,9.57">This is in accordance with our aim of providing a visual interpretation whose quality will be objectively evaluated as accurate and understandable to non-experts.</s></p><p><s coords="4,88.94,360.04,328.32,9.57">The susceptibility to distributional shift is in effect what Nguyen et.</s><s coords="4,421.12,360.04,118.88,9.57;4,72.00,373.59,157.03,9.57">al., 2015 <ref type="bibr" coords="4,465.25,360.04,10.91,9.57" target="#b0">[1]</ref>, exploited in the paper that inspires our work.</s><s coords="4,233.80,373.59,306.20,9.57;4,72.00,387.14,468.00,9.57;4,72.00,400.69,333.14,9.57">Employing either an evolutionary algorithm or a gradient ascent method, they managed to produce image samples that a convolutional network classifies with high confidence, however, they are completely unrecognizable to humans.</s><s coords="4,413.12,400.69,126.88,9.57;4,72.00,414.24,468.00,9.57;4,72.00,427.79,299.50,9.57">They stress that that was not their purpose; on the contrary, they hoped that the fooling images would be recognizable by humans, as was the case with previous similar work (see <ref type="bibr" coords="4,352.71,427.79,11.27,9.57" target="#b3">[4]</ref>).</s><s coords="4,378.80,427.79,161.21,9.57;4,72.00,441.34,468.00,9.57;4,72.00,454.89,468.00,9.57;4,72.00,468.43,468.00,9.57;4,72.00,481.98,39.09,9.57">They use the most popular CNN architectures (AlexNet, LeNet) and widely available datasets (MNIST, ImageNet2012) and manage to prove that the size of the dataset is not conducive to the robustness of the network, as it can still be fooled, and that using fooling images in the training set does not stop the network from being cheated.</s><s coords="4,118.03,481.98,421.97,9.57;4,72.00,495.53,226.91,9.57">Particularly interesting is the fact that they cheat different or the same but differently initialized architectures with the same images.</s><s coords="4,306.56,495.53,233.45,9.57;4,72.00,509.08,468.00,9.57;4,72.00,522.63,118.96,9.57">This reinforces our belief that a different model exhibiting similarities with the CNN, for instance an RNN, will experience the same problems under critical conditions.</s><s coords="4,195.89,522.63,344.11,9.57;4,72.00,536.18,468.01,9.57;4,72.00,549.73,468.01,9.57;4,72.00,563.28,468.00,9.57;4,72.00,576.83,89.77,9.57">Our assumption is also supported by the reason to which they attribute the failure of the network: the fact that the CNN learns to discriminate based on the details of a class rather than the sum of the typical features that the class entails (some images might be in the same class area of the image space, and at the same time be far apart from natural images in the same class).</s><s coords="4,168.33,576.83,371.67,9.57;4,72.00,590.38,114.62,9.57">Given that one of the uses of RNN is classification, we expect that they will exhibit the same pitfall.</s><s coords="4,192.10,590.38,347.91,9.57;4,72.00,603.93,422.87,9.57">On the other hand, they note that generative models could be harder to fool as they will take into account the low marginal probability of a bogus example input.</s><s coords="4,499.56,603.93,40.44,9.57;4,72.00,617.48,421.40,9.57">This is a crucial remark that we have to take into consideration if we select a generative task (e.g.</s><s coords="4,498.18,617.48,41.82,9.57;4,72.00,631.03,140.12,9.57">language generation) for our approach.</s></p><p><s coords="4,88.94,644.57,126.13,9.57">The findings of Nguyen et.</s><s coords="4,218.40,644.57,321.60,9.57;4,72.00,658.12,468.00,9.57;4,72.00,671.67,37.64,9.57">al. prove that machine learning models can be guided into wrongful decisions either accidentally or deliberately, regardless of their architecture, training parameters and dataset.</s><s coords="4,114.48,671.67,425.52,9.57;4,72.00,685.22,247.99,9.57">Consequently, how could we trust the behavior of such models, especially when many -if not all -of them still lack mathematical explanation?</s><s coords="4,324.55,685.22,215.46,9.57;4,72.00,698.77,351.30,9.57">This is the question posed by Lipton, 2016 <ref type="bibr" coords="4,525.46,685.22,10.91,9.57" target="#b4">[5]</ref>, who emphasizes the need for interpretability of machine learning models.</s><s coords="4,429.20,698.77,110.81,9.57;4,72.00,712.32,468.00,9.57;5,72.00,75.51,468.00,9.57;5,72.00,89.06,36.91,9.57">In his work, he tries to set the framework around interpretability of (supervised) machine learning models, by questioning the notion of interpretability, its motives and the properties that an interpretable model should possess.</s><s coords="5,113.73,89.06,426.27,9.57;5,72.00,102.61,227.33,9.57">The dangers that he presents that could make an ML model untrustworthy are similar to those presented more thoroughly in Amodei et.</s><s coords="5,303.24,102.61,236.75,9.57;5,72.00,116.16,384.40,9.57">al. <ref type="bibr" coords="5,318.66,102.61,11.52,9.57" target="#b2">[3]</ref> : wrong or over-simplistic training criteria, as well as divergence between the training data and the deployment environment.</s><s coords="5,461.08,116.16,78.92,9.57;5,72.00,129.71,468.00,9.57;5,72.00,143.25,275.27,9.57">This is often due to difficulty in expressing the actual task objective in the training function, and that is where the writer believes that an interpretation of the model helps.</s><s coords="5,353.43,143.25,186.57,9.57;5,72.00,156.80,468.00,9.57;5,72.00,170.35,399.09,9.57">Interpretable models could instill trust in their users by uncovering (potentially) causal relations between their input and their decisions, hence help verify if the training function encourages learning the desired behavior.</s><s coords="5,477.34,170.35,62.66,9.57;5,72.00,183.90,468.01,9.57;5,72.00,197.45,468.00,9.57;5,72.00,211.00,306.22,9.57">Furthermore, the long-term goal of transferability in machine learning models, i.e. be able to successfully use a model trained on different data than those of the deployment environment, would be much easier were we in a position to interpret the model's inner workings.</s><s coords="5,386.57,211.00,153.44,9.57;5,72.00,224.55,468.00,9.57;5,72.00,238.10,337.91,9.57">Finally, in the spirit of the EU regulation mentioned in the previous chapter, an interpretable model can account for its output and/or the input data even if its actual mechanism remains a mystery.</s></p><p><s coords="5,88.94,251.65,451.06,9.57;5,72.00,265.20,197.70,9.57">In our opinion, the most important contribution of the paper to the area is its effort to define the properties of an interpretable model.</s><s coords="5,276.69,265.20,263.31,9.57;5,72.00,278.75,468.00,9.57;5,72.00,292.30,317.30,9.57">They are divided into two categories: those related to transparency, i.e. explanation of the inner workings of the model and those related to 'post-hoc' interpretations, namely what can the model reveal about the task.</s><s coords="5,394.15,292.30,145.85,9.57;5,72.00,305.84,468.00,9.57;5,72.00,319.39,371.47,9.57">The first category includes the suitability of the model for human simulation, the explicability of the algorithm that the model implements as well as the ability to explain the parts that form the model: e.g.</s><s coords="5,448.13,319.39,91.88,9.57;5,72.00,332.94,67.33,9.57">parameters, inputs, computations.</s><s coords="5,143.96,332.94,396.04,9.57;5,72.00,346.49,468.00,9.57;5,72.00,360.04,468.00,9.57;5,72.00,373.59,153.40,9.57">On the other hand, post-hoc interpretations do not try to illuminate the inner workings of the model; such interpretations could be natural language (textual/spoken) explanations, visualizations of parameters or learned representations, as well as example explanations, similar to human explanations by analogy.</s></p><p><s coords="5,88.94,387.14,451.06,9.57;5,72.00,400.69,468.00,9.57;5,72.00,414.24,50.00,9.57">The terminology that Lipton introduces is of utmost importance for the area of ML interpretability, as it is still an emerging area and a common language between researchers has to be developed.</s></p><p><s coords="5,88.94,427.79,451.07,9.57;5,72.00,441.34,338.54,9.57">In our opinion, a work suitable to illustrate how the aforementioned notions bind with each other (although prior to above publication) is that of <ref type="bibr" coords="5,297.79,441.34,109.12,9.57">Chuang et. al., 2012 [6]</ref>.</s><s coords="5,415.27,441.34,124.73,9.57;5,72.00,454.89,468.00,9.57;5,72.00,468.43,468.00,9.57;5,72.00,481.98,367.86,9.57">They define interpretation as referring 'to the facility with which an analyst makes inferences about the data through the lens of a model abstraction' and trust as referring 'to the actual and perceived accuracy of an analysts inferences' and introduce a similarity measure for text collections.</s><s coords="5,448.37,481.98,91.62,9.57;5,72.00,495.53,468.00,9.57;5,72.00,509.08,468.00,9.57;5,72.00,522.63,468.00,9.57;5,72.00,536.18,55.24,9.57">However, the most important contribution of this work is the fact that they stress the importance of model driven visualizations and, even if not directly stated, the importance of human interpretability of such models ('...model abstractions should correspond to analysts' mental models of a domain to aid reasoning').</s><s coords="5,134.96,536.18,405.05,9.57;5,72.00,549.73,376.03,9.57">In addition, emphasis is put on the importance of principled approaches in modeldriven visualizations and proceed to present design guidelines for this purpose:</s></p><p><s coords="5,88.36,569.20,451.64,9.57;5,99.27,582.75,440.73,9.57;5,99.27,596.29,299.75,9.57">• The target audience's tasks and background should be aligned with the information that the visualization conveys; towards this direction it is useful to define 'units of analysis', i.e. entities, relations and concepts that the visualization captures.</s></p><p><s coords="5,88.36,617.59,451.64,9.57;5,99.27,631.14,237.55,9.57">• The units of analysis should answer potential questions of the target audience, but should also be appropriate and expressible by the model.</s></p><p><s coords="5,88.36,652.44,451.64,9.57;5,99.27,665.98,118.89,9.57">• Models should be continually assessed for their suitability to the target audience's needs, e.g. by means of comparison.</s><s coords="5,223.87,665.98,316.13,9.57;5,99.27,679.53,124.85,9.57">This imposes visualization methods and units of analysis that are independent of the model.</s></p><p><s coords="5,72.00,699.00,441.96,9.57">In general, the area of trust in machine learning models and neural networks is in its infancy.</s><s coords="5,518.73,699.00,21.27,9.57;5,72.00,712.55,468.00,9.57;6,72.00,75.51,468.00,9.57;6,72.00,89.06,193.91,9.57">This could also be attributed to the broad spectrum of the notion of 'trust': one should take into account the fact that trust is very task-dependent, as the scope and the degree of safety needed, is what will define trust in the system involved.</s><s coords="6,273.37,89.06,266.64,9.57;6,72.00,102.61,468.00,9.57;6,72.00,116.16,421.90,9.57">In a robotics/autonomous systems context, the idea of trust is more prominent in applications such as self-driving cars, healthcare assistants, human-robot cooperation in the workspace (e.g. in an industrial plant) and search-and-rescue missions.</s><s coords="6,498.51,116.16,41.49,9.57;6,72.00,129.71,468.00,9.57;6,72.00,143.25,127.55,9.57">All these applications are not only safety-critical, but also exhibit a wide variety of different conditions under which they should operate.</s><s coords="6,204.25,143.25,335.75,9.57;6,72.00,156.80,468.00,9.57;6,72.00,170.35,323.21,9.57">Therefore, trust under these applications entails not only a high degree of certainty that the system will perform well but also that it will perform well under all working conditions or know the working conditions that will lead to failure.</s><s coords="6,401.05,170.35,138.95,9.57;6,72.00,183.90,468.00,9.57;6,72.00,197.45,467.99,9.57;6,72.00,211.00,468.00,9.57;6,72.00,224.55,330.38,9.57">For instance, an autonomous robotic manipulator in a factory, working together with an employee should be able to reliably cooperate with any employee assigned to that post, and not only with the few that might have trained it; a search-and-rescue robot should be able to differentiate between debris and its target, regardless of the material or light conditions that it operates under.</s><s coords="6,409.21,224.55,130.79,9.57;6,72.00,238.10,468.00,9.57;6,72.00,251.65,468.01,9.57;6,72.00,265.20,397.37,9.57">Similarly, a self-driving car should be reliable in its driving operation regardless of the person behind the wheel, who might choose to customize the driving mode of the car (e. g. cruise or sport), and a robotic assistant for elderly people should be able to adapt to different mobility capabilities of its users.</s><s coords="6,88.94,278.75,451.07,9.57;6,72.00,292.30,468.00,9.57;6,72.00,305.84,468.00,9.57">Regarding research resources, there are very limited relevant publications, and based on our experience after reviewing the area, the papers presented above take the most important steps towards setting the foundations of the area and establishing a common language in the community.</s><s coords="6,72.00,319.39,468.00,9.57;6,72.00,332.94,468.00,9.57;6,72.00,346.49,468.00,9.57;6,72.00,360.04,468.00,9.57;6,72.00,373.59,468.01,9.57;6,72.00,387.14,115.51,9.57">However, the related few and very recent workshops introduced in big AI conferences (Workshop on Visualization for Deep Learning -ICML 2016, Workshop on Human Interpretability in Machine Learning -ICML 2016, Interpretable ML for Complex Systems -NIPS 2016, IEEE Visualization, Interpretation and Visualization of Deep Neural Nets -ACCV 2016, Methods for Interpreting and Understanding Deep Neural Networks -ICASSP 2017 (upcoming)), prove that it is an area that will thrive in the future.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="6,72.00,419.12,8.07,12.62;6,96.21,419.12,188.77,12.62">Recurrent neural networks</head><p><s coords="6,72.00,445.86,468.00,9.57;6,72.00,459.41,239.44,9.57">While reviewing the literature on trusting AI systems, we came across no paper that dealt with the issue in connection with Recurrent Networks.</s><s coords="6,318.38,459.41,221.62,9.57;6,72.00,472.96,432.06,9.57">Nonetheless, as already mentioned, we expect that they too exhibit dubious behavior in a setting similar to the work of Nguyen (see <ref type="bibr" coords="6,485.27,472.96,11.27,9.57" target="#b0">[1]</ref>).</s><s coords="6,508.87,472.96,31.13,9.57;6,72.00,486.51,468.00,9.57;6,72.00,500.06,258.55,9.57">In this section we will briefly introduce the RNN architecture and shed light on similarities and differences with CNN, which further strengthen our assumptions.</s></p><p><s coords="6,88.94,513.61,451.06,9.57;6,72.00,527.16,219.05,9.57">The most basic form of recurrent network (sometimes referred to in the literature as vanilla RNN ) was presented in 1990 by Elman ( <ref type="bibr" coords="6,272.26,527.16,11.27,9.57" target="#b6">[7]</ref>).</s><s coords="6,296.15,527.16,243.85,9.57;6,72.00,540.71,468.00,9.57;6,72.00,554.26,468.00,9.57;6,72.00,567.81,24.24,9.57">RNN are feedforward networks with the innovative element of states: each unit is connected not only to the previous layer of the architecture, but also to its own previous state in time, i.e. information it withheld from the previous pass of the data.</s><s coords="6,101.71,567.81,438.29,9.57;6,72.00,581.36,306.54,9.57">In essence, depth in RNN refers not to the number of layers (although there are RNN with multiple computational layers) but rather to depth in time steps.</s><s coords="6,383.21,581.36,156.79,9.57;6,72.00,594.91,300.25,9.57">This makes them ideal to process sequences where the order of appearance of the data matters.</s><s coords="6,379.30,594.91,160.71,9.57;6,72.00,608.45,308.63,9.57">The mathematical formulation of the RNN update rule is expressed by the following equations <ref type="bibr" coords="6,366.09,608.45,10.91,9.57" target="#b7">[8]</ref>:</s></p><formula xml:id="formula_0" coords="6,225.81,632.93,160.39,35.31">h t = H(W xh x t + W hh h t-1 + b h ) y t = H(W hy h t + b y )</formula><p><s coords="6,72.00,677.46,406.43,10.67;6,478.43,682.12,3.06,6.99;6,481.99,677.50,56.49,9.57;6,72.00,691.01,468.00,10.81;6,72.00,704.59,327.15,9.57">where x t is the input in timestep t = 1, ..., T , h t is the hidden state in timestep t, y t , t = 1, ..., T is the output sequence, b h , b y are the bias vectors, and W xh , W hh , W hy are the input-to-hidden, hidden-to-hidden and hidden-to-output weight matrices respectively.</s><s coords="7,88.94,324.19,451.07,9.57;7,72.00,337.74,410.29,9.57">The problem with RNN is the fact that depending on the type of activation unit, gradient values could diminish to zero or saturate the neuron at a large value during training.</s><s coords="7,488.09,337.74,51.90,9.57;7,72.00,351.29,468.00,9.57">Given that the past information is stored in the weights, this problem poses a threat to the network's training.</s><s coords="7,72.00,364.83,468.00,9.57;7,72.00,378.38,242.23,9.57">To counter this, Hochreiter and Schmidhuber <ref type="bibr" coords="7,291.55,364.83,11.52,9.57" target="#b8">[9]</ref> introduced in 1997 a new type of activation unit, called the Long Short-Time Memory cell (LSTM).</s><s coords="7,318.24,378.38,221.76,9.57;7,72.00,391.93,468.00,9.57;7,72.00,405.48,468.00,9.57;7,72.00,419.03,418.58,9.57">The operation of this type of cells is based on three gates: the input gate, that controls the influx of information from the previous layer into the cell, the output gate that controls the outflux to the next cell and the 'forget gate' <ref type="bibr" coords="7,491.67,405.48,16.97,9.57" target="#b9">[10]</ref> which determines how much of the current state's information will remain in the cell memory.</s><s coords="7,495.57,419.03,44.43,9.57;7,72.00,432.58,468.00,9.57;7,72.00,446.13,63.39,9.57">Formally, the operation of the LSTM cell (the function H in the above notation) is described by the following equations <ref type="bibr" coords="7,120.85,446.13,10.91,9.57" target="#b7">[8]</ref>:</s></p><formula xml:id="formula_1" coords="7,197.62,459.65,216.76,90.77">i t = σ(W xi x t + W hi h t-1 + W ci c t-1 + b i ) f t = σ(W xf x t + W hf h t-1 + W cf c t-1 + b f ) o t = σ(W xo x t + W ho h t-1 + W co c t-1 + b o ) c t = f t c t-1 + i t tanh(W xc x t + W hc h t-1 + b c ) h t = o t tanh(c t )</formula><p><s coords="7,72.00,559.80,468.00,9.57;7,72.00,573.35,60.03,9.57">where σ is the sigmoid function, and i, f, o, c are respectively the input, forget, output gates and cell memory.</s><s coords="7,136.88,573.35,250.70,9.57">An illustration of an LSTM cell is shown in figure <ref type="figure" coords="7,379.10,573.35,4.24,9.57" target="#fig_0">1</ref>.</s></p><p><s coords="7,88.94,586.90,451.06,9.57;7,72.00,600.45,104.74,9.57">Having presented the basics of RNN and LSTM networks, it is useful to outline similarities and differences with CNN.</s><s coords="7,180.14,600.45,359.87,9.57;7,72.00,614.00,468.00,9.57;7,72.00,627.55,468.00,9.57;7,72.00,641.10,410.96,9.57">To begin with, the notion of weight sharing is present in both architectures, although in a different domain: CNN share weights in space, i.e. the same weight matrix is used on the whole image thus inducing depth in the spatial domain, whereas RNN share weights in time, since the same weight matrix is used over all time steps, thus inducing depth in time.</s><s coords="7,488.26,641.10,51.74,9.57;7,72.00,654.65,468.00,9.57;7,72.00,668.20,468.01,9.57;7,72.00,681.75,468.00,9.57;7,72.00,695.30,188.12,9.57">Given that the locality properties of images can also be captured as a sequence by feeding pixels in order of appearance to the network, one would expect that an RNN could suffer from the same 'fooling' problems as a CNN as they are rooted to the fact that the network learns minute details of image regions rather than global class traits.</s><s coords="7,268.89,695.30,271.11,9.57;7,72.00,708.85,468.00,9.57;8,72.00,75.51,468.00,9.57;8,72.00,89.06,262.15,9.57">Furthermore, the idea of local receptive fields in CNN, namely that each neuron receives input from a neighborhood of neurons in the previous layer, is similar to the memory property (or state) of the RNN/LSTM: each unit receives also input from a few processing steps back, or a 'neighborhood in time'.</s></p><p><s coords="8,88.94,102.61,451.06,9.57;8,72.00,116.16,208.11,9.57">On the other hand, CNN need to have input of a predefined size, whereas an RNN is suited to processing sequences of arbitrary length.</s><s coords="8,285.61,116.16,254.39,9.57;8,72.00,129.71,468.00,9.57;8,72.00,143.25,37.11,9.57">This induces context to each input, which is actually the strength of the RNN architecture, and thus we expect that it will be harder to 'fool' than a CNN.</s><s coords="8,113.50,143.25,426.50,9.57;8,72.00,156.80,468.00,9.57;8,72.00,170.35,468.00,9.57;8,72.00,183.90,83.75,9.57">Regarding the visualization component, one of the challenges of our goal for interactive visualization will be the fact that RNN have a strong task dependency when it comes to visualizing them, as it is not obvious what to visualize or how in the network, contrary to the obvious way for CNN parameters.</s></p><p><s coords="8,88.94,197.45,451.06,9.57;8,72.00,211.00,468.00,9.57;8,72.00,224.55,32.79,9.57">We will present research into visualization techniques in a later section, however since they are dependent on our task of 'fooling' the network, we first review techniques for building 'adversarial' inputs.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="8,72.00,256.53,8.07,12.62;8,96.21,256.53,150.54,12.62">Adversarial examples</head><p><s coords="8,72.00,283.27,339.20,9.57">The notion of 'adversarial' examples was first introduced by Szegedy et.</s><s coords="8,414.37,283.27,125.63,9.57;8,72.00,296.82,232.98,9.57">al., 2014 <ref type="bibr" coords="8,457.14,283.27,11.52,9.57" target="#b3">[4]</ref> and since then it has developed into a very active research area.</s></p><p><s coords="8,88.94,310.37,266.23,9.57">Formally, they present adversarial examples as follows.</s><s coords="8,362.29,310.37,47.68,9.57;8,409.97,308.42,7.49,6.99;8,422.26,310.37,117.74,9.57;8,72.00,323.92,468.00,10.77;8,72.00,337.47,9.25,9.57;8,81.25,335.52,7.49,6.99;8,91.62,337.47,68.57,9.57;8,160.18,335.52,6.59,6.99;8,167.27,337.47,3.03,9.57">Let f : R m → {1...k} be a mapping classifying image pixel vectors to a set of labels, and an associated continuous loss function loss f : R m × {1...k} → R + .</s><s coords="8,175.14,337.47,117.40,9.57;8,292.54,335.52,7.49,6.99;8,304.15,337.47,235.85,9.57;8,72.00,351.02,54.58,9.57">For a given image x ∈ R m and label l, the following optimization problem is formulated:</s></p><p><s coords="8,88.94,392.92,301.42,10.63">Minimize the norm of a minute perturbation r, r 2 subject to:</s></p><formula xml:id="formula_2" coords="8,272.03,417.42,67.43,34.08">f (x + r) = l x + r ∈ [0, 1] m</formula><p><s coords="8,72.00,461.96,468.00,9.57;8,72.00,475.51,468.00,9.57;8,72.00,489.05,90.45,9.57">The new image x + r is an adversarial example if f (x) = l, namely it is an image that differs imperceptibly to a human from the original one, and yet the network classifies it with high confidence to a different class.</s></p><p><s coords="8,88.94,502.60,451.06,9.57;8,72.00,516.15,468.00,9.57;8,72.00,529.70,468.00,9.57;8,72.00,543.25,468.00,9.57;8,72.00,556.80,57.66,9.57">Their work drew important conclusions: first, it is the whole space rather than the individual neurons that maintain the learned information; second, the mapping learned by the network is in general discontinuous since a very small perturbation of the input can lead to its wrong classification; third, adversarial examples can fool different networks and even networks trained on a different training set.</s></p><p><s coords="8,88.94,570.35,451.06,9.57;8,72.00,583.90,468.00,9.57;8,72.00,597.45,247.23,9.57">In our opinion, their first conclusion is particularly useful to visual interpretations of neural networks, since it implies that a visualization of the individual activations, which is common practice, is not particularly helpful to human understanding.</s><s coords="8,324.72,597.45,215.29,9.57;8,72.00,611.00,400.67,9.57">On the contrary, a visualization of the whole space of activations could be more helpful towards interpreting the network's output.</s><s coords="8,477.33,611.00,62.67,9.57;8,72.00,624.55,468.00,9.57;8,72.00,638.10,373.35,9.57">Furthermore, since they specifically refer to neural networks that learn by back-propagation, it makes us wonder whether a different training strategy would discover a more robust mapping.</s><s coords="8,452.54,638.10,87.47,9.57;8,72.00,651.65,468.00,9.57;8,72.00,665.19,128.54,9.57">RNN is a suitable architecture to examine this idea, given new training methods that have appeared, such as the primal-dual algorithm <ref type="bibr" coords="8,180.54,665.19,16.00,9.57" target="#b10">[11]</ref>.</s></p><p><s coords="8,88.94,678.74,451.06,9.57;8,72.00,692.29,286.25,9.57">Further methods for generating and exploiting adversarial examples to improve the network's training have been suggested by <ref type="bibr" coords="8,224.70,692.29,129.55,9.57">Goodfellow et. al., 2015 [12]</ref>.</s><s coords="8,362.84,692.29,177.16,9.57;8,72.00,705.84,468.00,9.57;9,72.00,75.51,468.00,9.57;9,72.00,89.06,174.39,9.57">They argue that adversarial examples are due to the linearity present in many machine learning models, and that deep neural networks are not more vulnerable, but instead more robust to adversarial examples because of their ability to express more complex functions.</s><s coords="9,256.18,89.06,283.83,9.57;9,72.00,102.61,468.00,9.57;9,72.00,116.16,468.00,9.57;9,72.00,129.71,183.90,9.57">Furthermore, they support that it is the direction of the perturbation that matters to the misclassification, and the fact that although they might seem minute, if perturbations are gathered along all dimensions of the model, then the total amount can be expected to become significant.</s><s coords="9,260.74,129.71,279.26,9.57;9,72.00,143.25,115.24,9.57">This intuitive explanation is in contrast to the assumption presented in Szegedy et.</s><s coords="9,190.88,143.25,289.70,9.57">al. (see <ref type="bibr" coords="9,229.12,143.25,11.82,9.57" target="#b3">[4]</ref>) that adversarial examples are 'counter-intuitive'.</s></p><p><s coords="9,88.94,156.80,451.06,9.57;9,72.00,170.35,155.44,9.57">More importantly to our research direction, Goodfellow and his colleagues imply what we said, based on the work of Szegedy et.</s><s coords="9,230.82,170.35,309.19,9.57;9,72.00,183.90,266.85,9.57">al. : different optimization strategies should be sought for, which will help models develop a more locally stable behavior.</s></p><p><s coords="9,88.94,197.45,451.06,9.57;9,72.00,211.00,205.72,9.57">In addition to developing a new method for generating adversarial examples, which outperforms previous approaches, Moosavi-Dezfooli et.</s><s coords="9,283.09,211.00,256.91,9.57;9,72.00,224.55,121.54,9.57">al., 2016 <ref type="bibr" coords="9,330.64,211.00,16.97,9.57" target="#b12">[13]</ref> present a quantitative measure of the robustness of a classifier.</s><s coords="9,201.69,224.55,338.31,9.57;9,72.00,238.10,467.99,9.57;9,72.00,251.65,468.00,9.57;9,72.00,265.20,10.30,9.57">Starting from the binary classification problem and advancing to the multi-class case, they develop a gradient-descent based algorithm that although does not guarantee to converge to the minimal perturbation, in practice it does find an approximation very close to it.</s><s coords="9,88.19,265.20,307.70,9.57">Compared to their method, the one proposed by Goodfellow et.</s><s coords="9,399.87,265.20,140.13,9.57;9,72.00,278.75,468.00,9.57">al. (as cited above) produces adversarial examples with too much perturbation, which are unlikely to be present in the test data.</s></p><p><s coords="9,88.94,292.30,281.35,9.57">During our research, we came across the work of Xu et.</s><s coords="9,375.45,292.30,67.08,9.57">al., 2016 <ref type="bibr" coords="9,422.53,292.30,16.00,9.57" target="#b13">[14]</ref>.</s><s coords="9,451.98,292.30,88.03,9.57;9,72.00,305.84,468.00,9.57;9,72.00,319.39,280.49,9.57">Although it refers to a different area of interest (malware classification), it describes a genetic-programming based approach to, essentially, constructing adversarial examples.</s><s coords="9,357.23,319.39,182.77,9.57;9,72.00,332.94,221.14,9.57">We believe it is worth mentioning here as it is clearly presented and classifier-agnostic.</s><s coords="9,297.72,332.94,242.28,9.57;9,72.00,346.49,397.45,9.57">Their goal is to find a variant of a malicious sample, which is being classified as non-malicious while maintaining its malicious behavior.</s><s coords="9,474.86,346.49,65.15,9.57;9,72.00,360.04,468.00,9.57;9,72.00,373.59,119.77,9.57">First, a set of random variations of the malicious sample are collected, and each of them is evaluated by the classifier and an 'oracle'.</s><s coords="9,199.23,373.59,340.77,9.57;9,72.00,387.14,468.00,9.57;9,72.00,400.69,43.97,9.57">The classifier computes a measure of the maliciousness of the sample, whereas the 'oracle' is a system that decides if the sample actually has a particular malicious behavior.</s><s coords="9,121.10,400.69,418.90,9.57;9,72.00,414.24,205.32,9.57">The classifier has been 'fooled' if it classifies a sample as non-malicious when the oracle decides that it exhibits malicious behavior.</s><s coords="9,282.31,414.24,257.69,9.57;9,72.00,427.79,468.00,9.57;9,72.00,441.34,413.97,9.57">As long as no 'fooling' example has been found or the maximum generations number has not been reached, a subset of the variants is selected based on a fitness function, and the next generation of the sample set is developed by mutation.</s></p><p><s coords="9,88.94,454.89,451.07,9.57;9,72.00,468.43,144.93,9.57">With regards to our goals, we believe that adversarial examples are a good and established way to check the model's behavior.</s><s coords="9,221.75,468.43,318.25,9.57;9,72.00,481.98,468.00,9.57">However, without simultaneous monitoring of the network's intrinsics, adversarial examples can only hint at modeling deficiencies, without providing interpretability.</s></p><p><s coords="9,88.94,495.53,451.06,9.57;9,72.00,509.08,468.00,9.57;9,72.00,522.63,129.45,9.57">As far as the construction of adversarial examples is concerned, we consider an evolutionary approach especially useful, as it could potentially be applied to any task, by simply changing the fitness evaluation function.</s><s coords="9,208.49,522.63,331.51,9.57;9,72.00,536.18,330.66,9.57">Given that we aim to develop a more generic approach to our RNN research in the future, it is something that should be further studied.</s><s coords="9,407.51,536.18,132.49,9.57;9,72.00,549.73,468.00,9.57;9,72.00,563.28,244.43,9.57">In addition, the generations developed after each mutation could be visualized to effectively illustrate which feature is most significant to the change in the network's behavior.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="9,72.00,595.26,8.07,12.62;9,96.21,595.26,345.04,12.62">Visualizing recurrent networks for interpretation</head><p><s coords="9,72.00,622.00,468.00,9.57;9,72.00,635.55,468.00,9.57;9,72.00,649.10,194.25,9.57">As already stated, one of the goals of the project is to produce an interactive, data-driven visualization tool for neural networks, which we aspire will be useful to the user -expert or not -for understanding the network's mechanism.</s><s coords="9,271.04,649.10,268.96,9.57;9,72.00,662.65,468.00,9.57">This implies that we have first to identify what elements of the network to visualize and how to evaluate their effectiveness when it comes to interpretability.</s><s coords="9,72.00,676.20,468.00,9.57;9,72.00,689.75,62.79,9.57">The former is the content of this section, where we review current approaches in deep networks' visualization.</s><s coords="9,139.63,689.75,193.19,9.57">The latter will be the scope of section 6.</s></p><p><s coords="9,88.94,703.30,451.07,9.57;10,72.00,75.51,467.99,9.57;10,72.00,89.06,68.00,9.57">We will start our review from the fundamental feedforward deep neural network, then present visualization approaches for convolutional networks and finally review relevant literature for recurrent networks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" coords="10,72.00,116.69,434.48,10.52">Visualization in deep feedforward and convolutional neural networks</head><p><s coords="10,72.00,138.04,260.75,9.57;10,283.48,300.63,256.52,9.57;10,72.00,314.18,44.00,9.57">Although not adding any novel method in their work  <ref type="bibr" coords="10,283.48,300.63,16.97,9.57" target="#b15">[16]</ref> suggest a new approach for visualizing neuron activations.</s><s coords="10,121.08,314.18,418.92,9.57;10,72.00,327.73,468.00,9.57;10,72.00,341.27,220.38,9.57">Their method, termed 'DeepLIFT' is based on comparing the activation of each neuron with a corresponding reference activation, and according to the difference, it evaluates the contribution of the neuron to the network's decision.</s><s coords="10,297.06,341.27,242.94,9.57;10,72.00,354.82,468.00,9.57;10,72.00,368.37,100.56,9.57">They base their idea on the fact that low activation or gradient values (of sigmoid or tanh activations for instance) do not mean that the corresponding input is insignificant.</s><s coords="10,177.60,368.37,362.40,9.57;10,72.00,381.92,258.73,9.57">However, comparing activations with activations stemming from a reference input, can reveal the actual importance of a feature.</s><s coords="10,338.71,381.92,201.29,9.57;10,72.00,395.47,383.16,9.57">This method is an important addition to visualization techniques as most methods to date have relied on gradient values.</s></p><p><s coords="10,88.94,409.02,451.07,9.57;10,72.00,422.57,468.00,9.57;10,72.00,436.12,48.43,9.57">The most prolific literature regarding visualization in neural networks concerns convolutional neural networks in image tasks, which is expected given the obvious way to visualize their weights as images.</s><s coords="10,125.22,436.12,40.52,9.57">Grun et.</s><s coords="10,169.20,436.12,370.80,9.57;10,72.00,449.67,320.21,9.57">al., 2016 <ref type="bibr" coords="10,212.51,436.12,16.97,9.57" target="#b16">[17]</ref> provide a survey of such methods and divide them in three main categories based on what they aim to visualize and how they do it:</s></p><p><s coords="10,88.36,467.79,142.70,9.57">• Input modification methods.</s><s coords="10,235.85,467.79,304.15,9.57;10,99.27,481.34,185.49,9.57">These methods alter the input and measure the resulting effects on the output and intermediate layers.</s><s coords="10,290.29,481.34,249.71,9.57;10,99.27,494.89,440.72,9.57;10,99.27,508.44,20.61,9.57">As a result they visualize properties of the mapping that the network learns, and help study the effect and locality of features in the input space (e.g.</s><s coords="10,124.72,508.44,42.46,9.57">Zeiler et.</s><s coords="10,170.82,508.44,67.88,9.57">al., 2013 <ref type="bibr" coords="10,214.46,508.44,16.16,9.57" target="#b17">[18]</ref>).</s></p><p><s coords="10,88.36,529.20,131.91,9.57">• Deconvolutional methods.</s><s coords="10,227.51,529.20,312.50,9.57;10,99.27,542.75,298.04,9.57">In contrast to the previous approach, these methods use the architecture of the network as the center of their visualization.</s><s coords="10,405.07,542.75,134.93,9.57;10,99.27,556.30,440.72,9.57;10,99.27,569.85,230.52,9.57">They are based on the idea of examining a single neuron by examining its input from its layer through the network, all the way to the original part of the input sample.</s><s coords="10,334.57,569.85,205.43,9.57;10,99.27,583.40,271.16,9.57;10,152.61,712.55,24.24,9.57">Such methods allow users to identify which features contributed to the activation of a specific unit (e.  <ref type="bibr" coords="10,152.61,712.55,16.16,9.57" target="#b21">[22]</ref>).</s></p><p><s coords="11,88.94,75.51,352.62,9.57">For our purposes, input modification methods will definitely be valuable.</s><s coords="11,447.79,75.51,92.20,9.57;11,72.00,89.06,468.01,9.57;11,72.00,102.61,108.01,9.57">Since our approach to changing the network's decision will be based on varying the input, it will be beneficial to visualize such changes.</s><s coords="11,184.81,102.61,355.19,9.57;11,72.00,116.16,468.00,9.57;11,72.00,129.71,223.01,9.57">This is similar to our previously mentioned idea of visualizing the different generations of a population of inputs during evolution, in order to find the feature (or 'gene') that mostly affects the output of the network.</s><s coords="11,301.34,129.71,238.66,9.57;11,72.00,143.25,468.00,9.57;11,72.00,156.80,224.58,9.57">Similarly to deconvolution methods, we could try visualizing the cell states of the RNN, in order to discover what it is that the network retains (or also forgets) and what determines its behavior.</s></p><p><s coords="11,88.94,170.35,157.75,9.57">Similarly to the work of Bach et.</s><s coords="11,250.41,170.35,289.58,9.57;11,72.00,183.90,327.96,9.57">al. <ref type="bibr" coords="11,265.65,170.35,16.00,9.57" target="#b19">[20]</ref>, the group of <ref type="bibr" coords="11,351.84,170.35,118.61,9.57">Zintgraf et. al., 2016 [23]</ref> try to express the relevance of image regions to the classification decision of a CNN.</s><s coords="11,403.16,183.90,136.84,9.57;11,72.00,197.45,468.01,9.57;11,72.00,211.00,49.72,9.57">They exploit the strong local relations between image features and the fact that a pixel's value is independent from its location in the image.</s><s coords="11,126.59,211.00,413.41,9.57;11,72.00,224.55,468.00,9.57;11,72.00,238.10,204.09,9.57">An advantage of their method is that they manage to identify features that contribute for and against the classification decision of the network, regardless of whether they use the actual object for classification or its background.</s><s coords="11,283.22,238.10,216.15,9.57">The difference between the work of Bach et.</s><s coords="11,503.77,238.10,36.23,9.57;11,72.00,251.65,53.56,9.57">al. and Zintgraf et.</s><s coords="11,128.78,251.65,411.22,9.57;11,72.00,265.20,423.67,9.57">al. is the fact that the former constraints the relevance of each layer (sum of relevances of each unit) to be equal to the relevance of the surrounding (previous and next) layers.</s><s coords="11,501.25,265.20,38.75,9.57;11,72.00,278.75,468.01,9.57">In other words, the evidence for the decision of the classifier should point to the same direction in each layer.</s></p><p><s coords="11,88.94,292.30,451.06,9.57;11,72.00,305.84,229.05,9.57">A work based on a similar idea, i.e. relating neuron activation to input components/features, was presented by <ref type="bibr" coords="11,161.44,305.84,135.61,9.57">Nguyen et. al., in 2016 [24]</ref>.</s><s coords="11,310.35,305.84,229.65,9.57;11,72.00,319.39,468.01,9.57;11,72.00,332.94,383.04,9.57">The interesting thing in their work is the fact that they discover multiple inputs that highly stimulate each neuron; in this way they manage to demonstrate the distributed nature of the learned representation of the network.</s></p><p><s coords="11,88.94,346.49,451.06,9.57;11,72.00,360.04,103.33,9.57">Taking things a step further, <ref type="bibr" coords="11,231.17,346.49,118.50,9.57">Smilkov et. al., 2016 [25]</ref> added interactivity to the visualization of deep architectures.</s><s coords="11,182.25,360.04,357.74,9.57;11,72.00,373.59,468.00,9.57;11,72.00,387.14,377.33,9.57">They introduced TensorFlow Playground, an interactive visualization tool based on the TensorFlow framework <ref type="bibr" coords="11,246.55,373.59,16.00,9.57" target="#b25">[26]</ref>, in the hope to help beginners in the field of deep learning get an intuition into the effect of the network's parameters and architecture.</s><s coords="11,457.52,387.14,82.48,9.57;11,72.00,400.69,468.00,9.57;11,72.00,414.24,468.00,9.57;11,72.00,427.79,145.83,9.57">The tool has the ability to visualize all obvious hyperparameters (as presented in <ref type="bibr" coords="11,383.09,400.69,16.16,9.57" target="#b14">[15]</ref>), even the activation of each unit separately, yet in our opinion the visualization methods used (heatmaps) are not helpful to a complete beginner in the field.</s><s coords="11,223.16,427.79,316.84,9.57;11,72.00,441.34,440.90,9.57">This does not mean that the tool cannot help someone acquire an understanding of deep architectures faster by playing with structure and parameter tuning.</s><s coords="11,518.94,441.34,21.06,9.57;11,72.00,454.89,468.00,9.57;11,72.00,468.43,468.00,9.57;11,72.00,481.98,30.67,9.57">But, if someone wanted to perform a failure analysis of the network or acquire a more human-friendly interpretation of the network's intrinsics, then plain heatmaps without further explanation are not useful.</s><s coords="11,88.94,495.53,274.89,9.57">This is exactly the gap in research we intend to cover.</s><s coords="11,373.61,495.53,166.39,9.57;11,72.00,509.08,468.00,9.57;11,72.00,522.63,84.90,9.57">Naturally, a tool that serves that purpose will probably not be as generic as TensorFlow Playground, but rather more focused on a particular task.</s><s coords="11,165.72,522.63,374.28,9.57;11,72.00,536.18,399.51,9.57">An approach closer to this goal is presented by <ref type="bibr" coords="11,405.57,522.63,101.56,9.57">Liu et. al., 2016 [27]</ref> where they develop CNNVis, an interactive tool for examining convolutional networks.</s><s coords="11,481.77,536.18,58.23,9.57;11,72.00,549.73,468.00,9.57;11,72.00,563.28,468.00,9.57;11,72.00,576.83,40.09,9.57">Contrary to TensorFlow Playground, CNNVis provides the user with many visualization possibilities: visualize neuron activations, visualize learned features as heatmaps or images, or even examine groups of neurons.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" coords="11,72.00,605.29,238.61,10.52">Visualization in recurrent networks</head><p><s coords="11,72.00,626.64,468.00,9.57;11,72.00,640.19,346.12,9.57">An interactive visualization tool specifically focused on RNN is developed by <ref type="bibr" coords="11,442.29,626.64,97.72,9.57;11,72.00,640.19,16.97,9.57">Strobelt et. al., 2016 [28]</ref> in order to study and understand the dynamics of the hidden states.</s><s coords="11,422.92,640.19,117.08,9.57;11,72.00,653.73,378.32,9.57">They also focus not only on visualizing the network but also on doing it in a human understandable way.</s><s coords="11,455.08,653.73,84.93,9.57;11,72.00,667.28,468.00,9.57;11,72.00,680.83,177.62,9.57">Although the tool allows for loading any RNN model and interacting with its units, the authors present their method in the context of language modeling.</s><s coords="11,256.32,680.83,283.69,9.57;11,72.00,694.38,468.00,9.57;11,72.00,707.93,258.93,9.57">To illustrate the use of the tool, they first present a set of goals that a user could have for the specific task: first, make a hypothesis about the data that one would like to verify, e.g. a certain linguistic property.</s><s coords="11,337.19,707.93,202.80,9.57;12,72.00,75.51,468.00,9.57;12,72.00,89.06,235.06,9.57">Then, examine the similar patterns in the dynamics of the network's states, as exposed by the tool, and finally, compare different models and datasets in order to verify the initial hypothesis.</s><s coords="12,313.67,89.06,226.33,9.57;12,72.00,102.61,468.00,9.57;12,72.00,116.16,468.00,9.57;12,72.00,129.71,468.00,9.57;12,72.00,143.25,468.00,9.57;12,72.00,156.80,169.89,9.57">With these goals in mind, the authors proceed to present the relevant tasks for visual analysis: visualize the hidden states and allow selection of them based on the initial input (text in this specific case); find similar patterns with the previous selections by examining the hidden states; match selections and labeling visually as well as possibly add multiple labels for examining the hypothesis under multiple angles, and, finally perform the above with different trained models.</s><s coords="12,246.52,156.80,293.49,9.57;12,72.00,170.35,327.36,9.57">By providing different analysis aspects (based on time steps or matching patterns) LSTMVis is an invaluable tool for RNN analysis.</s><s coords="12,404.08,170.35,135.91,9.57;12,72.00,183.90,468.00,9.57;12,72.00,197.45,468.00,9.57;12,72.00,211.00,265.56,9.57">Although they use heatmaps in their visualizations, they criticize them for not scaling well with high-dimensional data, and for not being actually useful for understanding the network, since they show the order of hidden states, which is not really relevant to the network's intrinsics.</s><s coords="12,344.59,211.00,195.41,9.57;12,72.00,224.55,468.00,9.57;12,72.00,238.10,228.35,9.57">In addition to heatmaps, they introduce a new way of visualizing the hidden states, i.e. by treating each hidden state as a function of time and plotting its value in various timesteps.</s><s coords="12,305.18,238.10,234.83,9.57;12,72.00,251.65,468.00,9.57;12,72.00,265.20,141.14,9.57">All in all, in this work the authors have managed to develop an interactive tool for visual analysis of an RNN that provides some human-friendly interpretation of the network.</s><s coords="12,217.94,265.20,322.05,9.57;12,72.00,278.75,468.00,9.57;12,72.00,292.30,58.85,9.57">Although we argue that the visualizations provided are still too low level for non-experts, their methodology and presentation of the development process are crucial to our work.</s></p><p><s coords="12,88.94,305.84,433.12,9.57">The idea of non-expert interpretation is expressed more clearly in the work of Karpathy et.</s><s coords="12,525.46,305.84,14.55,9.57;12,72.00,319.39,44.67,9.57">al., 2015 <ref type="bibr" coords="12,96.67,319.39,16.00,9.57" target="#b28">[29]</ref>.</s><s coords="12,121.25,319.39,418.75,9.57;12,72.00,332.94,468.00,9.57;12,72.00,346.49,215.66,9.57">They manage not only to explore the behavior of LSTM units in the context of characterlevel language modeling, but also to provide interpretable visualizations of their function as longrange information-maintaining memory cells.</s><s coords="12,293.77,346.49,246.23,9.57;12,72.00,360.04,317.49,9.57">Their method is based on tracking gate activations and the distribution of gate saturation patterns in the network.</s><s coords="12,399.04,360.04,140.95,9.57;12,72.00,373.59,468.00,9.57;12,72.00,387.14,351.35,9.57">This allows them to identify interesting traits, as, for instance, the fact that if the value of the forget gate of a cell is often above a certain threshold, then that cell functions as a 'long-term memory' cell.</s><s coords="12,428.39,387.14,111.61,9.57;12,72.00,400.69,468.00,9.57;12,72.00,414.24,253.69,9.57">Furthermore, they note that this method has enabled them to spot differences between the layers of the network, as different patterns in gate values arise depending on the layer.</s><s coords="12,332.05,414.24,207.95,9.57;12,72.00,427.79,468.00,9.57;12,72.00,441.34,468.00,9.57;12,72.00,454.89,468.00,9.57;12,72.00,468.43,468.00,9.57;12,72.00,481.98,290.21,9.57">They proceed to compare the LSTM-based language model with an N-gram based model; for this purpose, they examine the closing brace character on the Linux Kernel dataset, which is amenable to their visualization : they highlight each character of a sentence with a color according to the activation level, and one can clearly see that certain neurons experience a spike in activity in response to an opening bracket, then they die out and they fire up again when the closing bracket appears.</s></p><p><s coords="12,88.94,495.53,451.07,9.57;12,72.00,509.08,468.00,9.57;12,72.00,522.63,468.00,9.57;12,72.00,536.18,156.48,9.57">Karpathy's work on visualization has been inspiring for our approach, because he manages to present a strength of the recurrent architecture and LSTM cell, namely the memory property, in a way that is easily understandable by non-experts in the field: using color levels on the input to highlight the interesting pattern.</s></p><p><s coords="12,88.94,549.73,355.33,9.57">Understanding LSTM in Natural Language tasks is also the focus of <ref type="bibr" coords="12,424.26,549.73,16.00,9.57" target="#b29">[30]</ref>.</s><s coords="12,450.53,549.73,26.08,9.57">Li et.</s><s coords="12,480.72,549.73,59.29,9.57;12,72.00,563.28,468.00,9.57;12,72.00,576.83,253.80,9.57">al., 2015 are interested in visualizing compositionality in LSTM networks, i.e. how to identify which parts of a sentence contribute most to the sentence meaning.</s><s coords="12,332.19,576.83,207.81,9.57;12,72.00,590.38,468.00,9.57;12,72.00,603.93,386.74,9.57">For a start, they use heatmaps to visualize the pre-trained representations of input sentences, where they manage to effectively express the intensification of some dimensions in the presence of modifiers such as 'a lot'.</s><s coords="12,467.95,603.93,72.05,9.57;12,72.00,617.48,468.00,9.57;12,72.00,631.03,140.69,9.57">In addition, in such a setting, they utilize first-order derivatives of a unit to determine its contribution to the final decision of the network.</s><s coords="12,219.82,631.03,320.18,9.57;12,72.00,644.57,468.00,9.57;12,72.00,658.12,190.06,9.57">They point out that such a visualization method is inadequate to globally capture information about the input, especially in non-linear systems like LSTM, as it is very focused on each individual unit.</s><s coords="12,268.36,658.12,271.64,9.57;12,72.00,671.67,468.00,9.57;12,72.00,685.22,63.43,9.57">However, we believe that the first-order derivative could prove useful to identify the evolution of the activation of a particular unit and thus hint about its learning rate.</s><s coords="12,140.20,685.22,399.81,9.57;12,72.00,698.77,468.00,9.57;12,72.00,712.32,468.00,9.57;13,72.00,82.09,468.00,9.57;13,72.00,95.64,468.00,9.57;13,72.00,109.19,203.23,9.57">On the other hand, for the case where the network has to learn the word embedding as extra parameters, the writers suggest averaging the representation of the words in a sentence and then measure their deviation from the average, which will indicate the contribution of each to Figure <ref type="figure" coords="13,106.01,82.09,4.24,9.57">2</ref>: Instance of LAMVI tool, a highly interactive visual tool for language modeling debugging; it provides multiple views of the word embeddings and allows the end-user to examine inter-word relations helpful for the model's debugging.</s><s coords="13,72.00,346.81,102.82,9.57">the network's output.</s></p><p><s coords="13,88.94,360.36,192.01,9.57">In one of their more recent work, Li et.</s><s coords="13,285.17,360.36,254.83,9.57;13,72.00,373.91,468.00,9.57;13,72.00,387.46,468.00,9.57;13,72.00,401.01,468.00,9.57;13,72.00,414.56,40.67,9.57">al., 2016 <ref type="bibr" coords="13,330.11,360.36,16.97,9.57" target="#b30">[31]</ref> are still concerned with the idea of the importance of each word in a sentiment classification task, yet they examine it in a way more relevant to our aim: they remove various parts of the representation of the network (input embedding dimensions, input words, or hidden units) and assess the effect of the removal on the network's decision.</s><s coords="13,118.80,414.56,421.20,9.57;13,72.00,428.11,468.00,9.57;13,72.00,441.66,468.00,9.57">To achieve this they use methods ranging from computing the evaluation metric before and after the removal (a measure of the importance of the removed part) to using reinforcement learning to identify the minimum set of input words required to change the network's decision.</s><s coords="13,72.00,455.21,468.00,9.57;13,72.00,468.76,468.00,9.57;13,72.00,482.31,468.00,9.57;13,72.00,495.86,102.58,9.57">The visualization method they employ is based once again on heatmaps, however they manage to express insight into the role of each dimension for the classification (which is fundamental for error analysis as it uncovers the root of misclassification errors), and the superiority of LSTM over RNN in sentiment analysis.</s><s coords="13,179.37,495.86,360.63,9.57;13,72.00,509.40,467.99,9.57;13,72.00,522.95,468.00,9.57;13,72.00,536.50,31.82,9.57">It is interesting to note the difference between their method and adversarial examples: the writers point out that contrary to the method they present, adversarial training does not uncover the inner workings of a network's decision, but merely identifies some defects of the model.</s></p><p><s coords="13,88.94,550.05,279.81,9.57">Regarding the relevance of our work to the work of Li et.</s><s coords="13,372.96,550.05,167.04,9.57;13,72.00,563.60,468.01,9.57;13,72.00,577.15,468.00,9.57;13,72.00,590.70,196.18,9.57">al., we trust that it is an excellent starting point: they set off with a particular task (sentiment classification), find a way to affect the network's decision by varying the input, and manage to come up with a visually interpretable way to transmit the effect of that change.</s><s coords="13,272.91,590.70,267.09,9.57;13,72.00,604.25,468.00,9.57;13,72.00,617.80,362.00,9.57">The visualization method may lack interactivity and the fact that it is based on heatmaps means that it is not guaranteed to be understood by non-experts, however, we believe it is conducive to building trust in a system's decisions.</s></p><p><s coords="13,88.94,631.35,451.06,9.57;13,72.00,644.90,468.00,9.57;13,72.00,658.45,468.00,9.57;13,72.00,672.00,59.97,9.57">LAMVI (figure <ref type="figure" coords="13,167.95,631.35,4.24,9.57">2</ref>), an interactive tool for examining and debugging neural language models based on word embedding, was created by Rond and Adar, 2016 <ref type="bibr" coords="13,382.52,644.90,16.97,9.57" target="#b31">[32]</ref> in the hope that it will allow end-users to make guided decisions about the training data, the architecture and the parameters of the network.</s><s coords="13,137.56,672.00,402.45,9.57;13,72.00,685.54,468.00,9.57;13,72.00,699.09,468.00,9.57;14,72.00,75.51,329.59,9.57">The tool allows the user to play with model parameters, examine input data, pause training and examine the model at that stage, examine neurons (activations, associations between layers and multifaceted view of neurons, namely multiple input instances where they contribute to), in general a comprehensive view of the model at various stages.</s><s coords="14,407.94,75.51,132.06,9.57;14,72.00,89.06,468.00,9.57;14,72.00,102.61,468.00,9.57;14,72.00,116.16,118.58,9.57">As future work, the writers aim to increase the scalability of the model to larger datasets, add the ability to directly compare two differently trained models, and add more embedding models, among which models that deal with sequential contexts.</s><s coords="14,195.82,116.16,344.18,9.57;14,72.00,129.71,468.01,9.57;14,72.00,143.25,46.18,9.57">The latter is particularly relevant to our goals, and the writers note the complexity of sequential models as a challenge when trying to identify relations between training instances.</s></p><p><s coords="14,88.94,156.80,451.06,9.57;14,72.00,170.35,437.75,9.57">Considering all the aforementioned methods, it is clear that there is a growing interest in the research community in visualizing neural networks and gaining insight into their workings.</s><s coords="14,516.00,170.35,24.00,9.57;14,72.00,183.90,468.00,9.57;14,72.00,197.45,290.16,9.57">Most visualization methods so far, have focused on visualizations and comparisons of the input with representations learned in intermediate stages or the output.</s><s coords="14,367.28,197.45,172.72,9.57;14,72.00,211.00,468.00,9.57;14,72.00,224.55,385.45,9.57">Although such approaches can show what the network has learned and, in some cases, what part of the input was conducive to that, they are mostly useful in image-related applications and convolutional networks.</s><s coords="14,462.68,224.55,77.32,9.57;14,72.00,238.10,468.00,9.57;14,72.00,251.65,146.78,9.57">Regarding other tasks and architectures, there is limited experience, mostly because it is difficult to know what will be useful to visualize and how.</s></p><p><s coords="14,88.94,265.20,451.07,9.57;14,72.00,278.75,104.34,9.57">Especially for recurrent networks, it is crucial to be able to convey their temporal nature, as that is their strength.</s><s coords="14,182.31,278.75,357.69,9.57;14,72.00,292.30,468.00,9.57;14,72.00,305.84,357.99,9.57">One would have to come up with ways to visualize sequences of events, in order to be able, for instance, to express the input sequence that lead to a particular output or activation pattern, or a recurring activation pattern (a metastable state <ref type="bibr" coords="14,405.74,305.84,16.16,9.57" target="#b32">[33]</ref>).</s><s coords="14,434.45,305.84,105.55,9.57;14,72.00,319.39,468.01,9.57;14,72.00,332.94,468.00,9.57;14,72.00,346.49,468.01,9.57;14,72.00,360.04,311.18,9.57">Sequence visualization techniques that could be relevant for our work are Sankey diagrams <ref type="bibr" coords="14,399.99,319.39,16.97,9.57" target="#b33">[34]</ref> and MatrixWave <ref type="bibr" coords="14,504.82,319.39,16.00,9.57" target="#b34">[35]</ref>, an interactive sequence visualization tool, that manages to explicitly show the step-by-step evolution of a sequence by placing consecutive steps of the sequence in a 'zig-zag' manner and connecting them with a matrix that captures the relation between the steps.</s></p><p><s coords="14,88.94,373.59,451.07,9.57;14,72.00,387.14,468.00,9.57;14,72.00,400.69,303.37,9.57">These visualization methods as well as the often used, yet difficult to interpret heatmaps naturally lead us to the question of which we should use, and how we could quantitatively evaluate the visualization in terms of interpretability and understanding.</s><s coords="14,380.12,400.69,159.89,9.57;14,72.00,414.24,53.39,9.57">The latter is the focus of the next section <ref type="bibr" coords="14,108.42,414.24,12.73,9.57" target="#b5">(6)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="14,72.00,446.22,8.07,12.62;14,96.21,446.22,340.50,12.62">Evaluating the interpretability of a visualization</head><p><s coords="14,72.00,472.96,468.00,9.57;14,72.00,486.51,331.12,9.57">We have already stated that one of our goals is a human-understandable visualization of the network's intrinsics which will allow end-users to interpret its output.</s><s coords="14,412.52,486.51,127.48,9.57;14,72.00,500.06,468.00,9.57;14,72.00,513.61,385.97,9.57">Towards that goal is thus necessary to objectively identify what makes a visualization human-understandable, as something that is visually understandable to one person, can be confusing for someone else.</s></p><p><s coords="14,88.94,527.16,451.07,9.57;14,72.00,540.71,450.12,9.57">A paper related to the idea of evaluating visualizations, is one by <ref type="bibr" coords="14,420.32,527.16,115.68,9.57">Samek et. al., 2016 [36]</ref>, where they present a method to assess ordered collections of pixels, for instance, heatmaps.</s><s coords="14,530.00,540.71,10.00,9.57;14,72.00,554.26,468.00,9.57;14,72.00,567.81,467.99,9.57;14,72.00,581.36,275.27,9.57">In their work, they compare approaches to generating heatmaps -as we have already seen in section 5 (partial derivatives-based approach, the deconvolution method and the relevance propagation)and provide a general framework for heatmap evaluation.</s><s coords="14,352.76,581.36,187.24,9.57;14,72.00,594.91,468.00,9.57;14,72.00,608.45,289.37,9.57">What is more important and useful for our work is the fact that they suggest a set of properties that heatmaps should have in order to transmit as much information about the network as possible.</s><s coords="14,366.14,608.45,173.86,9.57;14,72.00,622.00,48.61,9.57">These properties can be summarized as follows:</s></p><p><s coords="14,88.36,644.52,451.64,9.57;14,99.27,658.07,440.73,9.57;14,99.27,671.62,23.63,9.57">• Give a global explanation of the network's output, i. e. indicate what made the network produce a particular output, rather than show the degree to which various areas contributed to it.</s></p><p><s coords="14,88.36,694.13,451.63,9.57;14,99.27,707.68,440.73,9.57;15,99.27,75.51,64.90,9.57">• Visualize a continuous relation between input and the heatmap content, i. e. a small input change should invoke a proportionally small heatmap change, thus making the visualization more reliable.</s></p><p><s coords="15,88.36,98.02,451.64,9.57;15,99.27,111.57,440.73,9.57;15,99.27,125.12,32.79,9.57">• Provide input-specific explanations, namely heatmaps should visualize specifics of each input that contribute to the model's decision, rather than an average of salient features over multiple inputs.</s></p><p><s coords="15,88.36,147.64,415.94,9.57">• Present features that are in favor to, as well as those against, the network's decision.</s></p><p><s coords="15,88.36,170.15,451.64,9.57;15,99.27,183.70,153.45,9.57">• Allow for comparison between input regions and datasets, which implies a proper normalization of the visualized quantities.</s></p><p><s coords="15,88.36,206.22,451.64,9.57;15,99.27,219.77,146.09,9.57">• Visualize an explicit relation between the heatmap and the network's decision, which supports interpretation of the heatmap.</s><s coords="15,251.39,219.77,288.61,9.57;15,99.27,233.32,440.73,9.57;15,99.27,246.87,69.60,9.57">This naturally implies that the quality of the heatmap does not depend only on the algorithm used to construct it but also on the performance of the classifier itself.</s></p><p><s coords="15,72.00,269.38,468.00,9.57;15,72.00,282.93,220.31,9.57">The method they describe is very similar to the input modification methods we described above, as well as the representation erasure method.</s><s coords="15,299.41,282.93,240.59,9.57;15,72.00,296.48,355.24,9.57">It is based on dividing an image into regions and perturbing them according to their importance to the classification decision.</s><s coords="15,431.74,296.48,108.26,9.57;15,72.00,310.03,468.00,9.57;15,72.00,323.58,468.00,9.57;15,72.00,337.13,177.49,9.57">Although their method is limited to image recognition, it is important that they derive a quantitative measure whose values can relate to the quality of the heatmap, and give the interested party the ability to compare algorithms for heatmap construction.</s><s coords="15,88.94,350.68,451.07,9.57;15,72.00,364.23,468.00,9.57">The idea of human interpretable visualizations is also explored by <ref type="bibr" coords="15,413.19,350.68,103.18,9.57">Kim et. al., 2016 [37]</ref> who evaluate feature-compression methods and how helpful they are to human classification decisions.</s><s coords="15,72.00,377.77,468.00,9.57;15,72.00,391.32,468.00,9.57;15,72.00,404.87,418.98,9.57">In their work, they first use pre-defined metrics (Dunn index, Shannon entropy) to assess the compression methods, and then use human subjects; the result is that the method that performed best according to the metrics, was ranked as the most helpful by the human evaluators.</s></p><p><s coords="15,88.94,418.42,451.07,9.57;15,72.00,431.97,173.89,9.57">A different area of research in trust and interpretation, which could be valuable to our work, is the field of recommender systems.</s><s coords="15,251.27,431.97,288.73,9.57;15,72.00,445.52,467.99,9.57;15,72.00,459.07,306.94,9.57">In a comprehensive handbook by <ref type="bibr" coords="15,413.44,431.97,103.56,9.57">Ricci et. al., 2011 [38]</ref>, the writers present, among others, recommender systems that benefit from trust social networks, namely networks which express how much the users trust each other (e.g.</s><s coords="15,381.90,459.07,24.24,9.57"><ref type="bibr" coords="15,381.90,459.07,16.16,9.57" target="#b38">[39]</ref>).</s><s coords="15,410.77,459.07,129.22,9.57;15,72.00,472.62,468.00,9.57;15,72.00,486.17,223.28,9.57">Such recommender systems are considered more reliable by the users since the recommendation is coming from 'trusted friends' rather than a simple recommender algorithm.</s><s coords="15,303.42,486.17,236.59,9.57;15,72.00,499.72,247.50,9.57">Furthermore, they also present design guidelines for providing explanations of recommender systems.</s><s coords="15,324.30,499.72,215.70,9.57;15,72.00,513.27,468.00,9.57;15,72.00,526.82,340.06,9.57">Given that a recommender system essentially classifies and ranks suggestions as relevant or not to a user, we could benefit by taking into account those guidelines in our project (summarized and adapted to our goals):</s></p><p><s coords="15,88.36,549.33,451.63,9.57;15,99.27,562.88,53.90,9.57">• Think of the goal of the intended visualization and the metrics to evaluate the attainment of the goal.</s><s coords="15,158.42,562.88,381.58,9.57;15,99.27,576.43,157.12,9.57">In our case, the goal is human interpretability and possible metrics are variants of <ref type="bibr" coords="15,111.69,576.43,16.97,9.57" target="#b35">[36]</ref> with respect to heatmaps.</s></p><p><s coords="15,88.36,598.95,451.63,9.57;15,99.27,612.49,90.94,9.57">• The evaluation of the explanation is directly related to the recommender (or classification) engine underneath.</s><s coords="15,195.07,612.49,172.45,9.57">This idea was also expressed in <ref type="bibr" coords="15,347.52,612.49,16.00,9.57" target="#b35">[36]</ref>.</s></p><p><s coords="15,88.36,635.01,339.33,9.57">• Allow for multiple views and interactivity in the visual explanations.</s></p><p><s coords="15,88.94,657.53,451.06,9.57;15,72.00,671.08,265.84,9.57">To conclude, there is little research that focuses on evaluating visualizations without the help of human subjects, even at some stages of the process.</s><s coords="15,344.52,671.08,195.48,9.57;15,72.00,684.62,468.00,9.57">This will require more effort on our side to develop such an approach or use other methods that will be widely accepted as objective (e.g.</s><s coords="15,72.00,698.17,74.97,9.57">crowdsourcing).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" coords="16,72.00,73.12,8.07,12.62;16,96.21,73.12,77.20,12.62">Conclusion</head><p><s coords="16,72.00,99.86,468.00,9.57;16,72.00,113.41,468.00,9.57;16,72.00,126.96,55.11,9.57">Having reached the end of this literature review, it would be useful to summarize what we have learned in the process, with respect to the initial goals, and set the framework for the first steps in the project.</s><s coords="16,131.83,126.96,408.18,9.57;16,72.00,140.51,468.00,9.57;16,72.00,154.06,36.70,9.57">We will start by re-visiting what we were looking for in the papers that we included in the review, what we found, and then we will continue to introduce the tentative first steps of the project.</s></p><p><s coords="16,88.36,179.56,71.71,9.57">• Architecture.</s><s coords="16,168.99,179.56,371.02,9.57;16,99.27,193.11,440.73,9.57;16,99.27,206.66,77.40,9.57">Regarding the architecture of the recurrent networks, all papers considered relevant used either the standard recurrent architecture with a tanh activation function or the LSTM unit.</s><s coords="16,184.21,206.66,355.79,9.57;16,99.27,220.21,440.72,9.57;16,99.27,233.76,167.89,9.57">LSTM networks have gained ground in recent years and are prevalent in the RNN community because of the stability they express during training (avoid vanishing and exploding gradients problem).</s><s coords="16,275.84,233.76,264.16,9.57;16,99.27,247.31,440.73,9.57;16,99.27,260.86,124.61,9.57">We came across no paper that utilized a special kind of recurrent architecture while being relevant to our goals or being more interpretable and amenable to visualization.</s></p><p><s coords="16,88.36,283.37,123.47,9.57">• Domain of application.</s><s coords="16,221.78,283.37,318.22,9.57;16,99.27,296.92,440.72,9.57">The most popular application domains for RNN remain natural language (text generation), speech processing/language modeling and emotion recognition.</s></p><p><s coords="16,99.27,310.47,440.73,9.57;16,99.27,324.02,272.56,9.57">When it comes to visualization, textual sequence generation in the work of <ref type="bibr" coords="16,462.59,310.47,66.27,9.57">Karpathy [29]</ref> is the leading example of human-interpretable visualization.</s><s coords="16,376.53,324.02,163.47,9.57;16,99.27,337.57,440.73,9.57;16,99.27,351.12,440.73,9.57;16,99.27,364.67,137.11,9.57">When we decide on the domain we will use for our task, it is important to take into account the type of problem associated with it (NLP as sequence generation vs emotion recognition or other type of classification problem) and the scope of the project.</s><s coords="16,241.22,364.67,298.78,9.57;16,99.27,378.22,440.73,9.57;16,99.27,391.77,440.73,9.57;16,99.27,405.32,283.46,9.57">As we discussed at the beginning, we are currently considering a language generation task, due to the visualization opportunities it offers (highlight groups of neurons that fire at specific syntactic, grammar rules or punctuation marks), and because of the many related RNN-codebases and datasets available.</s><s coords="16,88.36,585.84,226.34,9.57">• Visualization, interpretability and evaluation.</s></p><p><s coords="16,319.43,585.84,220.57,9.57;16,99.27,599.39,440.73,9.57;16,99.27,612.94,133.28,9.57">Based on what we have seen, all approaches to neural network visualization try to show the response of individual neurons with respect to the whole or parts of the input.</s><s coords="16,238.20,612.94,301.80,9.57;16,99.27,626.49,371.82,9.57">However, as was presented above, groups of neuron activations could be more helpful to understand what is happening in with the network.</s><s coords="16,477.34,626.49,62.66,9.57;16,99.27,640.03,440.73,9.57;16,99.27,653.58,354.23,9.57">Furthermore, the most popular visualization approach is heatmaps, when there is no straightforward way to express the network's parameters (e. g. as images in the case of CNN).</s><s coords="16,457.26,653.58,82.73,9.57;16,99.27,667.13,440.73,9.57;16,99.27,680.68,440.73,9.57;16,99.27,694.23,175.48,9.57">Heatmaps can be difficult to understand, however if we follow the design guidelines already mentioned, they are a good tool to begin with, and we can use the method described in <ref type="bibr" coords="16,440.99,680.68,16.97,9.57" target="#b35">[36]</ref> to evaluate their ability to express useful information.</s><s coords="16,279.89,694.23,260.11,9.57;16,99.27,707.78,440.72,9.57;17,99.27,75.51,116.75,9.57">At a later stage, we will employ visualization methods more suitable for sequence and time series analysis, hoping to show the strength of RNN in capturing time relations.</s><s coords="17,220.71,75.51,319.30,9.57;17,99.27,89.06,440.72,9.57;17,99.27,102.61,286.54,9.57">Towards this direction it will be useful to demonstrate the sequence of events that lead to particular activation patterns in groups of neurons and then be able to examine more closely the role of each individual neuron.</s><s coords="17,392.09,102.61,147.91,9.57;17,99.27,116.16,255.80,9.57">The latter adds the element of interactivity which is one of the goals of the project.</s><s coords="17,361.68,116.16,178.32,9.57;17,99.27,129.71,363.62,9.57">Interactive visualization methods are scarce in literature and are usually found in word embedding tasks for NLP.</s><s coords="17,466.50,129.71,73.51,9.57;17,99.27,143.25,440.73,9.57;17,99.27,156.80,87.28,9.57">Extending such methods to interpretation of RNN behavior will be challenging yet an important contribution to the community.</s></p><p><s coords="17,72.00,182.31,450.37,9.57">As we said at the beginning of the review, the main goal of our work could be summarized as:</s></p><p><s coords="17,88.36,204.82,328.33,9.57">• generate inputs that will help illustrate critical failures of an RNN</s></p><p><s coords="17,88.36,227.34,451.64,9.57;17,99.27,240.89,212.94,9.57">• employ visualization techniques to allow end-users to investigate, understand such failures, and associate them with the networks input.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,72.00,82.09,468.00,9.57;7,72.00,95.64,100.50,9.57;7,165.60,106.81,280.81,193.94"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="7,72.00,82.09,180.74,9.57">Figure1: An LSTM activation unit.</s><s coords="7,261.40,82.09,278.60,9.57;7,72.00,95.64,100.50,9.57">Its structure with three distinct gates provides it with its memory property.<ref type="bibr" coords="7,160.24,95.64,12.27,9.57" target="#b7">[8]</ref></s></p></div></figDesc><graphic coords="7,165.60,106.81,280.81,193.94" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="13,142.20,119.75,327.60,203.63"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="13,142.20,119.75,327.60,203.63" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,72.00,138.04,468.01,172.16"><head/><label/><figDesc><div><p><s coords="10,332.75,138.04,207.25,9.57;10,72.00,151.59,468.00,9.57;10,72.00,165.14,468.00,9.57;10,72.00,178.68,468.00,9.57;10,72.00,192.23,468.01,9.57;10,72.00,205.78,468.00,9.57;10,72.00,219.33,468.00,9.57;10,72.00,232.88,468.00,9.57;10,72.00,246.43,468.00,9.57;10,72.00,259.98,468.00,9.57;10,72.00,273.53,468.00,9.57;10,72.00,287.08,211.73,9.57;10,88.94,300.63,147.83,9.57"><ref type="bibr" coords="10,171.16,300.63,108.88,9.57" target="#b15">Shrikumar et. al., 2016</ref>15], provide a brief discussion about what can be visualized over the whole process of deep neural network training.They begin by suggesting the importance of visualizing the available data and designing the network's architecture as a dataflow graph, since both can provide valuable hints and help in avoiding configuration errors later.During training, plotting the learning rate, the training criterion and the accuracy on the validation set with respect to the training strategy is an easy and expressive way of quickly assessing the network's progress; furthermore, displaying the weights' and gradients' change can reveal issues such as saturation to extreme values.Of course, they suggest visualizing the weights and activations of a CNN as images, as well as linking particular parts of an input image to corresponding neuron activations.To study individual neurons, they also propose trying synthetic (training and test) data which achieve a certain effect on them.For classification tasks, they highlight the use of confusion matrices.On the contrary,Shrikumar et.</s><s coords="10,240.21,300.63,39.83,9.57">al., 2016</s></p></div></figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="16,88.36,427.83,451.64,145.06"><head>•</head><label/><figDesc><div><p><s coords="16,99.27,427.83,440.73,9.57;16,99.27,441.38,440.72,9.57;16,99.27,454.93,376.75,9.57">[1] of Adversarial examples.Before doing the review we had a slight confusion about adversarial examples, mainly because Nguyen et.al.[1]mention that they did not want to produce adversarial examples, and yet, their work seemed to do exactly that.</s><s coords="16,482.95,454.93,57.04,9.57;16,99.27,468.48,440.73,9.57;16,99.27,482.03,440.72,9.57;16,99.27,495.58,123.25,9.57">Having now studied adversarial examples, the reason behind their claim becomes clear: adversarial examples manage to alter the network's outcome while being correctly identifiable by humans; the examples that Nguyen et.</s><s coords="16,226.40,495.58,313.60,9.57;16,99.27,509.13,54.97,9.57">al. had constructed 'fooled' the network but where unidentifiable by humans.</s><s coords="16,159.28,509.13,380.72,9.57;16,99.27,522.68,440.73,9.57;16,99.27,536.22,149.01,9.57">Ideally, in our task we would like to work with adversarial examples, i. e. being recognizable by humans, so that we can provide a better understanding of what confuses the network and leads it to failure.</s><s coords="16,254.11,536.22,285.90,9.57;16,99.27,549.77,440.72,9.57;16,99.27,563.32,259.58,9.57">However, we should keep in mind what was mentioned in a previous section, namely that adversarial examples expose the weakness of the learned model but do not provide any explanation for that weakness.</s></p></div></figDesc><table/></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Although more details will be presented in a later formal proposal, the first tentative steps of the project will be to select an application area; the most probable to this point is text generation.Then using an evolutionary algorithm we will try to generate input to change the network's output.We expect this to be a challenge, given that the network now learns the joint probability of the input and output, and thus it will be harder to fool.However, we could revert to classifying the input text sequence, try to fool the network in a classification task, and then draw conclusions that could be useful for fooling the generative network.We will start the visualizations with heatmaps and by highlighting input parts according to activation patters, like in [29], and then move on to more sophisticated methods and add interactivity.</p><p>Towards the end of the project, we hope to provide a generic tool to analyze recurrent networks in both generative and classification tasks with a high degree of non-expert interpretability and interactivity.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="17,94.43,471.17,445.57,9.57;17,94.42,484.72,445.57,9.57;17,94.42,498.27,296.49,9.57" xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName coords=""><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Anh Nguyen, Jason Yosinski, and Jeff Clune, "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 427-436.</note>
</biblStruct>

<biblStruct coords="17,94.43,520.78,445.57,9.57;17,94.42,534.33,375.60,9.57" xml:id="b1">
	<monogr>
		<title level="m" type="main">European union regulations on algorithmic decisionmaking and a" right to explanation</title>
		<author>
			<persName coords=""><forename type="first">Bryce</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08813</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Bryce Goodman and Seth Flaxman, "European union regulations on algorithmic decision- making and a" right to explanation"," arXiv preprint arXiv:1606.08813, 2016.</note>
</biblStruct>

<biblStruct coords="17,94.43,556.85,445.58,9.57;17,94.42,570.40,350.60,9.57" xml:id="b2">
	<monogr>
		<title level="m" type="main">Concrete problems in ai safety</title>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané, "Concrete problems in ai safety," arXiv preprint arXiv:1606.06565, 2016.</note>
</biblStruct>

<biblStruct coords="17,94.43,592.91,445.57,9.57;17,94.42,606.46,445.57,9.57;17,94.42,620.01,109.46,9.57" xml:id="b3">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus, "Intriguing properties of neural networks," arXiv preprint arXiv:1312.6199, 2013.</note>
</biblStruct>

<biblStruct coords="17,94.43,642.53,445.57,9.57;17,94.42,656.08,24.85,9.57" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><surname>Zachary C Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03490</idno>
		<title level="m">The mythos of model interpretability</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Zachary C Lipton, "The mythos of model interpretability," arXiv preprint arXiv:1606.03490, 2016.</note>
</biblStruct>

<biblStruct coords="17,94.43,678.59,445.58,9.57;17,94.42,692.14,445.58,9.57;17,94.42,705.69,384.23,9.57" xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpretation and trust: Designing model-driven visualizations for text analysis</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="443" to="452"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jason Chuang, Daniel Ramage, Christopher Manning, and Jeffrey Heer, "Interpretation and trust: Designing model-driven visualizations for text analysis," in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2012, pp. 443-452.</note>
</biblStruct>

<biblStruct coords="18,94.43,75.51,445.58,9.57;18,94.42,89.06,24.85,9.57" xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName coords=""><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211"/>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jeffrey L Elman, "Finding structure in time," Cognitive science, vol. 14, no. 2, pp. 179-211, 1990.</note>
</biblStruct>

<biblStruct coords="18,94.43,111.57,445.58,9.57;18,94.42,125.12,445.58,9.57;18,94.42,138.67,439.07,9.57" xml:id="b7">
	<analytic>
		<title level="a" type="main">A deep recurrent approach for acoustic-to-articulatory inversion</title>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quanjie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shiyin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianhong</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="4450" to="4454"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Peng Liu, Quanjie Yu, Zhiyong Wu, Shiyin Kang, Helen Meng, and Lianhong Cai, "A deep recurrent approach for acoustic-to-articulatory inversion," in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4450-4454.</note>
</biblStruct>

<biblStruct coords="18,94.43,161.19,445.57,9.57;18,94.42,174.74,162.42,9.57" xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780"/>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sepp Hochreiter and Jürgen Schmidhuber, "Long short-term memory," Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997.</note>
</biblStruct>

<biblStruct coords="18,94.42,197.25,445.58,9.57;18,94.42,210.80,368.75,9.57" xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471"/>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Felix A Gers, Jürgen Schmidhuber, and Fred Cummins, "Learning to forget: Continual pre- diction with lstm," Neural computation, vol. 12, no. 10, pp. 2451-2471, 2000.</note>
</biblStruct>

<biblStruct coords="18,94.42,233.32,445.57,9.57;18,94.42,246.87,378.69,9.57" xml:id="b10">
	<monogr>
		<title level="m" type="main">A primal-dual method for training recurrent neural networks constrained by the echo-state property</title>
		<author>
			<persName coords=""><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.6091</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jianshu Chen and Li Deng, "A primal-dual method for training recurrent neural networks constrained by the echo-state property," arXiv preprint arXiv:1311.6091, 2013.</note>
</biblStruct>

<biblStruct coords="18,94.42,269.38,445.58,9.57;18,94.42,282.93,282.30,9.57" xml:id="b11">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy, "Explaining and harnessing ad- versarial examples," arXiv preprint arXiv:1412.6572, 2014.</note>
</biblStruct>

<biblStruct coords="18,94.42,305.45,445.58,9.57;18,94.42,319.00,445.57,9.57;18,94.42,332.54,323.01,9.57" xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard, "Deepfool: a simple and accurate method to fool deep neural networks," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2574-2582.</note>
</biblStruct>

<biblStruct coords="18,94.42,355.06,445.57,9.57;18,94.42,368.61,304.34,9.57" xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically evading classifiers</title>
		<author>
			<persName coords=""><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Network and Distributed Systems Symposium</title>
		<meeting>the 2016 Network and Distributed Systems Symposium</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weilin Xu, Yanjun Qi, and David Evans, "Automatically evading classifiers," in Proceedings of the 2016 Network and Distributed Systems Symposium, 2016.</note>
</biblStruct>

<biblStruct coords="18,94.42,391.12,445.57,9.57;18,94.42,404.67,445.57,9.57;18,94.42,418.22,73.06,9.57" xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective visualizations for training and evaluating deep models</title>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Yeager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Mancewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Houston</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2016 Workshop on Visualization for Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luke Yeager, Greg Heinrich, Joe Mancewicz, and Houston Michael, "Effective visualizations for training and evaluating deep models," ICML 2016 Workshop on Visualization for Deep Learning, 2016.</note>
</biblStruct>

<biblStruct coords="18,94.42,440.74,445.57,9.57;18,94.42,454.29,445.58,9.57;18,94.42,467.84,156.85,9.57" xml:id="b15">
	<monogr>
		<title level="m" type="main">Not just a black box: Learning important features through propagating activation differences</title>
		<author>
			<persName coords=""><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Shcherbina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01713</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje, "Not just a black box: Learning important features through propagating activation differences," arXiv preprint arXiv:1605.01713, 2016.</note>
</biblStruct>

<biblStruct coords="18,94.42,490.35,445.58,9.57;18,94.42,503.90,445.58,9.57;18,94.42,517.45,115.03,9.57" xml:id="b16">
	<monogr>
		<title level="m" type="main">A taxonomy and library for visualizing learned features in convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Grün</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07757</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Felix Grün, Christian Rupprecht, Nassir Navab, and Federico Tombari, "A taxonomy and library for visualizing learned features in convolutional neural networks," arXiv preprint arXiv:1606.07757, 2016.</note>
</biblStruct>

<biblStruct coords="18,94.42,539.97,445.58,9.57;18,94.42,553.52,346.70,9.57" xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew D Zeiler and Rob Fergus, "Visualizing and understanding convolutional networks," in European conference on computer vision. Springer, 2014, pp. 818-833.</note>
</biblStruct>

<biblStruct coords="18,94.42,576.03,445.58,9.57;18,94.42,589.58,358.94,9.57;18,468.58,589.58,71.42,9.57;18,94.42,603.13,109.46,9.57" xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman, "Deep inside convolutional networks: Visualising image classification models and saliency maps," arXiv preprint arXiv:1312.6034, 2013.</note>
</biblStruct>

<biblStruct coords="18,94.42,625.65,445.58,9.57;18,94.42,639.19,445.57,9.57;18,94.42,652.74,378.42,9.57" xml:id="b19">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015">0130140. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek, "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation," PloS one, vol. 10, no. 7, pp. e0130140, 2015.</note>
</biblStruct>

<biblStruct coords="18,94.42,675.26,445.58,9.57;18,94.42,688.81,354.55,9.57" xml:id="b20">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName coords=""><forename type="first">Jonathan L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1601" to="1609"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jonathan L Long, Ning Zhang, and Trevor Darrell, "Do convnets learn correspondence?," in Advances in Neural Information Processing Systems, 2014, pp. 1601-1609.</note>
</biblStruct>

<biblStruct coords="19,94.42,75.51,445.58,9.57;19,94.42,89.06,445.58,9.57;19,94.42,102.61,160.58,9.57" xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName coords=""><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5188" to="5196"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Aravindh Mahendran and Andrea Vedaldi, "Understanding deep image representations by inverting them," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 5188-5196.</note>
</biblStruct>

<biblStruct coords="19,94.42,124.88,445.58,9.57;19,94.42,138.43,242.93,9.57" xml:id="b22">
	<monogr>
		<title level="m" type="main">A new method to visualize deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Luisa</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02518</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Luisa M Zintgraf, Taco S Cohen, and Max Welling, "A new method to visualize deep neural networks," arXiv preprint arXiv:1603.02518, 2016.</note>
</biblStruct>

<biblStruct coords="19,94.42,160.71,445.58,9.57;19,94.42,174.26,445.57,9.57;19,94.42,187.81,115.03,9.57" xml:id="b23">
	<monogr>
		<title level="m" type="main">Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03616</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Anh Nguyen, Jason Yosinski, and Jeff Clune, "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks," arXiv preprint arXiv:1602.03616, 2016.</note>
</biblStruct>

<biblStruct coords="19,94.42,210.08,445.58,9.57;19,94.42,223.63,380.93,9.57" xml:id="b24">
	<analytic>
		<title level="a" type="main">Directmanipulation visualization of deep networks</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Biegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2016</title>
		<meeting>ICML 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel Smilkov, Shan Carter, D. Sculley, Fernanda B. Biegas, and Martin Wattenberg, "Direct- manipulation visualization of deep networks," Proceedings of ICML 2016, 2016.</note>
</biblStruct>

<biblStruct coords="19,94.42,245.91,445.58,9.57;19,94.42,259.46,445.58,9.57;19,94.42,273.00,445.58,9.57;19,94.42,286.55,24.85,9.57" xml:id="b25">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al., "Tensorflow: Large-scale machine learning on heterogeneous distributed systems," arXiv preprint arXiv:1603.04467, 2016.</note>
</biblStruct>

<biblStruct coords="19,94.42,308.83,445.57,9.57;19,94.42,322.38,358.33,9.57" xml:id="b26">
	<monogr>
		<title level="m" type="main">Interactive demo: A visual analysis system for analyzing deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">Mengchen Liu, , Zhen Li, Chongxuan Li, Jun Zhu, and Shixia Liu, "Interactive demo: A visual analysis system for analyzing deep convolutional neural networks," .</note>
</biblStruct>

<biblStruct coords="19,94.42,344.65,445.58,9.57;19,94.42,358.20,445.58,9.57;19,94.42,371.75,115.03,9.57" xml:id="b27">
	<monogr>
		<title level="m" type="main">Visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernd</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07461</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Hendrik Strobelt, Sebastian Gehrmann, Bernd Huber, Hanspeter Pfister, and Alexander M Rush, "Visual analysis of hidden state dynamics in recurrent neural networks," arXiv preprint arXiv:1606.07461, 2016.</note>
</biblStruct>

<biblStruct coords="19,94.42,394.03,445.58,9.57;19,94.42,407.58,242.93,9.57" xml:id="b28">
	<monogr>
		<title level="m" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName coords=""><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Andrej Karpathy, Justin Johnson, and Li Fei-Fei, "Visualizing and understanding recurrent networks," arXiv preprint arXiv:1506.02078, 2015.</note>
</biblStruct>

<biblStruct coords="19,94.42,429.85,445.58,9.57;19,94.42,443.40,265.33,9.57" xml:id="b29">
	<monogr>
		<title level="m" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName coords=""><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01066</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky, "Visualizing and understanding neural models in nlp," arXiv preprint arXiv:1506.01066, 2015.</note>
</biblStruct>

<biblStruct coords="19,94.42,465.68,445.58,9.57;19,94.42,479.23,266.90,9.57" xml:id="b30">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName coords=""><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jiwei Li, Will Monroe, and Dan Jurafsky, "Understanding neural networks through represen- tation erasure," arXiv preprint arXiv:1612.08220, 2016.</note>
</biblStruct>

<biblStruct coords="19,94.42,501.50,445.58,9.57;19,94.42,515.05,250.57,9.57" xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual tools for debugging neural language models</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eytan</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2016 Workshop on Visualization for Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xin Rong and Eytan Adar, "Visual tools for debugging neural language models," ICML 2016 Workshop on Visualization for Deep Learning, 2016.</note>
</biblStruct>

<biblStruct coords="19,94.42,537.33,445.57,9.57;19,94.42,550.88,445.57,9.57;19,94.42,564.43,95.15,9.57" xml:id="b32">
	<analytic>
		<title level="a" type="main">Finding metastable states in real-world time series with recurrence networks</title>
		<author>
			<persName coords=""><forename type="first">Iliusi</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ch</forename><surname>Schütte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim Of</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">445</biblScope>
			<biblScope unit="page" from="1" to="17"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Iliusi Vega, Ch Schütte, and Tim OF Conrad, "Finding metastable states in real-world time series with recurrence networks," Physica A: Statistical Mechanics and its Applications, vol. 445, pp. 1-17, 2016.</note>
</biblStruct>

<biblStruct coords="19,94.42,586.70,445.57,9.57;19,94.42,600.25,445.58,9.57;19,94.42,613.80,41.21,9.57" xml:id="b33">
	<analytic>
		<title level="a" type="main">Interactive sankey diagrams</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Riehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manfred</forename><surname>Hanfler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernd</forename><surname>Froehlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Visualization, 2005. INFOVIS 2005. IEEE Symposium</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="233" to="240"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Patrick Riehmann, Manfred Hanfler, and Bernd Froehlich, "Interactive sankey diagrams," in Information Visualization, 2005. INFOVIS 2005. IEEE Symposium on. IEEE, 2005, pp. 233-240.</note>
</biblStruct>

<biblStruct coords="19,94.42,636.08,445.57,9.57;19,94.42,649.63,445.58,9.57;19,94.42,663.18,327.78,9.57" xml:id="b34">
	<analytic>
		<title level="a" type="main">Matrixwave: Visual comparison of event sequence data</title>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mira</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="259" to="268"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jian Zhao, Zhicheng Liu, Mira Dontcheva, Aaron Hertzmann, and Alan Wilson, "Matrixwave: Visual comparison of event sequence data," in Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 2015, pp. 259-268.</note>
</biblStruct>

<biblStruct coords="19,94.42,685.45,445.58,9.57;19,94.42,699.00,445.58,9.57;19,94.42,712.55,329.66,9.57" xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating the visualization of what a deep neural network has learned</title>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and Klaus- Robert Müller, "Evaluating the visualization of what a deep neural network has learned," IEEE Transactions on Neural Networks and Learning Systems, 2016.</note>
</biblStruct>

<biblStruct coords="20,94.42,75.51,445.57,9.57;20,94.42,89.06,427.83,9.57" xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable and interpretable data representation for high-dimensional, complex data</title>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kayur</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="1763" to="1769"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Been Kim, Kayur Patel, Afshin Rostamizadeh, and Julie A Shah, "Scalable and interpretable data representation for high-dimensional, complex data.," in AAAI, 2015, pp. 1763-1769.</note>
</biblStruct>

<biblStruct coords="20,94.42,111.57,445.58,9.57;20,94.42,125.12,122.37,9.57" xml:id="b37">
	<monogr>
		<title level="m" type="main">Introduction to recommender systems handbook</title>
		<author>
			<persName coords=""><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bracha</forename><surname>Shapira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Francesco Ricci, Lior Rokach, and Bracha Shapira, Introduction to recommender systems handbook, Springer, 2011.</note>
</biblStruct>

<biblStruct coords="20,94.42,147.64,445.58,9.57;20,94.42,161.19,445.57,9.57;20,94.42,174.74,283.10,9.57" xml:id="b38">
	<analytic>
		<title level="a" type="main">Filmtrust: Movie recommendations using trust in web-based social networks</title>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><surname>Golbeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Hendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Consumer communications and networking conference</title>
		<meeting>the IEEE Consumer communications and networking conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="282" to="286"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jennifer Golbeck, James Hendler, et al., "Filmtrust: Movie recommendations using trust in web-based social networks," in Proceedings of the IEEE Consumer communications and networking conference. Citeseer, 2006, vol. 96, pp. 282-286.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
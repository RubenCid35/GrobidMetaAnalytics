<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable Artificial Intelligence: a Systematic Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-13">October 13, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,243.48,174.12,52.69,8.90"><forename type="first">Giulia</forename><surname>Vilone</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">School of Computer Science, College of Science and Health, Technological University Dublin, Dublin, Republic of Ireland</note>
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">College of Science and Health</orgName>
								<orgName type="institution">Technological University Dublin</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country>Republic of Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.88,174.12,48.42,8.90"><forename type="first">Luca</forename><surname>Longo</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">School of Computer Science, College of Science and Health, Technological University Dublin, Dublin, Republic of Ireland</note>
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">College of Science and Health</orgName>
								<orgName type="institution">Technological University Dublin</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country>Republic of Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable Artificial Intelligence: a Systematic Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-13">October 13, 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">6B2B5EBF20EC8F97F52FC09B505DE3DE</idno>
					<idno type="arXiv">arXiv:2006.00093v4[cs.AI]</idno>
					<note type="submission">Preprint submitted to Elsevier</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Explainable artificial intelligence</term>
					<term>method classification</term>
					<term>survey</term>
					<term>systematic literature review</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,106.36,283.45,382.56,8.90;1,106.36,295.41,23.51,8.90">Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years.</s><s coords="1,135.44,295.41,353.48,8.90;1,106.36,307.36,382.56,8.90;1,106.36,319.32,53.31,8.90">This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability.</s><s coords="1,163.54,319.32,325.38,8.90;1,106.36,331.27,25.73,8.90">A plethora of methods to tackle this problem have been proposed, developed and tested.</s><s coords="1,136.94,331.27,351.98,8.90;1,106.36,343.23,382.56,8.90;1,106.36,355.18,173.08,8.90">This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation.</s><s coords="1,285.09,355.18,203.82,8.90;1,106.36,367.14,157.14,8.90">It also summarises the state-of-the-art in XAI and recommends future research directions.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="46" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="47" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="48" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="49" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="50" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="51" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="52" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="53" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="54" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="55" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="56" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="57" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="58" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="59" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="60" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="61" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="62" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="63" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="64" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="65" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="66" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="67" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="68" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="69" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="70" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="71" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="72" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="73" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="74" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="75" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="76" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="77" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="78" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="79" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="80" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="81" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." coords="1,106.36,431.19,66.52,8.82">Introduction</head><p><s coords="1,121.30,452.16,367.62,8.90;1,106.36,464.11,330.15,8.90">The number of scientific articles, conferences and symposia around the world in eXplainable Artificial Intelligence (XAI) has significantly increased over the last decade <ref type="bibr" coords="1,412.20,464.11,10.79,8.90" target="#b0">[1,</ref><ref type="bibr" coords="1,425.72,464.11,7.19,8.90" target="#b1">2]</ref>.</s><s coords="1,440.30,464.11,48.62,8.90;1,106.36,476.07,382.56,8.90;1,106.36,488.02,382.56,8.90;1,106.36,499.98,33.48,8.90">This has led to the development of a plethora of domain-dependent and context-specific methods for dealing with the interpretation of machine learning (ML) models and the formation of explanations for humans.</s><s coords="1,145.36,499.98,343.56,8.90;1,106.36,511.93,205.64,8.90">Unfortunately, this trend is far from being over, with an abundance of knowledge in the field which is scattered and needs organisation.</s><s coords="1,316.52,511.93,172.40,8.90;1,106.36,523.89,357.94,8.90">The goal of this article is to systematically review research works in the field of XAI and to try to define some boundaries in the field.</s><s coords="1,467.33,523.89,21.59,8.90;1,106.36,535.84,382.56,8.90;1,106.36,547.80,285.86,8.90">From several hundreds of research articles focused on the concept of explainability, about 350 have been considered for review by using the following search methodology.</s><s coords="1,395.68,547.80,93.24,8.90;1,106.36,559.75,382.56,8.90;1,106.36,571.71,229.92,8.90">In a first phase, Google Scholar was queried to find papers related to "explainable artificial intelligence", "explainable machine learning" and "interpretable machine learning".</s><s coords="1,341.86,571.71,147.06,8.90;1,106.36,583.66,363.83,8.90">Subsequently, the bibliographic section of these articles was thoroughly examined to retrieve further relevant scientific studies.</s><s coords="1,473.43,583.66,15.49,8.90;1,106.36,595.62,382.56,8.90;1,106.36,607.57,382.56,8.90;1,106.36,619.53,363.92,8.90">The first noticeable thing, as shown in figure <ref type="figure" coords="1,271.15,595.62,18.03,8.90" target="#fig_1">2 (a)</ref>, is the distribution of the publication dates of selected research articles: sporadic in the 70s and 80s, receiving preliminary attention in the 90s, showing raising interest in 2000 and becoming a recognised body of knowledge after 2010.</s><s coords="1,473.43,619.53,15.49,8.90;1,106.36,631.48,382.56,8.90;1,106.36,643.44,254.15,8.90">The first research concerned the development of an explanation-based system and its integration in a computer program designed to help doctors make diagnoses <ref type="bibr" coords="1,346.40,643.44,10.58,8.90" target="#b2">[3]</ref>.</s><s coords="1,363.53,643.44,125.39,8.90;1,106.36,655.40,382.56,8.90;1,106.36,667.35,151.40,8.90">Some of the more recent papers focus on work devoted to the clustering of methods for explainability, motivating the need for organising the XAI literature <ref type="bibr" coords="1,223.81,667.35,10.79,8.90" target="#b3">[4,</ref><ref type="bibr" coords="1,237.05,667.35,7.47,8.90" target="#b4">5,</ref><ref type="bibr" coords="1,246.97,667.35,7.19,8.90" target="#b5">6]</ref>.</s><s coords="1,260.84,667.35,228.08,8.90;1,106.36,679.31,382.56,8.90;1,106.36,691.26,382.56,8.90;2,106.36,144.23,382.56,8.90;2,106.36,156.19,220.33,8.90">The upturn in the XAI research outputs of the last decade is prominently due to the fast increase in the popularity of ML and in particular of deep learning (DL), with many applications in several business areas, spanning from e-commerce <ref type="bibr" coords="1,439.40,691.26,11.62,8.90" target="#b6">[7]</ref> to games <ref type="bibr" coords="2,106.36,144.23,11.62,8.90" target="#b7">[8]</ref> and including applications in criminal justice <ref type="bibr" coords="2,305.80,144.23,10.79,8.90" target="#b8">[9,</ref><ref type="bibr" coords="2,319.53,144.23,11.83,8.90" target="#b9">10]</ref>, healthcare <ref type="bibr" coords="2,382.27,144.23,15.27,8.90" target="#b10">[11]</ref>, computer vision <ref type="bibr" coords="2,472.32,144.23,16.60,8.90" target="#b9">[10]</ref> and battlefield simulations <ref type="bibr" coords="2,215.96,156.19,15.27,8.90" target="#b11">[12]</ref>, just to mention a few.</s><s coords="2,331.73,156.19,157.19,8.90;2,106.36,168.14,382.56,8.90;2,106.36,180.10,382.57,8.90;2,106.36,192.05,92.29,8.90">Unfortunately, most of the models that have been built with ML and deep learning have been labelled 'black-box' by scholars because their underlying structures are complex, non-linear and extremely difficult to be interpreted and explained to laypeople.</s><s coords="2,201.69,192.05,287.23,8.90;2,106.36,204.01,382.56,8.90;2,106.36,215.96,382.56,8.90;2,106.36,227.92,171.41,8.90">This opacity has created the need for XAI architectures that is motivated mainly by three reasons, as suggested by <ref type="bibr" coords="2,276.14,204.01,15.77,8.90" target="#b11">[12,</ref><ref type="bibr" coords="2,295.08,204.01,12.04,8.90" target="#b12">13]</ref>: i) the demand to produce more transparent models; ii) the need of techniques that enable humans to interact with them; iii) the requirement of trustworthiness of their inferences.</s><s coords="2,280.85,227.92,208.07,8.90;2,106.36,239.87,382.56,8.90;2,106.36,251.83,42.89,8.90">Additionally, as proposed by many scholars <ref type="bibr" coords="2,457.39,227.92,15.77,8.90" target="#b12">[13,</ref><ref type="bibr" coords="2,475.64,227.92,13.28,8.90" target="#b13">14]</ref> [ <ref type="bibr" coords="2,110.30,239.87,11.83,8.90" target="#b14">15,</ref><ref type="bibr" coords="2,124.60,239.87,11.83,8.90" target="#b15">16]</ref>, models induced from data must be liable as liability will likely soon become a legal requirement.</s><s coords="2,152.54,251.83,336.38,8.90;2,106.36,263.78,209.27,8.90">Article 22 of the General Data Protection Regulation (GDPR) sets out the rights and obligations of the use of automated decision making.</s><s coords="2,318.65,263.78,170.27,8.90;2,106.36,275.74,382.57,8.90;2,106.36,287.70,382.56,8.90;2,106.36,299.65,325.02,8.90">Noticeably, it introduces the right of explanation by giving individuals the right to obtain an explanation of the inference/s automatically produced by a model, confront and challenge an associated recommendation, particularly when it might negatively affect an individual legally, financially, mentally or physically.</s><s coords="2,434.53,299.65,54.38,8.90;2,106.36,311.61,382.56,8.90;2,106.36,323.56,382.56,8.90;2,106.36,335.52,133.63,8.90">By approving this GDPR article, the European Parliament attempted to tackle the problem related to the propagation of potentially biased inferences to society, that a computational model might have learnt from biased and unbalanced data.</s></p><p><s coords="2,121.30,359.43,367.62,8.90;2,106.36,371.38,328.34,8.90">Many authors surveyed scientific articles surrounding explainability within Artificial Intelligence (AI) in specific sub-domains, motivating the need for literature organisation.</s><s coords="2,437.73,371.38,51.19,8.90;2,106.36,383.34,382.56,8.90;2,106.36,395.29,382.56,8.90;2,106.36,407.25,160.51,8.90">For instance, <ref type="bibr" coords="2,106.36,383.34,15.77,8.90" target="#b16">[17,</ref><ref type="bibr" coords="2,124.94,383.34,13.28,8.90" target="#b17">18]</ref> respectively reviewed the methods for explanations with neural and bayesian networks while <ref type="bibr" coords="2,131.29,395.29,16.60,8.90" target="#b18">[19]</ref> clustered the scientific contributions devoted to extracting rules from models trained with Support Vector Machines (SVMs).</s><s coords="2,271.41,407.25,217.51,8.90;2,106.36,419.20,362.76,8.90">The goal was, and in general is, to create rules highly interpretable by humans while maintaining a degree of accuracy offered by trained models.</s><s coords="2,472.32,419.20,16.60,8.90;2,106.36,431.16,382.56,8.90;2,106.36,443.11,330.74,8.90"><ref type="bibr" coords="2,472.32,419.20,16.60,8.90" target="#b19">[20]</ref> carried out a literature review of all the methods focused on the production of visual representations of the inferential process of deep learning techniques, such as heat-maps.</s><s coords="2,443.02,443.11,45.90,8.90;2,106.36,455.07,382.56,8.90;2,106.36,467.02,132.63,8.90">Only a few scholars attempted to make a more comprehensive survey and organization of the methods for explainability as a whole <ref type="bibr" coords="2,209.55,467.02,10.79,8.90" target="#b0">[1,</ref><ref type="bibr" coords="2,223.22,467.02,11.83,8.90" target="#b20">21]</ref>.</s><s coords="2,243.21,467.02,245.71,8.90;2,106.36,478.98,382.56,8.90;2,106.36,490.93,99.67,8.90">This paper builds on these efforts to organise the vast knowledge surrounding explanations and XAI as a discipline, and it aims at defining a classification system of a larger scope.</s><s coords="2,209.92,490.93,279.00,8.90;2,106.36,502.89,83.67,8.90">The conceptual framework at the basis of the proposed system is represented in Figure <ref type="figure" coords="2,182.55,502.89,3.74,8.90" target="#fig_0">1</ref>.</s><s coords="2,194.86,502.89,294.06,8.90;2,106.36,514.84,382.56,8.90;2,106.36,526.80,217.10,8.90">Most of the methods for explainability focus on interpreting and making the entire process of building an AI system transparent, from the inputs to the outputs via the application of a learning approach to generate a model.</s><s coords="2,326.45,526.80,162.47,8.90;2,106.36,538.75,382.56,8.90;2,106.36,550.71,137.00,8.90">The outcome of these methods are explanations that can be of different formats, such as rules, numerical, textual or visual information, or a combination of the former ones.</s><s coords="2,247.61,550.71,241.30,8.90;2,106.36,562.66,382.56,8.90;2,106.36,574.62,166.01,8.90">These explanations can be theoretically evaluated according to a set of notions that can be formalised as metrics, usually borrowed from the discipline of Human-Computer Interaction (HCI) <ref type="bibr" coords="2,253.27,574.62,15.27,8.90" target="#b21">[22]</ref>.</s></p><p><s coords="2,121.30,598.53,214.63,8.90">The remainder of this paper is organised as it follows.</s><s coords="2,339.16,598.53,149.76,8.90;2,106.36,610.48,338.07,8.90">Section 2 provides a detailed description of the research methods employed for searching for relevant research articles.</s><s coords="2,450.55,610.48,38.37,8.90;2,106.36,622.44,382.56,8.90;2,106.36,634.40,55.47,8.90">Section 3 proposes a classification structure of XAI describing top branches while Sections 5-4 expand this structure.</s><s coords="2,168.62,634.40,320.30,8.90;2,106.36,646.35,378.95,8.90">Eventually, section 8 concludes this systematic review by trying to define the boundaries of the discipline of XAI, as well as suggesting future research work and challenges.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." coords="3,106.36,323.97,90.02,8.82">Research methods</head><p><s coords="3,121.30,344.85,367.62,8.90;3,106.36,356.81,231.56,8.90">Organizing the literature of explainability within AI in a precise and indisputable way as well as setting clear boundaries is far from being an easy task.</s><s coords="3,342.19,356.81,146.73,8.90;3,106.36,368.76,382.56,8.90;3,106.36,380.72,296.06,8.90">This is due to the multidisciplinarity surroundings of this new fascinating field of research spanning from Computer Science to Mathematics, from Psychology to Human Factors, from Philosophy to Ethics.</s><s coords="3,407.64,380.72,81.28,8.90;3,106.36,392.67,382.56,8.90;3,106.36,404.63,382.56,8.90;3,106.36,416.58,84.02,8.90">The development of computational models from data belongs mainly to Computer Science, Statistics and Mathematics, whereas the study of explainability belongs more to Human Factors and Psychology since humans are involved.</s><s coords="3,193.35,416.58,295.57,8.90">Reasoning over the notion of explainability touches Ethics and Philosophy.</s><s coords="3,106.36,428.54,372.30,8.90">Therefore, some constraints had to be set, and the following publication types were excluded:</s></p><p><s coords="3,121.33,447.59,367.60,9.96;3,131.26,460.29,217.18,8.90">• scientific studies discussing the notion of explainability in different contexts than <ref type="bibr" coords="3,461.07,448.34,27.86,8.90">AI and</ref> Computer Science, such as Philosophy or Psychology;</s></p><p><s coords="3,121.33,479.41,318.47,9.96">• articles or technical reports that have not gone through a peer-review process;</s></p><p><s coords="3,121.33,499.27,367.59,9.96;3,131.26,511.97,215.91,8.90">• methods that could be employed for enhancing the explainability of AI techniques but that were not designed specifically for this purposes.</s><s coords="3,353.15,511.97,135.77,8.90;3,131.26,523.93,357.66,8.90;3,131.26,535.88,151.64,8.90">For example, the scientific literature contains a considerable amount of articles related to methods designed for improving data visualization or feature selection.</s><s coords="3,286.21,535.88,202.71,8.90;3,131.26,547.84,357.66,8.90;3,131.26,559.79,82.60,8.90">These methods can indeed help researchers to gain deeper insights into computational models, but they were not specifically designed for producing explanations.</s><s coords="3,217.81,559.79,271.11,8.90;3,131.26,571.75,273.68,8.90">In other words, those methods developed only for enhancing model transparency but not directly focused on explanation were discarded.</s></p><p><s coords="3,106.36,591.55,380.88,8.90">Taking into account the above constraints, this systematic review was carried out in two phases:</s></p><p><s coords="3,118.81,609.34,370.11,8.90;3,131.26,621.29,357.66,8.90;3,131.26,633.32,75.45,8.71">1. papers discussing explainability were searched by using Google Scholar and the following terms: 'explainable artificial intelligence', 'explainable machine learning', 'interpretable machine learning'.</s><s coords="3,209.73,633.25,279.19,8.90;3,131.26,645.21,285.93,8.90">The queries returned several thousands of results, but it became immediately clear that only the first ten pages could contain relevant articles.</s><s coords="3,420.96,645.21,67.96,8.90;3,131.26,657.16,305.89,8.90;3,118.81,670.26,348.78,8.90">Altogether, these searches provided a basis of almost two hundred peer-reviewed publications; 2. the bibliographic section of the articles found in phase one was checked thoroughly.</s><s coords="3,471.21,670.26,17.71,8.90;3,131.26,682.22,357.66,8.90;3,131.26,694.17,37.35,8.90">This led to the selection of one hundred articles whose bibliographic section was recursively analysed.</s><s coords="3,171.70,694.17,306.01,8.90">This process was iterated until it converged and no more articles were found.</s><s coords="3,295.15,706.13,4.98,8.90">3</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." coords="4,106.36,144.18,226.94,8.82">Classification of scientific articles on explainability</head><p><s coords="4,121.30,165.15,367.62,8.90;4,106.36,177.11,151.44,8.90">After a thorough analysis of all the selected articles, four main categories were extracted as depicted in Fig. <ref type="figure" coords="4,171.71,177.11,4.98,8.90" target="#fig_1">2</ref> and as listed below:</s></p><p><s coords="4,121.33,196.29,367.59,9.96;4,131.26,208.99,357.66,8.90;4,131.26,220.95,213.09,8.90">• reviews on methods for explainability -it includes either literature or systematic reviews of those methods devoted to the proposal and/or testing of solutions for the explainability of data-and knowledge-driven models;</s></p><p><s coords="4,121.33,240.12,371.17,9.96;4,131.26,252.83,357.66,8.90;4,131.26,264.78,357.66,8.90">• notions related to the concept of explainability -it includes studies focused on the definition of those notions related to the concept of explainability and on the determination of the main characteristics as well as the requirements of an effective explanation;</s></p><p><s coords="4,121.33,283.96,367.59,9.96;4,131.26,296.66,357.66,8.90;4,131.26,308.62,31.55,8.90">• development of new methods for explainability -it includes articles that propose novel and original methods for enhancing the explainability of data/knowledge-driven models;</s></p><p><s coords="4,121.33,327.79,367.59,9.96;4,131.26,340.50,357.66,8.90;4,131.26,352.45,56.49,8.90">• evaluation of methods for explainability -it includes articles reporting the results of scientific studies aiming at evaluating the performance of different methods for explainability.</s><s coords="4,121.30,539.19,367.62,8.90;4,106.36,551.14,339.21,8.90">Following the proposed classification, it was possible to design a map of the XAI literature in form of a tree whose root contains the above four categories (figure <ref type="figure" coords="4,403.95,551.14,3.74,8.90" target="#fig_1">2</ref>, part b).</s><s coords="4,452.50,551.14,36.42,8.90;4,106.36,563.10,338.23,8.90">This tree expands into branches of different depth where leaves represent scientific articles.</s><s coords="4,451.63,563.10,37.29,8.90;4,106.36,575.05,382.56,8.90;4,106.36,587.01,359.39,8.90">Figure <ref type="figure" coords="4,481.44,563.10,3.74,8.90" target="#fig_1">2</ref>, part b, also shows the percentage of articles grouped by each category, clearly highlighting the distribution of the research efforts towards the development of methods for explainability.</s><s coords="4,469.55,587.01,19.37,8.90;4,106.36,598.96,382.56,8.90;4,106.36,610.92,47.88,8.90">Note that, a paper might appear in multiple branches of this classification, as it might cover multiple dimensions.</s><s coords="4,158.76,610.92,282.57,8.90">Figure <ref type="figure" coords="4,187.74,610.92,3.74,8.90" target="#fig_1">2</ref>, part c, depicts the dependencies of the main four categories.</s><s coords="4,445.84,610.92,43.08,8.90;4,106.36,622.87,382.56,8.90;4,106.36,634.83,382.56,8.90;4,106.36,646.78,190.77,8.90">In general, scholars would not be able to carry out reviews of the XAI literature without the existence and consideration of relevant notions and methods for explainability as well as the approaches for evaluating the performances of these methods.</s><s coords="4,303.75,646.78,185.16,8.90;4,106.36,658.74,382.56,8.90;4,106.36,670.69,153.73,8.90">Evaluation approaches naturally followed the creation of methods for explainability which have been engineered to meet as many requirements of an effective explanation as possible.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." coords="5,106.36,144.18,137.18,8.82">Reviews of the XAI literature</head><p><s coords="5,121.30,165.15,367.62,8.90;5,106.36,177.11,382.56,8.90;5,106.36,189.06,382.56,8.90;5,106.36,201.02,10.79,8.90">This category contains literature and systematic reviews devoted to specific classes of solutions for explainability, such as systems generating textual explanations <ref type="bibr" coords="5,398.62,177.11,15.27,8.90" target="#b22">[23]</ref>, or constrained to specific AI techniques as, for instance, neural networks <ref type="bibr" coords="5,333.90,189.06,16.60,8.90" target="#b23">[24]</ref> (summary in table A.2 and figure <ref type="figure" coords="5,106.36,201.02,3.60,8.90" target="#fig_2">3</ref>).</s><s coords="5,120.77,201.02,368.15,8.90;5,106.36,212.98,340.37,8.90">These reviews provide an entry point for researchers to acquire information and get familiar with the key aspects of the rapidly growing body of research related to explainability.</s><s coords="5,450.00,212.98,38.92,8.90;5,106.36,224.93,382.56,8.90;5,106.36,236.89,45.12,8.90">They also attempt to summarise the main techniques for explainability and to highlight their strengths and limitations.</s><s coords="5,155.22,236.89,333.70,8.90;5,106.36,248.84,33.26,8.90">Seven clusters emerged based on distinct aspects of explainability covered by these reviews:</s></p><p><s coords="5,121.33,270.01,367.59,9.96;5,131.26,282.71,27.12,8.90">• application fields -reviews on methods for explainability in a specific field of application;</s></p><p><s coords="5,121.33,301.89,369.96,9.96;5,131.26,314.59,174.44,8.90">• construction approaches -reviews on methods for explainability specifically designed to explain the inferential process of models.</s><s coords="5,308.80,314.59,173.92,8.90">This category has been further divided into:</s></p><p><s coords="5,143.22,334.47,345.71,8.99;5,153.18,346.47,335.74,8.90">data-driven approaches which focus on extracting new knowledge from trained models from data, but without accounting for the prior knowledge of domain experts.</s></p><p><s coords="5,143.22,362.36,345.71,8.99;5,153.18,374.37,179.31,8.90">knowledge-driven approaches focused on capturing an expert's knowledge and logic, often embedded in the notion of agent;</s></p><p><s coords="5,121.33,393.55,364.05,9.96">• theories &amp; concepts -reviews of the notions related to the concept of explainability;</s></p><p><s coords="5,121.33,413.47,367.59,9.96;5,131.26,426.18,189.67,8.90">• output formats -reviews on methods for explainability focused on generating specific formats of explanations, such as visual or rules;</s></p><p><s coords="5,121.33,445.35,367.59,9.96;5,131.26,458.06,357.66,8.90;5,131.26,470.01,39.30,8.90">• problem types -review articles on methods designed to explain the logic of data and knowledge-driven models applied to a specific type of problem, namely regression or classification;</s></p><p><s coords="5,121.33,489.19,367.59,9.96;5,131.26,501.89,357.66,8.90;5,131.26,513.85,35.99,8.90">• generic reviews -generic reviews that cover a wide range of data/knowledge-driven models as well as their methods for explainability and cannot be placed within any other category.</s></p><p><s coords="5,121.30,535.76,367.62,8.90;5,106.36,547.72,382.56,8.90;5,106.36,559.68,382.56,8.90;5,106.36,571.63,58.98,8.90">In the application fields cluster, the assumption of the methods for explainability is that it is not possible to accept the inference made by a model without understanding its functioning because a decision, supported by a wrong prediction, can have a dramatic impact on people's lives <ref type="bibr" coords="5,151.23,571.63,10.58,8.90" target="#b0">[1]</ref>.</s><s coords="5,172.40,571.63,316.52,8.90;5,106.36,583.59,382.56,8.90;5,106.36,595.54,382.56,8.90;5,106.36,607.50,382.56,8.90;5,106.36,619.45,96.70,8.90">The second cluster, construction data-driven approaches, contains reviews of methods for explainability for specific data-driven learning approaches, mainly neural networks <ref type="bibr" coords="5,106.36,595.54,15.77,8.90" target="#b24">[25,</ref><ref type="bibr" coords="5,125.09,595.54,12.45,8.90" target="#b19">20,</ref><ref type="bibr" coords="5,140.51,595.54,12.45,8.90" target="#b25">26,</ref><ref type="bibr" coords="5,155.92,595.54,12.45,8.90" target="#b26">27,</ref><ref type="bibr" coords="5,171.34,595.54,12.45,8.90" target="#b27">28,</ref><ref type="bibr" coords="5,186.75,595.54,12.45,8.90" target="#b28">29,</ref><ref type="bibr" coords="5,202.18,595.54,12.45,8.90" target="#b16">17,</ref><ref type="bibr" coords="5,217.59,595.54,11.83,8.90" target="#b29">30]</ref>, bayesian networks <ref type="bibr" coords="5,313.66,595.54,16.60,8.90" target="#b17">[18]</ref> and SVMs <ref type="bibr" coords="5,378.99,595.54,15.77,8.90" target="#b30">[31,</ref><ref type="bibr" coords="5,397.73,595.54,11.83,8.90" target="#b18">19]</ref>, not constrained to a specific type of input data for the approach or a particular output format for an explanation, such as images or texts.</s><s coords="5,208.00,619.45,280.92,8.90;5,106.36,631.41,327.17,8.90">Other scholars instead focused on reviewing methods for knowledgebased approaches such as Expert Systems (ES) <ref type="bibr" coords="5,298.07,631.41,16.60,8.90" target="#b31">[32]</ref> and Intelligent Systems <ref type="bibr" coords="5,414.44,631.41,15.27,8.90" target="#b32">[33]</ref>.</s><s coords="5,437.55,631.41,51.37,8.90;5,106.36,643.36,382.56,8.90;5,106.36,655.32,143.95,8.90">In particular, these surveys analysed what types and formats of explanations were tested on these systems and which ones work better than others.</s><s coords="5,254.49,655.32,234.43,8.90;5,106.36,667.27,382.57,8.90;5,106.36,679.23,274.09,8.90">For instance, <ref type="bibr" coords="5,309.12,655.32,16.60,8.90" target="#b33">[34]</ref> showed that rich explanations, based on a combination of information regarding users, items and features, are very effective, while <ref type="bibr" coords="5,472.32,667.27,16.60,8.90" target="#b32">[33]</ref> claimed that explanations should be context-specific to be effective.</s><s coords="5,385.46,679.23,103.46,8.90;5,106.36,691.18,382.56,8.90;6,106.36,342.09,207.32,8.90">The third cluster contains those reviews focused on objectively defining the concept of explainability and its set of related notions, which are discussed in depth in section 5.1.</s><s coords="6,316.79,342.09,172.13,8.90;6,106.36,354.05,382.56,8.90;6,106.36,366.00,222.79,8.90">One of these studies presented an overview of different theories of explanation borrowed from the cognitive science and philosophy disciplines, contextualised within case-based reasoning <ref type="bibr" coords="6,310.06,366.00,15.27,8.90" target="#b34">[35]</ref>.</s><s coords="6,332.36,366.00,156.56,8.90;6,106.36,377.96,382.56,8.90;6,106.36,389.91,382.56,8.90;6,106.36,401.87,382.56,8.90">In details, it is believed that, in order to be effective, an AI system should: (I) explain how it reached the answer and (II) why it is a good answer, (III) why a question is relevant or not, (IV) clarify the meaning of the terms used in the system that might not be understood by the users and, lastly, (V) teach the user about the domain.</s><s coords="6,106.36,413.82,382.56,8.90;6,106.36,425.78,144.63,8.90">In short, the goals that an explanation must achieve depend on the domain under consideration, the underlying model and end-users.</s><s coords="6,254.02,425.78,234.90,8.90;6,106.36,437.73,227.55,8.90">Similarly, <ref type="bibr" coords="6,295.32,425.78,16.60,8.90" target="#b22">[23]</ref> suggested that explanations should take into account the preferences and preconceptions of end-users.</s><s coords="6,336.98,437.73,151.94,8.90;6,106.36,449.69,382.56,8.90">This can be achieved by incorporating more findings from the behavioural and social sciences into the newly emerging field of XAI.</s><s coords="6,106.36,461.64,382.56,8.90;6,106.36,473.60,215.11,8.90">For example, people explain their behaviour based on their beliefs, desires and intentions hence these elements must be considered in an explanation.</s><s coords="6,326.21,473.60,162.71,8.90;6,106.36,485.55,382.56,8.90;6,106.36,497.51,314.07,8.90">Eventually, explanations based on counterfactual examples should help end-users to understand the logic of an underlying model by leveraging on people's capability to infer general rules from a few examples.</s><s coords="6,425.94,497.51,62.98,8.90;6,106.36,509.46,382.56,8.90;6,106.36,521.42,283.84,8.90">Counterfactuals add also something new to what is already known from the existing data and provide additional information on how a model behaves in novel, unseen situations <ref type="bibr" coords="6,371.11,521.42,15.27,8.90" target="#b35">[36]</ref>.</s><s coords="6,395.32,521.42,93.60,8.90;6,106.36,533.37,382.56,8.90;6,106.36,545.33,128.32,8.90">The fourth cluster contains reviews of methods for explainability generating a specific output format for an explanation (further discussed in section 6).</s><s coords="6,239.99,545.33,248.93,8.90;6,106.36,557.28,382.56,8.90;6,106.36,569.24,226.65,8.90">Methods generating textual explanations are surveyed in <ref type="bibr" coords="6,472.32,545.33,16.60,8.90" target="#b31">[32]</ref> and compared according to some requirements about the structure and content of the explanations to adapt them to the users' needs and knowledge.</s><s coords="6,339.04,569.24,149.88,8.90;6,106.36,581.19,314.83,8.90"><ref type="bibr" coords="6,339.04,569.24,16.60,8.90" target="#b36">[37]</ref> focused on written explanations generated from fuzzy rules integrated with natural language generation tools.</s><s coords="6,426.89,581.19,62.03,8.90;6,106.36,593.15,382.56,8.90">The underlying reasonable assumption is that the understandability of these rules cannot be given for granted.</s><s coords="6,106.36,605.10,382.56,8.90;6,106.36,617.06,382.56,8.90;6,106.36,629.01,225.06,8.90">Researchers studied the capabilities of 'data-to-text' approaches that automatically create linguistic descriptions from a complex dataset by means of aggregation functions, implemented as fuzzy rules, that aggregate 'computational perceptions'.</s><s coords="6,336.27,629.01,152.65,8.90;6,106.36,640.97,382.56,8.90;6,106.36,652.92,233.49,8.90">A computational perception is "a unit of meaning for the phenomenon under analysis and is identified by a set of linguistic expressions and their corresponding validity values given a situation."</s><s coords="6,344.78,652.92,144.14,8.90;6,106.36,664.88,224.00,8.90">Some methods combine Logical AI and Statistical AI to generate textual explanations <ref type="bibr" coords="6,311.27,664.88,15.27,8.90" target="#b37">[38]</ref>.</s><s coords="6,335.41,664.88,153.51,8.90;6,106.36,676.83,382.56,8.90;6,106.36,688.79,183.32,8.90">The former is concerned with 'formal languages' to represent and reason with qualitative specifications, while the latter is focused on learning quantitative specifications from data.</s><s coords="6,293.80,688.79,195.12,8.90;7,106.36,144.23,382.57,8.90;7,106.36,156.19,41.23,8.90">However, the authors claimed that the search for an effective way to learn representations of the inferential process of data-driven models is still open <ref type="bibr" coords="7,128.50,156.19,15.27,8.90" target="#b37">[38]</ref>.</s><s coords="7,151.55,156.19,319.03,8.90">A body of literature focused on the visual explanation of deep learning models.</s><s coords="7,474.53,156.19,14.39,8.90;7,106.36,168.14,382.56,8.90;7,106.36,180.10,382.56,8.90;7,106.36,192.05,113.71,8.90">Explanators generating salient masks were investigated in <ref type="bibr" coords="7,324.61,168.14,15.77,8.90" target="#b19">[20,</ref><ref type="bibr" coords="7,342.39,168.14,13.28,8.90" target="#b29">30]</ref> whilst <ref type="bibr" coords="7,384.05,168.14,15.77,8.90" target="#b19">[20,</ref><ref type="bibr" coords="7,401.84,168.14,13.28,8.90" target="#b38">39]</ref> reviewed methods that graphically represent the inner structure and functioning of neural networks with flow-charts or other explanatory graphs.</s><s coords="7,224.69,192.05,264.23,8.90;7,106.36,204.01,382.56,8.90;7,106.36,215.96,283.69,8.90">An interesting alternative was proposed in <ref type="bibr" coords="7,398.26,192.05,16.60,8.90" target="#b39">[40]</ref> whereby methods based on nomograms, rule induction, fuzzy logic, graphical models and topographic mapping can be utilised to explain data-driven models and learning techniques.</s><s coords="7,395.10,215.96,93.82,8.90;7,106.36,227.92,382.56,8.90;7,106.36,239.87,220.15,8.90">Similarly to textual explanation, the problem of visually inspecting data-driven models has not been resolved and there are still challenges and open questions to be answered.</s><s coords="7,330.49,239.87,158.43,8.90;7,106.36,251.83,382.56,8.90;7,106.36,263.78,382.56,8.90;7,106.36,275.74,382.56,8.90;7,106.36,287.70,382.56,8.90;7,106.36,299.65,50.74,8.90">Some reviews summarised the methods for explainability that generate sets of rules from underlying trained models <ref type="bibr" coords="7,416.55,251.83,16.60,8.90" target="#b40">[41]</ref> by extracting frequent relations from a dataset using fuzzy logic and fuzzy rules <ref type="bibr" coords="7,377.67,263.78,15.77,8.90" target="#b41">[42,</ref><ref type="bibr" coords="7,396.43,263.78,12.45,8.90" target="#b42">43,</ref><ref type="bibr" coords="7,411.87,263.78,11.83,8.90" target="#b39">40]</ref>, the integration of symbolic logic with the neural networks <ref type="bibr" coords="7,279.41,275.74,15.77,8.90" target="#b43">[44,</ref><ref type="bibr" coords="7,297.62,275.74,13.28,8.90" target="#b44">45]</ref> and, more generally, the usage of automated reasoning to shed a light over the inferential process of automatically constructed data-driven models <ref type="bibr" coords="7,138.01,299.65,15.27,8.90" target="#b45">[46]</ref>.</s><s coords="7,161.34,299.65,327.58,8.90;7,106.36,311.61,240.00,8.90">The fifth cluster contains reviews that analysed the methods for explainability for either regression <ref type="bibr" coords="7,174.47,311.61,16.60,8.90" target="#b46">[47]</ref> or classification <ref type="bibr" coords="7,258.33,311.61,15.77,8.90" target="#b30">[31,</ref><ref type="bibr" coords="7,276.42,311.61,12.45,8.90" target="#b47">48,</ref><ref type="bibr" coords="7,291.18,311.61,13.28,8.90" target="#b48">49]</ref> problems.</s><s coords="7,349.38,311.61,139.54,8.90;7,106.36,323.56,356.05,8.90">They have a broader scope than the previous reviews as they range over several fields, AI techniques and explanation types.</s><s coords="7,467.34,323.56,21.58,8.90;7,106.36,335.52,382.56,8.90;7,106.36,347.47,382.56,8.90;7,106.36,359.43,87.61,8.90">Their goal was to summarise the important issues, still unresolved, of interpreting prediction models for both problem types and encouraging researchers to improve the existing or discover novel methods for explainability.</s><s coords="7,198.09,359.43,214.17,8.90">Eventually, some reviews have a more generic scope.</s><s coords="7,416.38,359.43,72.54,8.90;7,106.36,371.38,382.56,8.90;7,106.36,383.34,127.65,8.90">They are aimed at proposing a comprehensive way of organizing the several methods for explainability <ref type="bibr" coords="7,447.07,371.38,10.79,8.90" target="#b0">[1,</ref><ref type="bibr" coords="7,460.52,371.38,12.45,8.90" target="#b49">50,</ref><ref type="bibr" coords="7,475.64,371.38,13.28,8.90" target="#b20">21]</ref> or describing them <ref type="bibr" coords="7,184.44,383.34,15.77,8.90" target="#b49">[50,</ref><ref type="bibr" coords="7,202.99,383.34,12.45,8.90" target="#b50">51,</ref><ref type="bibr" coords="7,218.24,383.34,11.83,8.90" target="#b51">52]</ref>.</s><s coords="7,237.97,383.34,250.95,8.90;7,106.36,395.29,68.27,8.90">A group of these reviews tried to evaluate the performances of various methods.</s><s coords="7,179.07,395.29,309.85,8.90;7,106.36,407.25,382.56,8.90;7,106.36,419.20,382.56,8.90;7,106.36,431.16,51.75,8.90">This is done by comparing the explanations automatically produced by these methods <ref type="bibr" coords="7,143.62,407.25,16.60,8.90" target="#b18">[19]</ref> or by measuring how much they fulfil certain notions of explainability, such as completeness, through the use of either quantitative or qualitative metrics <ref type="bibr" coords="7,398.55,419.20,16.70,8.90" target="#b52">[53]</ref> (further discussed in section 7).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." coords="7,106.36,461.00,210.41,8.82">Notions related to the concept of explainability</head><p><s coords="7,121.30,481.97,367.62,8.90;7,106.36,493.92,45.19,8.90">Explaining a model induced from data by employing a specific learning technique is not a trivial goal.</s><s coords="7,154.47,493.92,334.44,8.90;7,106.36,505.88,382.57,8.90;7,106.36,517.83,127.78,8.90">A body of literature focused on achieving such a goal by investigating and attempting to define the concept of explainability, leading to many types of explanation and the formation of several attributes and structures.</s><s coords="7,237.22,517.83,249.68,8.90">To organise these, the specific following clusters are proposed:</s></p><p><s coords="7,121.33,537.01,367.64,9.96;7,131.26,549.71,203.51,8.90">• attributes of explainability -it contains criteria and characteristics used by scholars to try to define the construct of 'explainability';</s></p><p><s coords="7,121.33,568.89,367.59,9.96;7,131.26,581.59,320.68,8.90">• types of explanation -it includes the different ways scholars reported explanations for their ad-hoc applications, what pieces of information are included or left out;</s></p><p><s coords="7,121.33,600.77,367.59,9.96;7,131.26,613.47,357.66,8.90;7,131.26,625.43,98.22,8.90">• structure of an explanation -it contains the various components an explanation can be constructed on, such as causes, context, and consequences of a model's prediction as well as their ordering.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1." coords="7,106.36,649.41,126.36,8.71">Attributes of explainability</head><p><s coords="7,121.30,664.28,367.62,8.90">One of the principal reasons to produce an explanation is to gain the trust of users <ref type="bibr" coords="7,469.83,664.28,15.27,8.90" target="#b53">[54]</ref>.</s><s coords="7,106.36,676.24,382.56,8.90;7,106.36,688.19,198.01,8.90">Trust is the main way to increase users' confidence with a system <ref type="bibr" coords="7,377.85,676.24,16.60,8.90" target="#b54">[55]</ref> and to make them feel comfortable while controlling and using it <ref type="bibr" coords="7,285.27,688.19,15.27,8.90" target="#b55">[56]</ref>.</s><s coords="7,311.67,688.19,177.24,8.90;8,106.36,144.23,164.58,8.90">Besides trust, researchers determined other positive effects brought by explainability.</s><s coords="8,274.13,144.23,214.79,8.90;8,106.36,156.19,108.45,8.90">According to <ref type="bibr" coords="8,328.98,144.23,15.27,8.90" target="#b56">[57]</ref>, it is part of human nature to assign causal attribution of events.</s><s coords="8,217.78,156.19,271.14,8.90;8,106.36,168.14,382.56,8.90;8,106.36,180.10,135.80,8.90">A system that provides a causal explanation on its inferential process is perceived more human-like by end-users as a consequence of the innate tendency of human psychology to anthropomorphism.</s><s coords="8,245.04,180.10,243.87,8.90;8,106.36,192.05,282.66,8.90">Thus, several scholars spoke at length about causality which is considered a fundamental attribute of explainability <ref type="bibr" coords="8,311.94,192.05,15.77,8.90" target="#b11">[12,</ref><ref type="bibr" coords="8,329.76,192.05,12.45,8.90" target="#b57">58,</ref><ref type="bibr" coords="8,344.25,192.05,12.45,8.90" target="#b58">59,</ref><ref type="bibr" coords="8,358.75,192.05,12.45,8.90" target="#b55">56,</ref><ref type="bibr" coords="8,373.24,192.05,11.83,8.90" target="#b22">23]</ref>.</s><s coords="8,391.95,192.05,96.97,8.90;8,106.36,204.01,382.56,8.90;8,106.36,215.96,194.53,8.90">Explanations must make the causal relationships between the inputs and the model's predictions explicit, especially when these relationships are not evident to end-users.</s><s coords="8,306.88,215.96,182.04,8.90;8,106.36,227.92,382.56,8.90;8,106.36,239.87,84.81,8.90">Data-driven models are designed to discover and exploit associations in the data, but they cannot guarantee that there is a causal relationship in these associations.</s><s coords="8,195.70,239.87,293.22,8.90;8,106.36,251.83,382.56,8.90;8,106.36,263.78,64.36,8.90">As pointed out in <ref type="bibr" coords="8,269.03,239.87,15.27,8.90" target="#b55">[56]</ref>, the task of inferring causal relationships strongly depends on prior knowledge, but some associations might be completely unexpected and not explainable yet.</s><s coords="8,178.07,263.78,310.85,8.90;8,106.36,275.74,356.88,8.90">Scientists can use these associations to generate hypotheses to be tested in scientific experiments; however, this is outside the scope of the methods for explainability.</s><s coords="8,466.23,275.74,22.68,8.90;8,106.36,287.70,382.56,8.90;8,106.36,299.65,129.49,8.90">Other four reasons supporting the necessity to explain the logic of an inferential system or a learning algorithm were suggested in <ref type="bibr" coords="8,221.46,299.65,10.79,8.90" target="#b0">[1]</ref>:</s></p><p><s coords="8,121.33,318.83,367.59,9.96;8,131.26,331.53,150.81,8.90">• explain to justify -the decisions made by utilising an underlying model should be explained in order to increase their justifiability;</s></p><p><s coords="8,121.33,350.71,367.59,9.96;8,131.26,363.41,285.00,8.90">• explain to control -explanations should enhance the transparency of a model and its functioning, allowing its debugging and the identification of potential flaws;</s></p><p><s coords="8,121.33,382.59,367.59,9.96;8,131.26,395.29,63.09,8.90">• explain to improve -explanations should help scholars improve the accuracy and efficiency of their models;</s></p><p><s coords="8,121.33,414.47,367.59,9.96;8,131.26,427.17,164.91,8.90">• explain to discover -explanations should support the extraction of novel knowledge and the learning of relationships and patterns.</s></p><p><s coords="8,121.30,447.10,367.62,8.90;8,106.36,459.05,336.44,8.90">Despite the widely recognised importance of explainability, researchers are striving to determine universal, objective criteria on how to build and validate explanations <ref type="bibr" coords="8,423.70,459.05,15.27,8.90" target="#b21">[22]</ref>.</s><s coords="8,447.41,459.05,41.50,8.90;8,106.36,471.01,382.56,8.90;8,106.36,482.96,79.27,8.90">Numerous notions underlying the effectiveness of explanations were proposed in the literature (as summarised in table <ref type="table" coords="8,174.84,482.96,3.60,8.90" target="#tab_0">1</ref>).</s><s coords="8,191.07,482.96,297.85,8.90;8,106.36,494.92,382.56,8.90;8,106.36,506.87,52.70,8.90"><ref type="bibr" coords="8,191.07,482.96,16.60,8.90" target="#b21">[22]</ref> surveyed 250 articles from the fields of Philosophy, Psychology and Cognitive Science to analyse in depth how people define, generate, select, evaluate and present explanations.</s><s coords="8,162.15,506.87,326.77,8.90;8,106.36,518.83,382.56,8.90;8,106.36,530.78,32.37,8.90">The author also presented an interesting definition of XAI as a human-agent interaction problem where the agent reveals the underlying causes to its or another agent's decision process.</s><s coords="8,142.19,530.78,346.73,8.90;8,106.36,542.74,254.81,8.90">In other words, XAI is believed to be a subset of the human-agent interaction field that can be defined as the intersection of AI, social science and HCI.</s></p><p><s coords="8,121.30,566.65,367.62,8.90;8,106.36,578.60,382.56,8.90;8,106.36,590.56,96.83,8.90">Two studies on explainability demonstrated that this concept is utilised in several fields, spanning from Mathematics, Physics, Computer Science to Engineering, Psychology, Medicine and Social sciences <ref type="bibr" coords="8,169.24,590.56,15.77,8.90" target="#b62">[63,</ref><ref type="bibr" coords="8,187.41,590.56,11.83,8.90" target="#b74">75]</ref>.</s><s coords="8,206.23,590.56,282.69,8.90;8,106.36,602.51,382.56,8.90;8,106.36,614.47,382.56,8.90;8,106.36,626.43,159.16,8.90">Explainability is often replaced with the notion of interpretability, considered as synonyms within the general AI community, and in particular by those scholars in automated learning and reasoning, whereas it seems that the software engineering community prefers the term understandability <ref type="bibr" coords="8,246.43,626.43,15.27,8.90" target="#b62">[63]</ref>.</s><s coords="8,269.97,626.43,218.95,8.90;8,106.36,638.38,382.56,8.90;8,106.36,650.34,314.89,8.90">Generally speaking, interpretability is often defined as the capacity to provide or bring out the meaning of an abstract concept and understandability as the capacity to make the model understandable by end-users (see table <ref type="table" coords="8,410.45,650.34,3.60,8.90" target="#tab_0">1</ref>).</s><s coords="8,426.92,650.34,62.00,8.90;8,106.36,662.29,164.21,8.90">However, other definitions are proposed in the literature.</s><s coords="8,275.22,662.29,213.70,8.90;8,106.36,674.25,382.56,8.90;8,106.36,686.20,118.13,8.90">Explainability or interpretability is defined in <ref type="bibr" coords="8,461.01,662.29,16.60,8.90" target="#b25">[26]</ref> as "the degree to which a human observer can understand the reason behind a decision (or a prediction) made by the model".</s><s coords="8,228.81,686.20,260.12,8.90;9,172.48,176.66,278.71,7.12;9,112.33,190.86,42.07,7.12;9,172.48,190.86,270.52,7.12;9,112.33,205.05,30.11,7.12;9,172.48,200.32,308.98,7.12;9,172.48,209.79,73.05,7.12">An interesting distinction between the concepts of interpretation The degree of confidence of a learning algorithm to behave 'sensibly' in general <ref type="bibr" coords="9,429.94,176.66,12.62,7.12" target="#b25">[26,</ref><ref type="bibr" coords="9,444.54,176.66,6.64,7.12" target="#b1">2]</ref> Actionability The capacity of a learning algorithm to transfer new knowledge to end-users <ref type="bibr" coords="9,417.76,190.86,12.62,7.12" target="#b59">[60,</ref><ref type="bibr" coords="9,432.37,190.86,10.62,7.12" target="#b60">61]</ref> Causality The capacity of a method for explainability to clarify the relationship between input and output <ref type="bibr" coords="9,172.48,209.79,12.62,7.12" target="#b11">[12,</ref><ref type="bibr" coords="9,187.09,209.79,9.96,7.12" target="#b57">58,</ref><ref type="bibr" coords="9,199.04,209.79,9.96,7.12" target="#b56">57,</ref><ref type="bibr" coords="9,211.00,209.79,9.96,7.12" target="#b58">59,</ref><ref type="bibr" coords="9,222.95,209.79,9.96,7.12" target="#b55">56,</ref><ref type="bibr" coords="9,234.91,209.79,10.62,7.12" target="#b22">23]</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,112.33,219.25,44.72,7.12">Completeness</head><p><s coords="9,172.48,219.25,299.48,7.12;10,106.36,144.23,154.23,8.90">The extent to which an underlying inferential system is described by explanations <ref type="bibr" coords="9,434.77,219.25,12.62,7.12" target="#b52">[53,</ref><ref type="bibr" coords="9,449.38,219.25,9.96,7.12" target="#b59">60,</ref><ref type="bibr" coords="9,461.34,219.25,10.62,7.12" target="#b60">61]</ref> and explanation was proposed in <ref type="bibr" coords="10,241.49,144.23,15.27,8.90" target="#b28">[29]</ref>.</s><s coords="10,264.74,144.23,224.18,8.90;10,106.36,156.19,382.56,8.90;10,106.36,168.14,289.74,8.90">On one hand, an interpretation is the mapping of an abstract concept (as a predicted class) into a domain that the human can make sense of, such as, for instance, images or texts that can be inspected and classified by people.</s><s coords="10,400.71,168.14,88.21,8.90;10,106.36,180.10,382.56,8.90;10,106.36,192.05,108.70,8.90">On the other hand, an explanation is the collection of features of an interpretable domain that contributed to produce a prediction for a given item.</s><s coords="10,218.40,192.05,270.51,8.90;10,106.36,204.01,45.36,8.90">The authors of <ref type="bibr" coords="10,279.27,192.05,16.60,8.90" target="#b28">[29]</ref> did not specify how to determine this collection of features.</s><s coords="10,156.44,204.01,332.48,8.90;10,106.36,215.96,382.56,8.90">The selection criteria are to be decided by researchers according to several factors like the type of input data and the degree of refinement in the explanation demanded by end-users.</s><s coords="10,106.36,227.92,382.56,8.90;10,106.36,239.87,153.32,8.90">An expansion of the definition of interpretability through the determination of its main characteristics was presented in <ref type="bibr" coords="10,210.02,239.87,15.77,8.90" target="#b73">[74,</ref><ref type="bibr" coords="10,228.63,239.87,12.45,8.90" target="#b21">22,</ref><ref type="bibr" coords="10,243.91,239.87,11.83,8.90" target="#b84">85]</ref>.</s><s coords="10,263.81,239.87,225.11,8.90;10,106.36,251.83,382.56,8.90;10,106.36,263.78,382.57,8.90;10,106.36,275.74,382.57,8.90;10,106.36,287.70,194.97,8.90">In detail, <ref type="bibr" coords="10,302.50,239.87,16.60,8.90" target="#b73">[74]</ref> suggested the following requirements: (I) fidelity -the representation of inputs and models in terms of concepts should preserve and present to end-users their relevant features and structures, (II) diversity -inputs and models should be representable with few non-overlapping concepts, and (III) grounding -concepts should have an immediate human-understandable interpretation.</s><s coords="10,305.92,287.70,183.00,8.90;10,106.36,299.65,289.56,8.90">These requirements were further expanded in <ref type="bibr" coords="10,106.36,299.65,16.60,8.90" target="#b21">[22]</ref> by listing a set of characteristics that an explanation should possess:</s></p><p><s coords="10,121.33,318.83,367.59,9.96;10,131.26,331.53,203.61,8.90">• contrastive nature of explanations -people seek for an explanation when they are presented with counterfactual and/or counter-intuitive events;</s></p><p><s coords="10,121.33,350.71,367.59,9.96;10,131.26,363.41,357.66,8.90;10,131.26,375.37,209.77,8.90">• selectivity of explanations -people usually do not expect that an explanation contains the actual and complete list of the causes of an event, but only a selection of the few causes deemed to be necessary and sufficient to explain it.</s><s coords="10,346.82,375.37,142.11,8.90;10,131.26,387.32,198.29,8.90">Authors point out the risk that this selection might be influenced by cognitive biases;</s></p><p><s coords="10,121.33,406.50,367.59,9.96;10,131.26,419.20,347.62,8.90">• social nature of explanations -explanations are part of a dialogue aiming at transferring knowledge, therefore, they are based on the beliefs of both the explainer and explainee;</s></p><p><s coords="10,121.33,438.38,367.59,9.96;10,131.26,451.08,357.66,8.90;10,131.26,463.04,150.31,8.90">• irrelevance of probabilities to explanations -referring to the occurrence probabilities of events or to the statistical relationships between causes and events does not produce a satisfactory and intuitive explanation.</s><s coords="10,285.62,463.04,203.30,8.90;10,131.26,474.99,149.05,8.90">Explanations are more effective when they refer to the causes and not to their likelihood.</s></p><p><s coords="10,106.36,494.92,382.56,8.90;10,106.36,506.87,382.57,8.90;10,106.36,518.83,382.56,8.90;10,106.36,530.78,382.57,8.90;10,106.36,542.74,382.56,8.90;10,106.36,554.69,382.57,8.90;10,106.36,566.65,188.97,8.90">Four further requirements for enhancing the interpretability of visual explanations were added in <ref type="bibr" coords="10,117.11,506.87,15.49,8.90" target="#b84">[85]</ref>: i) graphical integrity -the representations should highlight the features that contribute the most to the final predictions and distinguish those with positive and negative attribution, ii) coverage -a large fraction of the most important features should be visible in the representation, iii) morphological clarity -the important features should be clearly displayed, their visualization cannot be 'noisy', and iv) layer separation -the representation cannot occlude the raw image which should be visible for human inspection.</s><s coords="10,300.88,566.65,188.03,8.90;10,106.36,578.60,255.07,8.90">Other two notions strongly correlated with interpretability are comprehensibility <ref type="bibr" coords="10,250.44,578.60,16.60,8.90" target="#b63">[64]</ref> and intelligibility <ref type="bibr" coords="10,342.34,578.60,15.27,8.90" target="#b74">[75]</ref>.</s><s coords="10,366.48,578.60,122.44,8.90;10,106.36,590.56,69.72,8.90">However, scholars highlighted some differences.</s><s coords="10,179.48,590.56,309.44,8.90;10,106.36,602.51,382.57,8.90;10,106.36,614.47,293.01,8.90"><ref type="bibr" coords="10,179.48,590.56,16.60,8.90" target="#b65">[66]</ref> proposed to distinguish between interpretable systems, systems in which end-users can mathematically analyse algorithms, and comprehensible systems that "emit symbols enabling user-driven explanations of how a conclusion is reached".</s><s coords="10,404.90,614.47,84.01,8.90;10,106.36,626.43,382.56,8.90;10,106.36,638.38,382.56,8.90;10,106.36,650.34,155.67,8.90">Two studies <ref type="bibr" coords="10,456.56,614.47,15.77,8.90" target="#b74">[75,</ref><ref type="bibr" coords="10,475.64,614.47,13.28,8.90" target="#b77">78]</ref> defined intelligibility as an attribute of user-centric reasoned explanations that are easily interpretable by end-users and that draws from foundational concepts of other disciplines such as Philosophy and Cognitive Psychology.</s><s coords="10,266.88,650.34,222.04,8.90;10,106.36,662.29,382.56,8.90;10,106.36,674.25,382.56,8.90;10,106.36,686.20,21.31,8.90">Additionally, both studies recommended exploiting the experience and knowledge of the HCI community in making interfaces that empower people to assure that intelligibility will be one of the core requirements of the next generation of AI systems.</s><s coords="10,130.58,686.20,358.33,8.90;11,106.36,144.23,146.22,8.90">Other authors focused on breaking some of the notions identified in table 1 into sub-notions or on assigning further requirements.</s><s coords="11,255.56,144.23,233.36,8.90;11,106.36,156.19,268.39,8.90">For example, three sub-notions related to transparency that should be achieved by any learning model were defined in <ref type="bibr" coords="11,340.44,156.19,15.77,8.90" target="#b25">[26,</ref><ref type="bibr" coords="11,358.70,156.19,12.04,8.90" target="#b55">56]</ref>:</s></p><p><s coords="11,121.33,175.37,367.59,9.96;11,131.26,188.07,81.63,8.90">• simulatability -the capacity of a model to allow a user to understand its structure and functioning entirely;</s></p><p><s coords="11,121.33,207.25,367.59,9.96;11,131.26,219.95,314.91,8.90">• decomposability -the degree to which a model can be decomposed into its individual components (input, parameters and output) and of their intuitive explainability;</s></p><p><s coords="11,121.33,239.13,367.59,9.96;11,131.26,251.83,153.01,8.90">• algorithmic transparency -the degree of confidence of a learning algorithm to behave 'sensibly' in general (see also table <ref type="table" coords="11,273.48,251.83,3.60,8.90" target="#tab_0">1</ref>).</s></p><p><s coords="11,121.30,271.75,367.62,8.90;11,106.36,283.71,382.56,8.90;11,106.36,295.67,329.12,8.90">However, according to <ref type="bibr" coords="11,215.33,271.75,15.27,8.90" target="#b55">[56]</ref>, it is not possible to achieve algorithmic transparency in neural networks because of the current incapacity of experts to understand the inferential process of these models and to prove that they work correctly on new, unseen observations.</s><s coords="11,440.85,295.67,48.07,8.90;11,106.36,307.62,382.56,8.90;11,106.36,319.58,181.26,8.90">Scholars attempted to overcome this shortcoming by finding methods to trace the predictions of a model to the most influential features of the input.</s><s coords="11,294.61,319.58,194.31,8.90;11,106.36,331.53,382.56,8.90;11,106.36,343.49,94.35,8.90">Examples of these methods are heat-maps <ref type="bibr" coords="11,472.32,319.58,16.60,8.90" target="#b97">[98]</ref> which are created by back-propagating the predictions of a model to the input space and highlighting relevant pixels.</s><s coords="11,204.76,343.49,284.16,8.90;11,106.36,355.44,382.56,8.90;11,106.36,367.40,50.20,8.90">Alternatively, <ref type="bibr" coords="11,261.66,343.49,16.60,8.90" target="#b98">[99]</ref> proposed a solution to satisfy the simulatability and decomposability properties by substituting black-box models with Generalized Additive Models (GAMs).</s><s coords="11,160.84,367.40,328.08,8.90;11,106.36,379.35,382.56,8.90;11,106.36,391.31,24.07,8.90">GAMs are linear combinations of simple models trained on a single feature of an input dataset, thus allowing end-users to quantify the contribution of each feature to the outcome.</s><s coords="11,134.60,391.31,354.32,8.90;11,106.36,403.26,182.77,8.90">However, transparency must be handled with caution because it can be dangerous under certain circumstances, as highlighted in <ref type="bibr" coords="11,270.04,403.26,15.27,8.90" target="#b95">[96]</ref>.</s><s coords="11,294.36,403.26,194.56,8.90;11,106.36,415.22,382.56,8.90;11,106.36,427.17,139.16,8.90">Requiring that data and models are fully visible to end-users prevents the creation of intellectual properties; this can significantly slow down the development of new technologies.</s><s coords="11,251.73,427.17,237.19,8.90;11,106.36,439.13,277.59,8.90">Moreover, data can contain sensitive or personal information which cannot be made public without affecting people's privacy.</s><s coords="11,388.16,439.13,100.76,8.90;11,106.36,451.08,382.56,8.90;11,106.36,463.04,246.69,8.90">Finally, the displaying of more information might push a researcher to optimise a model on specific instance(s) but deteriorating its overall performance and degree of generalisability.</s><s coords="11,356.80,463.04,132.11,8.90;11,106.36,474.99,78.13,8.90">Scholars extensively investigated sensitivity <ref type="bibr" coords="11,149.57,474.99,15.77,8.90" target="#b91">[92,</ref><ref type="bibr" coords="11,168.71,474.99,11.83,8.90" target="#b92">93]</ref>.</s><s coords="11,190.20,474.99,298.72,8.90;11,106.36,486.95,382.56,8.90;11,106.36,498.90,34.04,8.90">In this context, sensitivity is considered as the sensibility of explanations to variations in the input features, model implementation and, subsequently, in the model's predictions.</s><s coords="11,146.94,498.90,341.98,8.90;11,106.36,510.86,382.56,8.90;11,106.36,522.81,382.56,8.90"><ref type="bibr" coords="11,146.94,498.90,16.60,8.90" target="#b91">[92]</ref> introduced the requirement of input invariance meaning that a method for explainability must mirror the sensitivity of the underlying model with respect to transformations of the inputs in order to ensure a reliable interpretation of their contribution to each prediction.</s><s coords="11,106.36,534.77,382.56,8.90;11,106.36,546.72,382.56,8.90;11,106.36,558.68,280.36,8.90"><ref type="bibr" coords="11,106.36,534.77,16.60,8.90" target="#b92">[93]</ref> focused on the sensitivity of methods for explainability specifically designed for neural networks, in particular those that quantify the contribution of input features to the predictions, such as DeepLift <ref type="bibr" coords="11,154.98,558.68,21.58,8.90" target="#b99">[100]</ref> and Layer-wise Relevance Propagation (LRP) <ref type="bibr" coords="11,362.64,558.68,20.06,8.90" target="#b100">[101]</ref>.</s><s coords="11,389.70,558.68,99.22,8.90;11,106.36,570.63,382.56,8.90;11,106.36,582.59,382.56,8.90;11,106.36,594.54,46.76,8.90">In this case, a method for explainability satisfies the sensitivity requirement if it assigns a non-zero contribution to an input feature when two instances, in the input space, differ in that feature only but lead to different predictions.</s><s coords="11,158.36,594.54,330.56,8.90;11,106.36,606.57,108.79,8.71">According to <ref type="bibr" coords="11,214.58,594.54,15.27,8.90" target="#b92">[93]</ref>, methods for explainability must also fulfill the requirement of implementation invariance.</s><s coords="11,218.11,606.50,270.81,8.90;11,106.36,618.45,295.34,8.90">This suggests that a method applied to functionally equivalent neural networks should assign identical contributions to the features of the input.</s><s coords="11,405.06,618.45,83.86,8.90;11,106.36,630.41,382.57,8.90;11,106.36,642.37,140.19,8.90">Two neural networks are functionally equivalent if their predictions are equal for all inputs despite having different implementations and architectures.</s><s coords="11,251.15,642.37,237.77,8.90;11,106.36,654.32,299.03,8.90">Finally, scholars identified various factors that might affect the interestingness of a model, in particular of the rule-based ones <ref type="bibr" coords="11,371.38,654.32,15.77,8.90" target="#b80">[81,</ref><ref type="bibr" coords="11,389.62,654.32,11.83,8.90" target="#b66">67]</ref>.</s><s coords="11,408.47,654.32,80.45,8.90;11,106.36,666.28,156.08,8.90">First, rule size is the number of instances satisfied by a rule.</s><s coords="11,265.96,666.28,222.96,8.90;11,106.36,678.23,85.27,8.90">Usually, small size rules are undesirable as they explain only a few instances.</s><s coords="11,196.67,678.23,292.25,8.90;11,106.36,690.19,19.09,8.90">The main aim is to discover rules that cover a large portion of the input data.</s><s coords="11,130.05,690.19,358.87,8.90;12,106.36,144.23,162.76,8.90">However, there are situations where small rules might capture exception occurring in the data that can be of interest for scientists.</s><s coords="12,273.18,144.23,215.75,8.90;12,106.36,156.19,330.41,8.90">Second, imbalance of class distributions occurs when the instances belonging to a class are more frequent than those of another class.</s><s coords="12,443.06,156.19,45.86,8.90;12,106.36,168.14,382.57,8.90;12,106.36,180.10,30.16,8.90">It might be more difficult, hence more interesting, to discover those rules aimed at predicting the minority classes.</s><s coords="12,140.86,180.10,348.06,8.90;12,106.36,192.05,19.09,8.90">Attribute costs represent the cost to get access to the actual value of an attribute of the data.</s><s coords="12,131.69,192.05,357.23,8.90;12,106.36,204.01,254.96,8.90">For example, it is easy to assess the gender of a patient but the determination of some health-related attributes can require an expensive investigation.</s><s coords="12,366.54,204.01,122.38,8.90;12,106.36,215.96,123.26,8.90">Rules that utilise only 'cheap' attributes are more interesting.</s><s coords="12,234.40,215.96,254.51,8.90;12,106.36,227.92,106.26,8.90">Eventually, the interestingness of a rule must take into account the misclassification costs.</s><s coords="12,216.10,227.92,272.82,8.90;12,106.36,239.87,264.92,8.90">In some domain of application, the erroneous classification of an instance might have a significant impact, not only in terms of money.</s><s coords="12,374.31,239.87,114.61,8.90;12,106.36,251.83,365.76,8.90">In case of medical diagnosis, classifying as healthy a patient affected by a lethal disease might lead to premature death.</s><s coords="12,477.30,251.83,11.62,8.90;12,106.36,263.78,382.56,8.90;12,106.36,275.74,363.06,8.90">Interestingness was also examined for Reinforcement Learning (RL) agents which are designed to take actions in a specific environment with the aim to maximize a cumulative reward <ref type="bibr" coords="12,450.32,275.74,15.27,8.90" target="#b81">[82]</ref>.</s><s coords="12,473.43,275.74,15.49,8.90;12,106.36,287.70,382.56,8.90;12,106.36,299.65,382.57,8.90">The authors proposed a framework to make the behaviour of these agents explainable by analysing their historical interactions with the environment and extracting a few interestingness elements.</s><s coords="12,106.36,311.61,382.56,8.90;12,106.36,323.56,382.56,8.90;12,106.36,335.52,109.01,8.90">Examples of interesting elements of these interactions are the portion of environment observed by the agent, the frequency of certain types of interactions and the cost (in terms of a reward) of the interactions carried out.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2." coords="12,106.36,359.50,105.88,8.71">Types of explanations</head><p><s coords="12,121.30,374.37,367.62,8.90;12,106.36,386.33,177.63,8.90">Researchers tried to create a classification system for the types of explanation suitable for interpreting the logic of learning algorithms.</s><s coords="12,287.40,386.33,201.52,8.90;12,106.36,398.28,177.69,8.90">A method for explainability should answer several questions to form an exhaustive explanation.</s><s coords="12,287.26,398.28,201.67,8.90;12,106.36,410.24,306.38,8.90">The two most common questions are why and how the model under scrutiny produces its predictions/inferences <ref type="bibr" coords="12,348.82,410.24,20.75,8.90" target="#b101">[102,</ref><ref type="bibr" coords="12,372.06,410.24,17.43,8.90" target="#b102">103,</ref><ref type="bibr" coords="12,391.98,410.24,7.47,8.90" target="#b1">2,</ref><ref type="bibr" coords="12,401.95,410.24,7.19,8.90" target="#b6">7]</ref>.</s><s coords="12,415.82,410.24,73.10,8.90;12,106.36,422.19,382.57,8.90;12,106.36,434.15,89.24,8.90">However, scholars identified other questions that might arise and that require different answers, thus different types of explanations <ref type="bibr" coords="12,171.53,434.15,20.06,8.90" target="#b103">[104]</ref>.</s><s coords="12,201.22,434.15,287.70,8.90;12,106.36,446.10,263.90,8.90">Additionally, as pointed out in <ref type="bibr" coords="12,328.98,434.15,20.75,8.90" target="#b104">[105,</ref><ref type="bibr" coords="12,353.06,434.15,16.60,8.90" target="#b106">106]</ref>, distinct behaviours, distinct problems and distinct types of users require distinct explanations.</s><s coords="12,374.51,446.10,114.41,8.90;12,106.36,458.06,303.93,8.90">This has led to many ad-hoc classifications that are domain-dependent and are hard to be merged into one.</s><s coords="12,413.25,458.06,75.67,8.90;12,106.36,470.01,239.74,8.90">For example, <ref type="bibr" coords="12,467.34,458.06,21.58,8.90" target="#b107">[107]</ref> focused on the types of users of methods for explainability.</s><s coords="12,350.60,470.01,138.32,8.90;12,106.36,481.97,382.56,8.90;12,106.36,493.92,382.56,8.90;12,106.36,505.88,191.23,8.90">They proposed a two-class system consisting of traced-based explanations, useful for system designers, that accurately reflects the reasoning implemented within a model, and reconstructive explanations, designed for end-users, based on an active, problem-solving approach.</s><s coords="12,304.09,505.88,184.83,8.90;12,106.36,517.83,251.52,8.90">A reconstructive explanation tends to build a 'story' exposing the input features contributing to a prediction.</s><s coords="12,361.50,517.83,127.42,8.90;12,106.36,529.79,267.21,8.90">For instance, an image of a bird was assigned to a certain class because of the colour of the bird.</s><s coords="12,379.77,529.79,109.15,8.90;12,106.36,541.74,382.56,8.90;12,106.36,553.70,30.71,8.90">However, the model might have analysed other features that did not influence the final assessment, like the image's background.</s><s coords="12,140.01,553.70,348.91,8.90;12,106.36,565.65,126.56,8.90">These characteristics can be included in the traced-based explanations but excluded from the reconstructive explanations.</s><s coords="12,237.02,565.65,251.89,8.90;12,106.36,577.61,382.56,8.90;12,106.36,589.56,76.23,8.90">The same scholars also developed Reconstructive EXplanation (REX) <ref type="bibr" coords="12,135.67,577.61,20.75,8.90" target="#b107">[107,</ref><ref type="bibr" coords="12,159.18,577.61,16.60,8.90" target="#b108">108]</ref>, an explanatory tool capable of producing reconstructive textual explanations for expert systems.</s><s coords="12,186.96,589.56,301.96,8.90;12,106.36,601.52,161.82,8.90">REX is built on a model that maps the execution of the expert system onto a textbook representation of the domain.</s><s coords="12,271.82,601.52,217.10,8.90;12,106.36,613.47,364.03,8.90">A textbook representation presents the domain knowledge in human-understandable explanations, much of which comes from domain textbooks.</s><s coords="12,473.43,613.47,15.49,8.90;12,106.36,625.43,382.56,8.90;12,106.36,637.38,382.56,8.90;12,106.36,649.34,382.56,8.90;12,106.36,661.29,382.56,8.90;12,106.36,673.25,162.06,8.90">The explanation consists of mapping over key elements from the execution trace and expanding on them using the more structured textbook knowledge, which is a collection of relationships between cues, hypotheses and goals as illustrated by this example: "The presence of damages to the drainage pipes is a sign that the cause of an excessive high uplift pressures on a concrete dam is internal erosion of soil under the dam.</s><s coords="12,271.94,673.25,216.98,8.90;12,106.36,685.20,178.81,8.90">Erosion would lead to broken pipes, therefore slowing drainage and causing high uplift pressures".</s><s coords="12,290.61,685.20,198.31,8.90;13,106.36,144.23,382.56,8.90;13,106.36,156.19,104.26,8.90">The goal is to determine the cause of high uplift pressure on a concrete dam, the cues consist of the presence of broken pipes and the hypothesis is the erosion of soil.</s><s coords="13,215.81,156.19,273.10,8.90;13,106.36,168.14,382.56,8.90;13,106.36,180.10,382.56,8.90">Another classification of the types of explanations was proposed in <ref type="bibr" coords="13,106.36,168.14,21.58,8.90" target="#b109">[109]</ref> for intelligent systems which include intelligent agents, such as those AI assistants utilised in customer support chats, or other support decision systems like those for medical diagnoses.</s><s coords="13,106.36,192.05,382.56,8.90;13,106.36,204.01,185.18,8.90">Here, traced-based explanations were defined as mechanistic explanations and correspond to the answer of the question "How does it work?".</s><s coords="13,297.31,204.01,191.62,8.90;13,106.36,215.96,382.57,8.90;13,106.36,227.92,212.78,8.90">Hence, they must offer insights into the causes and consequences of events and how these events and the different components of the intelligent systems interact to give rise to complex actions.</s><s coords="13,323.26,227.92,165.66,8.90;13,106.36,239.87,382.57,8.90;13,106.36,251.83,283.43,8.90">Reconstructive explanations were instead called ontological explanations and describe the structural properties of the intelligent systems: its components, their attributes, and how they are related to each other.</s><s coords="13,393.48,251.83,95.44,8.90;13,106.36,263.78,382.57,8.90;13,106.36,275.74,14.39,8.90"><ref type="bibr" coords="13,393.48,251.83,21.58,8.90" target="#b109">[109]</ref> also added a third category, referred to as operational explanations which respond to the question "How do I use it?"</s><s coords="13,123.71,275.74,234.12,8.90">by relating goals to the mechanics designed to realise them.</s><s coords="13,360.80,275.74,128.12,8.90;13,106.36,287.70,382.56,8.90;13,106.36,299.65,158.82,8.90">A more articulated classification of the types of explanations was introduced in <ref type="bibr" coords="13,291.85,287.70,21.58,8.90" target="#b110">[110]</ref> and it is based on five types of explanations that intelligent systems should produce.</s><s coords="13,268.86,299.65,220.06,8.90;13,106.36,311.61,382.56,8.90;13,106.36,323.56,292.51,8.90">The first one, teaching explanations, aims at informing humans about the concepts learned by the system such as, for example, the presence of some physical constraints (walls or other obstacles) that can limit its actions.</s><s coords="13,405.16,323.63,83.76,8.71;13,106.36,335.52,382.57,8.90;13,106.36,347.47,382.56,8.90;13,106.36,359.43,131.09,8.90">Introspective tracing explanations have the goal of finding the cause of and the solution to a fault whilst introspective informative explanations aim at explaining predictions based on the reasoning process to improve human-system interaction.</s><s coords="13,241.29,359.43,247.63,8.90;13,106.36,371.38,382.56,8.90;13,106.36,383.34,382.56,8.90;13,106.36,395.29,19.10,8.90">The last two types of explanations, post-hoc explanations and execution explanations, are respectively focused on explaining the decisions and their execution without necessarily following the same reasoning process and directly linking them with the inputs.</s><s coords="13,129.66,395.29,359.26,8.90;13,106.36,407.25,361.80,8.90">An example of post-hoc and execution explanation is a robot describing the path it wants to follow to go from point A to point B and all the movements it must do to cover that path.</s><s coords="13,471.21,407.25,17.71,8.90;13,106.36,419.20,382.56,8.90;13,106.36,431.16,382.56,8.90;13,106.36,443.11,213.81,8.90">This explanation can mention the characteristics of the surrounding environment that have been considered while planning the path, but it does not mention that alternative paths were considered and discarded and the reasons beyond these decisions.</s><s coords="13,323.16,443.11,165.76,8.90;13,106.36,455.07,257.90,8.90">Finally, <ref type="bibr" coords="13,355.52,443.11,21.58,8.90" target="#b111">[111]</ref> presented a classification of the types of knowledge intrinsically embedded in an explanation.</s><s coords="13,367.29,455.07,121.64,8.90;13,106.36,467.02,382.57,8.90;13,106.36,478.98,90.39,8.90">Explanations based on reasoning domain knowledge focus on the domain knowledge needed to perform reasoning, including rules and terminology.</s><s coords="13,201.49,478.98,287.43,8.90;13,106.36,490.93,382.56,8.90;13,106.36,502.89,301.33,8.90">Communication domain knowledge is instead about the domain knowledge needed to inform, clearly and comprehensively, end-users about the underlying domain, and it might include additional information not strictly necessary for reasoning.</s><s coords="13,411.41,502.89,77.51,8.90;13,106.36,514.84,382.56,8.90;13,106.36,526.80,382.56,8.90;13,106.36,538.75,382.57,8.90">Eventually, domain communication knowledge focuses on how to communicate within a certain domain of application and it deals with practical aspects of the communication process, such as the language to be used, the most effective strategies for effective explanations and the communication medium.</s><s coords="13,106.36,550.71,346.36,8.90">This knowledge must be tuned to the prior knowledge and cognitive state of the hearer.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3." coords="13,106.36,573.19,123.96,8.71">Structures of explanations</head><p><s coords="13,121.30,586.57,367.62,8.90;13,106.36,598.53,77.26,8.90">The most effective way to structure explanations is still an open problem despite being tackled by several scholars.</s><s coords="13,186.60,598.53,302.32,8.90;13,106.36,610.48,382.56,8.90;13,106.36,622.44,288.85,8.90">As highlighted in <ref type="bibr" coords="13,257.34,598.53,20.06,8.90" target="#b112">[112]</ref>, two properties of the structure of an explanation can have a significant effect on learning, namely the capacity to "accommodate novel information in the context of prior beliefs and do so in a way that fosters generalization".</s><s coords="13,398.14,622.44,90.78,8.90;13,106.36,634.40,382.56,8.90;13,106.36,646.35,346.51,8.90">As prior beliefs greatly vary according to the application field and the domain knowledge of end-users, researchers examined and proposed different structures for explanations which are domain-dependent.</s><s coords="13,455.82,646.35,33.10,8.90;13,106.36,658.31,382.56,8.90;13,106.36,670.26,311.49,8.90">The first studies on the most suitable and effective structures of textual explanations were carried out in the 80-90s and focused on interpreting the inferential process of expert models.</s><s coords="13,420.80,670.26,68.11,8.90;13,106.36,682.22,382.56,8.90;13,106.36,694.17,131.90,8.90">Most of these explanations were planned as dialogues where end-users were allowed to ask a (limited) number of questions via an explanatory tool.</s><s coords="13,241.17,694.17,247.75,8.90;14,106.36,144.23,275.17,8.90">Blah <ref type="bibr" coords="13,261.95,694.17,20.06,8.90" target="#b113">[113]</ref>, an example of these tools, was primarily concerned with structuring explanations so that they do not appear too complex.</s><s coords="14,384.60,144.23,104.32,8.90;14,106.36,156.19,382.56,8.90;14,106.36,168.14,58.93,8.90">It was based on a series of psycho-linguistic studies that analyzed how human beings explain decisions, choices, and plans to one another.</s><s coords="14,168.37,168.14,320.55,8.90;14,106.36,180.10,382.56,8.90">Different ways to structure a conversational explanation, or dialogue, to successfully transfer knowledge from an explainer to an explainee were listed in <ref type="bibr" coords="14,392.23,180.10,20.75,8.90" target="#b114">[114,</ref><ref type="bibr" coords="14,414.95,180.10,17.43,8.90" target="#b115">115,</ref><ref type="bibr" coords="14,434.34,180.10,17.43,8.90" target="#b116">116,</ref><ref type="bibr" coords="14,453.75,180.10,17.43,8.90" target="#b117">117,</ref><ref type="bibr" coords="14,473.15,180.10,11.83,8.90" target="#b79">80]</ref>.</s><s coords="14,106.36,192.05,382.56,8.90;14,106.36,204.01,38.55,8.90">All these studies proposed to split a dialogue into three stages: opening, explanation and closing stage.</s><s coords="14,149.11,204.01,339.80,8.90;14,106.36,215.96,259.18,8.90">Each stage has to obey a set of rules to ensure that the knowledge about the model's inferential process can be successfully transferred to end-users.</s><s coords="14,371.81,215.96,117.11,8.90;14,106.36,227.92,382.56,8.90;14,106.36,239.87,382.56,8.90;14,106.36,251.83,139.77,8.90">On one hand, <ref type="bibr" coords="14,431.15,215.96,16.60,8.90" target="#b79">[80]</ref> grounded this three-stage formal protocol on the data collected from almost four hundred real dialogues which were examined to detect the key components of an explanation, the relationships between them and their order of occurrence.</s><s coords="14,249.17,251.83,239.75,8.90;14,106.36,263.78,382.56,8.90;14,106.36,275.74,382.56,8.90;14,106.36,287.70,126.53,8.90">These main components can be synthesised by a set of questions (mainly how, why and what) and the relative arguments presented by an explainer to an explainee who, respectively, answer the questions and acknowledge the explanation or challenge it with counterfactual examples.</s><s coords="14,235.87,287.70,253.05,8.90;14,106.36,299.65,382.56,8.90;14,106.36,311.61,239.33,8.90">On the other hand, <ref type="bibr" coords="14,311.19,287.70,20.75,8.90" target="#b115">[115,</ref><ref type="bibr" coords="14,334.08,287.70,17.43,8.90" target="#b116">116,</ref><ref type="bibr" coords="14,353.66,287.70,18.26,8.90" target="#b117">117]</ref> focused on the most effective set of rules to manage interactive dialogues with interruptions from the user while maintaining coherence between the different sections of an explanation.</s><s coords="14,350.44,311.61,138.48,8.90;14,106.36,323.56,220.58,8.90">They also developed a tool, called EDGE, that generates dialogues based on these rules.</s><s coords="14,333.60,323.56,155.32,8.90;14,106.36,335.52,382.56,8.90;14,106.36,347.47,114.43,8.90">EDGE updates assumptions about the user's knowledge based on his/her questions and uses this information to influence the further planning of the explanation.</s><s coords="14,226.66,347.47,262.26,8.90;14,106.36,359.43,382.56,8.90;14,106.36,371.38,128.92,8.90">Other studies on interactive dialogues <ref type="bibr" coords="14,383.92,347.47,20.75,8.90" target="#b118">[118,</ref><ref type="bibr" coords="14,408.09,347.47,17.43,8.90" target="#b119">119,</ref><ref type="bibr" coords="14,428.95,347.47,17.43,8.90" target="#b120">120,</ref><ref type="bibr" coords="14,449.81,347.47,17.43,8.90" target="#b121">121,</ref><ref type="bibr" coords="14,470.66,347.47,18.26,8.90" target="#b122">122]</ref> focused on the structure, the language and main components (what pieces of information must be included) of these dialogues.</s><s coords="14,240.20,371.38,248.72,8.90;14,106.36,383.34,373.00,8.90">Based in these early studies, <ref type="bibr" coords="14,358.55,371.38,20.75,8.90" target="#b123">[123,</ref><ref type="bibr" coords="14,382.41,371.38,17.43,8.90" target="#b124">124,</ref><ref type="bibr" coords="14,402.95,371.38,17.43,8.90" target="#b125">125,</ref><ref type="bibr" coords="14,423.49,371.38,18.26,8.90" target="#b126">126]</ref> proposed a modular architecture for explaining the behavior of simulated entities in military simulations.</s><s coords="14,482.83,383.34,6.09,8.90;14,106.36,395.29,364.00,8.90">It consists of three modules: a reasoner, a natural language generator and a dialogue manager.</s><s coords="14,473.43,395.29,15.49,8.90;14,106.36,407.25,382.56,8.90;14,106.36,419.20,83.66,8.90">The user can stop simulation and query about what happened at the current time point by selecting questions from a list.</s><s coords="14,193.18,419.20,295.74,8.90;14,106.36,431.16,382.56,8.90;14,106.36,443.11,212.18,8.90">The dialogue manager orchestrates the system's response: firstly, by using the reasoner to retrieve the relevant information from a relational database, then producing English responses using the natural language generator.</s><s coords="14,322.71,443.11,166.21,8.90;14,106.36,455.07,382.56,8.90">More recently, interactive dialogues were used as the explanation format of choice in knowledge-based systems other than expert systems.</s><s coords="14,106.36,467.02,382.56,8.90;14,106.36,478.98,103.79,8.90">AutoTutor <ref type="bibr" coords="14,150.74,467.02,20.06,8.90" target="#b127">[127]</ref>, designed to be integrated into tutoring systems, is grounded on learning theories and tutoring research.</s><s coords="14,213.54,478.98,275.38,8.90;14,106.36,490.93,78.85,8.90">It simulates a human tutor by holding a conversation with the learner in natural language.</s></p><p><s coords="14,121.30,514.84,367.62,8.90;14,106.36,526.80,382.56,8.90;14,106.36,538.75,382.56,8.90;14,106.36,550.71,382.56,8.90;14,106.36,562.66,19.65,8.90">The explanations of task planning systems, according to <ref type="bibr" coords="14,346.86,514.84,15.77,8.90" target="#b11">[12,</ref><ref type="bibr" coords="14,365.07,514.84,16.60,8.90" target="#b128">128]</ref>, must contain information on (I) why a planner choose an action, (II) why a planner did not choose another action, (III) why the decisions of a planner are the best among a set of possible alternatives, (IV) why certain actions cannot be executed and (V) why one needs or does not need to change the original plan.</s><s coords="14,130.00,562.66,358.93,8.90;14,106.36,574.62,382.56,8.90;14,106.36,586.57,382.56,8.90;14,106.36,598.53,248.56,8.90">The criterion of episodic memory was added to the above list by <ref type="bibr" coords="14,391.11,562.66,20.06,8.90" target="#b128">[128]</ref>, whereby an agent should remember all the factors that influenced the generation and execution of a plan such as "states, actions, and values considered during plan generation, traces of plan execution in the environment, and anomalous events that led to plan revision".</s><s coords="14,359.01,598.53,129.91,8.90;14,106.36,610.48,233.69,8.90">A formal framework to generate preferred explanations of a plan was introduced in <ref type="bibr" coords="14,315.98,610.48,20.06,8.90" target="#b129">[129]</ref>.</s><s coords="14,345.57,610.48,143.35,8.90;14,106.36,622.44,269.16,8.90">Preferences over explanations must be contextualized with respect to complex observational patterns.</s><s coords="14,382.49,622.44,106.43,8.90;14,106.36,634.40,382.56,8.90;14,106.36,646.35,186.68,8.90">Actions might be affected by several causes and requires reflecting on the past, meaning that explanations must take into consideration previous events and information.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." coords="15,106.36,144.18,212.58,8.82">Development of new methods for explainability</head><p><s coords="15,121.30,164.56,367.62,8.90;15,106.36,176.51,27.31,8.90">More than 200 scientific articles were found that aim at developing new methods for explainability.</s><s coords="15,136.74,176.51,352.18,8.90;15,106.36,188.47,203.67,8.90">Over time, researchers have tried to comprehend and unfurl the inner mechanics of datadriven, knowledge-driven models in various ways.</s><s coords="15,315.01,188.47,173.91,8.90;15,106.36,200.42,256.22,8.90">From an examination of these articles, two main criteria exist for discriminating methods for explainability:</s></p><p><s coords="15,121.33,218.80,339.46,9.96">• scope -it refers to the scope of an explanation that can be either global or local.</s><s coords="15,465.44,219.55,23.48,8.90;15,131.26,231.51,357.66,8.90;15,131.26,243.46,112.21,8.90">In the former case, the goal is to make the entire inferential process of a model transparent and comprehensible as a whole.</s><s coords="15,248.66,243.46,240.26,8.90;15,131.26,255.42,153.83,8.90">In the latter case, the objective is to explicitly explain each inference of a model <ref type="bibr" coords="15,215.91,255.42,20.75,8.90" target="#b130">[130,</ref><ref type="bibr" coords="15,239.16,255.42,12.45,8.90" target="#b25">26,</ref><ref type="bibr" coords="15,254.10,255.42,12.45,8.90" target="#b55">56,</ref><ref type="bibr" coords="15,269.04,255.42,12.04,8.90" target="#b16">17]</ref>;</s></p><p><s coords="15,121.33,274.20,292.47,9.96">• stage -it refers to the stage at which a method generates explanations.</s><s coords="15,416.84,274.94,72.08,8.90;15,131.26,286.90,357.66,8.90;15,131.26,298.85,357.66,8.90;15,131.26,310.81,357.66,8.90;15,131.26,322.76,357.66,8.90;15,131.26,334.72,84.12,8.90">Ante-hoc methods are generally aimed at considering explainability of a model from the beginning and during training to make it naturally explainable whilst still trying to achieve optimal accuracy or minimal error <ref type="bibr" coords="15,202.18,310.81,15.77,8.90" target="#b12">[13,</ref><ref type="bibr" coords="15,221.30,310.81,12.45,8.90" target="#b98">99,</ref><ref type="bibr" coords="15,237.10,310.81,16.82,8.90" target="#b131">131]</ref>; post-hoc methods are aimed at keeping a trained model unchanged and mimic or explain its behaviour by using an external explainer at testing time <ref type="bibr" coords="15,151.47,334.72,15.77,8.90" target="#b12">[13,</ref><ref type="bibr" coords="15,169.73,334.72,12.45,8.90" target="#b55">56,</ref><ref type="bibr" coords="15,184.67,334.72,12.45,8.90" target="#b28">29,</ref><ref type="bibr" coords="15,199.62,334.72,11.83,8.90" target="#b96">97]</ref>.</s></p><p><s coords="15,121.30,353.85,367.62,8.90;15,106.36,365.80,222.47,8.90">Taking into account the articles examined in this systematic review, and inspired by the classification system in <ref type="bibr" coords="15,185.79,365.80,15.27,8.90" target="#b20">[21]</ref>, we propose additional criteria:</s></p><p><s coords="15,121.33,384.18,367.59,9.96;15,131.26,396.89,109.56,8.90">• problem type -methods for explainability can vary according to the underlying problem: classification or regression;</s></p><p><s coords="15,121.33,415.67,367.59,9.96;15,131.26,428.37,357.66,8.90;15,131.26,440.32,357.65,8.90;15,131.26,452.28,162.56,8.90">• input data -the mechanisms followed by a model to classify images can substantially differ from those used to classify textual documents, thus the input format of a model (numerical/categorical, pictorial, textual or times series) can play an important role in constructing a method for explainability;</s></p><p><s coords="15,121.33,471.06,367.59,9.96;15,131.26,483.76,357.66,8.90;15,131.26,495.72,37.07,8.90">• output format -similarly, different formats of explanations useful for different circumstances can be considered by a method for explainability: numerical, rules, textual, visual or mixed.</s></p><p><s coords="15,121.30,514.84,367.62,8.90;15,106.36,526.80,144.15,8.90">Figure <ref type="figure" coords="15,150.05,514.84,4.98,8.90" target="#fig_3">4</ref> depicts the main branches of methods for explainability and shows the distribution of the articles across these branches.</s><s coords="15,253.53,526.80,235.39,8.90;15,106.36,538.75,382.56,8.90;15,106.36,550.71,214.75,8.90">Each of the many methods for explainability retrieved from the scientific literature can be robustly described by using the five categories of figure <ref type="figure" coords="15,454.78,538.75,4.98,8.90" target="#fig_3">4</ref> (stage, scope, problem type, input data and output format).</s><s coords="15,328.19,550.71,160.73,8.90;15,106.36,562.66,382.57,8.90;15,106.36,574.62,56.27,8.90">Additionally, as it is possible to notice from Figure <ref type="figure" coords="15,157.02,562.66,3.74,8.90" target="#fig_3">4</ref>, the post-hoc methods are further divided into model-agnostic and model-specific methods <ref type="bibr" coords="15,143.54,574.62,15.27,8.90" target="#b20">[21]</ref>.</s><s coords="15,168.48,574.62,320.44,8.90;15,106.36,586.57,363.59,8.90">The former methods do not consider the internal components of a model such as weights or structural information, therefore they can be applied to any black-box model.</s><s coords="15,473.43,586.57,15.49,8.90;15,106.36,598.53,247.19,8.90">The latter methods are instead limited to specific classes of models.</s><s coords="15,356.51,598.53,132.41,8.90;15,106.36,610.48,382.56,8.90">For example, the interpretation of the weights of a linear regression model is specific to the learning approach (linear regression).</s><s coords="15,106.36,622.44,382.56,8.90;15,106.36,634.40,50.55,8.90">Similarly, methods that only work with the interpretation of neural networks are model-specific <ref type="bibr" coords="15,106.36,634.40,15.77,8.90" target="#b24">[25,</ref><ref type="bibr" coords="15,125.40,634.40,12.45,8.90" target="#b25">26,</ref><ref type="bibr" coords="15,141.13,634.40,11.83,8.90" target="#b29">30]</ref>.</s><s coords="15,162.35,634.40,326.56,8.90;15,106.36,646.35,382.56,8.90;15,106.36,658.31,146.71,8.90">Model agnosticity and specificity do not usually apply to the class of 'ante-hoc' methods because their goal is to make the functioning of a model transparent, so almost all them are intrinsically model-specific <ref type="bibr" coords="15,233.98,658.31,15.27,8.90" target="#b12">[13]</ref>.</s><s coords="15,257.88,658.31,231.04,8.90;15,106.36,670.26,382.56,8.90;15,106.36,682.22,24.07,8.90">Some post-hoc methods for explainability can be applied both at a global or local scope <ref type="bibr" coords="15,226.39,670.26,21.58,8.90" target="#b132">[132]</ref> and can work for either regression or classification problems <ref type="bibr" coords="15,106.36,682.22,20.06,8.90" target="#b133">[133]</ref>.</s><s coords="16,121.30,338.54,367.62,8.90;16,106.36,350.49,382.56,8.90;16,106.36,362.45,318.84,8.90">The following sections try to succinctly describe the main classes of methods for explainability found during this systematic review, accompanied by tables for reporting their stage, scope, problem type, input data and output format and sorting them in alphabetic order.</s><s coords="16,428.26,362.45,60.66,8.90;16,106.36,374.40,327.96,8.90">Given the large number of methods found, it was decided to group them into five thematic classes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1." coords="16,106.36,398.38,80.54,8.71">Output formats</head><p><s coords="16,121.30,413.26,367.62,8.90;16,106.36,425.21,124.67,8.90">Visual explanations are probably the most natural way of communicating things and a very appealing way to explain them.</s><s coords="16,234.23,425.21,254.69,8.90;16,106.36,437.17,156.10,8.90">Visual explanations can also be used to illustrate the inner functioning of a model via graphical tools.</s><s coords="16,267.64,437.17,221.28,8.90;16,106.36,449.12,382.56,8.90;16,106.36,461.08,154.36,8.90">For instance, heat-maps can highlight specific areas of an image or specific words of a text that mostly influence the inferential process of a model by using different colours <ref type="bibr" coords="16,215.55,461.08,20.75,8.90" target="#b134">[134,</ref><ref type="bibr" coords="16,239.96,461.08,16.60,8.90" target="#b135">135]</ref>.</s><s coords="16,267.33,461.08,221.59,8.90;16,106.36,473.03,382.56,8.90;16,106.36,484.99,309.81,8.90">Similarly, a graphical representation can be employed to represent the inner structure of a model, such as the graphs proposed in <ref type="bibr" coords="16,417.67,473.03,21.58,8.90" target="#b136">[136]</ref> where each node is a layer of the network and the edges the connections between layers.</s><s coords="16,420.54,484.99,68.38,8.90;16,106.36,496.95,382.56,8.90;16,106.36,508.90,138.69,8.90">Another intuitive form of explanation for humans are textual explanations, natural language statements that can be either written or orally uttered.</s><s coords="16,250.67,508.90,238.25,8.90;16,106.36,520.86,382.56,8.90;16,106.36,532.81,159.47,8.90">An example is the phrase "This is a Brewer Blackbird because this is a blackbird with a white eye and long pointy black beak" shown by an explainer of an image classification model <ref type="bibr" coords="16,241.76,532.81,20.06,8.90" target="#b137">[137]</ref>.</s><s coords="16,271.31,532.81,217.61,8.90;16,106.36,544.77,382.57,8.90;16,106.36,556.72,246.58,8.90">A schematic, logical format, more structured than visual and textual explanations but still intuitive for humans, are rules that can be used to explain the inferences produced by models induced from data.</s><s coords="16,358.96,556.72,129.96,8.90;16,106.36,568.68,382.57,8.90;16,106.36,580.63,253.20,8.90">Rules can be in the form of 'IF ... THEN' statements with AND/OR operators and they are very useful for expressing combinations of input features and their activation values <ref type="bibr" coords="16,315.16,580.63,20.75,8.90" target="#b138">[138,</ref><ref type="bibr" coords="16,338.80,580.63,16.60,8.90" target="#b139">139]</ref>.</s><s coords="16,363.82,580.63,125.10,8.90;16,106.36,592.59,382.56,8.90;16,106.36,603.79,311.92,9.96">Technically, rules of these type employ symbolic logic, a formalized system of primitive symbols and their combinations (example: '(Country = US A) ∧ (28 &lt; Age &lt;= 37) → (S alary &gt; 50K)' <ref type="bibr" coords="16,390.89,604.54,19.56,8.90" target="#b140">[140]</ref>).</s><s coords="16,422.72,604.54,66.20,8.90;16,106.36,616.40,355.61,9.00">The parts before and after the → logical operator are respectively referred to as antecedent and consequent.</s><s coords="16,464.97,616.50,23.95,8.90;16,106.36,628.45,382.56,8.90;16,106.36,640.41,293.51,8.90">Given this logic, rules can be implemented as fuzzy rules, linking one or more premises to a consequent that can be true to a degree, instead of being entirely true or false.</s><s coords="16,404.58,640.41,84.34,8.90;16,106.36,652.36,267.23,8.90">This can be obtained by representing each antecedent and consequent as fuzzy sets <ref type="bibr" coords="16,354.49,652.36,15.27,8.90" target="#b42">[43]</ref>.</s><s coords="16,377.04,652.36,111.88,8.90;16,106.36,664.32,382.56,8.90;16,106.36,676.27,164.05,8.90">Combining fuzzy rules with learning algorithms can become a powerful tool to perform reasoning and, for instance, explain the inner logic of neural networks <ref type="bibr" coords="16,246.34,676.27,20.06,8.90" target="#b141">[141]</ref>.</s><s coords="16,274.92,676.27,214.00,8.90;16,106.36,688.23,382.56,8.90;17,106.36,144.23,382.56,8.90;17,106.36,156.19,127.20,8.90">Similarly, the combination of antecedents and consequent can be seen as an argument in the discipline of argumentation, and a set of arguments can be put together in a dialogical structure by employing attacks, the link between arguments that model conflictuality <ref type="bibr" coords="17,189.31,156.19,20.75,8.90" target="#b142">[142,</ref><ref type="bibr" coords="17,212.81,156.19,16.60,8.90" target="#b143">143]</ref>.</s><s coords="17,237.38,156.19,251.54,8.90;17,106.36,168.14,351.39,8.90">Arguments and attacks form a complex structure but with high explanatory power, suitable for explaining the inner functioning of data-driven models.</s><s coords="17,462.36,168.14,26.56,8.90;17,106.36,180.10,382.56,8.90;17,106.36,192.05,382.56,8.90;17,106.36,204.01,128.53,8.90">Explanations can also be constructed by only employing numerical formats as crisp values, vectors of numbers, matrices or tensors as in Probe <ref type="bibr" coords="17,275.05,192.05,21.58,8.90" target="#b144">[144]</ref> and Concept Activation Vectors (CAVs) <ref type="bibr" coords="17,464.85,192.05,20.06,8.90" target="#b145">[145]</ref>, two methods for explainability.</s><s coords="17,242.30,204.01,246.61,8.90;17,106.36,215.96,256.70,8.90">A Probe consists of a linear classifier fitted to the features, treated independently, learned by each layer of a neural network.</s><s coords="17,366.09,215.96,122.83,8.90;17,106.36,227.92,232.78,8.90">Probes are engineered to better understand the roles and dynamics of the internal layers.</s><s coords="17,345.16,227.92,143.76,8.90;17,106.36,239.87,243.32,8.90">The numerical explanations are the probability scores assigned by the probes to each class <ref type="bibr" coords="17,325.60,239.87,20.06,8.90" target="#b144">[144]</ref>.</s><s coords="17,352.75,239.87,136.17,8.90;17,106.36,251.83,382.56,8.90;17,106.36,263.78,350.36,8.90">CAVs separates the activation values of a neural network's hidden layer relative to instances belonging to a class, forming a set, from those generated by the remaining part of the input dataset, forming a second set.</s><s coords="17,461.80,263.78,27.12,8.90;17,106.36,275.74,382.56,8.90">Subsequently, a binary linear classifier is trained to distinguish the activation values of the two sets.</s><s coords="17,106.36,287.70,382.56,8.90;17,106.36,299.65,224.04,8.90">Then, CAVs computes directional derivatives on this classifier to measure the sensitivity of the model to changes in inputs towards the class of interest.</s><s coords="17,334.12,299.65,154.80,8.90;17,106.36,311.61,382.56,8.90;17,106.36,323.56,216.28,8.90">This is a scalar quantity, calculated for each class over the whole dataset, which quantifies how important a user-defined concept is to classify the input instances in the class under analysis.</s><s coords="17,325.79,323.56,163.13,8.90;17,106.36,335.52,261.24,8.90">For example, CAVs measures how sensitive the class 'zebra' is to the presence of stripes in an input image.</s><s coords="17,370.53,335.52,118.38,8.90;17,106.36,347.47,382.56,8.90;17,106.36,359.43,96.72,8.90">Eventually, the most powerful format of explanations are those that employ one or more of the formats described so far (visual, textual, rules, numeric).</s><s coords="17,208.52,359.43,280.40,8.90;17,106.36,371.38,382.56,8.90;17,106.36,383.34,271.56,8.90">An example of a combination of visual and numerical explanation is utilized by Important Support Vectors and Border Classification <ref type="bibr" coords="17,367.40,371.38,21.58,8.90" target="#b146">[146]</ref> that provide insight into local classifications produced by a Support Vector Machine (SVM).</s><s coords="17,380.63,383.34,108.29,8.90;17,106.36,395.29,364.05,8.90">The former method returns the support vectors which influence the most the final classification for a particular instance.</s><s coords="17,473.43,395.29,15.49,8.90;17,106.36,407.25,382.56,8.90;17,106.36,419.20,226.12,8.90">The latter determines which features of a data point would need to be altered (and by how much) to be placed on the separating surface between two classes.</s><s coords="17,335.88,419.20,153.04,8.90;17,106.36,431.16,382.56,8.90;17,106.36,443.11,247.87,8.90">The explanations are in the form of an interactive interface where the user can select a point and the tool shows the attributes that had the largest effect on classifying it and the closest border value.</s><s coords="17,357.39,443.11,131.54,8.90;17,106.36,455.07,210.36,8.90">The user can modify the selected point's attributes to see how the SVM reclassifies it.</s><s coords="17,321.13,455.07,167.79,8.90;17,106.36,467.02,341.44,8.90">Image Caption Generation with Attention Mechanism <ref type="bibr" coords="17,155.78,467.02,21.58,8.90" target="#b147">[147]</ref> is an example of visual and textual explanations jointly employed.</s><s coords="17,452.23,467.02,36.69,8.90;17,106.36,478.98,382.56,8.90;17,106.36,490.93,382.56,8.90;17,106.36,502.89,125.62,8.90">It returns attention maps for a combination of a Convolutional Neural Network (CNN) and a Long-Short Term Memory (LSTM) network where the CNN performs object recognition in images and the LSTM generates their captions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2." coords="17,106.36,526.87,188.43,8.71">Model agnostic methods for explainability</head><p><s coords="17,121.30,541.74,342.20,8.90">Several methods for explainability were designed to work with any learning technique.</s><s coords="17,466.48,541.74,22.44,8.90;17,106.36,553.70,382.56,8.90;17,106.36,565.65,382.56,8.90">However, this does not mean that they can be universally applied as they might be constrained to the types of inputs of the technical problem they try to solve and the explanation they try to provide.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1." coords="17,106.36,589.63,114.92,8.71">Numeric explanations</head><p><s coords="17,121.30,602.51,367.62,8.90;17,106.36,614.47,71.82,8.90">A few model agnostic methods for explainability produce numerical explanations (see table A.3 and figure <ref type="figure" coords="17,167.39,614.47,3.60,8.90">5</ref>).</s><s coords="17,182.54,614.47,306.38,8.90;17,106.36,626.43,166.63,8.90">Most of them focus on measuring the contribution of an input variable (or a group of them) with quantitative metrics.</s><s coords="17,278.01,626.43,210.91,8.90;17,106.36,638.38,382.56,8.90;17,106.36,650.34,161.45,8.90">Distill-and-Compare <ref type="bibr" coords="17,363.61,626.43,21.58,8.90" target="#b148">[148]</ref> trains a transparent, simpler model, called student, on the output obtained from a large, complex model, considered as a teacher, to mimic its inferential process.</s><s coords="17,272.15,650.34,216.77,8.90;17,106.36,662.29,357.38,8.90">In this study, the student model was constrained to be GAMs which allow to easily assess the contribution of each feature in a numerical format.</s><s coords="17,466.77,662.29,22.15,8.90;17,106.36,674.25,382.56,8.90;17,106.36,686.20,382.56,8.90;18,106.36,144.23,150.11,8.90">Similarly, SHapley Additive exPlanations (SHAP) <ref type="bibr" coords="17,289.81,674.25,21.58,8.90" target="#b149">[149]</ref> utilizes additive feature attribution methods, basically linear combinations of the input features, to build a model which is an interpretable approximation of the original model.</s><s coords="18,261.92,144.23,227.00,8.90;18,106.36,156.19,382.56,8.90;18,106.36,168.14,312.06,8.90">Some methods for explainability are based on an 'input perturbation' approach and, generally speaking, they work by modifying the reported values of the variables of an input instance to cause a change in the model's prediction.</s><s coords="18,422.44,168.14,66.48,8.90;18,106.36,180.10,382.56,8.90;18,106.36,192.05,22.96,8.90">Explain and Ime <ref type="bibr" coords="18,106.36,180.10,20.75,8.90" target="#b150">[150,</ref><ref type="bibr" coords="18,130.90,180.10,18.26,8.90" target="#b151">151]</ref> assess respectively the contribution of a particular input variable or a set of variables.</s><s coords="18,132.52,192.05,356.40,8.90;18,106.36,204.01,382.56,8.90;18,106.36,215.96,73.64,8.90">This is done by replacing the actual values of the variables describing each input instance with other values sampled from the same variable(s) and measuring the differences in the output probability scores.</s><s coords="18,183.21,215.96,305.71,8.90;18,106.36,227.92,185.30,8.90">The assumption is that the larger the difference in the outcome, the more relevant the variable is for the prediction process.</s><s coords="18,294.73,227.92,194.19,8.90;18,106.36,239.87,382.56,8.90;18,106.36,251.83,234.91,8.90">Similarly, the Global Sensitivity Analysis (GSA) method <ref type="bibr" coords="18,139.26,239.87,20.75,8.90" target="#b152">[152,</ref><ref type="bibr" coords="18,163.02,239.87,18.26,8.90" target="#b153">153]</ref> ranks input features by quantifying the effects on the predictions of a given model when they are varied through their range of values.</s><s coords="18,345.96,251.83,142.96,8.90;18,106.36,263.78,382.56,8.90;18,106.36,275.74,382.56,8.90;18,106.36,287.70,161.48,8.90"><ref type="bibr" coords="18,345.96,251.83,20.75,8.90" target="#b154">[154,</ref><ref type="bibr" coords="18,369.73,251.83,17.43,8.90" target="#b155">155,</ref><ref type="bibr" coords="18,390.19,251.83,17.43,8.90" target="#b156">156,</ref><ref type="bibr" coords="18,410.65,251.83,17.43,8.90" target="#b157">157,</ref><ref type="bibr" coords="18,431.11,251.83,18.26,8.90" target="#b158">158]</ref> proposed a method to explain the prediction of a model at instance level also based on the contribution of each feature estimated by comparing the model output when all the features are known and when one or more of them are omitted.</s><s coords="18,273.49,287.70,215.43,8.90;18,106.36,299.65,382.56,8.90;18,106.36,311.61,151.65,8.90">The contribution is positive for the features that lead to the prediction towards a class, negative for those that push the prediction against a class and zero when they don't have influence.</s><s coords="18,264.18,311.61,224.74,8.90;18,106.36,323.56,382.56,8.90;18,106.36,335.52,382.56,8.90;18,106.36,347.47,86.94,8.90">Four methods, Quantitative Input Influence (QII) functions <ref type="bibr" coords="18,129.14,323.56,20.06,8.90" target="#b159">[159]</ref>, Gradient Feature Auditing (GFA) <ref type="bibr" coords="18,296.49,323.56,20.06,8.90" target="#b160">[160]</ref>, Influence functions <ref type="bibr" coords="18,405.17,323.56,21.58,8.90" target="#b161">[161]</ref> and Monotone Influence Measures <ref type="bibr" coords="18,187.87,335.52,20.06,8.90" target="#b162">[162]</ref>, utilize influence functions to assess the contribution of each feature to certain predictions.</s><s coords="18,196.90,347.47,292.02,8.90;18,106.36,359.43,362.38,8.90">An influence function is a classic technique from statistics <ref type="bibr" coords="18,432.79,347.47,21.58,8.90" target="#b161">[161]</ref> measuring the sensitivity of a model to changes in the distributions of the independent variables.</s><s coords="18,473.43,359.43,15.49,8.90;18,106.36,371.38,382.57,8.90;18,106.36,383.34,382.56,8.90;18,106.36,395.29,258.48,8.90">The perturbation of the input can be done in different ways such as applying a constant shift (Influence functions <ref type="bibr" coords="18,166.72,383.34,19.56,8.90" target="#b161">[161]</ref>), obscuring parts of the input (GFA <ref type="bibr" coords="18,333.85,383.34,19.56,8.90" target="#b160">[160]</ref>), rotating, reflecting or randomly assign labels to the input (Monotone Influence Measures <ref type="bibr" coords="18,337.45,395.29,19.56,8.90" target="#b162">[162]</ref>).</s><s coords="18,369.08,395.29,119.84,8.90;18,106.36,407.25,382.56,8.90;18,106.36,419.20,382.57,8.90;18,106.36,431.16,164.77,8.90">Feature Importance <ref type="bibr" coords="18,450.07,395.29,21.58,8.90" target="#b163">[163]</ref> and Feature Perturbation <ref type="bibr" coords="18,190.26,407.25,21.58,8.90" target="#b164">[164]</ref> are also based on algorithms that modify subsets of the input features to find groups of interacting attributes used by different classifiers and to determine the extent to which a model exploits such interactions.</s></p><p><s coords="18,123.76,593.50,96.06,7.12;18,297.33,593.50,46.48,7.12;18,423.69,593.50,45.45,7.12;18,133.08,612.93,329.11,7.12">(a) Distill-and-Compare <ref type="bibr" coords="18,202.56,593.50,17.26,7.12" target="#b148">[148]</ref> (b) GSA <ref type="bibr" coords="18,326.55,593.50,17.26,7.12" target="#b152">[152]</ref> (c) GFA <ref type="bibr" coords="18,451.88,593.50,17.26,7.12" target="#b160">[160]</ref> Figure <ref type="figure" coords="18,155.89,612.93,3.10,7.12">5</ref>: Examples of numerical explanations generated by model-agnostic methods for explainability.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2." coords="19,106.36,144.30,125.43,8.71">Rule-based explanations</head><p><s coords="19,121.30,157.18,367.62,8.90;19,106.36,169.14,382.56,8.90;19,106.36,181.09,72.69,8.90">A few model-agnostic methods for explainability produce rule-based explanations by exploiting several rule-extraction techniques (see table A.5 and figure <ref type="figure" coords="19,358.07,169.14,3.60,8.90" target="#fig_4">6</ref>), such as automated reasoningbased approaches.</s><s coords="19,182.74,181.09,306.18,8.90;19,106.36,193.05,382.56,8.90;19,106.36,205.01,248.53,8.90">The method presented in <ref type="bibr" coords="19,284.83,181.09,21.58,8.90" target="#b165">[165]</ref> extracts logical formulas as decision trees by combining split predicates along paths from inputs to predictions into logical conjunctions and all the paths related to an output class into logical disjunctions.</s><s coords="19,357.91,205.01,131.01,8.90;19,106.36,216.96,356.50,8.90">These rules can be analyzed with logical reasoning techniques to extract information about the decision-making process.</s><s coords="19,469.54,216.96,19.38,8.90;19,106.36,228.92,382.56,8.90;19,106.36,240.87,163.14,8.90">Similarly, Genetic Rule EXtraction (G-REX) <ref type="bibr" coords="19,277.22,228.92,20.75,8.90" target="#b166">[166,</ref><ref type="bibr" coords="19,301.41,228.92,18.26,8.90" target="#b167">167]</ref> employed genetic algorithms to generate IF-THEN rules with AND/OR operators.</s><s coords="19,272.56,240.87,216.37,8.90;19,106.36,252.83,382.56,8.90;19,106.36,264.78,123.79,8.90">Anchor <ref type="bibr" coords="19,304.84,240.87,21.58,8.90" target="#b140">[140]</ref> uses two algorithms to extract IF-THEN rules which highlight the features of an input instance, called 'anchors', that are sufficient for a classifier to make a prediction.</s><s coords="19,234.62,264.78,254.29,8.90;19,106.36,276.74,382.56,8.90;19,106.36,288.69,19.65,8.90">In an analogical manner, the words "not bad" are often used in sentences expressing a positive sentiment, and thus can be considered anchors in sentiment analyses.</s><s coords="19,130.52,288.69,358.39,8.90;19,106.36,300.65,382.56,8.90;19,106.36,312.60,149.11,8.90">These two algorithms, a bottom-up formation of and a beam-search for anchors, identify the candidate rules with the highest estimated precision over a dataset where precision is equal to the fraction of correct predictions.</s><s coords="19,260.38,312.60,228.54,8.90;19,106.36,324.56,220.55,8.90">The first algorithm starts from an empty set of rules and adds, at each iteration, a rule for each feature predicate.</s><s coords="19,329.99,324.56,158.93,8.90;19,106.36,336.51,382.56,8.90">The second one instead starts from a set containing all the possible candidate rules and then selects the best ones in terms of precision.</s><s coords="19,106.36,348.47,382.56,8.90;19,106.36,360.42,382.56,8.90;19,106.36,372.38,382.56,8.90;19,106.36,384.33,20.75,8.90">Model Extraction <ref type="bibr" coords="19,180.34,348.47,21.58,8.90" target="#b168">[168]</ref> and Partition Aware Local Model (PALM) <ref type="bibr" coords="19,380.73,348.47,21.58,8.90" target="#b170">[169]</ref> utilize decision trees (DTs) to approximate complex models with the assumption that, as long as the approximation quality is good, the statistical properties of the complex model are reflected in the interpretable ones.</s><s coords="19,131.22,384.33,357.70,8.90;19,106.36,396.29,85.51,8.90">End-users have also the faculty to examine the DT's structure and determine whether the rules match intuition.</s><s coords="19,195.78,396.29,293.14,8.90;19,106.36,408.24,382.56,8.90;19,106.36,420.20,186.76,8.90">Model Extraction generates DTs by using the Classification And Regression Trees algorithm (CART) and trains them over a mixture of Gaussian distributions fitted to the input data using expectation maximization.</s><s coords="19,296.45,420.20,192.47,8.90;19,106.36,432.15,382.56,8.90;19,106.36,444.11,133.65,8.90">PALM uses a two-part surrogate model: a metamodel, constrained to be a DT, that partitions the training data, and a set of sub-models fitting the patterns within each partition.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3." coords="19,106.36,651.24,105.33,8.71">Visual explanations</head><p><s coords="19,121.30,664.13,367.62,8.90;19,106.36,676.08,280.20,8.90">Visual explanations try to explain the inner functioning of a model via graphical aids and many model-agnostic methods exploit them (table A.6 and figure <ref type="figure" coords="19,375.76,676.08,3.60,8.90" target="#fig_5">7</ref>).</s><s coords="19,392.11,676.08,96.81,8.90;19,106.36,688.04,382.57,8.90;20,106.36,144.23,382.56,8.90;20,106.36,156.19,166.68,8.90">One of the most widely used among these aids is represented by 'salient masks' that are efficient ways to point out what parts of input, especially when images or texts are treated, most affect a model's prediction by superimposing a mask highlighting them.</s><s coords="20,277.23,156.19,211.69,8.90;20,106.36,168.14,382.56,8.90;20,106.36,180.10,245.23,8.90">Layer-Wise Relevance Propagation (LRP) <ref type="bibr" coords="20,449.09,156.19,21.58,8.90" target="#b100">[101]</ref> was developed as a model-agnostic solution to the problem of understanding image classification predictions by pixel-wise decomposition of nonlinear classifiers.</s><s coords="20,355.19,180.10,133.73,8.90;20,106.36,192.05,382.56,8.90;20,106.36,204.01,382.56,8.90">In its general form, LRP assumes that the classifier can be decomposed into several layers of computation and it traces back contributions of each pixel to the final output, layer by layer, to attribute relevance to individual inputs.</s><s coords="20,106.36,215.96,224.75,8.90">The pixel contributions can be visualized as heat-maps.</s><s coords="20,336.03,215.96,152.89,8.90;20,106.36,227.92,382.56,8.90;20,106.36,239.87,264.34,8.90">Spectral Relevance Analysis (SpRAy) <ref type="bibr" coords="20,106.36,227.92,11.62,8.90" target="#b7">[8]</ref> consists of spectral clustering on a set of LRP explanations in order to identify typical and atypical decision behaviours of an underlying data-driven model.</s><s coords="20,376.09,239.87,112.83,8.90;20,106.36,251.83,382.56,8.90;20,106.36,263.78,123.55,8.90">For example, to analyse the inferential process of a classifier trained on a dataset of images of animals, SpRAy produces an LRP heat-map for each image.</s><s coords="20,234.15,263.78,254.77,8.90;20,106.36,275.74,382.56,8.90;20,106.36,287.70,182.01,8.90">Then, it checks if the heat-maps highlight the area representing the animal or if, for a specific animal, the classifier is focusing on other parts, such as the presence of a rider in case the animal is a horse.</s><s coords="20,293.85,287.70,195.07,8.90;20,106.36,299.65,382.56,8.90;20,106.36,311.61,204.36,8.90">Image Perturbation <ref type="bibr" coords="20,374.59,287.70,21.58,8.90" target="#b171">[170]</ref> produces explanations in the forms of saliency maps by blurring different areas of the image and checking which ones most affect the prediction accuracy when perturbed.</s><s coords="20,313.68,311.61,175.23,8.90;20,106.36,323.56,382.56,8.90;20,106.36,335.52,181.26,8.90">Similarly, the Restricted Support Region Set (RSRS) Detection method <ref type="bibr" coords="20,215.06,323.56,21.58,8.90" target="#b172">[171]</ref> visualizes a set of size-restricted and non-overlapping regions of an image that are critical to classification.</s><s coords="20,292.74,335.52,196.18,8.90;20,106.36,347.47,127.70,8.90">This means that if any of them is removed, then the image is wrongly classified.</s><s coords="20,238.49,347.47,250.43,8.90;20,106.36,359.43,161.35,8.90">The explanation consists of the original image with its critical regions determined by RSRS greyed out.</s><s coords="20,270.69,359.43,218.23,8.90;20,106.36,371.38,63.93,8.90">The IVisClassifier <ref type="bibr" coords="20,344.71,359.43,21.58,8.90" target="#b173">[172]</ref> is based on linear discriminant analysis (LDA).</s><s coords="20,172.92,371.38,316.00,8.90;20,106.36,383.34,382.56,8.90;20,106.36,395.29,282.95,8.90">It attempts at reducing the dimension of the input data and produces heat-maps that gives an overview of the relationship among clusters in terms of pairwise distances between cluster centroids both in the original and reduced dimensional spaces.</s><s coords="20,394.28,395.29,94.63,8.90;20,106.36,407.25,382.56,8.90;20,106.36,419.20,240.58,8.90">The Saliency Detection method <ref type="bibr" coords="20,139.82,407.25,21.58,8.90" target="#b174">[173]</ref> utilizes a U-Net neural network trained to generate a saliency map, in a single forward pass, for any image and classifier received as inputs.</s><s coords="20,349.97,419.20,138.95,8.90;20,106.36,431.16,257.60,8.90">The output map then highlights the parts of the image that are considered important by the classifier.</s><s coords="20,121.30,455.07,367.62,8.90;20,106.36,467.02,31.27,8.90">Some methods use other visual aids, like graphs and scatter-plots, to generate visual explanations.</s><s coords="20,141.85,467.02,347.07,8.90;20,106.36,478.98,39.01,8.90">The Sensitivity Analysis method <ref type="bibr" coords="20,275.97,467.02,21.58,8.90" target="#b175">[174]</ref> generates explanations that correspond to local gradients.</s><s coords="20,149.65,478.98,339.27,8.90;20,106.36,490.93,14.66,8.90">These gradients indicate how a data point must be moved to change its predicted label.</s><s coords="20,126.17,490.93,362.75,8.90;20,106.36,502.89,289.27,8.90">The explanations can be either scatter-plots of the gradient vectors or heat-maps showing which parts of the inputs must be modified to change the predicted class.</s><s coords="20,398.71,502.89,90.21,8.90;20,106.36,514.84,382.56,8.90;20,106.36,526.80,382.56,8.90;20,106.36,538.75,225.09,8.90">Individual Conditional Expectation (ICE) plots <ref type="bibr" coords="20,203.33,514.84,21.58,8.90" target="#b176">[175]</ref> are line charts graphing the functional relationship between a predicted response and a feature for each individual observation when keeping all the other features fixed and varying the value of the feature under analysis.</s><s coords="20,334.52,538.75,154.40,8.90;20,106.36,550.71,382.56,8.90;20,106.36,562.66,230.83,8.90"><ref type="bibr" coords="20,334.52,538.75,21.58,8.90" target="#b177">[176]</ref> proposed two alternatives to ICE plots, called Partial Importance (PI) and Individual Conditional Importance (ICI) plots, which visualize the feature importance rather than its prediction.</s><s coords="20,340.55,562.66,148.37,8.90;20,106.36,574.62,190.41,8.90">Both plots are aimed at showing how changes in a feature affect model performance.</s><s coords="20,301.74,574.62,187.18,8.90;20,106.36,586.57,382.56,8.90;20,106.36,598.53,173.20,8.90">PI works at the global level by visualizing the point-wise average of all ICI curves across all observations, whereas ICI works at the local level by presenting changes for each observation.</s><s coords="20,282.55,598.53,206.37,8.90;20,106.36,610.48,382.56,8.90;20,106.36,622.44,185.62,8.90">The importance of each feature is assessed using the Shapley Feature Importance measure which fairly distributes the model's performance among them according to their marginal contribution.</s><s coords="20,296.09,622.44,192.83,8.90;20,106.36,634.40,113.64,8.90">Explanation Graph <ref type="bibr" coords="20,374.80,622.44,21.58,8.90" target="#b178">[177]</ref> is based on the perturbations of the input features.</s><s coords="20,223.08,634.40,265.84,8.90;20,106.36,646.35,19.09,8.90">It works by training a model on both the original and the perturbed data.</s><s coords="20,128.41,646.35,360.51,8.90;20,106.36,658.31,206.81,8.90">Subsequently, a comparison of the original and perturbed input-output pairs is performed to infer causal dependencies between input and output.</s><s coords="20,316.18,658.31,172.74,8.90;20,106.36,670.26,320.41,8.90">This method was tested across several word sequence generation tasks in Natural Language Processing (NLP) applications.</s><s coords="20,432.03,670.26,56.89,8.90;20,106.36,682.22,382.57,8.90;20,106.36,694.17,161.84,8.90">The perturbed input contains statements that are semantically similar to the originals but differ in some elements (words and punctuation) and their order.</s><s coords="20,272.16,694.17,216.76,8.90;21,106.36,144.23,382.56,8.90;21,106.36,156.19,185.34,8.90">The inferred dependencies are shown in graphs where the nodes contain the words of the original and perturbed inputs and their relative outputs and the edges represent the connections between them.</s><s coords="21,294.68,156.19,194.24,8.90;21,106.36,168.14,382.56,8.90;21,106.36,180.10,66.65,8.90">A Worst-Case Perturbation <ref type="bibr" coords="21,403.83,156.19,21.58,8.90" target="#b179">[178]</ref> corresponds instead to the smallest perturbation such that the perturbed input leads to an incorrect answer with high confidence.</s><s coords="21,178.49,180.10,310.42,8.90;21,106.36,192.05,71.89,8.90">This method was applied only to images and the explanation consists of the perturbed images.</s><s coords="21,182.87,192.05,306.05,8.90;21,106.36,204.01,382.56,8.90;21,106.36,215.96,338.45,8.90">Class Signatures <ref type="bibr" coords="21,252.53,192.05,21.58,8.90" target="#b180">[179]</ref> is a visual analytic interface that allows end-users to detect and interpret input-output relationships by presenting a mix of charts (line, bar charts and scatter plots) and tables organised in such a way that relationships become evident.</s><s coords="21,449.99,215.96,38.93,8.90;21,106.36,227.92,382.56,8.90;21,106.36,239.87,175.37,8.90">Similarly, ExplainD <ref type="bibr" coords="21,147.99,227.92,21.58,8.90" target="#b181">[180]</ref> was designed to explain predictions made by classifiers that use additive evidence, such as linear SVMs and regressors.</s><s coords="21,286.20,239.87,202.72,8.90;21,106.36,251.83,382.56,8.90;21,106.36,263.78,165.81,8.90">The graphs produced by this method represent the contribution of each feature to the prediction and how the prediction changes when the value of a feature varies across their value ranges.</s><s coords="21,276.38,263.78,212.54,8.90;21,106.36,275.74,310.16,8.90">Manifold <ref type="bibr" coords="21,316.33,263.78,21.58,8.90" target="#b182">[181]</ref> and MLCube Explorer <ref type="bibr" coords="21,434.59,263.78,21.58,8.90" target="#b183">[182]</ref> are two visual analytical tools that provide comparative analysis for multiple models.</s><s coords="21,420.77,275.74,68.14,8.90;21,106.36,287.70,382.56,8.90;21,106.36,299.65,382.56,8.90;21,106.36,311.61,382.56,8.90">They also enable end-users to define instance subsets using feature conditions, to identify instances that generate erroneous results so to explain potential reasons of these errors, and to iteratively refine the performance of a model by using different graphical aids such as scatter-plots, bar and line charts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4." coords="21,106.36,495.02,105.50,8.71">Mixed explanations</head><p><s coords="21,121.30,507.90,367.62,8.90;21,106.36,519.86,382.56,8.90;21,106.36,531.81,26.99,8.90">There are many methods for explainability that produce numerical explanations along with graphical representations to make them more interpretable for lay people (see table A.4 and figure <ref type="figure" coords="21,122.55,531.81,3.60,8.90">8</ref>).</s><s coords="21,139.41,531.81,349.51,8.90;21,106.36,543.77,382.56,8.90;21,106.36,555.72,338.77,8.90">The Functional ANOVA decomposition <ref type="bibr" coords="21,304.23,531.81,21.58,8.90" target="#b184">[183]</ref> quantifies the influence of non-additive interactions within any set of input variables and depict them with Variable Interaction Network (VIN) graphs where the nodes represent the variables and the edges the interactions.</s><s coords="21,449.06,555.72,39.86,8.90;21,106.36,567.68,382.56,8.90;21,106.36,579.63,382.56,8.90;21,106.36,591.59,382.56,8.90;21,106.36,603.54,313.12,8.90">The Justification Narratives method for explainability <ref type="bibr" coords="21,285.63,567.68,21.58,8.90" target="#b185">[184]</ref> consists of a simple model-agnostic mapping of the essential values underlying a classification (identified with any feature selection method) to a semantic space that automatically produces these narratives and realizes them visually (as bar-charts reporting the assessed relevance value of each variable) or textually.</s><s coords="21,422.74,603.54,66.18,8.90;21,106.36,615.50,382.56,8.90;21,106.36,627.45,31.27,8.90">ExplAIner <ref type="bibr" coords="21,467.34,603.54,21.58,8.90" target="#b133">[133]</ref> and Rivelo <ref type="bibr" coords="21,152.68,615.50,21.58,8.90" target="#b186">[185]</ref> are two user interfaces showing mixes of numerical, visual and textual explanations.</s><s coords="21,140.64,627.45,348.28,8.90;21,106.36,639.41,382.56,8.90;21,106.36,651.36,45.10,8.90">ExplAIner was designed to display visual and textual explanations of ML models which are the outcome of an iterative workflow of three stages: model understanding, diagnosis, and refinement.</s><s coords="21,154.51,651.36,334.40,8.90;21,106.36,663.32,382.56,8.90">Using TensorBoard (a visualization tool developed by Google for machine learning) as a starting point, ExplAIner produces an interactive graph view of the model to be explained.</s><s coords="21,106.36,675.27,382.56,8.90;21,106.36,687.23,382.56,8.90;22,106.36,144.23,50.64,8.90">The nodes of the graph represent the model's components, such as inputs, parameters and outputs, accompanied by textual definitions, and the edges represent the relationships between the components.</s><s coords="22,160.33,144.23,328.59,8.90;22,106.36,156.19,382.56,8.90;22,106.36,168.14,304.36,8.90">There are also other visual explanatory tools in support of the model's graph, such as line-charts of metrics, like loss and accuracy, and examples of input data together with their relative heat-maps generated with other visual methods for explainability.</s><s coords="22,418.08,168.14,70.83,8.90;22,106.36,180.10,286.68,8.90">Rivelo works exclusively with binary classification problems and binary input features.</s><s coords="22,397.84,180.10,91.08,8.90;22,106.36,192.05,382.56,8.90;22,106.36,204.01,382.56,8.90;22,106.36,215.96,382.57,8.90;22,106.36,227.92,144.93,8.90">It enables end-users to understand the causes behind predictions by interactively exploring a set of visual and textual instance-level explanations which lists the most relevant input features (words or image areas in a document/image), their frequency, number of instances with the feature with positive labels and are correctly/wrongly classified.</s></p><p><s coords="22,121.30,251.83,367.62,8.90;22,106.36,263.78,382.56,8.90;22,106.36,275.74,382.56,8.90;22,106.36,287.70,252.14,8.90">Other mixed explanations-based methods utilize a selection of prototypes, which are samples from the input that are correctly predicted by the model and can be considered as positive and iconic examples, or adversarial examples, which are samples misrepresented by the model and are used to generate contrastive explanations (see Section 5.1).</s><s coords="22,362.55,287.70,126.37,8.90;22,106.36,299.65,382.56,8.90">This subset helps end-users understand the model by leveraging on the human ability to induce principles from a few examples.</s><s coords="22,106.36,311.61,382.56,8.90;22,106.36,323.56,176.61,8.90">Being a subset of a training dataset, these explanations were classified as mixed as their format depends on the nature of the input data.</s><s coords="22,286.80,323.56,202.12,8.90;22,106.36,335.52,382.56,8.90;22,106.36,347.47,128.41,8.90">The Bayesian Teaching methods for explainability <ref type="bibr" coords="22,106.36,335.52,21.58,8.90" target="#b187">[186]</ref> selects a small subset of prototypes that would lead the model to the correct inference as if trained on the overall dataset.</s><s coords="22,239.39,347.47,249.53,8.90;22,106.36,359.43,290.13,8.90"><ref type="bibr" coords="22,239.39,347.47,21.58,8.90" target="#b188">[187]</ref> proposed to use Sequential Bayesian Quadrature (SBQ) in conjunction with Fisher kernels to select salient training data points.</s><s coords="22,402.05,359.43,86.86,8.90;22,106.36,371.38,323.01,8.90">All the instances in a training dataset are firstly embedded in the space induced by the Fisher kernels.</s><s coords="22,434.02,371.38,54.89,8.90;22,106.36,383.34,382.56,8.90;22,106.36,395.29,85.18,8.90">This provides a way to quantify the closeness of pairs of instances which, if close enough, should be treated similarly by a model.</s><s coords="22,195.12,395.29,293.80,8.90;22,106.36,407.25,382.56,8.90;22,106.36,419.20,90.91,8.90">The embedded instances are inputted into SBQ, an importance-samplingbased algorithm that estimates the expected value of a function under a distribution using discrete samples drawn from it.</s><s coords="22,200.26,419.20,288.66,8.90;22,106.36,431.16,382.56,8.90;22,106.36,443.11,382.57,8.90;22,106.36,455.07,35.15,8.90">Set Cover Optimization (SCO) <ref type="bibr" coords="22,324.52,419.20,21.58,8.90" target="#b189">[188]</ref> aims at selecting prototypes in such a way that they capture the full structure of the training examples in each class of the dataset, no points have a prototype of a different class in its neighbourhood and the prototypes are as few as possible.</s><s coords="22,145.68,455.07,343.24,8.90;22,106.36,467.02,382.56,8.90">This leads to a set cover optimization problem that can be solved approximately with standard approaches such as, for instance, 'linear program relaxation with randomized rounding'.</s><s coords="22,106.36,478.98,382.56,8.90">Neighbourhood-Based Explanations <ref type="bibr" coords="22,252.66,478.98,21.58,8.90" target="#b190">[189]</ref> is based on a Case-Based Reasoning (CBR) approach.</s><s coords="22,106.36,490.93,382.56,8.90;22,106.36,502.89,160.72,8.90">It presents to end-users the entries of a training dataset that are the most similar to the new input instance that needs to be explained.</s><s coords="22,272.42,502.89,216.50,8.90;22,106.36,514.84,128.25,8.90">Similarity is measured through the Euclidean metrics applied to all the input features.</s><s coords="22,238.94,514.84,249.98,8.90;22,106.36,526.80,382.56,8.90;22,106.36,538.75,24.07,8.90">Adversarial examples are instead used in Evasion-Prone Samples Selection <ref type="bibr" coords="22,164.49,526.80,20.06,8.90" target="#b191">[190]</ref>, Maximum Mean Discrepancy (MMD)-critic <ref type="bibr" coords="22,370.23,526.80,21.58,8.90" target="#b192">[191]</ref> and Pertinent Negatives <ref type="bibr" coords="22,106.36,538.75,20.06,8.90" target="#b193">[192]</ref>.</s><s coords="22,134.01,538.75,354.91,8.90;22,106.36,550.71,382.56,8.90;22,106.36,562.66,382.56,8.90;22,106.36,574.62,254.68,8.90">Evasion-Prone Samples Selection aims at detecting the instances closed to the classification boundaries that can be easily misclassified if slightly perturbed whereas MMD-critic utilizes the maximum mean discrepancy and an associated witness function to identify the portions of the input space most misrepresented by the underlying model.</s><s coords="22,366.97,574.62,121.95,8.90;22,106.36,586.57,365.28,8.90">Pertinent Negatives highlights what should be minimally and necessarily absent to justify the classification of an instance.</s><s coords="22,475.23,586.57,13.69,8.90;22,106.36,598.53,382.56,8.90">For example, the absence of glasses is a necessary condition to say if a person has a good sight.</s><s coords="22,106.36,610.48,382.56,8.90;22,106.36,622.44,245.09,8.90">The input data are modified by removing some parts and the pertinent negatives are identified as those perturbations that maximise the prediction accuracy.</s><s coords="22,355.57,622.44,133.35,8.90;22,106.36,634.40,382.56,8.90;22,106.36,646.35,382.56,8.90">eventually, some methods for explainability produce mixed explanations by approximating a black-box model with simpler, more comprehensible models that the end-users can inspect to assess the contribution of each feature.</s><s coords="22,106.36,658.31,382.56,8.90;22,106.36,670.26,382.56,8.90;22,106.36,682.22,382.56,8.90;22,106.36,694.17,187.72,8.90">Local Interpretable Model-Agnostic Explanations (LIME) <ref type="bibr" coords="22,342.12,658.31,20.75,8.90" target="#b194">[193,</ref><ref type="bibr" coords="22,365.64,658.31,18.26,8.90" target="#b134">134]</ref> explains the prediction of any classifiers by learning a local self-interpretable model (such as linear models or decision trees), sometimes referred to as 'white-box' modes, trained on a new dataset which contains interpretable representations of the original data.</s><s coords="22,298.23,694.17,190.68,8.90;23,106.36,144.23,382.56,8.90;23,106.36,156.19,217.58,8.90">These representations can be the binary vectors representing the presence or absence of certain characteristics, such as words in texts or superpixels (contiguous patch of similar pixels) in images.</s><s coords="23,329.80,156.19,159.12,8.90;23,106.36,168.14,382.56,8.90;23,106.36,180.10,295.01,8.90">The black-box model can be explained through the weights of the white-box estimator which does not need to fully work globally, but it should approximate the black-box well in the vicinity of a single instance.</s><s coords="23,404.47,180.10,84.45,8.90;23,106.36,192.05,382.56,8.90;23,106.36,204.01,345.44,8.90">However, the authors proposed the Sub-modular Pick (SP-LIME) to select, from an original dataset, a representative non-redundant explanation set of instances that is a global representation of the model.</s></p><p><s coords="23,192.53,405.89,63.74,7.12;23,382.01,405.89,69.50,7.12;23,106.36,425.32,382.56,7.12;23,106.36,434.78,376.55,7.12">(a) ExplAIner <ref type="bibr" coords="23,239.00,405.89,17.26,7.12" target="#b133">[133]</ref> (b) MMD-critic <ref type="bibr" coords="23,434.24,405.89,17.26,7.12" target="#b192">[191]</ref> Figure <ref type="figure" coords="23,129.41,425.32,3.10,7.12">8</ref>: Examples of mixed explanations generated by model-agnostic methods for explainability which consists of a combination of visual and textual explanations in (a) interactive interfaces or (c) a selection of prototypes from inputs.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3." coords="23,106.36,470.04,290.48,8.71">Model-specific methods for explainability based on neural networks</head><p><s coords="23,121.30,484.92,367.62,8.90;23,106.36,496.87,245.97,8.90">A considerable portion of the reviewed scientific articles about new methods for explainability is focused on interpreting deep neural networks (DNNs).</s><s coords="23,357.93,496.87,130.99,8.90;23,106.36,508.83,122.06,8.90">This is not surprising giving the momentum of Deep Learning.</s><s coords="23,232.92,508.83,256.00,8.90;23,106.36,520.78,382.56,8.90;23,106.36,532.74,382.56,8.90;23,106.36,544.69,208.91,8.90">Most of these methods produce visual explanations (table A.7), mostly in the form of salient masks and scatter-plots (figure <ref type="figure" coords="23,346.73,520.78,3.60,8.90">9</ref>), some as other visual aids (figure <ref type="figure" coords="23,106.36,532.74,7.89,8.90" target="#fig_0">10</ref>), rules (table A.8 and figure <ref type="figure" coords="23,231.28,532.74,7.89,8.90" target="#fig_7">11</ref>), textual and numerical explanations (table A.9 and figure <ref type="figure" coords="23,475.64,532.74,8.85,8.90" target="#fig_1">12</ref>) or a combination of them (table A.10 and figure <ref type="figure" coords="23,299.49,544.69,7.89,8.90" target="#fig_2">13</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1." coords="23,106.36,568.67,172.58,8.71">Visual explanations as salient masks</head><p><s coords="23,121.30,581.55,367.62,8.90;23,106.36,593.51,346.88,8.90">CLass-Enhanced Attentive Response (CLEAR) <ref type="bibr" coords="23,319.21,581.55,21.58,8.90" target="#b195">[194]</ref> produces attention maps for image classification applications by back-propagating the activation values of the output layer.</s><s coords="23,456.26,593.51,32.66,8.90;23,106.36,605.46,382.56,8.90;23,106.36,617.42,382.56,8.90;23,106.36,629.38,31.67,8.90">CLEAR was designed to return the attentive regions responsible for the prediction, along with their attentive levels to understand their influence and the dominant output class associated with these regions.</s><s coords="23,141.49,629.38,319.62,8.90">DeepResolve <ref type="bibr" coords="23,197.08,629.38,21.58,8.90" target="#b196">[195]</ref> and GradCam <ref type="bibr" coords="23,279.63,629.38,21.58,8.90" target="#b197">[196]</ref> are two gradient ascent-based methods.</s><s coords="23,464.58,629.38,24.34,8.90;23,106.36,641.33,382.56,8.90;23,106.36,653.29,274.97,8.90">Deep-Resolve computes and visualizes intermediate layer feature maps that summarize how a network combines elemental layer-specific features to predict a specific class.</s><s coords="23,384.79,653.29,104.13,8.90;23,106.36,665.24,382.56,8.90;23,106.36,677.20,382.56,8.90;23,106.36,689.15,19.09,8.90">GradCam instead uses the gradients of any target concept (say 'dog' for instance) flowing into the final convolutional layer to generate a heat-map highlighting the influential regions in the image for predicting that concept.</s><s coords="23,128.50,689.15,360.42,8.90;24,106.36,144.23,382.56,8.90;24,106.36,156.19,212.50,8.90">Heat-maps are generated by the last convolutional layer because the fully-connected layers do not retain spatial information and it is expected that it has the best compromise between high-level semantics and detailed spatial information.</s><s coords="24,321.90,156.19,167.02,8.90;24,106.36,168.14,382.56,8.90;24,106.36,180.10,161.52,8.90">Stacking with Auxiliary Features (SWAF) <ref type="bibr" coords="24,106.36,168.14,21.58,8.90" target="#b198">[197]</ref> utilizes heat-maps generated by GradCam to interpret and improve stacked ensembles for visual question answering (VQA) tasks.</s><s coords="24,272.79,180.10,216.13,8.90;24,106.36,192.05,382.56,8.90;24,106.36,204.01,200.25,8.90">VQA includes answering a natural language question about the content of an image by returning, usually, a word or phrase or, in this case, a heat-map highlighting the relevant regions for a prediction.</s><s coords="24,312.05,204.01,176.87,8.90;24,106.36,215.96,382.56,8.90;24,106.36,227.92,95.48,8.90">Guided BackProp and Occlusion <ref type="bibr" coords="24,448.56,204.01,21.58,8.90" target="#b199">[198]</ref> find what part of an input (pixels in images or words in questions) the VQA model focuses on while answering the question.</s><s coords="24,206.62,227.92,282.30,8.90;24,106.36,239.87,226.93,8.90">Guided BackProp is another gradient-based technique to visualize the activation values of neurons in different layers of CNNs.</s><s coords="24,337.23,239.87,151.69,8.90;24,106.36,251.83,382.56,8.90;24,106.36,263.78,265.24,8.90">It computes the gradients of the probability scores of predicted classes but restricts negative gradients from flowing back towards the input layer, resulting in sharper images showcasing the activation.</s><s coords="24,375.77,263.78,113.15,8.90;24,106.36,275.74,382.56,8.90;24,106.36,287.70,382.56,8.90;24,106.36,299.65,189.75,8.90">Occlusion consists of masking, or occluding, subsets of an input (either a region of the image or a word of the question), then forward propagating it through the VQA model and computing the change in the probability of the answer predicted with the original input.</s><s coords="24,300.35,299.65,188.57,8.90;24,106.36,311.61,382.56,8.90;24,106.36,323.56,382.56,8.90;24,106.36,335.52,42.55,8.90">A similar method, Occlusion Sensitivity <ref type="bibr" coords="24,464.85,299.65,20.06,8.90" target="#b200">[199]</ref>, maps those features considered relevant in the intermediate layers of a DNN, by projecting the top nine activation values of each layer down to the input pixel space and masking the rest of the image.</s><s coords="24,155.15,335.52,333.77,8.90;24,106.36,347.47,63.23,8.90">Net2Vec <ref type="bibr" coords="24,192.99,335.52,21.58,8.90" target="#b201">[200]</ref> maps instead semantic concepts to corresponding individual DNN filter responses.</s><s coords="24,174.76,347.47,314.16,8.90;24,106.36,359.43,333.94,8.90">It returns images that are entirely greyed out except in the region related to a semantic concept, such as for instance the area representing a door of a building.</s><s coords="24,446.31,359.43,42.61,8.90;24,106.36,371.38,382.56,8.90;24,106.36,383.34,223.43,8.90">The pixels of this region generate activation values that are above a threshold, corresponding to the 99.5th percentile of the distribution of all the activation values.</s><s coords="24,333.28,383.34,155.64,8.90;24,106.36,395.29,382.56,8.90;24,106.36,407.25,54.19,8.90">Inverting Representations <ref type="bibr" coords="24,438.15,383.34,21.58,8.90" target="#b202">[201]</ref> inverts the representations of images produced by the inner layers and projects them on the input image as heat-maps.</s><s coords="24,163.61,407.25,325.31,8.90;24,106.36,419.20,76.69,8.90">A representation can be thought of as a function of the image that characterise the image information.</s><s coords="24,187.08,419.20,301.84,8.90;24,106.36,431.16,192.52,8.90">By reconstructing an approximate inverse function, it should be possible to reproduce the representations built by the layers.</s><s coords="24,301.89,431.16,187.03,8.90;24,106.36,443.11,382.57,8.90;24,106.36,455.07,382.56,8.90;24,106.36,467.02,232.04,8.90">This method is based on the hypothesis that the layers consider only the relevant features and discard the irrelevant differences between images (such as, for instance, illumination or viewpoint) and consists of a reconstruction problem solved by optimizing an objective function with gradient descent.</s></p><p><s coords="24,121.30,490.93,367.62,8.90;24,106.36,502.89,382.56,8.90;24,106.36,514.84,382.56,8.90;24,106.36,526.80,58.25,8.90">Similarly, Guided Feature Inversion <ref type="bibr" coords="24,268.09,490.93,21.58,8.90" target="#b203">[202]</ref> generates an inversion image representation consisting of the weighted sum between the original image and another noisy background image, such as a grey-scale image with each pixel set to an average colour, a Gaussian white noise or a blurred image.</s><s coords="24,168.15,526.80,320.77,8.90;24,106.36,538.75,382.56,8.90;24,106.36,550.71,249.92,8.90">The weights are calculated in such a way to highlight the smallest area that contains the most relevant features and to blur out everything else, especially things that might lead to an erroneous prediction, like objects belonging to other classes.</s><s coords="24,359.25,550.71,129.67,8.90;24,106.36,562.66,382.56,8.90;24,106.36,574.62,229.74,8.90">SmoothGrad <ref type="bibr" coords="24,412.29,550.71,21.58,8.90" target="#b204">[203]</ref> was designed to sharpen in two ways gradient-based sensitivity maps, which are often visually noisy as they highlight pixels that, to a human, seem randomly selected.</s><s coords="24,339.08,574.62,149.84,8.90;24,106.36,586.57,177.29,8.90">The first approach considers an image of interest along with sample similar images.</s><s coords="24,286.65,586.57,202.27,8.90;24,106.36,598.53,230.00,8.90">The second approach generates a perturbed version of the image of interest by adding Gaussian white noise.</s><s coords="24,341.40,598.53,147.52,8.90;24,106.36,610.48,382.56,8.90;24,106.36,622.44,120.60,8.90">Both approaches generate individual saliency maps with other methods for explainability such as GradCam, for instance, and take the average of the resulting maps.</s><s coords="24,230.90,622.44,258.02,8.90;24,106.36,634.40,382.56,8.90;24,106.36,646.35,382.56,8.90">Deep Learning Important FeaTures (DeepLIFT) <ref type="bibr" coords="24,426.37,622.44,21.58,8.90" target="#b99">[100]</ref> computes the importance scores of features based on the difference between the activation of each neuron to a 'reference activation' value, computed by propagating a 'reference input' through the network.</s><s coords="24,106.36,658.31,382.56,8.90;24,106.36,670.26,31.99,8.90">This represents a default or neutral input, such as a white image, chosen according to the problem at hand.</s><s coords="24,142.77,670.26,346.15,8.90;24,106.36,682.22,382.56,8.90;24,106.36,694.17,382.56,8.90;25,106.36,144.23,382.56,8.90;25,106.36,156.19,182.71,8.90">According to the authors, this difference-from-reference approach has two advantages over the other methods producing saliency maps: (I) it can propagate importance signals even when the gradient is zero, avoiding artifacts caused by discontinuities in the gradient and (II) it can reveal dependencies missed by other approaches because it can separately consider the effects of positive and negative contributions.</s><s coords="25,293.65,156.19,195.27,8.90;25,106.36,168.14,359.78,8.90">Thus, the saliency maps produced by DeepLIFT contains all and only the important features that support or go against a certain prediction.</s><s coords="25,469.54,168.14,19.38,8.90;25,106.36,180.10,382.56,8.90">Similarly, Integrated Gradients <ref type="bibr" coords="25,216.10,180.10,16.60,8.90" target="#b92">[93]</ref> attributes the prediction of a DNN to specific parts of the input.</s><s coords="25,106.36,192.05,382.56,8.90;25,106.36,204.01,382.56,8.90;25,106.36,215.96,292.71,8.90">The attribution is measured as the cumulative sum of the gradients of the classification function representing the network calculated at all points along the straight-line path from a baseline input (a black image or an empty text, for example) to a specific input instance.</s></p><p><s coords="25,121.30,239.87,367.62,8.90;25,106.36,251.83,382.56,8.90;25,106.36,263.78,21.86,8.90">Feature Maps <ref type="bibr" coords="25,179.21,239.87,21.58,8.90" target="#b205">[204]</ref> and Prediction Difference Analysis <ref type="bibr" coords="25,347.68,239.87,21.58,8.90" target="#b206">[205]</ref> produce respectively featureand heat-maps highlighting areas in an input image that gives evidence for or against a predicted class.</s><s coords="25,134.06,263.78,354.86,8.90;25,106.36,275.74,348.12,8.90">Feature Maps utilizes a loss function that pushes each filter in a convolutional layer to encode a distinct and unique object part, exclusive of the object class under analysis.</s><s coords="25,460.15,275.74,28.77,8.90;25,106.36,287.70,382.57,8.90;25,106.36,299.65,141.21,8.90">Prediction Difference Analysis instead is based on Explain <ref type="bibr" coords="25,319.92,287.70,20.06,8.90" target="#b150">[150]</ref>, which was designed to evaluate the contribution of a feature at a time.</s><s coords="25,253.43,299.65,235.49,8.90;25,106.36,311.61,248.54,8.90">In this case, a feature should correspond to a pixel of the image, but the authors proposed to consider patches of pixels.</s><s coords="25,358.93,311.61,129.99,8.90;25,106.36,323.56,236.77,8.90">The assumption is that the value of each pixel is highly dependent on the surrounding pixels.</s><s coords="25,346.15,323.56,142.76,8.90;25,106.36,335.52,382.57,8.90;25,106.36,347.47,68.48,8.90">The patches are overlapping so that, ultimately, an individual pixel's relevance is calculated as the average relevance of the different patches it was in.</s><s coords="25,177.93,347.47,310.99,8.90;25,106.36,359.43,260.00,8.90">Two studies proposed variations of LRP, namely LRP with Relevance Conservation <ref type="bibr" coords="25,133.90,359.43,21.58,8.90" target="#b207">[206]</ref> and LRP with Local Renormalization Layers <ref type="bibr" coords="25,342.29,359.43,20.06,8.90" target="#b208">[207]</ref>.</s><s coords="25,370.64,359.43,118.28,8.90;25,106.36,371.38,382.56,8.90;25,106.36,383.34,135.63,8.90">LRP was used in conjunction with the Pixel-wise Decomposition methods for explaining the automated image classification process of neural networks <ref type="bibr" coords="25,217.92,383.34,20.06,8.90" target="#b100">[101]</ref>.</s><s coords="25,246.65,383.34,242.27,8.90;25,106.36,395.29,382.56,8.90;25,106.36,407.25,276.13,8.90">In both studies, the authors wanted to extend LRP to DNNs with non-linearities, such as LSTM models that have multiplicative interactions within their architecture <ref type="bibr" coords="25,148.46,407.25,21.58,8.90" target="#b207">[206]</ref> or networks with local renormalization layers <ref type="bibr" coords="25,358.42,407.25,20.06,8.90" target="#b208">[207]</ref>.</s><s coords="25,386.55,407.25,102.37,8.90;25,106.36,419.20,382.56,8.90;25,106.36,431.16,339.58,8.90"><ref type="bibr" coords="25,386.55,407.25,21.58,8.90" target="#b207">[206]</ref> proposed a strategy to back-propagate the relevance of the neurons in the output layer back to the input layer through the two-way multiplicative interactions between lower-layer neurons of the LSTM.</s><s coords="25,449.45,431.16,39.47,8.90;25,106.36,443.11,382.56,8.90;25,106.36,455.07,78.68,8.90">The algorithm sets to zero the relevance related to the gate neuron and propagate the relevance of the source neuron only.</s><s coords="25,188.84,455.07,300.08,8.90;25,106.36,467.02,196.45,8.90">The extension of LRP proposed in <ref type="bibr" coords="25,329.58,455.07,21.58,8.90" target="#b208">[207]</ref> is based on first-Taylor expansion for non-linearities in the renormalization layers.</s><s coords="25,309.22,467.02,179.70,8.90;25,106.36,478.98,382.56,8.90;25,106.36,490.93,382.56,8.90;25,106.36,502.89,21.86,8.90"><ref type="bibr" coords="25,309.22,467.02,16.60,8.90" target="#b97">[98]</ref> proposed to generate saliency maps by computing the first-order Taylor expansion of the function that links each pixel of an input image to the function, representing the neural network, that assigns a probability score to each output class.</s></p><p><s coords="25,121.30,526.80,367.62,8.90;25,106.36,538.75,382.56,8.90;25,106.36,550.71,382.56,8.90;25,106.36,562.66,54.49,8.90">Similarly, <ref type="bibr" coords="25,162.35,526.80,21.58,8.90" target="#b209">[208]</ref> analysed the use of Taylor decomposition for interpreting generic multi-layer DNNs by decomposing the network's output classification into the contributions of its input elements and back-propagating them from the output to the input layer, which are then visualized as heat-maps.</s><s coords="25,164.75,562.66,324.17,8.90;25,106.36,574.62,382.56,8.90;25,106.36,586.57,169.74,8.90">Receptive Fields <ref type="bibr" coords="25,234.08,562.66,21.58,8.90" target="#b210">[209]</ref> focused on visualizing the input patterns, called precisely receptive fields, that are most strongly related to individual neurons by reconstructing these from the highest activation values of each layer.</s><s coords="25,279.90,586.57,209.02,8.90;25,106.36,598.53,382.56,8.90;25,106.36,610.48,382.56,8.90;25,106.36,622.44,340.92,8.90">PatternNet and PatternAttribution <ref type="bibr" coords="25,417.10,586.57,21.58,8.90" target="#b211">[210]</ref> aim at measuring the contribution of the input 'signal' dimension, which is the part of the input that contains information about the output class, to the prediction as well as how good the network is at filtering out the 'distractor', which is the rest of the input (like the image background).</s><s coords="25,451.44,622.44,37.48,8.90;25,106.36,634.40,382.56,8.90;25,106.36,646.35,382.56,8.90;25,106.36,658.31,97.17,8.90">PatterNet yields a layer-wise back-projection of the estimated signal to the input space whereas PatternAttribution produces explanations consisting of neuron-wise contributions of the estimated signal to the classification scores.</s><s coords="25,207.52,658.31,281.40,8.90;25,106.36,670.26,248.07,8.90">Relevant Features Selection <ref type="bibr" coords="25,321.64,658.31,21.58,8.90" target="#b212">[211]</ref> automatically identifies the relevant internal features of a neural network via a two-step algorithm.</s><s coords="25,358.01,670.26,130.91,8.90;25,106.36,682.22,382.56,8.90;25,106.36,694.17,233.11,8.90">First, a set of relevant layer/filter pairs are identified for every class of interest by finding those pairs that reduce at the minimum the differences between the predicted and the actual labels.</s><s coords="25,342.51,694.17,146.41,8.90;26,106.36,144.23,246.31,8.90">This results in a relevance weight for every filter-wise response computed internally by the network.</s><s coords="26,355.64,144.23,133.28,8.90;26,106.36,156.19,382.56,8.90;26,106.36,168.14,271.30,8.90">Then, an image is pushed through the network producing the class prediction and it generates a heat-map by taking into account the internal responses and relevance weights for the predicted class.</s><s coords="26,381.33,168.14,107.59,8.90;26,106.36,180.10,326.77,8.90">A combination of a Neural Network and Case Base Reasoning (CBR) Twin-systems was proposed in <ref type="bibr" coords="26,409.06,180.10,20.06,8.90" target="#b213">[212]</ref>.</s><s coords="26,438.18,180.10,50.74,8.90;26,106.36,192.05,382.56,8.90;26,106.36,204.01,276.21,8.90">This method maps the features' weights from the DNN to the CBR system to find similar cases from a training dataset that explain the prediction of the network of a new instance.</s><s coords="26,387.63,204.01,101.29,8.90;26,106.36,215.96,382.56,8.90;26,106.36,227.92,382.56,8.90;26,106.36,239.87,198.25,8.90">To extract the weights of features, the authors proposed the Contributions Oriented Local Explanations (COLE) technique which is based on the premise that the feature contributions to the model's predictions are the most sensible basis to inform CBR explanations.</s><s coords="26,309.89,239.87,179.03,8.90;26,106.36,251.83,201.36,8.90">COLE uses saliency maps methods, such as LRP and DeepLift, to estimate these contributions.</s><s coords="26,310.75,251.83,178.17,8.90;26,106.36,263.78,382.56,8.90;26,106.36,275.74,141.53,8.90">This was tested on image classification problems with explanations generated in the form of similar images whose discriminating features were highlighted by saliency maps.</s><s coords="26,251.60,275.74,237.32,8.90;26,106.36,287.70,236.43,8.90">Compositionality <ref type="bibr" coords="26,323.49,275.74,21.58,8.90" target="#b214">[213]</ref> consists of building the meaning of a sentence from the meanings of single words and phrases.</s><s coords="26,346.70,287.70,142.22,8.90;26,106.36,299.65,382.56,8.90;26,106.36,311.61,115.73,8.90">This method is designed for visualizing compositionality in neural models trained for NLP tasks by plotting the salience value of each word as saliency maps.</s><s coords="26,227.29,311.61,261.63,8.90;26,106.36,323.56,74.51,8.90">The salience values indicate the contribution of the words to the sentence meaning.</s><s coords="26,186.69,323.56,302.23,8.90;26,106.36,335.52,382.56,8.90;26,106.36,347.47,35.70,8.90">For instance, the word 'hate' and 'boring' in the phrase 'I hate the movie because the plot is boring' can be considered the two most relevant ones in a sentiment analysis problem.</s><s coords="26,145.08,347.47,343.84,8.90;26,106.36,359.43,382.56,8.90;26,106.36,371.38,136.23,8.90">The OpenBox method <ref type="bibr" coords="26,235.56,347.47,21.58,8.90" target="#b215">[214]</ref> computes exact and consistent interpretations for the family of Piecewise Linear Neural Networks (PLNN) by transforming them into a mathematically equivalent set of linear classifiers.</s><s coords="26,246.75,371.38,242.17,8.90;26,106.36,383.34,382.56,8.90;26,106.36,395.29,309.96,8.90">Subsequently, each linear classifier is interpreted by the features that dominate its prediction and the decision boundaries of each feature can be determined and visualized as scatter-plots (for numeric inputs) or heat-maps (for images).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2." coords="26,106.36,418.28,168.79,8.71">Visual explanations as scatter-plots</head><p><s coords="26,121.30,431.16,367.62,8.90;26,106.36,443.11,382.56,8.90;26,106.36,455.07,146.84,8.90">The Convolutional Neural Network Interpretation method (Cnn-Inte) <ref type="bibr" coords="26,400.41,431.16,21.58,8.90" target="#b216">[215]</ref> uses a two-level k-means clustering algorithm to split into clusters the activation values of the neurons of hidden layers relative to each input feature.</s><s coords="26,258.97,455.07,229.95,8.90;26,106.36,467.02,120.93,8.90">Clusters might contain the activation values of instances belonging to different classes.</s><s coords="26,232.44,467.02,235.84,8.90">A random forest algorithm is then trained on each cluster.</s><s coords="26,473.43,467.02,15.49,8.90;26,106.36,478.98,382.56,8.90">The results are visually displayed using scatter plots to show how a specific test instance is classified.</s><s coords="26,106.36,490.93,382.56,8.90;26,106.36,502.89,382.56,8.90;26,106.36,514.84,189.71,8.90"><ref type="bibr" coords="26,106.36,490.93,21.58,8.90" target="#b217">[216]</ref> instead presented a method based on Principal Component Analysis (PCA) for analyzing the variation of features generated by CNNs to scene factors that occur in images such as object style, colour and lighting configuration.</s><s coords="26,302.08,514.84,186.84,8.90;26,106.36,526.80,382.57,8.90;26,106.36,538.75,382.56,8.90;26,106.36,550.71,79.91,8.90">It analyzes CNN feature responses (or activation values) in the different layers by decomposing them as a linear combination of uncorrelated components associated to the different factors of variation and visualizing them into scatterplots by using PCA.</s><s coords="26,188.64,550.71,300.28,8.90;26,106.36,562.66,382.56,8.90;26,106.36,574.62,245.03,8.90">t-Distributed Stochastic Neighbor Embedding (t-SNE) maps <ref type="bibr" coords="26,430.67,550.71,21.58,8.90" target="#b218">[217]</ref> analyzes Deep Q-networks (DQNs) in reinforcement learning applications, in particular for agents that autonomously learn, for instance how to play video-games.</s><s coords="26,358.32,574.62,130.60,8.90;26,106.36,586.57,382.56,8.90;26,106.36,598.53,320.11,8.90">This method extracts the neural activation values of the last DQN layer and apply t-SNE for dimensionality reduction and for generating cluster plots where each dot correspond to a particular learning phase.</s><s coords="26,429.45,598.53,59.47,8.90;26,106.36,610.48,382.56,8.90;26,106.36,622.44,382.56,8.90;26,106.36,634.40,109.54,8.90">Similarly, Hidden Activity Visualization <ref type="bibr" coords="26,213.87,610.48,21.58,8.90" target="#b219">[218]</ref> uses t-SNE to visualize the projections of the activation values of the hidden neurons as a 2D scatter-plot with points coloured according to the class of the instances originating them.</s><s coords="26,221.81,634.40,267.11,8.90;26,106.36,646.35,382.56,8.90;26,106.36,658.31,112.12,8.90">The distribution of the points in the scatter-plot gives a graphical representation of the data distribution, relationships between neurons and the presence of clusters in the activation values.</s><s coords="26,222.57,658.31,266.35,8.90;26,106.36,670.26,225.15,8.90">Finally, TreeView <ref type="bibr" coords="26,297.03,658.31,21.58,8.90" target="#b220">[219]</ref> consists of a scatter plot representation of a DNN via hierarchical partitioning of the feature space.</s><s coords="26,334.89,670.26,154.03,8.90;26,106.36,682.22,382.56,8.90;26,106.36,694.17,328.35,8.90">Features are clustered according to the activation values of the hidden neurons in such a way that each cluster comprised of a set of neurons with similar distribution of activation values across the whole training set.</s></p><p><s coords="27,110.24,257.12,61.08,7.12;27,187.36,190.98,65.75,7.12;27,177.71,257.12,92.47,7.12;27,177.71,266.58,59.58,7.12;27,270.84,257.12,75.02,7.12;27,270.84,266.58,17.26,7.12;27,360.12,257.12,126.04,7.12;27,157.64,286.01,279.99,7.12">(a) GradCam <ref type="bibr" coords="27,154.06,257.12,17.26,7.12" target="#b197">[196]</ref> (b) Guided BP <ref type="bibr" coords="27,235.84,190.98,17.26,7.12" target="#b199">[198]</ref> (c) Neural network-CBR Twin system <ref type="bibr" coords="27,220.02,266.58,17.26,7.12" target="#b213">[212]</ref> (d) Compositionality <ref type="bibr" coords="27,270.84,266.58,17.26,7.12" target="#b214">[213]</ref> (e) PCA <ref type="bibr" coords="27,388.45,257.12,17.26,7.12" target="#b217">[216]</ref> (f) t-SNE maps <ref type="bibr" coords="27,468.90,257.12,17.26,7.12" target="#b218">[217]</ref> Figure <ref type="figure" coords="27,180.45,286.01,3.10,7.12">9</ref>: Examples of visual explanations, as salient masks (a-d) and scatter-plots (e-f).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3." coords="27,106.36,317.46,170.08,8.71">Visual explanations -miscellaneous</head><p><s coords="27,121.30,330.34,229.37,8.90">A few other methods use alternative visualization tools.</s><s coords="27,357.23,330.34,131.69,8.90;27,106.36,342.30,382.57,8.90;27,106.36,354.25,382.56,8.90;27,106.36,366.21,382.56,8.90">Generative Adversarial Network (GAN) Dissection <ref type="bibr" coords="27,181.37,342.30,21.58,8.90" target="#b221">[220]</ref> was designed to understand the inferential process of GANs at different levels of abstraction, from each neuron to each object, and the relationship between objects, by identifying units (or groups of units) that are related to semantic classes (doors, for example).</s><s coords="27,106.36,378.17,382.56,8.90;27,106.36,390.12,218.01,8.90">This method intervenes on them by adding or removing these objects from the image and observing how the GAN network reacts to these changes.</s><s coords="27,327.44,390.12,161.48,8.90;27,106.36,402.08,362.78,8.90">These reactions are represented as a new version of the input image where other objects or areas of the background are modified.</s><s coords="27,475.23,402.08,13.69,8.90;27,106.36,414.03,382.56,8.90;27,106.36,425.99,71.79,8.90">For instance, if a door is intentionally removed from a building, the GAN might substitute it with a window or bricks.</s><s coords="27,181.20,425.99,307.71,8.90;27,106.36,437.94,365.49,8.90">The Important Neurons and Patches method <ref type="bibr" coords="27,358.70,425.99,21.58,8.90" target="#b222">[221]</ref> analyzes the predictions of a DNN in terms of its internal features by inspecting information flow through the network.</s><s coords="27,475.23,437.94,13.69,8.90;27,106.36,449.90,382.56,8.90;27,106.36,461.85,382.56,8.90;27,106.36,473.81,382.56,8.90;27,106.36,485.76,382.56,8.90;27,106.36,497.72,382.56,8.90;27,106.36,509.67,382.57,8.90;27,106.36,521.63,42.34,8.90">For instance, given a trained network and a test image, important neurons are selected according to two metrics, both measured over a set of perturbed images (each pixel is multiplied by a Gaussian noise): (I) the magnitude of the correlation between the neuron activation and the network output which approximates the influence of each neuron on the output, and (II) the precision of the activation of a neuron, which estimates the generalizability of the feature(s) encoded by it, by selecting those neurons whose activation values were not significantly affected by the perturbations.</s><s coords="27,153.24,521.63,335.68,8.90;27,106.36,533.58,382.56,8.90;27,106.36,545.54,107.62,8.90">Given a rank of neurons, the top N are selected and their related image patches are determined by using a multi-layered deconvolutional network and enclosed in bounding boxes applied to the input image.</s><s coords="27,217.87,545.54,271.05,8.90;27,106.36,557.49,382.56,8.90;27,106.36,569.45,212.44,8.90"><ref type="bibr" coords="27,217.87,545.54,21.58,8.90" target="#b223">[222]</ref> and <ref type="bibr" coords="27,259.35,545.54,20.75,8.90" target="#b224">[223,</ref><ref type="bibr" coords="27,282.86,545.54,18.26,8.90" target="#b225">224]</ref> proposed two similar methods, based on Activation Maximization, which modify the input images in such a way to maximise the activation of a given hidden neuron with respect to each pixel.</s><s coords="27,324.09,569.45,164.83,8.90;27,106.36,581.40,192.11,8.90">These modified images should provide a good representation of what a neuron is doing.</s><s coords="27,304.43,581.40,184.49,8.90;27,106.36,593.36,362.18,8.90"><ref type="bibr" coords="27,304.43,581.40,21.58,8.90" target="#b226">[225]</ref> instead presented a method to generate Activation maps which show what features activate the neurons in the penultimate layers.</s><s coords="27,473.18,593.36,15.74,8.90;27,106.36,605.31,382.56,8.90;27,106.36,617.27,128.44,8.90">It is based on the idea that the final prediction of a DNN is dominated by the most highly-weighted neuron activations of this layer.</s><s coords="27,240.57,617.27,248.35,8.90;27,106.36,629.22,382.56,8.90;27,106.36,641.18,382.56,8.90;27,106.36,653.13,181.83,8.90">Shifting from pictorial to textual inputs, Cell Activation Values <ref type="bibr" coords="27,122.39,629.22,21.58,8.90" target="#b227">[226]</ref> is a method of explainability for LSTMs and uses character-level language models as an interpretable test-bed for understanding the long-range dependencies learned by LSTMs by highlighting sequences of relevant characters.</s></p><p><s coords="28,121.30,144.23,297.98,8.90">A group of methods that produce visual explanations in the form of graphs.</s><s coords="28,422.31,144.23,66.61,8.90;28,106.36,156.19,382.56,8.90;28,106.36,168.14,56.09,8.90">The method proposed in <ref type="bibr" coords="28,142.16,156.19,21.58,8.90" target="#b136">[136]</ref> generates data-flow graphs to visualize the structure of DNNs created and trained in Tensorflow.</s><s coords="28,165.38,168.14,323.54,8.90;28,106.36,180.10,382.56,8.90;28,106.36,192.05,382.56,8.90;28,106.36,204.01,193.26,8.90">Similarly, Explanatory Graph <ref type="bibr" coords="28,284.18,168.14,21.58,8.90" target="#b228">[227]</ref> produces graphs from CNNs where each node represents a 'part pattern', which correspond to the peak activation in a layer related to a part of the input, and each edge connects two nodes in adjacent layers to encode co-activation relationships and spatial relationships between patterns.</s><s coords="28,303.77,204.01,185.15,8.90;28,106.36,215.96,382.56,8.90;28,106.36,227.92,382.56,8.90"><ref type="bibr" coords="28,303.77,204.01,21.58,8.90" target="#b229">[228]</ref> instead added to CNNs a new Symbolic Graph Reasoning (SGR) layer which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph.</s><s coords="28,106.36,239.87,382.56,8.90;28,106.36,251.83,382.56,8.90;28,106.36,263.78,382.56,8.90;28,106.36,275.74,382.56,8.90;28,106.36,287.70,382.56,8.90;28,106.36,299.65,165.93,8.90">To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; b) a graph reasoning module that propagates information over the knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping module that learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features.</s><s coords="28,278.85,299.65,210.07,8.90;28,106.36,311.61,182.95,8.90">Lastly, And-Or Graph (AOG) <ref type="bibr" coords="28,404.03,299.65,21.58,8.90" target="#b230">[229]</ref> is a method to grow a semantic AOG on a pretrained CNN.</s><s coords="28,292.68,311.61,196.24,8.90;28,106.36,323.56,382.56,8.90;28,106.36,335.52,58.49,8.90">An AOG is a graphical representation of the reduction of problems (or goals) to conjunctions (AND) and disjunctions (OR) of sub-problems (or sub-goals).</s><s coords="28,169.11,335.52,319.81,8.90;28,106.36,347.47,382.56,8.90;28,106.36,359.43,137.75,8.90">The AOG is used for parsing the part of the input images which corresponds to a semantic concept and the output explanation consists of the input image where the semantic part is included in a bounding box.</s><s coords="28,247.15,359.43,241.77,8.90;28,106.36,371.38,300.59,8.90">Many scholars studied ways to exploit the visual explanatory tools, described so far, to create interactive interfaces for the lay audience.</s><s coords="28,411.49,371.38,77.43,8.90;28,106.36,383.34,382.56,8.90;28,106.36,395.29,106.59,8.90">For example, <ref type="bibr" coords="28,467.34,371.38,21.58,8.90" target="#b231">[230]</ref> studied the usage of saliency maps as the building blocks of interactive interfaces to explain the inferential logic of CNNs.</s><s coords="28,218.57,395.29,270.34,8.90;28,106.36,407.25,382.56,8.90;28,106.36,419.20,209.84,8.90">ActiVis <ref type="bibr" coords="28,252.31,395.29,21.58,8.90" target="#b232">[231]</ref> is an interactive visualization system for DNNs that unifies instance-and subset-level inspection by using flowcharts that show how neurons are activated by user-specified instances or instance subsets.</s><s coords="28,319.27,419.20,169.65,8.90;28,106.36,431.16,104.55,8.90">Deep Visualization Toolbox <ref type="bibr" coords="28,433.12,419.20,21.58,8.90" target="#b233">[232]</ref> is based on two visualization tools.</s><s coords="28,213.93,431.16,274.99,8.90;28,106.36,443.11,263.26,8.90">The first one depicts the activation values produced, while processing an image or video, on every layer of a trained CNN as heat-maps.</s><s coords="28,373.17,443.11,115.75,8.90;28,106.36,455.07,382.56,8.90;28,106.36,467.02,185.00,8.90">The second tool modifies the input images via regularised optimization methods to enable a better visualization of the learned features by individual neurons at every layer.</s><s coords="28,297.50,467.02,191.42,8.90;28,106.36,478.98,382.56,8.90;28,106.36,490.93,172.97,8.90">Deep View (DV) <ref type="bibr" coords="28,370.38,467.02,21.58,8.90" target="#b234">[233]</ref> measures the evolution of a DNN by using two metrics that evaluate the class-wise discriminability of the neurons in the final layer and the output feature maps.</s><s coords="28,283.50,490.93,205.42,8.90;28,106.36,502.89,250.39,8.90">iNNvestigate <ref type="bibr" coords="28,338.73,490.93,21.58,8.90" target="#b235">[234]</ref> compares different methods for explainability, namely PatternNet, PatternAttribution and LRP.</s><s coords="28,359.42,502.89,129.50,8.90;28,106.36,514.84,382.56,8.90;28,106.36,526.80,112.66,8.90">LSTMVis <ref type="bibr" coords="28,401.89,502.89,21.58,8.90" target="#b135">[135]</ref> is a visual analysis tool for recurrent neural networks, LSTM in particular, that facilitates the understanding of their hidden state dynamics.</s><s coords="28,222.97,526.80,265.95,8.90;28,106.36,538.75,26.74,8.90">It is based on a set of interactive graphs and heat-maps of relevant words.</s><s coords="28,138.95,538.75,349.97,8.90;28,106.36,550.71,382.56,8.90;28,106.36,562.66,382.56,8.90;28,106.36,574.62,147.79,8.90">A user can select a range of text in the heat-maps, which results in the selection of a subset of hidden states visualized in a parallel coordinate plot where each state is a data item, time-steps are the coordinates, and the tool then matches this selection to similar patterns in the dataset for further statistical analysis.</s><s coords="28,257.16,574.62,231.76,8.90;28,106.36,586.57,382.56,8.90;28,106.36,598.53,31.68,8.90">Seq2seq-Vis <ref type="bibr" coords="28,309.21,574.62,21.58,8.90" target="#b236">[235]</ref> is similar to LSTMVis but it focuses on sequence-to-sequence models, also known as encoder-decoder models, for automatic translation of texts.</s><s coords="28,141.03,598.53,347.88,8.90;28,106.36,610.48,382.56,8.90;28,106.36,622.44,80.40,8.90">Seq2seq-Vis allows interactions with trained models trough each stage of the translation process intending to identify the learned pattern, detect errors and probe the model with counterfactual scenarios.</s><s coords="28,192.32,622.44,40.80,8.90;28,233.12,620.84,3.49,6.23;28,237.11,622.44,251.81,8.90;28,106.36,634.40,382.56,8.90;28,106.36,646.35,332.20,8.90">Finally, N 2 VIS <ref type="bibr" coords="28,256.48,622.44,21.58,8.90" target="#b237">[236]</ref> is an interactive visualization tool for feed-forward neural networks trained with evolutionary computation which allows end-users to adjust training parameters during adaptation and to immediately see the results of this interaction.</s><s coords="28,442.44,646.35,46.48,8.90;28,106.36,658.31,382.56,8.90;28,106.36,670.26,382.56,8.90;28,106.36,682.22,377.08,8.90">It considers graphs representing the network topology, connection weights and activation levels for specific inputs and weight volatility to facilitate the process of understanding the inferential process of a neural network and to improve its performances in terms of efficiency and prediction accuracy.</s></p><p><s coords="29,122.91,214.25,63.52,7.12;29,124.48,290.46,9.29,7.12;29,150.80,290.46,33.25,7.12;29,124.48,299.92,33.86,7.12;29,210.65,212.08,46.92,7.12;29,213.76,290.46,46.04,7.12;29,297.96,290.46,78.62,7.12;29,395.35,290.46,84.12,7.12;29,107.08,319.35,381.12,7.12">(a) GAN Diss <ref type="bibr" coords="29,169.17,214.25,17.26,7.12" target="#b221">[220]</ref> (b) Activation Max <ref type="bibr" coords="29,141.08,299.92,17.26,7.12" target="#b224">[223]</ref> (c) AOG <ref type="bibr" coords="29,240.30,212.08,17.26,7.12" target="#b230">[229]</ref> (d) SGR <ref type="bibr" coords="29,242.54,290.46,17.26,7.12" target="#b229">[228]</ref> (e) Cell Activation <ref type="bibr" coords="29,359.32,290.46,17.26,7.12" target="#b227">[226]</ref> (f) Data-flow graphs <ref type="bibr" coords="29,462.21,290.46,17.26,7.12" target="#b136">[136]</ref> Figure <ref type="figure" coords="29,129.88,319.35,6.79,7.12" target="#fig_0">10</ref>: Examples of miscellaneous visual explanations generated by methods for explainability for neural networks.</s><s coords="29,121.30,459.32,367.62,8.90;29,106.36,471.28,382.56,8.90;29,106.36,483.23,228.88,8.90">Regarding the decompositional methods, Discretizing Hidden Unit Activation Values by Clustering <ref type="bibr" coords="29,151.64,471.28,21.58,8.90" target="#b238">[237]</ref> generates IF-THEN rules by clustering the activation values of hidden neurons and replacing them with the cluster's average value.</s><s coords="29,339.40,483.23,149.52,8.90;29,106.36,495.19,271.71,8.90">The rules are extracted by examining the possible combinations in the outputs of the discretised network.</s><s coords="29,382.29,495.19,106.63,8.90;29,106.36,507.14,382.56,8.90;29,106.36,519.10,382.56,8.90;29,106.36,531.05,246.42,8.90">Similarly, Neural Network Knowledge eXtraction (NNKX) <ref type="bibr" coords="29,238.77,507.14,21.58,8.90" target="#b239">[238]</ref> produces binary decision trees from multi-layered feedforward sigmoidal artificial neural networks by clustering the activation values of the last layer and propagating them back to the input to generate clusters.</s><s coords="29,358.71,531.05,130.21,8.90;29,106.36,543.01,382.56,8.90;29,106.36,554.97,21.86,8.90">Interval Propagation <ref type="bibr" coords="29,444.42,531.05,21.58,8.90" target="#b141">[141]</ref> is an improved version of Validity Interval Analysis (VIA) <ref type="bibr" coords="29,321.96,543.01,21.58,8.90" target="#b240">[239]</ref> to extract IF-THEN crisp and fuzzy rules.</s><s coords="29,131.27,554.97,357.65,8.90;29,106.36,566.92,330.14,8.90">VIA consists of finding a set of validity intervals for the activation range of each unit (or a subset of units) such that the activation values of a DNN lie within these intervals.</s><s coords="29,440.28,566.92,48.64,8.90;29,106.36,578.88,382.56,8.90;29,106.36,590.83,21.86,8.90">The precondition of each extracted rule is given by a set of validity intervals and the output is a single target class.</s><s coords="29,131.18,590.83,357.74,8.90;29,106.36,602.79,318.05,8.90">According to <ref type="bibr" coords="29,185.23,590.83,20.06,8.90" target="#b141">[141]</ref>, VIA has two shortcomings: it fails sometimes to decide whether a rule is compatible or not with the network and the intervals are not always optimal.</s><s coords="29,428.72,602.79,60.19,8.90;29,106.36,614.74,382.56,8.90;29,106.36,626.70,179.00,8.90">Interval Propagation overcomes these limitations by setting intervals to either the input or output and feed-or back-propagating them through the network.</s><s coords="29,289.37,626.70,172.84,8.90">However, this method has still a drawback.</s><s coords="29,466.22,626.70,22.69,8.90;29,106.36,638.65,382.56,8.90;29,106.36,650.61,173.11,8.90">Some neural networks require a big number of crisp rules to be approximated and to reach similar performances in terms of prediction accuracy.</s><s coords="29,284.65,650.61,204.27,8.90;29,106.36,662.56,382.56,8.90;29,106.36,674.52,21.86,8.90">Then, <ref type="bibr" coords="29,310.98,650.61,21.58,8.90" target="#b141">[141]</ref> proposed to compact these crisp rules into fuzzy rules by using a fuzzy interactive operator which introduces the OR operators between rules.</s><s coords="29,132.31,674.52,356.61,8.90;29,106.36,686.47,382.56,8.90;30,106.36,144.23,382.56,8.90;30,106.36,156.19,382.56,8.90;30,106.36,168.14,35.70,8.90">Discretized Interpretable Multi-Layer Perceptrons (DIMLP) <ref type="bibr" coords="29,376.06,674.52,20.75,8.90" target="#b139">[139,</ref><ref type="bibr" coords="29,399.64,674.52,17.43,8.90" target="#b241">240,</ref><ref type="bibr" coords="29,419.91,674.52,17.43,8.90" target="#b242">241,</ref><ref type="bibr" coords="29,440.16,674.52,18.26,8.90" target="#b132">132]</ref> returns symbolic rules from Interpretable Multi-Layer Perceptrons (IMLP) which are CNNs where each neuron of the first hidden layer is connected to only an input neuron and its activation function is a step function while the remaining hidden layers are fully connected with a sigmoid activation function.</s><s coords="30,145.11,168.14,343.81,8.90;30,106.36,180.10,68.15,8.90">In DIMPL, the step activation function becomes a staircase function that approximates the sigmoid one.</s><s coords="30,180.61,180.10,308.31,8.90;30,106.36,192.05,382.56,8.90;30,106.36,204.01,30.16,8.90">The rule extraction is performed after a max-pool layer by determining the location of relevant discriminative hyperplanes, which are the boundaries between the output classes.</s><s coords="30,139.53,204.01,349.39,8.90;30,106.36,215.96,117.88,8.90">Their relevance corresponds to the number of points passing through each hyperplane as they move to a different class.</s><s coords="30,227.26,215.96,261.67,8.90;30,106.36,227.17,226.82,10.40">An example of a ruleset generated with DIMLP from a neural network with thirty neurons, represented as x i with i = 1, . . .</s><s coords="30,334.84,227.17,154.08,9.64;30,106.36,239.87,177.33,9.74;30,287.82,239.13,58.86,10.49;30,350.81,239.13,53.88,10.49;30,408.82,239.13,80.10,9.64;30,106.36,251.83,49.42,9.74;30,160.31,251.08,59.48,10.49;30,224.32,251.08,264.60,9.64;30,106.36,263.78,382.56,8.90;30,106.36,275.74,382.56,8.90;30,106.36,287.70,217.51,8.90">, 30, in a unique hidden layer and three output neurons is: Rule 1 -(¬x 3 ) (¬x 8 ) (x 17 &gt; 0.0061) (x 19 &lt; 0.151) (x 21 &gt; 0.065) Class 1, Rule 2: (x 17 &gt; 0.0061) (x 21 &lt; 0.065) Class 2, Default: Class 3. Rule Extraction by Reverse Engineering (RxREN) <ref type="bibr" coords="30,222.59,263.78,21.58,8.90" target="#b243">[242]</ref> relies on a reverse engineering technique to trace back input neurons that cause the final result, whilst pruning the insignificant ones, and to determine the data ranges of each significant neuron in respective classes.</s><s coords="30,327.15,287.70,161.77,8.90;30,106.36,299.65,382.56,8.90">The algorithm is recursive and generates hierarchical rules where conditions for discrete attributes are disjoint from the continuous ones.</s><s coords="30,106.36,311.61,382.56,8.90;30,106.36,323.56,103.63,8.90">Rule Extraction from Neural Network using Classified and Misclassified data (RxNCM) <ref type="bibr" coords="30,458.41,311.61,21.58,8.90" target="#b244">[243]</ref> is a modification of RxREN.</s><s coords="30,212.16,323.56,276.76,8.90;30,106.36,335.52,292.03,8.90">It incorporates also the input instances correctly classified in the range determination process, not only the misclassified ones as done by RxREN.</s><s coords="30,400.41,335.52,88.51,8.90;30,106.36,347.47,382.56,8.90;30,106.36,359.43,143.24,8.90">Most of the rule-based methods for explainability are monotonic, that means they produce an increasing set of rules, thus the prepositions that can be derived.</s><s coords="30,252.67,359.43,236.25,8.90;30,106.36,371.38,382.56,8.90;30,106.36,383.34,264.48,8.90">However, sometimes adding new rules might lead to the invalidation of some conclusion inferred by other rules, as in <ref type="bibr" coords="30,346.50,371.38,21.58,8.90" target="#b245">[244]</ref> where a method that captures non-monotonic symbolic rules coded in the network was presented.</s><s coords="30,373.73,383.34,115.19,8.90;30,106.36,395.29,382.56,8.90;30,106.36,407.25,65.91,8.90">The rule extraction algorithm starts by partially ordering the vectors of a training dataset according to the activation values of the output layer.</s><s coords="30,177.51,407.25,311.41,8.90;30,106.36,419.20,382.56,8.90">Then, it determines the minimum input point that activates an output neuron and creates a rule whose antecedents are based on the feature values of the selected instance.</s><s coords="30,106.36,431.16,238.84,9.74;30,345.70,430.41,14.94,9.53">Thus, the expected set of rules has the following form: L 1 , . . .</s><s coords="30,362.30,430.41,13.42,10.40;30,376.22,430.41,32.00,10.40;30,408.72,430.41,14.94,9.53">, L n , ∼ L n+1 , . . .</s><s coords="30,425.33,430.41,63.09,10.40;30,106.36,443.01,382.56,9.76;30,106.36,454.97,306.28,9.00">, ∼ L m → L m+1 where L i (1 ≤ i ≤ m) represents a neuron in the input layer, L m+1 represents a neuron in the output layer, ∼ stands for default negation and → means causal implication.</s><s coords="30,416.97,455.07,71.95,8.90;30,106.36,467.02,302.85,8.90">Finally, <ref type="bibr" coords="30,450.05,455.07,21.58,8.90" target="#b246">[245]</ref> and <ref type="bibr" coords="30,106.36,467.02,21.58,8.90" target="#b247">[246]</ref> proposed two algorithms that extract DTs from the weights of a DNN.</s><s coords="30,411.62,467.02,77.30,8.90;30,106.36,478.98,382.56,8.90;30,106.36,490.93,382.56,8.90;30,106.36,502.89,176.09,8.90">The former method produces a soft DT trained by stochastic gradient descent using the predictions of a neural network and its learned filters to make hierarchical decisions on where to split the data and how to create the paths from the root to the leaves.</s><s coords="30,287.27,502.89,201.65,8.90;30,106.36,514.84,382.56,8.90;30,106.36,526.80,271.66,8.90">The latter, which is designed only for image classification tasks, aims at explaining an underlying CNN semantically, meaning that the nodes of the tree should correspond to parts of the objects that can be named.</s><s coords="30,381.30,526.80,107.62,8.90;30,106.36,538.75,382.56,8.90;30,106.36,550.71,382.56,8.90;30,106.36,562.66,199.93,8.90">Nodes near the root should correspond to parts shared by many images (such as the presence of four legs in images showing animals) whereas the nodes close to the leaves should represent characteristics of minority images (a peculiar characteristic of each animal).</s><s coords="30,311.54,562.66,177.37,8.90;30,106.36,574.62,315.42,8.90">To build such DTs, the network's filters are forced to represent object parts by a special modification of the loss function.</s><s coords="30,426.87,574.62,62.05,8.90;30,106.36,586.57,268.31,8.90">The DT is then built on the part/filter pairs recursively on an image by image basis.</s></p><p><s coords="30,121.30,610.48,367.62,8.90;30,106.36,622.44,382.56,8.90">In regard to the pedagogical methods, Rule Extraction From Neural Network Ensemble (REFNE) <ref type="bibr" coords="30,147.27,622.44,21.58,8.90" target="#b248">[247]</ref> extracts symbolic rules from instances generated by neural network ensembles.</s><s coords="30,106.36,634.40,382.56,8.90;30,106.36,646.35,309.84,8.90">The algorithm randomly selects a categorical attribute and checks if there is a value satisfying the condition that all the instances possessing such a value fall into the same class.</s><s coords="30,419.19,646.35,69.73,8.90;30,106.36,658.31,382.56,8.90;30,106.36,670.26,308.41,8.90">If the condition is satisfied, a rule is created with the value as antecedent; otherwise, the algorithm selects another categorical attribute and examines all the combinations of the two attributes.</s><s coords="30,419.22,670.26,69.70,8.90;30,106.36,682.22,382.56,8.90;30,106.36,694.17,146.56,8.90">When all the categorical attributes are analysed, continuous attributes are considered and the process terminates when no more rules can be created.</s><s coords="30,258.88,694.17,178.13,8.90">Rules are limited to only three antecedents.</s><s coords="30,442.97,694.17,45.95,8.90;31,106.36,144.23,382.56,8.90;31,106.36,156.19,290.92,8.90">Continuous attributes are discretised and a fidelity evaluation mechanism checks that this process does not compromise the relationship between the attribute and the output classes.</s><s coords="31,400.34,156.19,88.58,8.90;31,106.36,168.14,382.56,8.90;31,106.36,180.10,138.94,8.90">An alternative method to extract IF-THEN rules from neural network ensembles, called C4.5Rule-PANE <ref type="bibr" coords="31,444.10,168.14,20.06,8.90" target="#b249">[248]</ref>, uses the C4.5 rule induction algorithm.</s><s coords="31,250.88,180.10,238.04,8.90;31,106.36,192.05,359.95,8.90">After a neural network ensemble was trained on a dataset, the original labels of the training dataset are replaced by those predicted by the ensemble.</s><s coords="31,470.10,192.05,18.82,8.90;31,106.36,204.01,382.56,8.90;31,106.36,215.96,97.08,8.90">Subsequently, C4.5Rule-PANE extracts a ruleset from the modified dataset to mimic the inferential process of the ensemble.</s><s coords="31,206.43,215.96,282.49,8.90">The DecText method <ref type="bibr" coords="31,291.87,215.96,21.58,8.90" target="#b250">[249]</ref> also extracts high fidelity DTs from a DNN.</s><s coords="31,106.36,227.92,382.56,8.90;31,106.36,239.87,168.71,8.90">It sorts input instances by increasing the value of a feature and split an input dataset by placing the cutpoint at the midpoint of the range.</s><s coords="31,280.72,239.87,208.20,8.90;31,106.36,251.83,256.80,8.90">Then, four splitting algorithms check the partitions such created and choose the best ones according to four criteria.</s><s coords="31,366.83,251.83,122.09,8.90;31,106.36,263.78,200.55,8.90">The first, SetZero, chooses the most discriminative features of the target variable.</s><s coords="31,310.38,263.78,178.54,8.90;31,106.36,275.74,382.56,8.90;31,106.36,287.70,382.56,8.90;31,106.36,299.65,17.42,8.90">The other three, SSE, ClassDiff and Fidelity, respectively select the feature which maximizes the possibility that a single class dominating each partition is created, the quality of the partition and the fidelity between the DNN and the tree.</s><s coords="31,126.77,299.65,362.15,8.90;31,106.36,311.61,259.50,8.90">According to the authors, these algorithms have comparable, if not better, prediction performance that a tree extraction technique based on entropy gain split.</s><s coords="31,368.81,311.61,120.11,8.90;31,106.36,323.56,382.56,8.90;31,106.36,335.52,35.68,8.90">TREPAN <ref type="bibr" coords="31,408.74,311.61,20.75,8.90" target="#b251">[250,</ref><ref type="bibr" coords="31,431.59,311.61,18.26,8.90" target="#b252">251]</ref> induces a DT that, like DecText, maintain a high level of fidelity to a DNN while being comprehensible and accurate.</s><s coords="31,145.43,335.52,343.49,8.90;31,106.36,347.47,382.56,8.90;31,106.36,359.43,360.50,8.90">It queries an underlying network to determine the predicted class of each instance and selects the splits for each node of the tree by using the 'gain ratio criterion' and by considering the previously selected splits that lie on the path from the root to that node as constraints.</s><s coords="31,471.02,359.43,17.90,8.90;31,106.36,371.38,382.56,8.90;31,106.36,383.34,372.60,8.90">Tree Regularization <ref type="bibr" coords="31,168.26,371.38,21.58,8.90" target="#b253">[252]</ref> consists of a penalty function of the parameters of a differentiable DNN which favours models whose decision boundaries can be approximated by small binary DTs.</s><s coords="31,482.83,383.34,6.09,8.90;31,106.36,395.29,382.56,8.90;31,106.36,407.25,165.49,8.90">It finds a binary DT that accurately reproduces the network's prediction and measures its complexity as the 'average decision path length'.</s><s coords="31,277.19,407.25,211.73,8.90;31,106.36,419.20,324.55,8.90">It then maps the parameter vector of each candidate network to an estimate of the average-path-length and chooses the shortest one.</s><s coords="31,436.31,419.20,52.61,8.90;31,106.36,431.16,382.56,8.90;31,106.36,443.11,29.06,8.90">Word Importance Scores <ref type="bibr" coords="31,160.20,431.16,21.58,8.90" target="#b254">[253]</ref> visualizes the importance of specific inputs for determining the output of a LSTM.</s><s coords="31,137.90,443.11,351.02,8.90;31,106.36,455.07,292.33,8.90">By searching for consistently important phrases and calculating their importance scores, the method extracts simple phrase patterns consisting of one to five words.</s><s coords="31,401.65,455.07,87.27,8.90;31,106.36,467.02,382.56,8.90;31,106.36,478.98,91.45,8.90">To concretely validate these patterns, they are inputted to a rule-based classifier which approximates the performance of the original LSTM.</s><s coords="31,201.45,478.98,287.47,8.90;31,106.36,490.93,362.92,8.90">Iterative Rule Knowledge Distillation <ref type="bibr" coords="31,358.14,478.98,21.58,8.90" target="#b255">[254]</ref> and Symbolic Logic Integration <ref type="bibr" coords="31,137.43,490.93,21.58,8.90" target="#b256">[255]</ref> are the only ante-hoc methods producing rule-based explanations for DNNs.</s><s coords="31,473.43,490.93,15.49,8.90;31,106.36,502.89,382.56,8.90;31,106.36,514.84,382.56,8.90;31,106.36,526.80,210.36,8.90">The former combines DNNs with declarative first-logic rules to allow integrating human knowledge and intentions into the networks via an iterative distillation procedure that transfers the structured information of logic rules into the weights of DNNs.</s><s coords="31,320.13,526.80,168.79,8.90;31,106.36,538.75,382.56,8.90;31,106.36,550.71,33.48,8.90">This is achieved by forcing the network to emulate the predictions of a rule-based teacher model and evolving both models throughout the training.</s><s coords="31,144.03,550.71,344.89,8.90;31,106.36,562.66,382.56,8.90;31,106.36,574.62,382.56,8.90;31,106.36,586.57,24.07,8.90">The latter instead encodes symbolic knowledge in an unsupervised neural network by converting background knowledge, in the form of propositional IF-THEN rules and first-order logic formulas, into confidence rules which can be represented in a Restricted Boltzmann Machine.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5." coords="31,106.36,609.56,169.70,8.71">Textual and numerical explanations</head><p><s coords="31,121.30,622.44,367.62,8.90;31,106.36,634.40,12.36,8.90">Textual and numerical explanations are jointly less used than other methods for explainability.</s><s coords="31,124.55,634.40,364.37,8.90;31,106.36,646.35,382.56,8.90;31,106.36,658.31,121.14,8.90">InterpNET <ref type="bibr" coords="31,171.11,634.40,21.58,8.90" target="#b257">[256]</ref> makes use of a DNN to generate natural language explanations of classifications done by external models and produces statements like "This is an image of a Red Winged Blackbird because...".</s><s coords="31,231.32,658.31,257.60,8.90;31,106.36,670.26,22.86,8.90">The explanations are built upon the activation values of this network.</s><s coords="31,132.58,670.26,356.34,8.90;31,106.36,682.22,296.97,8.90">Most-Weighted-Path, Most-Weighted-Combination and Maximum-Frequency-Difference <ref type="bibr" coords="31,106.36,682.22,21.58,8.90" target="#b258">[257]</ref> are three methods for explainability that generates textual statements.</s><s coords="31,406.30,682.22,82.62,8.90;31,106.36,694.17,382.56,8.90;32,106.36,286.36,174.52,8.90">Most-Weighted-Path starts from the output neuron and selects the corresponding input passing, layer-by-layer, via the neuron connected with the highest weight.</s><s coords="32,287.41,286.36,201.51,8.90;32,106.36,298.31,312.51,8.90">Then, it auto-generates a natural language explanation indicating the most relevant feature for predicting the output category.</s><s coords="32,423.86,298.31,65.06,8.90;32,106.36,310.27,332.19,8.90">Most-Weighted-Combination works similarly, but it selects the two most-weighted input features.</s><s coords="32,444.09,310.27,44.83,8.90;32,106.36,322.23,382.56,8.90;32,106.36,334.18,382.56,8.90;32,106.36,346.14,85.27,8.90">Maximum-Frequency-Difference retrieves from the training dataset the most similar cases for each instance, then perform the difference between the percentages of cases that have the same output and those with different output.</s><s coords="32,196.19,346.14,292.73,8.90;32,106.36,358.09,382.56,8.90;32,106.36,370.05,382.56,8.90;32,106.36,382.00,55.47,8.90">The explanation is generated according to the input with the highest difference and it is a statement like "the smart kitchen estimates that you are sad because you are eating chocolate, which is 50% more frequent in this emotional state than people in other emotional states".</s><s coords="32,168.62,382.00,320.30,8.90;32,106.36,393.96,382.56,8.90;32,106.36,405.91,134.81,8.90">Rationales <ref type="bibr" coords="32,214.40,382.00,21.58,8.90" target="#b259">[258]</ref> is a method for justifying the predictions made by DNNs in natural language processing tasks, such as sentiment analysis, by extracting pieces of the input text as justifications, or rationales.</s><s coords="32,244.16,405.91,244.76,8.90;32,106.36,417.87,210.54,8.90">These rationales must contain words that are interpretable and lead to the same prediction as to the entire input text.</s><s coords="32,319.97,417.87,168.95,8.90;32,106.36,429.82,382.56,8.90;32,106.36,441.78,382.56,8.90;32,106.36,453.73,155.88,8.90">These words are selected via the combination of a 'rationale generator' function, which works as a tagging model assigning to each word a binary tag saying whether it must be included in the rationale, and an 'encoder' function that maps a string of words to a target class.</s><s coords="32,265.28,453.73,223.64,8.90;32,106.36,465.69,382.56,8.90;32,106.36,477.64,35.82,8.90">Relevance and Discriminative Loss <ref type="bibr" coords="32,407.62,453.73,20.75,8.90" target="#b260">[259,</ref><ref type="bibr" coords="32,430.71,453.73,18.26,8.90" target="#b137">137]</ref> generates textual explanations for an image classifier like "The bird in the photo is a White Pelican because...".</s><s coords="32,145.47,477.64,343.45,8.90;32,106.36,489.60,382.56,8.90;32,106.36,501.55,34.02,8.90">It consists of a CNN that extracts visual features from the images, such as colours and object parts, and two LSTMs which produce a description of each image conditioned on visual features.</s><s coords="32,146.51,501.55,342.40,8.90;32,106.36,513.51,382.56,8.90;32,106.36,525.46,87.27,8.90">The training process aims at reducing two loss functions, called respectively 'Relevance' and 'Discriminative', which assure that the generated sentences are both image relevant and category-specific.</s></p><p><s coords="32,121.30,549.37,286.24,8.90">A few methods for explainability produce pure numerical explanations.</s><s coords="32,411.40,549.37,77.52,8.90;32,106.36,561.33,382.56,8.90;32,106.36,573.28,358.19,8.90">Concept Activation Vectors (CAVs) <ref type="bibr" coords="32,172.51,561.33,21.58,8.90" target="#b145">[145]</ref> separates the activation values of a hidden layer relative to the instances belonging to a class of interest from those generated by the remaining part of the dataset.</s><s coords="32,468.45,573.28,20.47,8.90;32,106.36,585.24,382.56,8.90;32,106.36,597.19,382.56,8.90;32,106.36,609.15,142.82,8.90">Then it trains a binary linear classifier to distinguish the activation values of the two sets and compute directional derivatives on this classifier to measure the sensitivity of the model to changes in inputs towards the class of interest.</s><s coords="32,254.16,609.15,234.76,8.90;32,106.36,621.10,59.28,8.90">This is a scalar quantity calculated over the whole dataset for each class.</s><s coords="32,172.57,621.10,316.35,8.90;32,106.36,633.06,227.32,8.90">Probe <ref type="bibr" coords="32,199.59,621.10,21.58,8.90" target="#b144">[144]</ref> consists of a linear classifier is fitted to a single feature learned by each layer of a DNN to predict the original classes.</s><s coords="32,340.14,633.06,148.78,8.90;32,106.36,645.01,143.97,8.90">The numeric explanations consist of the predictions made by the Probes.</s><s coords="32,254.46,645.01,234.46,8.90;32,106.36,656.97,382.56,8.90;32,106.36,668.93,250.62,8.90">Singular Vector Canonical Correlation Analysis (SVCCA) <ref type="bibr" coords="32,106.36,656.97,21.58,8.90" target="#b261">[260]</ref> returns the correlation matrix of the neurons' activation vectors, calculated over the entire dataset, of two instances of a given DNN trained separately.</s><s coords="32,364.10,668.93,124.82,8.90;32,106.36,680.88,382.56,8.90;32,106.36,692.84,113.88,8.90">The first network's instance is obtained at the end of the training process, whereas the latter consists of multiple snapshots of the network during training.</s><s coords="32,225.52,692.84,263.40,8.90;33,106.36,144.23,303.69,8.90">Every layer of the first instance is compared to every other layer of the other instance to calculate correlation factors between pairs of layers.</s><s coords="33,413.74,144.23,75.18,8.90;33,106.36,156.19,382.56,8.90;33,106.36,168.14,295.48,8.90">Causal Importance <ref type="bibr" coords="33,106.36,156.19,21.58,8.90" target="#b262">[261]</ref> is calculated by summing up the variations in the output when the values of a variable are perturbed instance by instance whilst all the other variables are kept fixed.</s><s coords="33,405.05,168.14,83.87,8.90;33,106.36,180.10,382.56,8.90;33,106.36,192.05,382.57,8.90;33,106.36,204.01,79.70,8.90">Firstly, the algorithm suppresses the irrelevant variables by measuring their predictive importance via a metric based on the absolute difference between the predictions made by a DNN with the original and the perturbed variables.</s><s coords="33,191.61,204.01,297.31,8.90;33,106.36,215.96,220.07,8.90">The network is then trained with the relevant variables only and data are clustered according to their hidden layer representation.</s><s coords="33,329.41,215.96,159.51,8.90;33,106.36,227.92,382.57,8.90">This is done by training an unsupervised Kohonen map on the matrix containing the activation values of each pair neuron/input instance.</s><s coords="33,106.36,239.87,382.56,8.90;33,106.36,251.83,251.34,8.90">Finally, causal importance is measured on a cluster by cluster basis, meaning that it is calculated only on the instances belonging to the cluster under analysis.</s><s coords="33,363.76,251.83,125.16,8.90;33,106.36,263.78,306.84,8.90"><ref type="bibr" coords="33,363.76,251.83,21.58,8.90" target="#b263">[262]</ref> proposed to measure the Contextual Importance and Contextual Utility of input on the output variable.</s><s coords="33,416.22,263.78,72.70,8.90;33,106.36,275.74,382.56,8.90;33,106.36,287.70,196.14,8.90">The former metric is the ratio between the range of the output values covered by varying a variable throughout its own range of values and the whole output space.</s><s coords="33,306.59,287.70,182.32,8.90;33,106.36,299.65,335.32,8.90">For example, a neural network was trained to predict the price of a car over a set of variable, among which there is the engine size.</s><s coords="33,444.73,299.65,44.18,8.90;33,106.36,311.61,263.32,8.90">By varying the engine size alone, the price varies only within a certain range.</s><s coords="33,373.47,311.61,115.45,8.90;33,106.36,323.56,258.51,8.90">Contextual Utility represents the position of the actual output within Contextual Importance.</s><s coords="33,370.83,323.56,118.09,8.90;33,106.36,335.52,382.56,8.90;33,106.36,347.47,47.69,8.90">So the price of cars with big engines, produced by the same manufacturer, are towards the upper end of the manufacturer's price range.</s><s coords="33,159.16,347.47,250.43,8.90">The ideal values for these two metrics are domain-dependent.</s><s coords="33,414.69,347.47,74.23,8.90;33,106.36,359.43,382.56,8.90;33,106.36,371.38,382.56,8.90;33,106.36,383.34,382.56,8.90;33,106.36,395.29,52.71,8.90">They are designed just to help end-users understand where each variable and instance lie within the input space, but they can be used to generate rule-based or textual explanations by structuring the domain in intermediate concepts which attach a positive or negative outlook to certain subsets of the output space.</s><s coords="33,162.57,395.29,326.34,8.90;33,106.36,407.25,163.98,8.90">A certain price range can be deemed very good for a manufacturer and all its cars in that range can be considered as a deal.</s><s coords="33,273.82,407.25,215.10,8.90;33,106.36,419.20,382.56,8.90;33,106.36,431.16,38.91,8.90">Finally, REcurrent LEXicon NETwork (RELEXNET) <ref type="bibr" coords="33,106.36,419.20,21.58,8.90" target="#b264">[263]</ref> combines the transparency of lexicon-based classifiers with the accuracy of recurrent neural networks.</s><s coords="33,150.45,431.16,338.47,8.90;33,106.36,443.11,327.88,8.90">Lexicons are linguistic tools for classification and feature extraction which take the form of a list of terms weighted by their strength of association with a given class.</s><s coords="33,437.45,443.11,51.47,8.90;33,106.36,455.07,382.56,8.90;33,106.36,467.02,183.18,8.90">RELEXNET uses lexicons as inputs of a naive gated recurrent neural network which returns the probability that the input belongs or not to a certain class.</s></p><p><s coords="33,177.09,567.55,64.62,7.12;33,136.49,651.88,140.00,7.12;33,368.71,581.15,56.67,7.12;33,373.66,651.88,49.39,7.12;33,106.36,671.31,382.56,7.12;33,106.36,680.77,31.13,7.12">(a) InterpNET <ref type="bibr" coords="33,224.44,567.55,17.26,7.12" target="#b257">[256]</ref> (b) Contextual Importance and Utility <ref type="bibr" coords="33,259.23,651.88,17.26,7.12" target="#b263">[262]</ref> (c) SVCCA <ref type="bibr" coords="33,408.12,581.15,17.26,7.12" target="#b261">[260]</ref> (d) CAVs <ref type="bibr" coords="33,405.78,651.88,17.26,7.12" target="#b145">[145]</ref> Figure <ref type="figure" coords="33,129.36,671.31,6.79,7.12" target="#fig_1">12</ref>: Examples of textual (a) and numerical (b-d) explanation generated by a method for explainability for neural networks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.6." coords="34,106.36,144.30,105.50,8.71">Mixed explanations</head><p><s coords="34,121.30,157.18,367.62,8.90;34,106.36,169.14,141.16,8.90">To help end-user interpret a model, some scholars have proposed the use of multiple types of output formats for explanations.</s><s coords="34,252.62,169.14,236.30,8.90;34,106.36,181.09,145.75,8.90">The Attention Alignment method <ref type="bibr" coords="34,390.90,169.14,21.58,8.90" target="#b265">[264]</ref> produces explanations in the form of attention maps.</s><s coords="34,257.93,181.09,230.99,8.90;34,106.36,193.05,320.49,8.90">These maps highlight parts of a scene that mattered to a control DNN utilized in self-driving cars in combination with a perception DNN.</s><s coords="34,429.10,193.05,59.82,8.90;34,106.36,205.01,382.56,8.90;34,106.36,216.96,382.56,8.90;34,106.36,228.92,32.93,8.90">The perception DNN combines the data received from cameras and other sensors, like radars and infrared, to 'understand' the environment and to generate manoeuvring commands like steering angles or braking.</s><s coords="34,144.56,228.92,344.36,8.90;34,106.36,240.87,382.56,8.90;34,106.36,252.83,22.86,8.90">The control DNN is trained to identify the presence of specific objects, such as road signs, and obstacles like pedestrians and bikers, that influence the output of the perception network.</s><s coords="34,133.01,252.83,355.90,8.90;34,106.36,264.78,382.56,8.90;34,106.36,276.74,34.73,8.90">Attention Alignment consists of an attention-based video-to-text algorithm that produces textual explanations of the model predictions such as "The car heads down the street because it is clear."</s><s coords="34,145.88,276.74,343.04,8.90;34,106.36,288.69,382.56,8.90;34,106.36,300.65,382.56,8.90">Similarly, Pointing and Justification Model (PJ-X) <ref type="bibr" coords="34,352.19,276.74,21.58,8.90" target="#b266">[265]</ref> and Image Caption Generation with Attention Mechanism <ref type="bibr" coords="34,234.61,288.69,21.58,8.90" target="#b147">[147]</ref> are two multi-modal methods for explainability, designed for VQA tasks, that provides joint textual rationale generation and attention-map visualization.</s><s coords="34,106.36,312.60,382.56,8.90;34,106.36,324.56,382.56,8.90">The attention-maps are extracted from a CNN, which performs the object recognition in images, whereas the textual justifications are produced by an LSTM network as image captions.</s><s coords="34,106.36,336.51,284.52,8.90">The word(s) in the caption related to the attention region is underlined.</s><s coords="34,394.65,336.51,94.27,8.90;34,106.36,348.47,251.35,8.90">According to <ref type="bibr" coords="34,449.90,336.51,20.06,8.90" target="#b266">[265]</ref>, the two explanations support each other in achieving high quality.</s><s coords="34,362.40,348.47,126.51,8.90;34,106.36,360.42,289.27,8.90">The visual explanations help to generate better textual explanations which lead to better visual pointing.</s><s coords="34,399.50,360.42,89.41,8.90;34,106.36,372.38,382.56,8.90;34,106.36,384.33,382.56,8.90;34,106.36,396.29,329.53,8.90">Image Caption Generation with Attention Mechanism is based on two algorithms: (I) a 'soft' deterministic attention mechanism trainable by standard back-propagation methods and (II) a 'hard' stochastic attention mechanism trainable by maximizing an approximate variational lower bound.</s><s coords="34,439.78,396.29,49.14,8.90;34,106.36,408.24,254.00,8.90">The word(s) in the caption related to the attention region is/are underlined.</s><s coords="34,366.27,408.24,122.65,8.90;34,106.36,420.20,382.56,8.90;34,106.36,432.15,382.56,8.90;34,106.36,444.11,337.24,8.90"><ref type="bibr" coords="34,366.27,408.24,21.58,8.90" target="#b267">[266]</ref> and <ref type="bibr" coords="34,409.10,408.24,21.58,8.90" target="#b268">[267]</ref> proposed two methods for replacing a DNN with a deterministic finite automaton that can be visualized as a graph where each node represent a cluster of values in the output space and the edges represent the presence of shared patterns in a network's internal layers between these clusters.</s><s coords="34,447.27,444.11,41.65,8.90;34,106.36,456.06,315.09,8.90">Lastly, the method in <ref type="bibr" coords="34,151.15,456.06,21.58,8.90" target="#b269">[268]</ref> uses prototypes to explain the prediction of a new instance.</s><s coords="34,427.79,456.06,61.13,8.90;34,106.36,468.02,357.20,8.90">The prototypes are selected according to the activation values of hidden neurons related to training data.</s><s coords="34,467.90,468.02,21.02,8.90;34,106.36,479.97,382.56,8.90;34,106.36,491.93,329.08,8.90">For a new observation, it is possible to foresee and justify its prediction by identifying the three most similar training samples based on cosine distance of their hidden activation values.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4." coords="34,106.36,527.86,304.09,8.71">Model-specific methods for explainability related to rule-based systems</head><p><s coords="34,121.30,542.74,367.62,8.90;34,106.36,554.69,204.73,8.90">Explainable Artificial Intelligence was ignited by the interpretability problem of machine learning, in particular of the deep learning models.</s><s coords="34,315.42,554.69,173.50,8.90;34,106.36,566.65,189.78,8.90">However, the problem of explainability exists even before the advent of neural networks.</s><s coords="34,301.14,566.65,187.77,8.90;34,106.36,578.60,382.56,8.90;34,106.36,590.56,345.94,8.90">Many rules-based learning approaches already existed and the majority of these were interpreted with ante-hoc methods that act during the model training stage to make it naturally explainable (see table A.11 and figure <ref type="figure" coords="34,436.53,590.56,7.89,8.90" target="#fig_3">14</ref>).</s><s coords="34,458.33,590.56,30.58,8.90;34,106.36,602.51,356.91,8.90">An Ant Colony Optimization (ACO) algorithm was proposed in <ref type="bibr" coords="34,329.17,602.51,21.58,8.90" target="#b270">[269]</ref> to create interpretable rules.</s><s coords="34,466.25,602.51,22.66,8.90;34,106.36,614.47,382.56,8.90;34,106.36,626.43,382.56,8.90;34,106.36,638.38,298.84,8.90">It follows a sequential covering strategy, one-rule-at-a-time or also known as separate-and-conquer, to generate unordered sets of IF-THEN classification rules which can be inspected individually and independently from the others, thus they are easier to be interpreted.</s><s coords="34,411.05,638.38,77.87,8.90;34,106.36,650.34,353.54,8.90">At each step, ACO creates a new unordered set of rules and compare it with those of previous iterations.</s><s coords="34,466.46,650.34,22.46,8.90;34,106.36,662.29,382.56,8.90">If the new set contains fewer rules or has a better prediction accuracy, it replaces the previous one.</s><s coords="34,106.36,674.25,382.56,8.90;34,106.36,686.20,382.56,8.90;35,106.36,144.23,242.64,8.90">Experiments run on thirty-two publicly available datasets showed that ACO gave the best results in terms of both predictive accuracy and model size, outperforming state-of-the-art rule induction algorithms with statistically significant differences.</s><s coords="35,352.03,144.23,136.89,8.90;35,106.36,156.19,382.56,8.90;35,106.36,168.14,382.56,8.90">Another method based on an ACO algorithm, called AntMinter+ <ref type="bibr" coords="35,228.00,156.19,20.06,8.90" target="#b271">[270]</ref>, constructs monotonic rulesets by allowing the inclusion of domain knowledge via the definition of a directed acyclic graph representing the solution space.</s><s coords="35,106.36,180.10,382.56,8.90;35,106.36,192.05,382.56,8.90">The nodes at the same depth in the graph represent the splitting values related to an input variable; the edges represent which values of the following variable can be reached from a node.</s><s coords="35,106.36,204.01,382.56,8.90;35,106.36,215.96,73.39,8.90">AntMinter+ uses an iterative max-min ant system to construct a set of IF-THEN rules, starting from an empty set.</s><s coords="35,182.69,215.96,265.27,8.90">A rule represents a path from the start to the end nodes of the graph.</s><s coords="35,450.90,215.96,38.02,8.90;35,106.36,227.92,382.56,8.90;35,106.36,239.87,302.88,8.90">The algorithm stops adding rules when either a predefined percentage of training points is covered or when the addition of new rules does not improve the performance of the classifier.</s><s coords="35,412.33,239.87,76.59,8.90;35,106.36,251.83,382.56,8.90;35,106.36,263.78,238.47,8.90">AntMinter+ can be combined with a non-linear SVM, in a method called Active Learning Based Approach (ALBA), to generate comprehensible and accurate rule-based models.</s><s coords="35,347.84,263.78,141.08,8.90;35,106.36,275.74,382.56,8.90;35,106.36,287.70,382.56,8.90;35,106.36,299.65,382.56,8.90;35,106.36,311.61,117.26,8.90">Exception Directed Acyclic Graphs (EDAGs) <ref type="bibr" coords="35,146.86,275.74,21.58,8.90" target="#b272">[271]</ref> is an empirical induction tool that generates rules from the knowledge base of expert systems to create comprehensible knowledge structures in the form of graphs where nodes are premises, some of which have attached conclusions, leaves are conclusions and edges represent exceptions to some node.</s><s coords="35,226.54,311.61,262.38,8.90;35,106.36,323.56,382.56,8.90">The 'meaning' of each node can be easily determined by following its path back to the root and by inspecting its child nodes, whilst the rest of the graph is irrelevant.</s></p><p><s coords="35,247.58,481.66,96.29,7.12;35,176.84,586.39,46.48,7.12;35,340.34,583.55,99.39,7.12;35,106.36,605.81,382.56,7.12;35,106.36,615.28,150.32,7.12">(a) Attention Alignment <ref type="bibr" coords="35,326.61,481.66,17.26,7.12" target="#b265">[264]</ref> (b) PJ-X <ref type="bibr" coords="35,206.06,586.39,17.26,7.12" target="#b266">[265]</ref> (c) Attention Mechanism <ref type="bibr" coords="35,422.46,583.55,17.26,7.12" target="#b147">[147]</ref> Figure <ref type="figure" coords="35,129.17,605.81,6.79,7.12" target="#fig_2">13</ref>: Examples of mixed explanations, consisting of combinations of images and texts, generated by the following methods for explainability for neural networks.</s></p><p><s coords="35,121.30,640.68,367.62,8.90;35,106.36,652.64,238.43,8.90">The Interpretable Decision Set <ref type="bibr" coords="35,246.01,640.68,21.58,8.90" target="#b273">[272]</ref> and the Bayesian Rule Lists (BRL) <ref type="bibr" coords="35,412.68,640.68,20.75,8.90" target="#b274">[273,</ref><ref type="bibr" coords="35,435.98,640.68,17.43,8.90" target="#b275">274,</ref><ref type="bibr" coords="35,455.95,640.68,18.26,8.90" target="#b276">275]</ref> are two methods that creates unordered sets of IF-THEN rules.</s><s coords="35,349.03,652.64,139.88,8.90;35,106.36,664.59,382.56,8.90;35,106.36,676.55,382.56,8.90;35,106.36,688.50,87.30,8.90">Interpretable Decision Set is based on an objective function that simultaneously optimizes accuracy and interpretability by learning short and non-overlapping rules that cover the whole feature space and pay attention to small but important classes.</s><s coords="35,198.07,688.50,290.85,8.90;36,106.36,144.23,382.56,8.90;36,106.36,156.19,113.36,8.90">BRL produces a posterior multinomial distribution over permutations of rules, starting from a large set of possible rules, to assess the probability of predicting a certain label from the selected rules.</s><s coords="36,222.69,156.19,266.23,8.90;36,106.36,168.14,222.63,8.90">The prior is the Dirichlet distribution and the permutation that maximises the posterior is included in the final decision set.</s><s coords="36,332.68,168.14,156.24,8.90;36,106.36,180.10,382.56,8.90;36,106.36,192.05,382.56,8.90;36,106.36,204.01,382.56,8.90;36,106.36,215.96,77.18,8.90">The Bayesian Rule Sets (BRS) method <ref type="bibr" coords="36,106.36,180.10,20.75,8.90" target="#b277">[276,</ref><ref type="bibr" coords="36,129.93,180.10,18.26,8.90" target="#b278">277]</ref> is similar to BRL but it uses different probabilities, with the posterior as a Bernoulli distribution, and the prior a Beta distribution whose parameters can be adjusted by end-users to guide BRS toward more interpretable solutions by specifying the desired balance between size and length of rules.</s><s coords="36,186.87,215.96,302.05,8.90;36,106.36,227.92,218.75,8.90">First Order Combined Learner (FOCL) <ref type="bibr" coords="36,345.85,215.96,21.58,8.90" target="#b279">[278]</ref> inductively constructs a set of rules in terms of predicates used to describe examples.</s><s coords="36,328.83,227.92,160.09,8.90;36,106.36,239.87,282.57,8.90">Each clause body consists of a conjunction of predicates that cover some positive and no negative examples.</s><s coords="36,394.12,239.87,94.79,8.90;36,106.36,251.83,382.56,8.90;36,106.36,263.78,64.68,8.90">The rules are displayed in a tree where the nodes are the predicates, the edge are the conjunctions and the leaves are the conclusions.</s><s coords="36,175.57,263.78,313.35,8.90;36,106.36,275.74,297.18,8.90">Non-monotonic argumentation-based approaches for increasing explainability and dealing with conflictual information were proposed in <ref type="bibr" coords="36,339.78,275.74,20.75,8.90" target="#b142">[142,</ref><ref type="bibr" coords="36,362.94,275.74,17.43,8.90" target="#b143">143,</ref><ref type="bibr" coords="36,382.79,275.74,16.60,8.90" target="#b280">279]</ref>.</s><s coords="36,406.60,275.74,82.32,8.90;36,106.36,287.70,382.56,8.90;36,106.36,299.65,382.56,8.90;36,106.36,311.61,210.89,8.90">They are based upon the concepts of defeasible arguments, in the form of rules, each composed by a set of premises, an inference rule and a conclusion as well as the notion of attacks between arguments to model conflictuality and the retraction of a final inference.</s><s coords="36,322.97,311.61,165.95,8.90;36,106.36,323.56,382.56,8.90;36,106.36,335.52,107.27,8.90">Argumentation-based approaches are believed to have a higher explainability as the notions of arguments and conflictuality are common to the way human reason.</s><s coords="36,220.67,335.52,268.25,8.90;36,106.36,347.47,382.56,8.90;36,106.36,359.43,86.63,8.90">Four methods based on fuzzy reasoning to generate interpretable sets of rules that clearly show the dependencies between inputs and outputs were presented in <ref type="bibr" coords="36,106.36,359.43,20.75,8.90" target="#b281">[280,</ref><ref type="bibr" coords="36,130.52,359.43,17.43,8.90" target="#b282">281,</ref><ref type="bibr" coords="36,151.38,359.43,17.43,8.90" target="#b283">282,</ref><ref type="bibr" coords="36,172.24,359.43,16.60,8.90" target="#b284">283]</ref>.</s><s coords="36,198.86,359.43,290.06,8.90;36,106.36,371.38,86.72,8.90">Both <ref type="bibr" coords="36,221.66,359.43,20.75,8.90" target="#b281">[280,</ref><ref type="bibr" coords="36,245.83,359.43,18.26,8.90" target="#b284">283]</ref> examine the interpretability-accuracy tradeoff in fuzzy rule-based classifiers.</s><s coords="36,199.04,371.38,289.88,8.90;36,106.36,383.34,382.56,8.90;36,106.36,395.29,382.56,8.90">A multiobjective fuzzy Genetics-Based Machine Learning (GBML) algorithm, analyzed by <ref type="bibr" coords="36,197.29,383.34,20.06,8.90" target="#b281">[280]</ref>, is implemented in the framework of evolutionary multiobjective optimization (EMO) and consists of a hybrid version of Michigan and Pittsburgh approaches.</s><s coords="36,106.36,407.25,382.56,8.90;36,106.36,419.20,382.56,8.90;36,106.36,431.16,188.59,8.90">Each fuzzy rule is represented by its antecedent fuzzy sets as an integer string of fixed length and the resulting fuzzy rule-based classifier, consisting of a set of fuzzy rules, is represented as a concatenated integer string of variable length.</s><s coords="36,298.01,431.16,190.91,8.90;36,106.36,443.11,382.56,8.90;36,106.36,455.07,250.46,8.90">Multi-Objective Evolutionary Algorithms based Interpretable Fuzzy (MOEAIF) <ref type="bibr" coords="36,236.30,443.11,21.58,8.90" target="#b284">[283]</ref> consists instead of a fuzzy rule-based model engineered to classify gene expression data from microarray technologies.</s><s coords="36,360.35,455.07,128.57,8.90;36,106.36,467.02,382.56,8.90;36,106.36,478.98,382.56,8.90;36,106.36,490.93,150.64,8.90">GBML and MOEAIF maximize the accuracy of rule sets, measured by the number of correctly classified training pattern, and minimize their complexity, measured by the number of fuzzy rules and/or the total number of antecedent conditions of fuzzy rules.</s><s coords="36,263.30,490.93,225.62,8.90">The method in <ref type="bibr" coords="36,327.11,490.93,21.58,8.90" target="#b282">[281]</ref> is based on a five-step algorithm.</s><s coords="36,106.36,502.89,288.67,8.90">First, it generates fuzzy rules that cover the extrema directly from data.</s><s coords="36,399.73,502.89,89.19,8.90;36,106.36,514.84,228.79,8.90">Second, it checks rule similarity to delete the redundant and inconsistent rules.</s><s coords="36,340.71,514.84,148.21,8.90;36,106.36,526.80,242.07,8.90">Third, it optimizes the rule structure using genetic algorithms based on a local performance index.</s><s coords="36,351.44,526.80,137.48,8.90;36,106.36,538.75,359.78,8.90">Fourth, it performs further training of the rule parameters using gradient-based learning method and deletes the inactive rules.</s><s coords="36,469.27,538.75,19.65,8.90;36,106.36,550.71,208.34,8.90">Last, it improves interpretability by using regularization.</s><s coords="36,320.85,550.71,168.07,8.90;36,106.36,562.66,382.56,8.90;36,106.36,574.62,30.16,8.90">The method presented in <ref type="bibr" coords="36,426.20,550.71,21.58,8.90" target="#b283">[282]</ref> generates fuzzy rules by starting from a set of relations and properties, selected by an expert, of an input dataset.</s><s coords="36,140.72,574.62,348.20,8.90">It then extracts the most relevant ones employing a frequent itemset mining algorithm.</s><s coords="36,106.36,586.57,382.56,8.90;36,106.36,598.53,382.56,8.90;36,106.36,610.48,68.02,8.90">The authors do not provide a specific metric for evaluating the relevancy of a relation, but they suggest using "measures like the number of relations and properties in the antecedent or the value of their support".</s><s coords="36,177.41,610.48,311.51,8.90;36,106.36,622.44,382.56,8.90;36,106.36,634.40,350.47,8.90">Interpretable Classification Rule Mining (ICRM) <ref type="bibr" coords="36,374.47,610.48,21.58,8.90" target="#b285">[284]</ref> consists of a three-step evolutionary programming algorithm producing comprehensible IF-THEN classification rules, where comprehensibility is achieved by minimizing the number of rules and conditions.</s><s coords="36,460.07,634.40,28.85,8.90;36,106.36,646.35,282.06,8.90">First, it creates a pool of rules composed of a single attribute-value comparison.</s><s coords="36,391.38,646.35,97.54,8.90;36,106.36,658.31,382.56,8.90;36,106.36,670.26,382.56,8.90;36,106.36,682.22,238.41,8.90">Second, it utilizes evolutionary processes, designed to use only relevant attributes which are to discriminate a class from the others and improve the accuracy of the ruleset, based on the Iterative Rule Learning (IRL) genetic algorithm (also known as the Pittsburgh approach).</s><s coords="36,349.50,682.22,139.42,8.90;36,106.36,694.17,217.88,8.90">IRL returns a rule per output class with the exception of one class which is set as default.</s><s coords="36,327.93,694.17,160.99,8.90;37,106.36,144.23,278.07,8.90">The third step optimizes the accuracy of the classifier by maximising the product of sensitivity and specificity.</s><s coords="37,388.26,144.23,100.66,8.90;37,106.36,156.19,382.56,8.90;37,106.36,168.14,382.56,8.90;37,106.36,180.10,66.88,8.90">Linear Programming Relaxation <ref type="bibr" coords="37,140.97,156.19,20.75,8.90" target="#b286">[285,</ref><ref type="bibr" coords="37,164.22,156.19,18.26,8.90" target="#b288">286]</ref> is a method for learning two-level Boolean rules in conjunctive normal form (AND-of-ORs) or disjunctive normal form (OR-of-ANDs) as a type of human-interpretable classification model.</s><s coords="37,177.71,180.10,311.21,8.90;37,106.36,192.05,382.56,8.90;37,106.36,204.01,173.64,8.90">A first version uses a generalization of a linear programming relaxation from one level to two-level rules whose objective function is a weighted combination of the total number of errors and features used in the rule.</s><s coords="37,285.85,204.01,203.07,8.90;37,106.36,215.96,382.56,8.90;37,106.36,227.92,113.30,8.90">In a second version, the 0-1 classification error is replaced with the Hamming distance between the current rule and the closest rule that correctly classifies a sample instance.</s><s coords="37,224.81,227.92,264.11,8.90;37,106.36,239.87,382.56,8.90;37,106.36,251.83,136.31,8.90">The main advantages for explainability of the Hamming distance is that it avoids identical clauses in the ruleset, thus repetitions, by training each clause with a different subset of input instances.</s></p><p><s coords="37,121.30,275.74,367.62,8.90;37,106.36,287.70,47.23,8.90">All the above methods were intrinsically ante-hoc, but other methods exist for post-hoc explainability.</s><s coords="37,159.66,287.70,329.25,8.90;37,106.36,299.65,382.56,8.90;37,106.36,311.61,382.56,8.90;37,106.36,323.56,235.72,8.90">For example, Mycin <ref type="bibr" coords="37,246.17,287.70,10.58,8.90" target="#b2">[3]</ref>, probably the first method for explainability ever developed, is a backward chaining expert system based upon a knowledge-based of IF-THEN rules composed by an expert, a database of the context set of facts that satisfy the condition part of the rules, and an inference engine that interpret the rules.</s><s coords="37,348.13,323.56,140.79,8.90;37,106.36,335.52,382.56,8.90;37,106.36,347.47,382.56,8.90;37,106.36,359.43,277.23,8.90">It also includes a natural language interface that allows end-users to interact with the system independently of the expert by asking English questions, and the system can respond to them by using its inference engine and performing the reasoning involved in composing an answer to them.</s><s coords="37,388.57,359.43,100.35,8.90;37,106.36,371.38,382.56,8.90">In details, it searches for facts that match the condition part of the productions that match the action part of the question.</s><s coords="37,106.36,383.34,382.57,8.90;37,106.36,395.29,382.56,8.90;37,106.36,407.25,47.23,8.90">This method allows the system to explain its reasoning and final inferences by using AND/OR trees created during the production system reasoning process, thus showing an element of explainability.</s><s coords="37,159.68,407.25,329.23,8.90;37,106.36,419.20,382.56,8.90;37,106.36,431.16,168.39,8.90">Similarly, the Sugeno-type fuzzy inference system proposed in <ref type="bibr" coords="37,420.51,407.25,21.58,8.90" target="#b289">[287]</ref> consists of an explicit declarative knowledge representation of rules, which are fired at the same time by a given input, and produce a final inference.</s><s coords="37,278.10,431.16,210.82,8.90;37,106.36,443.11,382.56,8.90;37,106.36,455.07,202.02,8.90">Besides this, the system includes an explanatory tool which shows a numerical representation of the input variables, the set of co-fired rules and an English statement exposing the reasoning process.</s><s coords="37,312.56,455.07,176.36,8.90;37,106.36,467.02,382.56,8.90;37,106.36,478.98,382.56,8.90;37,106.36,490.93,27.39,8.90">In an example taken from the application to an Unmanned Aerial Vehicle (UAV) sent on a fight mission, is a statement like "UAV aborted the mission because the weather was a thunderstorm and the distance from the enemy was too close".</s><s coords="37,137.01,490.93,351.91,8.90;37,106.36,502.89,382.56,8.90;37,106.36,514.84,382.56,8.90;37,106.36,526.80,382.56,8.90;37,106.36,538.75,36.80,8.90">Another method, the Fuzzy Inference-Grams (Fingrams) <ref type="bibr" coords="37,365.63,490.93,21.58,8.90" target="#b290">[288]</ref> produces inference maps of sets of fuzzy rules which graphically depict the interaction between co-fired rules and gives support to detect redundant or inconsistent rules as well as it identifies the most significant ones, by using network scaling methods that simplify the maps while maintaining their most important relations.</s><s coords="37,146.42,538.75,342.49,8.90;37,106.36,550.71,97.63,8.90">Fingrams also quantifies the comprehensibility of the ruleset, measured as the proportion of the co-fired rules.</s><s coords="37,206.99,550.71,281.93,8.90;37,106.36,562.66,268.51,8.90">The assumption is that the larger the number of rules which are co-fired for a given input, the smaller the comprehensibility of the ruleset.</s><s coords="37,380.21,562.66,108.71,8.90;37,106.36,574.62,382.56,8.90;37,106.36,586.57,382.56,8.90;37,106.36,598.53,52.70,8.90">ExpliClas <ref type="bibr" coords="37,422.75,562.66,21.58,8.90" target="#b291">[289]</ref> is a visual interface designed to explain, in an instance-based manner, rule-based classifiers (such as those algorithms extracting DTs from data, like C4.5 or CART, for instance) with visual and textual explanations.</s><s coords="37,162.98,598.53,325.94,8.90;37,106.36,610.48,212.76,8.90">The rules are graphically displayed as DTs and a natural language generation approach returns textual explanations of the fired rules.</s><s coords="37,323.40,610.48,165.52,8.90;37,106.36,622.44,382.56,8.90">ExpliClas was tested on in the context of recognising the role of basketball players from some physical characteristics and game statistics.</s><s coords="37,106.36,634.40,382.56,8.90;37,106.36,646.35,247.41,8.90">An example of textual explanation produced in this case is "The player is a Point-Guard because he is medium-height and he has a small number of rebounds."</s><s coords="37,357.16,646.35,131.76,8.90;37,106.36,658.31,246.59,8.90">and it is accompanied by a graph of the DT structure and bar-charts of the player's information.</s></p><p><s coords="38,152.19,198.21,45.60,7.12;38,146.95,305.36,61.98,7.12;38,271.09,305.36,60.21,7.12;38,370.03,200.69,113.77,7.12;38,377.64,305.36,89.83,7.12;38,377.64,314.83,17.26,7.12;38,120.66,334.26,353.96,7.12">(a) BRL <ref type="bibr" coords="38,180.53,198.21,17.26,7.12" target="#b276">[275]</ref> (b) ExpliClas <ref type="bibr" coords="38,191.67,305.36,17.26,7.12" target="#b291">[289]</ref> (c) Fingrams <ref type="bibr" coords="38,314.04,305.36,17.26,7.12" target="#b290">[288]</ref> (d) Interpretable Decision Set <ref type="bibr" coords="38,466.53,200.69,17.26,7.12" target="#b273">[272]</ref> (e) Fuzzy Inference Systems <ref type="bibr" coords="38,377.64,314.83,17.26,7.12" target="#b289">[287]</ref> Figure <ref type="figure" coords="38,143.46,334.26,6.79,7.12" target="#fig_3">14</ref>: Examples of explanations generated by methods for explainability for rule-based inference systems.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5." coords="38,106.36,365.71,210.10,8.71">Other model-specific methods for explainability</head><p><s coords="38,121.30,380.58,367.62,8.90;38,106.36,392.54,244.20,8.90">Scholars proposed other methods for explainability that are not strictly based on neural networks or rule-based classifiers (see table A.12 and figure <ref type="figure" coords="38,334.79,392.54,7.89,8.90" target="#fig_0">15</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1." coords="38,106.36,416.52,72.50,8.71">Ensembles.</head><p><s coords="38,121.30,429.40,297.97,8.90">Some methods were designed to interpret the logic followed by ensembles.</s><s coords="38,422.33,429.40,66.59,8.90;38,106.36,441.35,370.74,8.90"><ref type="bibr" coords="38,422.33,429.40,21.58,8.90" target="#b292">[290]</ref> introduced an algorithm to tweak the input features to change the output of a tree ensemble classifier.</s><s coords="38,482.83,441.35,6.09,8.90;38,106.36,453.31,382.56,8.90;38,106.36,465.26,382.56,8.90">It modifies a variable (or a set of variable) of an input instance by applying a linear shift, capped to a global tolerance value, until all the trees in the ensemble assign it to another target class.</s><s coords="38,106.36,477.22,382.56,8.90;38,106.36,489.17,382.56,8.90;38,106.36,501.13,196.36,8.90">The delta between the original and the tweaked value represents the 'effort', or 'tweaking cost', required to move the instance into the target class and provides a measure of the sensitivity of the model to changes to that particular feature(s).</s><s coords="38,306.01,501.13,182.91,8.90;38,106.36,513.08,382.56,8.90;38,106.36,525.04,316.00,8.90">This information can be used to rank the variables according to their tweaking costs and inform end-users on how a particular instance must be modified to change its output label and at what cost this can be achieved.</s><s coords="38,428.44,525.04,60.48,8.90;38,106.36,536.99,382.56,8.90;38,106.36,548.95,119.21,8.90">Three methods for extracting a single DT from ensemble models, and for generating global explanations were presented in <ref type="bibr" coords="38,159.46,548.95,20.75,8.90" target="#b293">[291,</ref><ref type="bibr" coords="38,183.79,548.95,17.43,8.90" target="#b294">292,</ref><ref type="bibr" coords="38,204.82,548.95,16.60,8.90" target="#b295">293]</ref>.</s><s coords="38,231.94,548.95,256.98,8.90;38,106.36,560.91,382.56,8.90;38,106.36,572.86,132.13,8.90">In detail, <ref type="bibr" coords="38,272.32,548.95,21.58,8.90" target="#b294">[292]</ref> uses the solution obtained from combining the several hypotheses (or models) of the ensemble as an oracle, and it selects the single hypothesis that is most similar to the oracle.</s><s coords="38,242.48,572.86,246.43,8.90;38,106.36,584.72,382.57,9.00;38,106.36,596.67,382.56,9.00;38,106.36,608.73,382.56,8.90;38,106.36,620.68,285.36,8.90">The similarity is measured according to three formal metrics: 'θmeasure' which determines the probability that both classifiers agree, 'κmeasure' which assesses the probability that two classifiers agree by chance and 'Qmeasure' which assigns values between 0 and 1 to classifiers that correctly predict the same input instances and values between -1 and 0 to classifiers that commit errors on different instances.</s><s coords="38,394.77,620.68,94.15,8.90;38,106.36,632.64,382.56,8.90">Instead, <ref type="bibr" coords="38,428.44,620.68,21.58,8.90" target="#b295">[293]</ref> proposed to generate a tree from the ensemble by using a divide-and-conquer algorithm analogous to C4.5.</s><s coords="38,106.36,644.59,382.56,8.90;38,106.36,656.55,232.75,8.90">Similarly, <ref type="bibr" coords="38,148.11,644.59,21.58,8.90" target="#b293">[291]</ref> combined an opaque learning algorithm (random forest), with a more transparent and inherently interpretable algorithm (decision tree).</s><s coords="38,344.43,656.55,144.49,8.90;38,106.36,668.50,263.57,8.90">On one hand, the opaque algorithm represents the 'oracle' which search for the most relevant output.</s><s coords="38,374.72,668.50,114.20,8.90;38,106.36,680.46,382.56,8.90;38,106.36,692.41,382.56,8.90;39,106.36,144.23,194.94,8.90">On the other hand, a natural language generation approach is aimed at composing a textual explanation for this output which is the interpretation of the inference process carried out by the correspondent decision tree, if the output of both the learning algorithms coincides.</s><s coords="39,304.85,144.23,184.07,8.90;39,106.36,156.19,382.56,8.90;39,106.36,168.14,207.26,8.90"><ref type="bibr" coords="39,304.85,144.23,21.58,8.90" target="#b296">[294]</ref> proposed instead a method to efficiently merge a set DTs, each trained independently on distributed data, into a single tree to overcome the lack of interpretability of the distributed models.</s><s coords="39,316.68,168.14,148.42,8.90">The algorithm consists of three steps.</s><s coords="39,468.16,168.14,20.76,8.90;39,106.36,180.10,382.56,8.90;39,106.36,192.05,143.79,8.90">First, each DT is converted into a ruleset where each rule replicates a path from the root to a leaf and defines a region in the output space.</s><s coords="39,253.68,192.05,235.24,8.90">All the regions are disjoint and they cover the entire space.</s><s coords="39,106.36,204.01,382.56,8.90;39,106.36,215.96,256.25,8.90">In the second phase, the regions are combined by using a line sweep algorithm which sorts the limits of each region and merges overlapping and adjacent ones.</s><s coords="39,365.92,215.96,123.00,8.90;39,106.36,227.92,382.56,8.90">Finally, a DT is extracted from the regions with an algorithm that mimics the C5.0 and uses the values in the regions as examples.</s></p><p><s coords="39,121.30,251.83,367.62,8.90;39,106.36,263.78,382.56,8.90;39,106.36,275.74,90.22,8.90">A similar approach is at the basis of the Factorized Asymptotic Bayesian (FAB) inference method <ref type="bibr" coords="39,139.36,263.78,21.58,8.90" target="#b297">[295]</ref> which consists of a bayesian model selection algorithm that simplified and optimized a tree ensemble.</s><s coords="39,199.55,275.74,289.37,8.90;39,106.36,287.70,382.56,8.90;39,106.36,299.65,325.89,8.90">FAB estimates the model's parameters and the optimal number of regions of the input space (ensemble methods often splits the input space into a huge number of regions) to derive a simplified model with appropriate complexity and prediction accuracy.</s><s coords="39,435.33,299.65,53.59,8.90;39,106.36,311.61,222.11,8.90">inTrees <ref type="bibr" coords="39,467.34,299.65,21.58,8.90" target="#b298">[296]</ref> extracts, prunes and selects rules from a tree ensemble.</s><s coords="39,332.69,311.61,156.23,8.90;39,106.36,323.56,325.97,8.90">The algorithm starts from the set of all the rules in the ensemble and excludes those covering a small number of instances.</s><s coords="39,435.31,323.56,53.61,8.90;39,106.36,335.52,382.56,8.90;39,106.36,347.47,382.56,8.90;39,106.36,359.43,382.56,8.90;39,106.36,371.38,113.79,8.90">At each iteration, the algorithm selects the rule with the minimum error and shorter condition, then it removes the instances satisfying this rule from the dataset and it updates the initial ruleset according to the instances left, by discarding rules that at this stage cover just a few, if not none, instances and recalculating their error.</s><s coords="39,224.15,371.38,264.77,8.90;39,106.36,383.34,382.56,8.90;39,106.36,395.29,382.56,8.90;39,106.36,407.25,331.61,8.90">Discriminative Patterns <ref type="bibr" coords="39,320.52,371.38,21.58,8.90" target="#b299">[297]</ref> aims at interpreting a random forest model that classifies sentences according to their contents by extracting a ruleset that enables interpretation and gains insight of useful information in texts which corresponds to discriminative sequential patterns of words, or sentences that determine the predicted class.</s><s coords="39,443.82,407.25,45.10,8.90;39,106.36,419.20,382.56,8.90;39,106.36,431.16,315.51,8.90">Tree Space Prototypes (TSP) <ref type="bibr" coords="39,177.76,419.20,21.58,8.90" target="#b300">[298]</ref> selects prototypes from a training dataset to explain the prediction made be ensembles of DTs and gradient boosted tree models on a new observation.</s><s coords="39,426.95,431.16,61.97,8.90;39,106.36,443.11,382.56,8.90;39,106.36,455.07,382.56,8.90;39,106.36,467.02,382.56,8.90;39,106.36,478.98,30.17,8.90">To measure the similarity between the new instance and the prototypes, the authors proposed a metric based on the weighted average of the number of trees in the ensemble that assigns the points to the same output class to quantify the contribution of the predictions made by each DT to the overall prediction.</s><s coords="39,139.52,478.98,349.40,8.90;39,106.36,490.93,382.56,8.90;39,106.36,502.89,184.38,8.90">By following the path root-to-leaf of the most relevant DT, it is possible to determine the values of the features deemed important by the tree for predicting the class of the new instance and select a prototype having the same values.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2." coords="39,106.36,525.87,130.46,8.71">Support Vector Machines.</head><p><s coords="39,121.30,538.75,367.62,8.90;39,106.36,550.71,382.56,8.90;39,106.36,562.66,253.18,8.90">ExtractRule <ref type="bibr" coords="39,171.41,538.75,21.58,8.90" target="#b138">[138]</ref> converts hyperplane-based linear classifiers, like SVMs, into a set of nonoverlapping symbolic rules which are human-understandable because they display, in a compact format, the inferential process of the underlying classifier.</s><s coords="39,363.93,562.66,124.99,8.90;39,106.36,574.52,382.57,9.00;39,106.36,585.83,307.83,9.96">An example of a rule extracted from a classifier trained to distinguish between malign and benign tumors is "(Cell S ize ≤ 3) ∧ (Bare Nuclei ≤ 1) ∧ (Normal Nucleoli ≤ 7) =⇒ mass is benign".</s><s coords="39,420.04,586.57,68.88,8.90;39,106.36,598.53,382.56,8.90;39,106.36,610.48,76.12,8.90">Each rule can be seen as a hypercube in the multidimensional space generated by the input variables with edges parallel to the axis.</s><s coords="39,185.89,610.48,303.03,8.90;39,106.36,622.44,172.89,8.90">To define these hypercubes, each iteration of this algorithm is formulated as one of two possible optimization problems.</s><s coords="39,282.52,622.44,206.40,8.90;39,106.36,634.40,382.56,8.90;39,106.36,646.35,19.64,8.90">The first formulation seeks to maximize the volume covered by each rule whereas the second formulation maximizes the number of samples covered.</s><s coords="39,130.79,646.35,358.13,8.90;39,106.36,658.31,218.69,8.90">Important support vectors and Border classification <ref type="bibr" coords="39,341.04,646.35,21.58,8.90" target="#b146">[146]</ref> are two methods for providing insight into local classifications produced by a SVM.</s><s coords="39,328.65,658.31,160.27,8.90;39,106.36,670.26,382.56,8.90;39,106.36,682.22,262.71,8.90">The former reports the most influential support vectors for the final classification of a particular data instance, thus determining the most similar instances to the test point that belong to the same class.</s><s coords="39,375.57,682.22,113.34,8.90;39,106.36,694.17,382.56,8.90;40,106.36,144.23,245.29,8.90">As in the previous methods based on prototypes, presenting this subset helps users understand the model by leveraging on the human ability to induce principles from a few examples.</s><s coords="40,356.76,144.23,132.16,8.90;40,106.36,156.19,382.56,8.90;40,106.36,168.14,382.56,8.90;40,106.36,180.10,284.09,8.90">The latter determines which features of a testing data instance would need to be altered (and by how much) to be classified on the separating surface between the two classes, thus providing, as in the feature tweaking method, a measure of the cost required to change a model's prediction.</s><s coords="40,394.11,180.10,94.80,8.90;40,106.36,192.05,382.56,8.90;40,106.36,204.01,167.68,8.90">SVM+Prototypes <ref type="bibr" coords="40,467.34,180.10,21.58,8.90" target="#b301">[299]</ref> is based on a clustering algorithm to detect the prototype vectors for each class, after the decision function is determined to employ a SVM.</s><s coords="40,276.86,204.01,212.05,8.90;40,106.36,215.96,382.56,8.90;40,106.36,227.92,382.56,8.90">These vectors are combined with the support vectors using geometric methods to define ellipsoids in the input space, which are later transferred to IF-THEN rules as the mathematical equations that defined the ellipsoids, so a rule looks like "If</s></p><formula xml:id="formula_0" coords="40,106.60,238.28,241.15,13.12">AX 2 1 + BX 2 2 + CX 1 X 2 + DX 1 + EX 2 + F ≤ G Then Class 1 "</formula><p><s coords="40,347.75,239.87,141.17,8.90;40,106.36,251.83,382.56,8.90;40,106.36,263.78,382.56,8.90;40,106.36,275.74,321.82,8.90">. Weighted Linear Classifier <ref type="bibr" coords="40,467.34,239.87,21.58,8.90" target="#b302">[300]</ref> generates weighted linear SVM classifiers or random hyperplanes to obtain models whose accuracy is comparable to that of a non-linear SVM classifier and whose results can be readily visualized by projecting them on separating hyperplanes and decision surfaces.</s><s coords="40,433.64,275.74,55.28,8.90;40,106.36,287.70,180.63,8.90">These projections are considered as a sort of explanations.</s><s coords="40,290.07,287.70,198.85,8.90;40,106.36,299.65,191.75,8.90">A method based on Self-Organizing Maps (SOM) used to visualise SVMs was proposed in <ref type="bibr" coords="40,274.04,299.65,20.06,8.90" target="#b303">[301]</ref>.</s><s coords="40,303.01,299.65,185.91,8.90;40,106.36,311.61,382.56,8.90;40,106.36,323.56,269.48,8.90">It is a specific unsupervised network aimed at investigating a high-dimensional space of data for a cluster of points by projecting these clusters onto a 2-dimensional map, but trying to preserve their topologies.</s><s coords="40,381.62,323.56,107.30,8.90;40,106.36,335.52,382.56,8.90;40,106.36,347.47,263.86,8.90">Thus, it allows visualising both data and the SVM models, providing an overall overview of the support vector decision surface which is not possible with other visualization approaches.</s><s coords="40,374.24,347.47,114.68,8.90;40,106.36,359.43,382.56,8.90;40,106.36,371.38,286.46,8.90"><ref type="bibr" coords="40,374.24,347.47,20.75,8.90" target="#b304">[302,</ref><ref type="bibr" coords="40,397.79,347.47,17.43,8.90" target="#b305">303,</ref><ref type="bibr" coords="40,418.03,347.47,18.26,8.90" target="#b306">304]</ref> introduced a method for automatically generating nomograms as the graphical tool for visual explanations of the inferential mechanisms of SVM and naïve bayesian-driven models.</s><s coords="40,397.39,371.38,91.53,8.90;40,106.36,383.34,382.56,8.90;40,106.36,395.29,382.56,8.90;40,106.36,407.25,36.80,8.90">A nomogram is a twodimensional diagram designed to allow approximating graphical computation of mathematical functions by showing a set of scales, one for each variable (dependent and independent) in an equation.</s><s coords="40,147.81,407.25,341.11,8.90;40,106.36,419.20,382.56,8.90;40,106.36,431.16,185.79,8.90">By drawing a line connecting specific values of all the scales related to the independent variables, it is possible to calculate the value of the dependent variable from the intersection point between the line and the variable's scale.</s><s coords="40,295.28,431.16,193.64,8.90;40,106.36,443.11,359.45,8.90">The advantages for explainability of nomograms are simplicity of presentation and clear display of the effects of individual attribute values.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3." coords="40,106.36,466.10,169.88,8.71">Bayesian and hierachical networks.</head><p><s coords="40,121.30,478.98,367.62,8.90;40,106.36,490.93,246.48,8.90">Explaining Bayesian network Inferences (EBI) <ref type="bibr" coords="40,316.12,478.98,21.58,8.90" target="#b307">[305]</ref> produces DT rules to show how the variables of a bayesian network interact to make predictions.</s><s coords="40,357.75,490.93,131.17,8.90;40,106.36,502.89,382.56,8.90;40,106.36,514.84,245.17,8.90">In detail, EBI explains the value of a target node in terms of the influential nodes in the target's Markov blanket which include the target's parents, children and the children's other parents.</s><s coords="40,355.49,514.84,133.43,8.90;40,106.36,526.80,382.56,8.90;40,106.36,538.75,382.56,8.90">Working backwards from the target node, EBI shows the derivation of each intermediate node and explains how missing and erroneous values are compensated by displaying these causal relationships in a DT hierarchy.</s><s coords="40,106.36,550.71,382.56,8.90;40,106.36,562.66,39.56,8.90"><ref type="bibr" coords="40,106.36,550.71,21.58,8.90" target="#b308">[306]</ref> instead proposed an explanation method for understanding bayesian networks in terms of scenarios.</s><s coords="40,149.63,562.66,339.29,8.90;40,106.36,574.62,382.56,8.90;40,106.36,586.57,382.56,8.90;40,106.36,598.53,183.54,8.90">Narrative approaches to reasoning with legal evidence, for instance, are based on the formulation of alternative scenarios which are subsequently compared according to two aspects: the relations with the evidence and the quality that depends on the completeness, internal consistency and plausibility of the scenario itself.</s><s coords="40,293.65,598.53,195.27,8.90;40,106.36,610.48,382.56,8.90;40,106.36,622.44,29.52,8.90">The aim is to explain the content of the bayesian network by reporting which scenarios were modelled and evaluating their evidential support and quality.</s><s coords="40,138.80,622.44,350.12,8.90;40,106.36,634.40,303.66,8.90">Probabilistically Supported Arguments (PSA) <ref type="bibr" coords="40,322.14,622.44,21.58,8.90" target="#b309">[307]</ref> is a two-phase method for extracting probabilistically explanatory supported arguments from a bayesian network.</s><s coords="40,413.21,634.40,75.71,8.90;40,106.36,646.35,382.56,8.90;40,106.36,658.31,75.98,8.90">In the first phase, a support graph is constructed from a bayesian network representing the structure for a particular variable of interest.</s><s coords="40,185.27,658.31,303.65,8.90;40,106.36,670.26,57.36,8.90">In the second phase, given a set of observations, arguments are built from that support graph.</s><s coords="40,166.74,670.26,322.18,8.90;40,106.36,682.22,188.08,8.90">To do so, the algorithm defines a logical language and a set of rules built from the support graph by following its edges and nodes.</s><s coords="40,297.41,682.22,191.51,8.90;40,106.36,694.17,131.73,8.90">The parents of a node are the rule conditions, the node itself is the rule's outcome.</s><s coords="40,242.51,694.17,246.40,8.90;41,106.36,144.23,19.64,8.90">Only the parents supported by pieces of evidence are consid-ered.</s><s coords="41,129.06,144.23,296.43,8.90">Then, the ASPIC+ framework for structured argumentation is instantiated.</s><s coords="41,428.56,144.23,60.36,8.90;41,106.36,156.19,382.56,8.90;41,106.36,168.14,382.56,8.90;41,106.36,180.10,382.56,8.90;41,106.36,192.05,382.56,8.90;41,106.36,204.01,136.71,8.90">Arguments can attack each other on the conclusion variable and defeat can be based on the inferential strength of the arguments which can be computed with two types of measures: 'incremental measures' which assign a number to the weight of the evidence (the Likelihood Ratio is an example of these measures) and 'absolute measures' which assign strength based on posterior probability, such as the posterior for instance.</s><s coords="41,248.16,204.01,240.76,8.90;41,106.36,215.96,327.11,8.90">Such arguments can help interpret and explain the relationship between hypotheses and evidence that is modelled in the Bayesian network.</s><s coords="41,438.19,215.96,50.73,8.90;41,106.36,227.92,382.56,8.90;41,106.36,239.87,306.28,8.90">Contribution propagation <ref type="bibr" coords="41,156.40,227.92,21.58,8.90" target="#b310">[308]</ref> is a per-instance method for hierarchical networks that explain which components of the input were responsible (and to what degree) for its classification.</s><s coords="41,415.71,239.87,73.21,8.90;41,106.36,251.83,382.56,8.90;41,106.36,263.78,144.21,8.90">The central idea is that a node was important to the classification if it was important to its parents, and its parents were important to the classification.</s><s coords="41,254.94,263.78,233.98,8.90;41,106.36,275.74,43.44,8.90">The contribution of each input component is visualized as heat-maps.</s></p><p><s coords="41,147.55,393.38,106.11,7.12;41,151.96,494.49,101.41,7.12;41,350.76,388.77,69.50,7.12;41,370.03,494.49,45.16,7.12;41,106.36,513.91,382.56,7.12;41,106.36,523.38,159.05,7.12">(a) Decision tree extraction <ref type="bibr" coords="41,236.40,393.38,17.26,7.12" target="#b294">[292]</ref> (b) Self-Organizing Maps <ref type="bibr" coords="41,236.11,494.49,17.26,7.12" target="#b303">[301]</ref> (c) Nomograms <ref type="bibr" coords="41,403.00,388.77,17.26,7.12" target="#b306">[304]</ref> (d) PSA <ref type="bibr" coords="41,397.93,494.49,17.26,7.12" target="#b309">[307]</ref> Figure <ref type="figure" coords="41,129.94,513.91,6.79,7.12" target="#fig_0">15</ref>: Examples of explanations generated by methods specifically designed to interpret ensembles (a), Support Vector Machines (b-c) and bayesian networks (d).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6." coords="41,106.36,558.64,191.75,8.71">Self-explainable and interpretable methods</head><p><s coords="41,121.30,573.51,367.62,8.90;41,106.36,585.47,126.65,8.90">Naturally interpretable models, sometimes referred to as 'white-box models', are 'ante-hoc' (see table A.13 and figure <ref type="figure" coords="41,217.24,585.47,7.89,8.90" target="#fig_4">16</ref>).</s><s coords="41,239.56,585.47,249.36,8.90;41,106.36,597.42,29.05,8.90">Their output format depends on their architecture and inputs format.</s><s coords="41,141.31,597.42,347.61,8.90;41,106.36,609.38,223.55,8.90">Bayesian Case Model (BCM) <ref type="bibr" coords="41,265.73,597.42,21.58,8.90" target="#b311">[309]</ref> is a method for explainability for bayesian casebased reasoning, prototype classification and clustering.</s><s coords="41,333.46,609.38,155.46,8.90;41,106.36,621.34,382.56,8.90;41,106.36,633.29,197.03,8.90">BCM learns prototypes, corresponding to the observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features.</s><s coords="41,308.23,633.29,180.69,8.90;41,106.36,645.25,231.65,8.90">Gaussian Process Regression (GPR) <ref type="bibr" coords="41,457.62,633.29,21.58,8.90" target="#b312">[310]</ref> is a powerful, but amenable to analysis, data-driven model.</s><s coords="41,343.56,645.25,145.35,8.90;41,106.36,657.20,382.56,8.90;41,106.36,669.16,382.56,8.90;41,106.36,681.11,240.22,8.90">GPR is a non-parametric regression algorithm, meaning that it does not make any assumption on the estimator function as linear and logistic regression algorithms do, robust to missing data and interpretable because the weights assigned to each feature provide a measure of its relevance.</s><s coords="41,351.01,681.11,137.91,8.90;41,106.36,693.07,206.51,8.90;41,312.87,691.47,3.49,6.23;41,316.85,693.07,172.07,8.90;42,106.36,144.23,382.56,8.90;42,106.36,156.19,39.97,8.90;42,146.33,154.59,3.49,6.23;42,150.31,156.19,18.54,8.90">Generalized Additive Models <ref type="bibr" coords="41,472.32,681.11,16.60,8.90" target="#b98">[99]</ref> and their extension with pairwise interactions (GA 2 Ms) <ref type="bibr" coords="41,336.26,693.07,20.75,8.90" target="#b313">[311,</ref><ref type="bibr" coords="41,360.38,693.07,18.26,8.90" target="#b131">131]</ref> are linear combinations of simple models, called 'shape functions', trained on a single feature (GAMs) or up to two features (GA 2 Ms).</s><s coords="42,173.16,156.19,315.75,8.90;42,106.36,168.14,382.56,8.90;42,106.36,180.10,103.49,8.90">Their simple structure allows end-user to easily understand the contribution of individual features to the predictions and to visualize them, together with the shape functions, with bar-and line-charts.</s><s coords="42,216.48,180.10,272.44,8.90;42,106.36,192.05,176.56,8.90">Oblique Treed Sparse Additive Models (OT-SpAMs) <ref type="bibr" coords="42,436.76,180.10,21.58,8.90" target="#b314">[312]</ref> are instances of region-specific predictive models.</s><s coords="42,285.98,192.05,202.94,8.90;42,106.36,204.01,331.41,8.90">They divide feature spaces into regions with sparse oblique tree splitting and assign local sparse additive experts to individual regions.</s><s coords="42,441.69,204.01,47.23,8.90;42,106.36,215.96,382.56,8.90;42,106.36,227.92,302.84,8.90">Transparent Generalized Additive Model Tree (TGAMT) <ref type="bibr" coords="42,286.35,215.96,21.58,8.90" target="#b315">[313]</ref> was proposed as an explainable and transparent method that uses a CART-like greedy recursive search to grow the tree.</s><s coords="42,413.51,227.92,75.41,8.90;42,106.36,239.87,382.56,8.90;42,106.36,251.83,216.84,8.90">Multi-Run Subtree Encapsulation, which comes from the genetic programming (GP) realm, was proposed in <ref type="bibr" coords="42,467.34,239.87,21.58,8.90" target="#b316">[314]</ref> as a way to generate simpler tree-based GP programs.</s><s coords="42,327.23,251.83,161.70,8.90;42,106.36,263.78,382.56,8.90">If the tree contains sub-trees of different makeup but evaluating the same vector of results, they are to be considered as the same sub-tree.</s><s coords="42,106.36,275.74,382.56,8.90;42,106.36,287.70,172.05,8.90">This reduces, according to the authors, the complexity of the entire tree structure and the resulting expressions, in favour of explainability.</s></p><p><s coords="42,121.30,311.61,367.62,8.90;42,106.36,323.56,382.56,8.90;42,106.36,335.52,382.56,8.90;42,106.36,347.47,108.27,8.90">Probabilistic Sentential Decision Diagrams (PSDD) <ref type="bibr" coords="42,331.69,311.61,21.58,8.90" target="#b317">[315]</ref> can be described as circuit representations where each parameter represents a conditional probability of deciding the input variables and each node is either a logical AND gate with two inputs or a logical OR gate with an arbitrary number of inputs.</s><s coords="42,218.63,347.47,270.29,8.90;42,106.36,359.43,37.29,8.90">The PSDD structure can be visualized as an easily-interpretable binary tree.</s><s coords="42,146.62,359.43,342.30,8.90;42,106.36,371.38,37.91,8.90">Mind the Gap Model (MGM) <ref type="bibr" coords="42,265.37,359.43,21.58,8.90" target="#b318">[316]</ref> is a method for interpretable feature extraction and selection.</s><s coords="42,147.60,371.38,341.32,8.90;42,106.36,383.34,168.36,8.90">The goal is to split the observation into clusters while returning the list of dimensions that are important for distinguishing them.</s><s coords="42,277.77,383.34,211.15,8.90;42,106.36,395.29,382.56,8.90;42,106.36,407.25,26.69,8.90">The results are presented as a mix of numbers, which are the relevance values of each dimension, texts and graphs that represent the dimensions themselves.</s><s coords="42,136.78,407.25,352.14,8.90;42,106.36,419.20,382.56,8.90;42,106.36,431.16,382.56,8.90;42,106.36,443.11,331.42,8.90">For example, in a classification problem of images representing the four seasons, MGM returns samples of images belonging to each class (spring, summer, autumn and winter) together with the list of their relevant features (like snow, sun and flowers) and the relevance values of each feature per target class (snow has a high relevance value for the class 'winter').</s><s coords="42,440.78,443.11,48.14,8.90;42,106.36,455.07,382.56,8.90;42,106.36,467.02,239.39,8.90">Supersparse Linear Integer Model (SLIM) <ref type="bibr" coords="42,227.24,455.07,21.58,8.90" target="#b319">[317]</ref> generates a scoring system from an input dataset by assigning a score to each variable that contributes to the prediction.</s><s coords="42,348.73,467.02,140.19,8.90;42,106.36,478.98,382.56,8.90;42,106.36,490.93,73.37,8.90">These scores are multiplied by a set of coefficients inferred from the training dataset and then added, subtracted, and/or multiplied to make a prediction.</s><s coords="42,182.76,490.93,306.16,8.90;42,106.36,502.79,382.56,9.84;42,106.36,514.84,382.56,8.90;42,106.36,526.80,200.96,8.90">Scores are generated by minimising the 0-1 loss to reach a high level of accuracy and to produce a classifier that is robust to outliers by applying a 0penalty to encourage a high level of sparsity and a set of interpretability constraints which restricts coefficients to a user-defined set of meaningful and intuitive values.</s><s coords="42,310.26,526.80,178.66,8.90;42,106.36,538.75,382.56,8.90;42,106.36,550.71,148.32,8.90">Eventually, Unsupervised Interpretable Word Sense Disambiguation <ref type="bibr" coords="42,198.28,538.75,21.58,8.90" target="#b320">[318]</ref> produces interpretable word sense disambiguation models that create clusters, or inventories, of words.</s><s coords="42,258.74,550.71,230.18,8.90;42,106.36,562.66,230.78,8.90">For example, an inventory can be the collection of all the words related to 'furniture' (such as table, chair and bed).</s><s coords="42,340.74,562.66,148.18,8.90;42,106.36,574.62,382.56,8.90;42,106.36,586.57,382.56,8.90;42,106.36,598.53,245.35,8.90">The words are clustered according to their co-occurrence and relative position in a text, where close words are assumed to be highly correlated, and their syntactic dependency extracted from the Stanford Dependencies (representation of grammatical relations between words in a sentence.)</s><s coords="42,355.01,598.53,133.91,8.90;42,106.36,610.48,382.56,8.90;42,106.36,622.44,382.56,8.90;42,106.36,634.40,208.33,8.90">The resulting word groups can be interpreted at three levels: (1) word sense inventory where each sense of the word under analysis is displayed as a separate network-graph where the nodes are the semantically related words and the edges represent their semantic relationships.</s><s coords="42,318.04,634.40,170.88,8.90;42,106.36,646.35,382.56,8.90;42,106.36,658.31,382.56,8.90;42,106.36,670.26,382.56,8.90;42,106.36,682.22,42.89,8.90">For example, the word 'table' is connected to two networks corresponding to 'furniture' and 'data' senses; (2) sense feature representation characterized by a list of sparse features (which consists of words) ordered by relevance; (3) results of disambiguation in context by highlighting the most discriminative words that caused the prediction.</s><s coords="42,153.73,682.22,335.19,8.90;42,106.36,694.17,121.45,8.90">Words like 'cookie' and 'website' indicate that 'table' refers to a collection of data and not as a piece of furniture.</s></p><p><s coords="43,148.92,229.39,47.81,7.12;43,149.88,305.55,49.58,7.12;43,272.29,232.56,50.47,7.12;43,264.04,304.27,55.72,8.67;43,398.17,234.86,50.02,7.12;43,357.51,305.55,132.26,7.12;43,357.51,315.01,90.76,7.12;43,128.51,334.44,338.25,7.12">(a) BCM <ref type="bibr" coords="43,179.47,229.39,17.26,7.12" target="#b311">[309]</ref> (b) SLIM <ref type="bibr" coords="43,182.19,305.55,17.26,7.12" target="#b319">[317]</ref> (c) PSDD <ref type="bibr" coords="43,305.49,232.56,17.26,7.12" target="#b317">[315]</ref> (d) GA 2 Ms <ref type="bibr" coords="43,302.50,305.81,17.26,7.12" target="#b313">[311]</ref> (e) MGM <ref type="bibr" coords="43,430.93,234.86,17.26,7.12" target="#b318">[316]</ref> (f) Unsupervised Interpretable Word Sense Disambiguation <ref type="bibr" coords="43,431.01,315.01,17.26,7.12" target="#b320">[318]</ref> Figure <ref type="figure" coords="43,151.31,334.44,6.79,7.12" target="#fig_4">16</ref>: Examples of ante-hoc methods for explainability designed to generate self-explainable models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7." coords="43,106.36,375.88,183.70,8.82">Evaluation of methods for explainability</head><p><s coords="43,121.30,395.74,367.62,8.90;43,106.36,407.70,22.42,8.90">The proposal of many methods for explainability pushed authors to focus also on their evaluation.</s><s coords="43,132.18,407.70,356.74,8.90;43,106.36,419.65,251.15,8.90">Different evaluation metrics were proposed and found in the literature as well as different types of evaluation were conducted, as summarised in figure <ref type="figure" coords="43,345.05,419.65,8.30,8.90" target="#fig_8">17</ref>.</s><s coords="43,360.43,419.65,128.49,8.90;43,106.36,431.61,382.56,8.90;43,106.36,443.56,38.19,8.90">A thorough review of these studies led to the identification of two main ways to evaluate methods for explainability, as shown in figure <ref type="figure" coords="43,132.09,443.56,8.30,8.90" target="#fig_8">17</ref>.</s></p><p><s coords="43,121.33,462.88,367.59,9.96;43,131.26,475.59,261.38,8.90">• objective evaluations -it includes research studies that employed objective metrics and automated approaches to evaluate methods for explainability;</s></p><p><s coords="43,121.33,494.03,367.59,9.96;43,131.26,506.73,357.66,8.90;43,131.26,518.68,132.80,8.90">• human-centred evaluations -it contains those studies that evaluated methods for explainability with a human-in-the-loop approach by involving end-users and exploited their feedback or informed judgement.</s></p><p><s coords="43,121.30,538.75,367.62,8.90;43,106.36,550.71,160.11,8.90">The same dual categorisation system was suggested in <ref type="bibr" coords="43,342.39,538.75,15.27,8.90" target="#b63">[64]</ref>, but they named the two classes heuristic-based and user-based metrics.</s><s coords="43,270.50,550.71,218.42,8.90;43,106.36,562.66,382.56,8.90">The former includes quantitative measures which consist of mathematical entities such as, for example, the size of models <ref type="bibr" coords="43,391.88,562.66,15.77,8.90" target="#b25">[26,</ref><ref type="bibr" coords="43,410.95,562.66,12.45,8.90" target="#b87">88,</ref><ref type="bibr" coords="43,426.70,562.66,17.43,8.90" target="#b321">319,</ref><ref type="bibr" coords="43,447.43,562.66,17.43,8.90" target="#b322">320,</ref><ref type="bibr" coords="43,468.17,562.66,16.60,8.90" target="#b270">269]</ref>.</s><s coords="43,106.36,574.62,382.56,8.90;43,106.36,586.57,283.49,8.90">This is a simple explainability indicator and it is based upon the assumption that the bigger the size of a model, the harder it becomes for the users to understand it.</s><s coords="43,396.05,586.57,92.87,8.90;43,106.36,598.53,89.15,8.90">However, this assumption was proved false.</s><s coords="43,200.82,598.53,288.10,8.90;43,106.36,610.48,382.56,8.90;43,106.36,622.44,382.56,8.90;43,106.36,634.40,382.56,8.90;43,106.36,646.35,48.96,8.90">One of the outcomes of the human-centred evaluation study conducted in <ref type="bibr" coords="43,116.98,610.48,16.60,8.90" target="#b87">[88]</ref> was that users found some larger models to be more comprehensible than some smaller models because larger models provided more classification-relevant information and users are unlikely to accept weaker, simpler models when the underlying modelled concept is believed to be complex.</s><s coords="43,159.64,646.35,329.29,8.90;43,106.36,658.31,382.56,8.90;43,106.36,670.26,382.56,8.90;43,106.36,682.22,129.12,8.90">An alternative categorisation was presented in <ref type="bibr" coords="43,347.92,646.35,11.62,8.90" target="#b1">[2]</ref> with three classes: applicationgrounded, functionally-grounded and human-grounded evaluation metrics where functionallygrounded and human-grounded metrics respectively corresponds to the heuristic-based and userbased metrics proposed in <ref type="bibr" coords="43,216.39,682.22,15.27,8.90" target="#b63">[64]</ref>.</s><s coords="43,241.41,682.22,247.51,8.90;43,106.36,694.17,382.56,8.90;44,106.36,538.81,382.56,8.90;44,106.36,550.77,118.74,8.90">Application-grounded metrics assess the quality of machineproduced explanations of data-driven models by comparing the increase in productivity of a few users of these models when following these explanations instead of those produced by human engineers, as done in <ref type="bibr" coords="44,195.35,550.77,15.77,8.90" target="#b25">[26,</ref><ref type="bibr" coords="44,214.31,550.77,7.19,8.90" target="#b1">2]</ref>.</s><s coords="44,230.27,550.77,258.65,8.90;44,106.36,562.72,71.94,8.90">Because they involve humans, they are merged into the humangrounded metrics.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1." coords="44,106.36,585.18,106.38,8.71">Objective evaluations</head><p><s coords="44,121.30,598.53,367.62,8.90;44,106.36,610.48,114.49,8.90">Scholars proposed several metrics to evaluate, formally and objectively, the methods for explainability, listed in table 1.</s><s coords="44,224.73,610.48,264.19,8.90;44,106.36,622.44,382.56,8.90;44,106.36,634.39,382.56,8.90;44,106.36,646.35,19.09,8.90">In the scientific literature, there is consensus that simpler learning techniques, such as linear regression and DTs, can lead to more transparent inferences than more complex techniques, such as neural network, as they are intrinsically self-interpretable <ref type="bibr" coords="44,106.36,646.35,15.27,8.90" target="#b25">[26]</ref>.</s><s coords="44,129.46,646.35,359.46,8.90;44,106.36,658.31,339.37,8.90">However, these simpler techniques usually do not lead to the construction of models with the same level of accuracy than those induced by more complex learning techniques.</s><s coords="44,449.37,658.31,39.55,8.90;44,106.36,670.26,382.56,8.90;44,106.36,682.22,212.50,8.90">The interpretability of these models depends on many factors such as the learning algorithm, the learning architecture and its configuration (hyper-parameters).</s></p><p><s coords="45,121.30,144.23,367.62,8.90;45,106.36,156.19,382.56,8.90;45,106.36,168.14,382.56,8.90;45,106.36,180.10,90.60,8.90">A few sale performance indicators were utilized as a formal metric to assess the increase in productivity of the sales department when two methods of explainability, Explain and Ime <ref type="bibr" coords="45,106.36,168.14,20.75,8.90" target="#b150">[150,</ref><ref type="bibr" coords="45,129.50,168.14,16.60,8.90" target="#b151">151]</ref>, were applied to a complex real-world business problem of business-to-business sales forecasting <ref type="bibr" coords="45,153.03,180.10,20.75,8.90" target="#b323">[321,</ref><ref type="bibr" coords="45,176.20,180.10,16.60,8.90" target="#b324">322]</ref>.</s><s coords="45,200.01,180.10,288.91,8.90;45,106.36,192.05,179.04,8.90">The system was tested for a long period in a real-world company and the sale performance indicators were monitored.</s><s coords="45,289.34,192.05,199.58,8.90;45,106.36,204.01,382.56,8.90;45,106.36,215.96,382.56,8.90;45,106.36,227.92,245.65,8.90">The indicators showed that the forecasts based on the Explain and Ime explanations outperformed initial sales forecasts, which supported the hypothesis that data-driven explanations better facilitate unbiased decision-making than the mental models of sale experts based upon their previous experience.</s><s coords="45,356.64,227.92,132.28,8.90;45,106.36,239.87,382.56,8.90;45,106.36,251.83,196.60,8.90">Two quantitative evaluation metrics to assess the interpretability of methods generating visual explanations of a neural network trained to classify images were presented in <ref type="bibr" coords="45,283.87,251.83,15.27,8.90" target="#b29">[30]</ref>.</s><s coords="45,306.35,251.83,182.57,8.90;45,106.36,263.78,382.56,8.90;45,106.36,275.74,293.60,8.90">The first metric, filter interpretability, considers six types of semantics for CNN filters that must be annotated by humans on testing images at the pixel level: objects, parts, scenes, textures, materials, and colours.</s><s coords="45,404.77,275.74,84.14,8.90;45,106.36,287.70,382.56,8.90;45,106.36,299.65,87.46,8.90">The metric measures the intersection areas between these annotations and the distribution of the activation values of a filter over a heat-map.</s><s coords="45,196.89,299.65,292.03,8.90;45,106.36,311.61,75.30,8.90">If there are overlapping areas, it can be said that the filter represents these semantic concepts.</s><s coords="45,184.86,311.61,304.06,8.90;45,106.36,323.56,382.56,8.90;45,106.36,335.52,266.50,8.90">The second metric, location instability, checks if a CNN locates the relevant parts of the same object, shown in different images, at an almost constant relative distance, as the distances between the parts of an object must be almost invariant.</s><s coords="45,377.65,335.52,111.27,8.90;45,106.36,347.47,382.56,8.90;45,106.36,359.43,382.56,8.90;45,106.36,371.38,382.57,8.90;45,106.36,383.34,382.57,8.90;45,106.36,395.29,382.57,8.90;45,106.36,407.25,382.57,8.90;45,106.36,419.20,382.56,8.90;45,106.36,431.16,153.53,8.90"><ref type="bibr" coords="45,377.65,335.52,21.58,8.90" target="#b257">[256]</ref> proposed to use three automated quantitative metrics, designed to assess the quality of text documents, to evaluate textual explanations automatically generated by methods for explainability: BiLingual Evaluation Understudy (BLEU) that assesses the similarity of sentences based on the average percentage of n-gram matches, Automatic NT Translation Metric (METEOR) that evaluates semantically the similarity between words of sentences by using pre-trained word embeddings and Consensusbased Image Description Evaluation (CIDEr) that compares sentences generated by neural networks to reference explanations written by humans by counting 'Term Frequency-Inverse Document Frequency' weighted n-grams.</s><s coords="45,267.24,431.16,221.68,8.90;45,106.36,443.11,382.56,8.90;45,106.36,455.07,382.56,8.90;45,106.36,467.02,37.36,8.90">A general evaluation metric for post-hoc methods for explainability was presented in <ref type="bibr" coords="45,233.20,443.11,21.58,8.90" target="#b325">[323]</ref> based on the risk of generating unjustified 'counterfactual examples' which are instances that do not depend on previous knowledge but are artefacts of the classifier.</s><s coords="45,146.87,467.02,342.05,8.90">This might happen when a model must predict an area not covered by the training set.</s><s coords="45,106.36,478.98,382.56,8.90;45,106.36,490.93,382.56,8.90;45,106.36,502.89,361.13,8.90">The algorithm that generates these examples applies the minimal perturbation that changes the predicted classes of observation in such a way that it is still connected to the input data and avoid the construction of examples representing situations that are neither feasible nor logical.</s><s coords="45,473.43,502.89,15.49,8.90;45,106.36,514.84,382.56,8.90;45,106.36,526.80,282.60,8.90">The explanations of the predictions made by an underlying model for these observations would not make sense and would not help the understanding of the model's logic.</s></p><p><s coords="45,121.30,550.71,367.62,8.90;45,106.36,562.66,358.54,8.90">Only a few scholars carried out formal comparisons, based on heuristic-based metrics, between different methods for explainability to evaluate their strengths and weaknesses.</s><s coords="45,473.43,562.66,15.49,8.90;45,106.36,574.62,274.48,8.90">The methodologies utilized for the comparisons are (see also table A.15):</s></p><p><s coords="45,121.33,593.80,367.59,9.96;45,131.26,606.50,357.66,8.90;45,131.26,618.45,313.23,8.90">• sensitivity to input perturbation -some features of the input dataset are removed, masked or altered and the explanations generated by a method for explainability from the model trained on both the original and modified inputs are compared;</s></p><p><s coords="45,121.33,637.63,367.59,9.96;45,131.26,650.34,357.66,8.90;45,131.26,662.29,279.18,8.90">• sensitivity to model parameter randomization -the outputs a method for explainability generated from a trained model and another model of the same architecture with some or all parameters replaced by random values are compared;</s></p><p><s coords="45,121.33,681.47,367.59,9.96;45,131.26,694.17,357.66,8.90">• explanation completeness -these approaches check which method generates explanations that describe the inferential process of the underlying model to the highest extent.</s></p><p><s coords="46,131.26,144.23,357.66,8.90;46,131.26,156.19,104.32,8.90">This consists of capturing the highest number of features of the input that affect the decision process of the model.</s></p><p><s coords="46,121.30,176.11,367.62,8.90;46,106.36,188.07,382.56,8.90;46,106.36,200.02,90.48,8.90">The vast majority of these scientific articles compared methods for explainability designed to generate visual explanations of the logic followed by neural networks for the classification of either images or texts.</s><s coords="46,202.55,200.02,286.37,8.90;46,106.36,211.98,382.56,8.90;46,106.36,223.93,228.74,8.90">All these methods produce maps, like heat-maps or feature maps, and the comparison is carried out by measuring the differences in these maps generated before and after the input or the model's parameters were perturbed.</s><s coords="46,338.98,223.93,149.94,8.90;46,106.36,235.89,382.56,8.90;46,106.36,247.84,382.56,8.90;46,106.36,259.80,382.56,8.90;46,106.36,271.75,222.52,8.90">The complete list of the methods that were compared, along with the type of input data that were analysed in these comparisons, is shown in table A.16. <ref type="bibr" coords="46,193.79,247.84,20.75,8.90" target="#b326">[324,</ref><ref type="bibr" coords="46,217.34,247.84,18.26,8.90" target="#b327">325]</ref> compared the saliency maps generated by various methods for visual explainability to either a randomly initialised untrained network or from a copy of the dataset in which the labels were randomly permutated.</s><s coords="46,334.14,271.75,154.78,8.90;46,106.36,283.71,339.72,8.90">The degree of correlation between the saliency maps was measured by calculating Spearman Rank Correlation coefficients.</s><s coords="46,449.99,283.71,38.93,8.90;46,106.36,295.67,382.56,8.90;46,106.36,307.62,382.56,8.90;46,106.36,319.58,64.03,8.90">Similarly, <ref type="bibr" coords="46,106.36,295.67,21.58,8.90" target="#b328">[326]</ref> proposed to vary input images by occluding with zero-valued pixels their portions sharing the same relevance level, according to the saliency maps generated by four gradient-based attribution methods.</s><s coords="46,173.54,319.58,315.38,8.90;46,106.36,331.53,382.56,8.90;46,106.36,343.49,181.62,8.90">The sensitivity of the four methods to this input perturbation was assessed with a formal metric, Sensitivity-n, which quantifies the variation in the output caused when features sharing the same relevance level are removed.</s><s coords="46,291.00,343.49,197.92,8.90;46,106.36,355.44,382.56,8.90;46,106.36,367.40,382.57,8.90;46,106.36,379.35,112.52,8.90">The results of this analysis showed that Occlusion Sensitivity (see Section 6) is the method that identifies the few most important features, in respect with the other methods, because it suffers the faster variations in the output when the most relevant pixels are removed.</s><s coords="46,222.55,379.35,266.37,8.90;46,106.36,391.31,382.56,8.90;46,106.36,403.26,382.56,8.90;46,106.36,415.22,304.69,8.90">Layer-wise Relevance Propagation (LRP) and Sensitivity Analysis were tested in <ref type="bibr" coords="46,165.06,391.31,20.75,8.90" target="#b329">[327,</ref><ref type="bibr" coords="46,188.60,391.31,17.43,8.90" target="#b330">328,</ref><ref type="bibr" coords="46,208.82,391.31,17.43,8.90" target="#b331">329,</ref><ref type="bibr" coords="46,229.03,391.31,18.26,8.90" target="#b332">330]</ref> by removing important words from input documents in text classification problems <ref type="bibr" coords="46,201.79,403.26,20.75,8.90" target="#b329">[327,</ref><ref type="bibr" coords="46,225.70,403.26,18.26,8.90" target="#b330">328]</ref> or replacing the most relevant pixels (in case of images) by randomly sampling new pixel values from a uniform distribution <ref type="bibr" coords="46,367.00,415.22,20.75,8.90" target="#b331">[329,</ref><ref type="bibr" coords="46,390.29,415.22,16.60,8.90" target="#b332">330]</ref>.</s><s coords="46,414.31,415.22,74.61,8.90;46,106.36,427.17,382.57,8.90;46,106.36,439.13,205.83,8.90">The metric used in this study assesses the differences in the model's classification accuracy between the original and the perturbed input points when fed into the model.</s><s coords="46,315.77,439.13,173.15,8.90;46,106.36,451.08,382.56,8.90;46,106.36,463.04,70.01,8.90">Both studies showed that LRP qualitatively and quantitatively provides a better explanation of what made a DNN reach a specific classification prediction.</s><s coords="46,180.30,463.04,308.62,8.90;46,106.36,474.99,187.05,8.90"><ref type="bibr" coords="46,180.30,463.04,21.58,8.90" target="#b333">[331]</ref> used the same evaluation approach of <ref type="bibr" coords="46,358.01,463.04,20.75,8.90" target="#b331">[329,</ref><ref type="bibr" coords="46,381.53,463.04,18.26,8.90" target="#b332">330]</ref> to compare LRP with Occlusion Sensitivity and Sensitivity Analysis.</s><s coords="46,296.63,474.99,192.29,8.90;46,106.36,486.95,250.52,8.90">LRP and Occlusion Sensitivity performed better than Sensitivity Analysis, seconding the findings of <ref type="bibr" coords="46,312.98,486.95,20.75,8.90" target="#b331">[329,</ref><ref type="bibr" coords="46,336.12,486.95,16.60,8.90" target="#b332">330]</ref>.</s><s coords="46,359.93,486.95,128.99,8.90;46,106.36,498.90,382.56,8.90;46,106.36,510.86,295.10,8.90">Input perturbation was also used in <ref type="bibr" coords="46,117.56,498.90,15.77,8.90" target="#b89">[90,</ref><ref type="bibr" coords="46,136.79,498.90,17.43,8.90" target="#b334">332,</ref><ref type="bibr" coords="46,157.67,498.90,13.28,8.90" target="#b91">92]</ref> to test the robustness of several methods that generate visual explanations of the inferential process of DNNs applied to image classification problems.</s><s coords="46,405.51,510.86,83.41,8.90;46,106.36,522.81,382.56,8.90;46,106.36,534.77,61.33,8.90">Robustness concerns variations of an explanation provided by a method with respect to changes in the input leading to that eplanation.</s><s coords="46,170.73,534.77,318.18,8.90;46,106.36,546.72,366.18,8.90">Intuitively, if the input being explained is modified slightly-subtly enough not to change the prediction of the model then the explanation must not change much either <ref type="bibr" coords="46,453.44,546.72,15.27,8.90" target="#b89">[90]</ref>.</s><s coords="46,476.74,546.72,12.17,8.90;46,106.36,558.68,382.56,8.90;46,106.36,570.63,366.82,8.90">On the one hand, <ref type="bibr" coords="46,163.55,558.68,16.60,8.90" target="#b89">[90]</ref> applied a Gaussian noise to input images and measured the relative changes in the output with respect to these perturbations with the Local Lipschitz Continuity metric.</s><s coords="46,476.74,570.63,12.17,8.90;46,106.36,582.59,311.79,8.90">On the other hand, <ref type="bibr" coords="46,168.20,582.59,20.75,8.90" target="#b334">[332,</ref><ref type="bibr" coords="46,191.40,582.59,13.28,8.90" target="#b91">92]</ref> added/subtracted a constant shift to the input images.</s><s coords="46,421.22,582.59,67.71,8.90;46,106.36,594.54,382.56,8.90;46,106.36,606.50,382.56,8.90;46,106.36,617.71,382.57,9.64">Then, <ref type="bibr" coords="46,446.64,582.59,21.58,8.90" target="#b334">[332]</ref> used two metrics to assess the similarity between the heat-maps generated from the original and the perturbed images: Spearman Rank Correlation coefficients and Top-κ intersection which measure the size of the intersection of the κ most important features before and after perturbation.</s><s coords="46,106.36,630.41,382.56,8.90;46,106.36,642.37,382.56,8.90"><ref type="bibr" coords="46,106.36,630.41,16.60,8.90" target="#b91">[92]</ref> instead measure the differences in the model's predictions by checking whether the methods for explainability under analysis satisfy the requirement of 'input invariance' (see Section 5.1).</s><s coords="46,106.36,654.32,382.56,8.90;46,106.36,666.28,382.57,8.90;46,106.36,678.23,207.70,8.90">These studies show that all tested methods (table A.16) are vulnerable even to small perturbations that do not affect the predictions of an underlying model but they significantly change the heat and saliency maps produced by the explainers.</s><s coords="46,318.55,678.23,170.37,8.90;46,106.36,690.19,382.56,8.90;47,106.36,144.23,55.06,8.90">These do not satisfy the 'input invariance' requirement <ref type="bibr" coords="46,157.34,690.19,15.27,8.90" target="#b91">[92]</ref>, that means they do not reflect the sensitivity of a model with respect to input perturbations.</s><s coords="47,165.50,144.23,323.42,8.90;47,106.36,156.19,382.56,8.90;47,106.36,168.14,368.76,8.90">Lastly, <ref type="bibr" coords="47,195.15,144.23,21.58,8.90" target="#b335">[333]</ref> compared the completeness of the explanations generated by seven methods for explainability that interpret the logic of DNNs by calculating the partial derivatives of the output according to the inputs, their perturbation and analysing the network's weights.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2." coords="47,106.36,192.12,126.48,8.71">Human-centred evaluation</head><p><s coords="47,121.30,207.00,367.62,8.90;47,106.36,218.95,235.50,8.90">Explanations are effective when they help end-users to build a complete and correct mental representation of the inferential process of a given model.</s><s coords="47,347.18,218.95,141.74,8.90;47,106.36,230.91,382.56,8.90;47,106.36,242.86,29.60,8.90">Many scientific articles focused on testing the degree of explainability of one or multiple methods, with a human-in-the-loop approach.</s><s coords="47,139.90,242.86,246.73,8.90">These experiments involved human participants of two kinds.</s><s coords="47,390.59,242.86,98.33,8.90;47,106.36,254.82,382.57,8.90;47,106.36,266.77,382.56,8.90;47,106.36,278.73,26.28,8.90">On the one hand, people randomly selected from the lay public and without any prior technical/domain knowledge who were asked to interact with one or more explanatory tools and give feedback by filling questionnaires.</s><s coords="47,138.60,278.73,350.32,8.90;47,106.36,290.68,382.56,8.90;47,106.36,302.64,78.32,8.90">On the other hand, domain experts who were asked to give informed opinions on the explanations produced by these methods and verify the consistency of the explanations with the domain knowledge.</s><s coords="47,187.75,302.64,301.17,8.90;47,106.36,314.59,264.60,8.90">Examined scientific articles can be clustered into two categories, depending on the nature of questions administered to people (see table <ref type="table" coords="47,345.50,314.59,4.24,8.90" target="#tab_1">A</ref> <ref type="bibr" coords="47,349.74,314.59,16.97,8.90">.14)</ref>.</s><s coords="47,374.07,314.59,114.85,8.90;47,106.36,326.55,382.57,8.90;47,106.36,338.50,277.38,8.90">Qualitative studies are based upon open-ended questions aimed at achieving deeper insights whereas quantitative studies make use of close-ended questions that can be easily analyzed statistically.</s><s coords="47,388.19,338.50,100.73,8.90;47,106.36,350.46,382.56,8.90;47,106.36,362.42,144.70,8.90">The first methods for explainability that were tested by human users are those generating textual explanations for Expert Systems (ES) in the 90's <ref type="bibr" coords="47,207.02,362.42,20.75,8.90" target="#b336">[334,</ref><ref type="bibr" coords="47,230.30,362.42,16.60,8.90" target="#b337">335]</ref>.</s><s coords="47,254.26,362.42,234.66,8.90;47,106.36,374.37,382.56,8.90;47,106.36,386.33,25.74,8.90">These researches aimed at collecting pieces of evidence on whether and how explanations could enhance the user's confidence in the decisions proposed by an ES.</s><s coords="47,134.32,386.33,354.60,8.90;47,106.36,398.28,382.56,8.90;47,106.36,410.24,93.91,8.90">Scholars carried out several human-centred evaluations over the years to assess the effects of textual explanations on end-users to other types of systems employing ML models, such as learning systems <ref type="bibr" coords="47,176.19,410.24,20.06,8.90" target="#b338">[336]</ref>.</s><s coords="47,204.34,410.24,284.58,8.90;47,106.36,422.19,195.02,8.90">The impact of explanations on the reliability, trust and reliance of endusers on automated systems was explored in <ref type="bibr" coords="47,282.28,422.19,15.27,8.90" target="#b53">[54]</ref>.</s><s coords="47,304.32,422.19,184.60,8.90;47,106.36,434.15,382.56,8.90;47,106.36,446.10,66.07,8.90">The participants were presented with photos of the Fort Sill terrain where the presence of a camouflaged soldier was indicated by an automated decision system.</s><s coords="47,176.13,446.10,312.78,8.90;47,106.36,458.06,382.56,8.90;47,106.36,470.01,270.98,8.90">Initially, they considered the inference produced by the system to be trustworthy and reliable but, after observing the system making errors, they distrusted even reliable aids, unless an explanation was provided regarding why the system failed.</s><s coords="47,380.32,470.01,108.60,8.90;47,106.36,481.97,382.56,8.90;47,106.36,493.92,382.56,8.90;47,106.36,505.88,56.71,8.90">In conclusion, explanations of these errors increased the trust of the participants in the automated system who were asked to estimate their perceived reliability on a 9-point Likert-format scale, ranging from 'not very well' to 'very well'.</s><s coords="47,166.37,505.88,322.55,8.90;47,106.36,517.83,382.56,8.90;47,106.36,529.79,95.60,8.90">The influence of explanation factors over the capacity of end-users to understand and develop a mental representation of the internal reference process of a model was investigated in <ref type="bibr" coords="47,116.98,529.79,20.75,8.90" target="#b339">[337,</ref><ref type="bibr" coords="47,140.60,529.79,17.43,8.90" target="#b340">338,</ref><ref type="bibr" coords="47,160.90,529.79,17.43,8.90" target="#b341">339,</ref><ref type="bibr" coords="47,181.20,529.79,16.60,8.90" target="#b342">340]</ref>.</s><s coords="47,206.18,529.79,282.74,8.90;47,106.36,541.74,264.61,8.90">The explanations produced by the analysed methods for explainability consisted of graphical representation of the most relevant features.</s><s coords="47,374.18,541.74,114.74,8.90;47,106.36,553.70,382.56,8.90;47,106.36,565.65,270.33,8.90"><ref type="bibr" coords="47,374.18,541.74,20.75,8.90" target="#b339">[337,</ref><ref type="bibr" coords="47,397.46,541.74,18.26,8.90" target="#b340">338]</ref> showed that users of simulation-based training systems with virtual players prefer short explanations to long ones where length is defined by the number of elements in an explanation.</s><s coords="47,379.64,565.65,109.28,8.90;47,106.36,577.61,129.75,8.90">An element can be a goal or an event of the training program.</s><s coords="47,239.13,577.61,249.79,8.90;47,106.36,589.56,382.56,8.90;47,106.36,601.52,382.56,8.90;47,106.36,613.47,201.97,8.90">This was tested by showing to the participants four explanation alternatives of different length (they contained either one or two elements), in the form of DTs, for each action of the virtual players and asked to indicate which alternative they considered the most useful for increasing end-user understanding.</s><s coords="47,311.42,613.47,177.50,8.90;47,106.36,625.43,382.56,8.90;47,106.36,637.38,109.16,8.90">The influence factors analysed in <ref type="bibr" coords="47,445.49,613.47,21.58,8.90" target="#b341">[339]</ref> were the number of independent variables of a trained linear regression model and the values of these variables for each instance.</s><s coords="47,219.57,637.38,269.35,8.90;47,106.36,649.34,258.44,8.90">Some participants were randomly assigned to check either a model that uses only two features or a model that uses eight features.</s><s coords="47,370.93,649.34,117.99,8.90;47,106.36,661.29,271.88,8.90">The coefficients of the linear regression model were also presented only to half of the participants.</s><s coords="47,381.23,661.29,107.69,8.90;47,106.36,673.25,382.56,8.90;47,106.36,685.20,26.83,8.90">Then, the participants were asked to estimate what would be the output of the model and to correct it in case it was not accurate.</s><s coords="47,136.69,685.20,352.23,8.90;48,106.36,144.23,382.56,8.90;48,106.36,156.19,157.72,8.90">As expected, users can easily simulate models with a small number of features whereas, surprisingly, displaying model internal parameters can hamper their ability to notice unusual inputs and correct inaccurate predictions.</s><s coords="48,268.21,156.19,220.71,8.90;48,106.36,168.14,230.69,8.90">The method for explainability tested in <ref type="bibr" coords="48,427.90,156.19,21.58,8.90" target="#b342">[340]</ref> listed the two most relevant features and the prediction of the model.</s><s coords="48,339.99,168.14,148.93,8.90;48,106.36,180.10,382.56,8.90;48,106.36,192.05,115.40,8.90">The prediction was coded with a solid colour taken from a scale running from red, representing the most negative possible outcome, to green, the most positive one.</s><s coords="48,225.72,192.05,263.20,8.90;48,106.36,204.01,382.56,8.90;48,106.36,215.96,382.56,8.90;48,106.36,227.92,34.59,8.90">Participants were asked to interact with the system for two weeks at the end of which they were interviewed to collect their feedback and to check whether they gained some insight on the logic of the model to be explained by the explanatory system under analysis.</s><s coords="48,144.80,227.92,344.12,8.90;48,106.36,239.87,382.56,8.90;48,106.36,251.83,321.16,8.90"><ref type="bibr" coords="48,144.80,227.92,21.58,8.90" target="#b102">[103]</ref> studied the explainability of an interactive interface, called Prospector, containing a set of diagnostic tools that allows end-user, via visual and numerical representations, to understand how the features of a dataset affect the prediction of a model overall.</s><s coords="48,431.07,251.83,57.84,8.90;48,106.36,263.78,382.56,8.90;48,106.36,275.74,65.65,8.90">Users can also inspect a specific instance to check its prediction and can weak feature values to see how the model responds.</s><s coords="48,176.13,275.74,312.79,8.90;48,106.36,287.70,382.56,8.90;48,106.36,299.65,109.32,8.90">A team of data-scientists was asked to interact with this tool to debug a set of models designed to predict if a patient is at risk of developing diabetes by using a database of electronic medical records.</s><s coords="48,221.32,299.65,267.60,8.90;48,106.36,311.61,344.71,8.90">The human experiment consists of interviewing, at the end of the experiment, the data scientists on whether they feel that it was beneficial for their work.</s><s coords="48,454.05,311.61,34.87,8.90;48,106.36,323.56,382.56,8.90;48,106.36,335.52,132.27,8.90">Methods for evaluating the interpretability of data-driven models with a human-in-the-loop approach were proposed in <ref type="bibr" coords="48,155.28,335.52,20.75,8.90" target="#b343">[341,</ref><ref type="bibr" coords="48,178.35,335.52,17.43,8.90" target="#b344">342,</ref><ref type="bibr" coords="48,198.12,335.52,17.43,8.90" target="#b345">343,</ref><ref type="bibr" coords="48,217.88,335.52,16.60,8.90" target="#b346">344]</ref>.</s><s coords="48,241.67,335.52,247.25,8.90;48,106.36,347.47,247.86,8.90">The approach proposed in <ref type="bibr" coords="48,347.24,335.52,21.58,8.90" target="#b343">[341]</ref> identifies some proxies which consist of other, simpler models inherently more explainable.</s><s coords="48,358.99,347.47,129.93,8.90;48,106.36,359.43,361.29,8.90">For example, a DT is inherently more interpretable than DNNs and the former methods can be used to explain the latter.</s><s coords="48,473.43,359.43,15.49,8.90;48,106.36,371.38,382.56,8.90;48,106.36,383.34,382.56,8.90;48,106.36,395.29,73.23,8.90">The authors presented to participants a list of the coefficients of the features used by each proxy and a graphical depiction of its structure (in the form of a DT) and they asked them to identify the correct prediction.</s><s coords="48,183.25,395.29,305.67,8.90;48,106.36,407.25,382.56,8.90;48,106.36,419.20,382.56,8.90;48,106.36,431.16,382.56,8.90;48,106.36,443.11,382.57,8.90;48,106.36,455.07,382.56,8.90;48,106.36,467.02,382.56,8.90;48,106.36,478.98,382.56,8.90;48,106.36,490.93,142.46,8.90"><ref type="bibr" coords="48,183.25,395.29,21.58,8.90" target="#b346">[344]</ref> proposed instead to assess the comprehensibility of DTs by asking the participants to perform the following tasks: (I) classify a data-point according to the classification tree, (II) explain the classification of a data-point by pointing out which attributes' values must be changed or retained to modify the instance's class, (III) validate a part of the classification tree by asking the participant to check whether a statement about the domain is confirmed/rejected by the tree, (IV) discover new knowledge by finding a property (attribute-value pair) that is unusual for instances from one class, (V) rate the classification tree by giving a subjective opinion on the comprehensibility of the tree and, lastly, (VI) compare two classification tree by saying which one is more comprehensible.</s><s coords="48,252.72,490.93,236.20,8.90;48,106.36,502.89,382.56,8.90;48,106.36,514.84,60.04,8.90">The two studies presented in <ref type="bibr" coords="48,370.45,490.93,20.75,8.90" target="#b344">[342,</ref><ref type="bibr" coords="48,393.97,490.93,18.26,8.90" target="#b345">343]</ref> analysed the interpretability of predictive models by asking participants to interact with them and fill self-reporting questionnaires.</s><s coords="48,170.02,514.84,318.90,8.90;48,106.36,526.80,37.92,8.90">The surveys carried out in <ref type="bibr" coords="48,277.12,514.84,21.58,8.90" target="#b344">[342]</ref> aimed at comparing six supervised learning algorithms.</s><s coords="48,150.26,526.80,338.66,8.90;48,106.36,538.75,347.99,8.90">These were ranked in order of preference based on the subjective quantification of understandability obtained from the self-reporting questionnaires filled by participants.</s><s coords="48,458.12,538.75,30.79,8.90;48,106.36,550.71,382.56,8.90;48,106.36,562.66,382.56,8.90;48,106.36,574.62,300.30,8.90">Pairs of models trained on the same dataset were generated and participants were asked to rate them on a scale where one extreme is 'the first model is the most understandable' to the other extreme 'the second model is the most understandable' via increasingly positive grades.</s><s coords="48,410.74,574.62,78.18,8.90;48,106.36,586.57,382.56,8.90;48,106.36,598.53,382.56,8.90;48,106.36,610.48,17.42,8.90">In the study carried out in <ref type="bibr" coords="48,133.31,586.57,20.06,8.90" target="#b345">[343]</ref>, participants were required to evaluate the explanations of a credit model, trained to accept or reject loan applications, consisting of IF-THEN rules and displayed as a decision tree.</s><s coords="48,128.01,610.48,360.91,8.90;48,106.36,622.44,382.56,8.90;48,106.36,634.40,382.56,8.90;48,106.36,646.35,116.21,8.90">They were asked to predict the model's outcome on a new loan application, answer a few yes/no questions such as "Does the model accept all the people with an age above 60?" and rate, for each question, the degree of confidence in the answer on a scale from 1 (Totally not confident) to 5 (Very confident).</s><s coords="48,227.53,646.35,261.39,8.90;48,106.36,658.31,382.56,8.90;48,106.36,670.26,196.43,8.90">The authors measured, besides the answer confidence, other two variables about task performance: accuracy, quantified as the percentage of correct answers, and the time in seconds spent to answer the questions.</s><s coords="48,305.80,670.26,183.13,8.90;48,106.36,682.22,382.56,8.90;48,106.36,694.17,256.45,8.90">The effectiveness of why-oriented explanation systems in debugging a naïve Bayes learning model for text classification and in context-aware applications were respectively tested in <ref type="bibr" coords="48,266.63,694.17,20.75,8.90" target="#b347">[345,</ref><ref type="bibr" coords="48,290.19,694.17,13.28,8.90" target="#b59">60]</ref> and <ref type="bibr" coords="48,323.47,694.17,20.75,8.90" target="#b348">[346,</ref><ref type="bibr" coords="48,347.03,694.17,11.83,8.90" target="#b76">77]</ref>.</s><s coords="48,366.84,694.17,122.08,8.90;49,106.36,144.23,382.57,8.90;49,106.36,156.19,382.56,8.90;49,106.36,168.14,73.47,8.90"><ref type="bibr" coords="48,366.84,694.17,20.75,8.90" target="#b347">[345,</ref><ref type="bibr" coords="48,390.40,694.17,13.28,8.90" target="#b59">60]</ref> asked participants to train a prototype system, based on a Multinomial naïve Bayes classifier, that can learn from users how to automatically classify emails by manually moving a few of them from the inbox into an appropriate folder.</s><s coords="49,182.88,168.14,306.04,8.90;49,106.36,180.10,96.72,8.90">The system was subsequently run over a new set of messages, some of which were wrongly classified.</s><s coords="49,206.09,180.10,282.83,8.90;49,106.36,192.05,382.56,8.90;49,106.36,204.01,382.56,8.90;49,106.36,215.96,336.72,8.90">The participants had to debug the system by asking 'why' questions via an interactive explanation tool producing textual answers and by giving two types of feedback: some participants could label the most relevant feature (words) whereas the others could only provide more labelled instances (moving more messages to the appropriate folders).</s><s coords="49,446.92,215.96,42.00,8.90;49,106.36,227.92,382.56,8.90">At the end of the session, the participants filled a questionnaire to express their opinions on the prototype.</s><s coords="49,106.36,239.87,382.56,8.90;49,106.36,251.83,382.56,8.90">In the experiment run in <ref type="bibr" coords="49,203.64,239.87,20.75,8.90" target="#b348">[346,</ref><ref type="bibr" coords="49,226.61,239.87,11.83,8.90" target="#b76">77]</ref>, participants were invited to interact with a model that predicts whether a person is doing physical exercise or not, based on the body temperature and the pace.</s><s coords="49,106.36,263.78,382.56,8.90;49,106.36,275.74,346.92,8.90">They were shown with some examples of inputs and outputs accompanied by graphical (in the form of decision trees) and textual explanations on the logic followed by the model.</s><s coords="49,459.41,275.74,29.51,8.90;49,106.36,287.70,382.56,8.90;49,106.36,298.90,382.56,9.64;49,106.36,310.86,382.57,9.64;49,106.36,322.81,121.32,9.53">Half of the participants were presented with why explanations, such as "Output classified as Not Exercising, because Body Temperature &lt; 37 and Pace &lt; 3" whereas the other half were presented with why not explanations, such as "Output not classified as Exercising, because Pace &lt; 3, but not Body Temperature &gt; 37".</s><s coords="49,233.62,323.56,255.30,8.90;49,106.36,335.52,382.56,8.90;49,106.36,347.47,256.43,8.90">Subsequently, the participants had to fill two questionnaires to check their understanding by asking questions how the system works and to give feedback on the explanations in terms of understandability, trust and usefulness.</s><s coords="49,367.32,347.47,121.60,8.90;49,106.36,359.43,370.83,8.90">Both questionnaires contained a mix of open and close questions, where the close ones consisted of a 7-point Likert scale.</s><s coords="49,481.73,359.43,7.19,8.90;49,106.36,371.38,382.56,8.90;49,106.36,383.34,137.53,8.90">A qualitative evaluation of the interpretability of the Mind the Gap Model (MGM) method for explainability was gathered in <ref type="bibr" coords="49,219.82,383.34,20.06,8.90" target="#b318">[316]</ref>.</s><s coords="49,247.52,383.34,241.40,8.90;49,106.36,395.29,90.25,8.90">MGM clustered the data of an input dataset according to the most relevant features.</s><s coords="49,200.10,395.29,288.82,8.90;49,106.36,407.25,382.56,8.90;49,106.36,419.20,127.86,8.90">The participants were presented with the raw data and the data clustered with MGM and k-means and were asked to write a 2-3 sentence executive summary of each data representation within 5 minutes.</s><s coords="49,237.24,419.20,251.67,8.90;49,106.36,431.16,382.56,8.90;49,106.36,443.11,118.48,8.90">They all found impossible to summarise the raw data, not being able to complete the task in the given amount of time, but they managed to do so on the data with clustered MGM and k-means.</s><s coords="49,227.87,443.11,261.05,8.90;49,106.36,455.07,382.56,8.90;49,106.36,467.02,382.56,8.90;49,106.36,478.98,78.18,8.90">To test Bayesian Case Model (BCM), <ref type="bibr" coords="49,379.11,443.11,21.58,8.90" target="#b311">[309]</ref> asked the participants to assign sixteen recipes, described only by a set of ingredients, to the right category (so a recipe of cookies had to be classified under 'cookie') and then they counted how many of them were correctly classified.</s><s coords="49,189.74,478.98,299.18,8.90;49,106.36,490.93,253.58,8.90">BCM was compared with Latent Dirichlet Allocation (LDA), a clustering approach based on extracting similar characteristics in the data.</s><s coords="49,363.44,490.93,125.48,8.90;49,106.36,502.89,184.17,8.90">The need for XAI in Intelligent Tutoring Systems (ITS) was explored in <ref type="bibr" coords="49,266.45,502.89,20.06,8.90" target="#b349">[347]</ref>.</s><s coords="49,293.50,502.89,195.42,8.90;49,106.36,514.84,382.56,8.90;49,106.36,526.80,131.93,8.90">The participants in the study were asked to use an ITS that provided tools to explore and explain an algorithm solving constraint satisfaction problems in an interactive simulation.</s><s coords="49,241.36,526.80,247.56,8.90;49,106.36,538.75,83.62,8.90">The participants were instructed by the exploration tool with a textual hint message.</s><s coords="49,193.04,538.75,295.88,8.90;49,106.36,550.71,382.56,8.90;49,106.36,562.66,382.56,8.90;49,106.36,574.62,382.56,8.90;49,106.36,586.57,251.26,8.90">They could select to have the hint explained by the explanatory tool which was also designed to solicit their suggestions on the explanations they would like to see for each hint by presenting them a checkbox list with the following options: 'why the system gave this hint', 'how the system chose this hint', 'some other explanation about this hint' (followed by an open-text field) and 'I do not want an explanation for this hint.'</s><s coords="49,121.30,610.48,367.62,8.90;49,106.36,622.44,136.08,8.90">Many scholars proposed human-centred evaluation approaches for methods for explainability generating visual explanations.</s><s coords="49,245.47,622.44,243.45,8.90;49,106.36,634.40,382.56,8.90;49,106.36,646.35,51.19,8.90">The participants selected in the study in <ref type="bibr" coords="49,405.14,622.44,21.58,8.90" target="#b350">[348]</ref> were presented with whole images misclassified by a DNN and the visual explanations generated by LIME and MMD-critic.</s><s coords="49,161.63,646.35,327.29,8.90;49,106.36,658.31,382.56,8.90">For example, a photo of a Jeep with a red-cross was wrongly classified as an ambulance and the visual explanations show the red-cross with the rest of the image greyed out.</s><s coords="49,106.36,670.26,382.56,8.90;49,106.36,682.22,382.56,8.90;49,106.36,694.17,19.09,8.90">Participants were asked to say whether the class predicted by the model was nonetheless relevant where the possible answers to questions like 'Is the label Red-Cross relevant?' were 'yes' and 'no'.</s><s coords="49,130.00,694.17,285.14,8.90">A similar experiment was carried out in <ref type="bibr" coords="49,293.92,694.17,21.58,8.90" target="#b351">[349]</ref> to test GAN Dissection.</s><s coords="49,419.69,694.17,69.23,8.90;50,106.36,144.23,382.56,8.90;50,106.36,156.19,264.04,8.90">Participants were presented with images reporting highlighted patches showing the most highly-activating regions for each unit at each intermediate convolutional layer of a DNN.</s><s coords="50,373.64,156.19,115.28,8.90;50,106.36,168.14,382.56,8.90;50,106.36,180.10,49.41,8.90">Each layer was aligned with a semantic and were given labels across a range of objects, parts, scenes, textures, materials, and colours.</s><s coords="50,161.01,180.10,327.91,8.90;50,106.36,192.05,382.56,8.90;50,106.36,204.01,34.33,8.90">For example, if a DNN was trained to recognize a list of object in input images, like flowers and cars, the semantic consists of this list and images showing flowers were labelled 'flower'.</s><s coords="50,144.94,204.01,343.98,8.90;50,106.36,215.96,143.17,8.90">The participants were asked to say if the highlighted patches were pertinent to the label by answering yes/no questions.</s><s coords="50,255.64,215.96,233.27,8.90;50,106.36,227.92,281.62,8.90">The capacity of Anchors and LIME in helping end-users forecasting the predictions of an image classifier was tested in <ref type="bibr" coords="50,363.91,227.92,20.06,8.90" target="#b140">[140]</ref>.</s><s coords="50,393.49,227.92,95.43,8.90;50,106.36,239.87,382.56,8.90;50,106.36,251.83,355.02,8.90">Participants were asked to predict the output class assigned by the classifier to ten random test instances before and ten instances after seeing two rounds of explanations generated by either Anchors or LIME.</s><s coords="50,464.20,251.83,24.72,8.90;50,106.36,263.78,382.56,8.90;50,106.36,275.74,39.83,8.90">A few scholars conducted human-centred studies to test the interpretability of the heat-maps generated with LRP.</s><s coords="50,148.64,275.74,340.28,8.90;50,106.36,287.70,382.56,8.90;50,106.36,299.65,382.56,8.90;50,106.36,311.61,80.19,8.90"><ref type="bibr" coords="50,148.64,275.74,20.75,8.90" target="#b352">[350,</ref><ref type="bibr" coords="50,171.85,275.74,18.26,8.90" target="#b353">351]</ref> applied it respectively to test models built with Fisher vector classifiers for object recognition in images and to SVMs, trained on videos, to understand and interpret action recognition and to check whether LRP allows identifying in which point of the video the important action happens.</s><s coords="50,190.79,311.61,298.13,8.90;50,106.36,323.56,382.56,8.90;50,106.36,335.52,382.56,8.90;50,106.36,347.47,382.56,8.90;50,106.36,359.43,39.84,8.90">By visually inspecting the heat-maps, the authors of the two studies could show a possible weakness of the underlying classifiers by looking at the regions highlighted in the heat-maps and examining whether they were relevant for the recognition of an object (or at least part of it) in images and of the areas of video frames showing the actions performed in the video.</s><s coords="50,150.94,359.43,337.98,8.90;50,106.36,371.38,34.59,8.90">Similarly, <ref type="bibr" coords="50,193.05,359.43,21.58,8.90" target="#b354">[352]</ref> employed LRP with DNNs for electroencephalography (EEG) data analysis.</s><s coords="50,146.63,371.38,342.29,8.90;50,106.36,383.34,214.99,8.90">The predictions of the trained DNNs are transformed with LRP, for every trial, into heat-maps indicating the relevance of each data point.</s><s coords="50,324.79,383.34,164.13,8.90;50,106.36,395.29,382.56,8.90;50,106.36,407.25,180.70,8.90">The relevance information can be plotted as a scalp topography that can be visually inspected by experts to check if there are neurophysiologically plausible patterns in the EEG data.</s><s coords="50,290.15,407.25,198.77,8.90;50,106.36,419.20,382.56,8.90;50,106.36,431.16,139.56,8.90"><ref type="bibr" coords="50,290.15,407.25,21.58,8.90" target="#b355">[353]</ref> used LRP for computing the contribution of contextual words to arbitrary hidden states in the attention-based encoder-decoder framework of neural machine translation (NMT).</s><s coords="50,248.44,431.16,240.48,8.90;50,106.36,443.11,382.56,8.90;50,106.36,455.07,91.37,8.90">As per the previous studies, the authors checked if the translation made by the NMT (Japanese-English) were right or wrong and what types of errors were made more frequently.</s><s coords="50,203.12,455.07,285.80,8.90;50,106.36,467.02,382.56,8.90;50,106.36,478.98,382.56,8.90;50,106.36,490.93,128.15,8.90">The participants in <ref type="bibr" coords="50,282.63,455.07,21.58,8.90" target="#b133">[133]</ref> were asked to interact with explAIner which showed them explanations generated by LRP and Saliency Analysis of both a simple and a complex network trained on the MNIST dataset and, in the meantime, to communicate their thoughts and actions by 'thinking aloud'.</s><s coords="50,238.45,490.93,221.62,8.90">The sessions were audio-recorded and screen-captured.</s><s coords="50,464.01,490.93,24.91,8.90;50,106.36,502.89,382.56,8.90;50,106.36,514.84,76.11,8.90">At the end of the sessions, participants were also interviewed to provide qualitative feedback on the overall experience.</s><s coords="50,187.85,514.84,301.07,8.90;50,106.36,526.80,382.56,8.90;50,106.36,538.75,382.56,8.90;50,106.36,550.71,53.64,8.90">Another method for explainability producing visual explanations as maps, GradCam, was applied to multivariate time series from photovoltaic power plants that were fed into a neural network to forecast the energy production of these plants in different weather conditions <ref type="bibr" coords="50,135.92,550.71,20.06,8.90" target="#b356">[354]</ref>.</s><s coords="50,163.06,550.71,325.86,8.90;50,106.36,562.66,382.57,8.90;50,106.36,574.62,151.32,8.90">GradCam was used to explain which features, such as environmental temperature, wind bearing or humidity, or any combinations of these features were responsible, at different time intervals, for a given prediction.</s><s coords="50,263.02,574.62,225.90,8.90;50,106.36,586.57,382.56,8.90">The results showed that GradCam was able to visualise the network's attention over the time dimension and the features of multivariate time series data.</s><s coords="50,106.36,598.53,382.56,8.90;50,106.36,610.48,382.56,8.90;50,106.36,622.44,68.01,8.90"><ref type="bibr" coords="50,106.36,598.53,21.58,8.90" target="#b357">[355]</ref> compared other three methods, namely Activation Maximization, Sampling Unit and Linear Combination, designed to produce explanations as heat-maps of the most relevant features of the input images.</s><s coords="50,177.41,622.44,311.51,8.90;50,106.36,634.40,160.09,8.90">Activation Maximization consists of selecting the input features that maximise the activation of a single hidden neuron.</s><s coords="50,269.72,634.40,219.20,8.90;50,106.36,646.35,349.52,8.90">Sampling Unit consists of setting the value of a neuron to one and calculating the probability with which each sample is assigned to a class.</s><s coords="50,462.17,646.35,26.75,8.90;50,106.36,658.31,382.56,8.90;50,106.36,670.26,89.79,8.90">Lastly, Linear Combination consists of choosing the largest weights of the connections between neurons of two adjacent layers.</s><s coords="50,199.21,670.26,289.71,8.90;50,106.36,682.22,344.45,8.90">The authors did not use any objective measure to compare these methods but a qualitative visual inspection of the heat-maps and the connections between them.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8." coords="51,106.36,144.18,169.55,8.82">Final remarks and recommendations</head><p><s coords="51,121.30,165.15,367.62,8.90;51,106.36,177.11,382.56,8.90;51,106.36,189.06,284.66,8.90">Research on methods to explain the inner logic either of a learning algorithm, a model induced from it, or a knowledge-based approach for inference is now generally recognized as a core area of AI and is referred to as eXplainable Artificial Intelligence (XAI).</s><s coords="51,393.08,189.06,95.84,8.90;51,106.36,201.02,382.56,8.90;51,106.36,212.98,289.17,8.90">Note that other common terms exist, such as 'Interpretable Machine Learning', but with XAI we would like to emphasise the wider applicability of this emerging and fascinating field of research.</s><s coords="51,398.61,212.98,90.31,8.90;51,106.36,224.93,382.56,8.90">Several scientific studies are published every year, with many workshops and conferences organised around the world.</s><s coords="51,106.36,236.89,203.94,8.90">to propose novel methods and disseminate findings.</s><s coords="51,313.28,236.89,175.64,8.90;51,106.36,248.84,246.22,8.90">Although this has led to the production of an abundance of knowledge, unfortunately, this is very scattered.</s><s coords="51,355.72,248.84,133.20,8.90;51,106.36,260.80,353.76,8.90">This systematic review attempted to fill this gap by organising this vast knowledge in a structured and hierarchical way.</s><s coords="51,466.22,260.80,22.69,8.90;51,106.36,272.75,382.56,8.90;51,106.36,284.71,234.96,8.90">Some scholars already tried to classify scientific studies but, given a large amount of literature, they decided to focus only on a specific aspect of explainability.</s><s coords="51,344.40,284.71,144.52,8.90;51,106.36,296.66,382.56,8.90;51,106.36,308.62,382.56,8.90;51,106.36,320.57,382.56,8.90;51,106.36,332.53,382.56,8.90;51,106.36,344.48,27.31,8.90">To the best of our knowledge, this is the first attempt to review a wider body of literature that has led to the definition of four clusters: (I) reviews focused on specific aspects of XAI, (II) the theories and notions related to the concept of explainability, (III) the methods aimed at explaining the inferential process of data-driven and knowledge-based modelling approaches, and (IV) the ways to evaluate the methods for explainability.</s><s coords="51,136.60,344.48,352.32,8.90;51,106.36,356.44,262.48,8.90">Many studies within XAI have focused on improving the quality and widening the variety of explanations for several types of learning approaches with data.</s><s coords="51,371.87,356.44,117.05,8.90;51,106.36,368.39,382.56,8.90;51,106.36,380.35,382.56,8.90;51,106.36,392.30,22.96,8.90">Since the early '80s and '90s, with research concerned only with textual explanations, to nowadays, scholars have been targeting new explanation formats such as visual aids, rules, numbers and different combinations of these.</s><s coords="51,133.13,392.30,355.78,8.90;51,106.36,404.26,382.56,8.90;51,106.36,416.21,57.84,8.90">For each of these formats, scholars designed, deployed and tested several solutions, such as saliency masks, attention maps, heat-maps, feature maps, as well as graphs, rules sets, trees and dialogues.</s><s coords="51,168.16,416.21,320.76,8.90;51,106.36,428.17,382.56,8.90;51,106.36,440.12,128.13,8.90">These advances were aimed at meeting the needs of different types of end-users operating in various fields, such as laypeople, doctors and lawyers, and adapting explanations to their domains of application.</s><s coords="51,240.06,440.12,248.86,8.90;51,106.36,452.08,382.56,8.90;51,106.36,464.03,262.20,8.90">Additionally, scholars widened their research horizons by incorporating the knowledge developed in other fields, like Psychology, Philosophy and Social Sciences, into the design of the novel methods for explainability.</s><s coords="51,373.12,464.03,115.80,8.90;51,106.36,475.99,382.57,8.90;51,106.36,487.94,21.87,8.90">The goal was to improve the structure, efficiency and efficacy on people's understanding of automatically generated explanations.</s><s coords="51,134.03,487.94,354.89,8.90;51,106.36,499.90,382.56,8.90;51,106.36,511.85,103.65,8.90">All this research has produced many definitions of explainability and identified several notions related to it, such as interpretability, understandability, comprehensibility and justifiability, just to mention a few.</s><s coords="51,214.85,511.85,274.07,8.90;51,106.36,523.81,131.37,8.90">Coupled to these notions, different objective metrics have also been produced for their measurement.</s><s coords="51,242.15,523.81,246.77,8.90;51,106.36,535.77,382.56,8.90">Despite the large number and variety of methods and metrics for explainability proposed so far, there are still important scientific issues that must be tackled.</s><s coords="51,106.36,547.72,382.56,8.90;51,106.36,559.68,382.56,8.90;51,106.36,571.63,126.47,8.90">Firstly, there is no agreement among scholars on what an explanation exactly is and which are the salient properties that should be considered to make it effective and understandable for endusers, in particular non-experts.</s><s coords="51,236.59,571.63,252.33,8.90;51,106.36,583.59,382.56,8.90;51,106.36,595.54,156.66,8.90">Secondly, the construct of explainability is a concept borrowed from Psychology, since it is strictly connected to humans, and it is also linked to other constructs such as trust, transparency and privacy.</s><s coords="51,266.81,595.54,222.11,8.90;51,106.36,607.50,251.96,8.90">Thirdly, this concept has been invoked in several fields, such as Physics, Mathematics, Social Sciences and Medicine.</s><s coords="51,364.15,607.50,124.77,8.90;51,106.36,619.45,197.92,8.90">All this make its formalisation and operationalisation a non-trivial research task.</s><s coords="51,308.39,619.45,180.53,8.90;51,106.36,631.41,128.65,8.90">This holds true for every explanation format, either textual, visual, numerical.</s><s coords="51,238.44,631.41,250.48,8.90;51,106.36,643.36,382.56,8.90;51,106.36,655.32,52.58,8.90">The same can be said for rule-based explanations, in particular for those that are generated after a model has been induced by employing deep-learning neural networks.</s><s coords="51,164.02,655.32,324.90,8.90;51,106.36,667.27,293.09,8.90">In accordance with <ref type="bibr" coords="51,244.31,655.32,10.58,8.90" target="#b0">[1]</ref>, we believe that scholars have produced enough material that can be used to construct a generally applicable framework for XAI.</s><s coords="51,402.60,667.27,86.32,8.90;51,106.36,679.23,382.56,8.90;51,106.36,691.18,382.56,8.90;52,106.36,144.23,290.41,8.90">This would guide the advancement of novel, end-to-end methods for explainability, rather than keep creating isolated methods that remain only fragments of a broad solution which should also be flexible enough to adapt to various contexts, fields of application and type of end-users.</s><s coords="52,401.62,144.23,87.30,8.90;52,106.36,156.19,382.56,8.90;52,106.36,168.14,382.56,8.90;52,106.36,180.10,104.54,8.90">Additionally, the ultimate scope of an explanation is to help end-users build a complete and correct mental model of the inferential process of either a learning algorithm or a knowledge-based system and to promote trust for its outputs.</s><s coords="52,217.03,180.10,271.89,8.90;52,106.36,192.05,382.56,8.90;52,106.36,204.01,223.53,8.90">An area for future research is the involvement of humans, as final users of artificial explanations, since their role has not been sufficiently studied in the creation and exploitation of existing explainability methods <ref type="bibr" coords="52,315.78,204.01,10.58,8.90" target="#b0">[1]</ref>.</s><s coords="52,335.24,204.01,153.68,8.90;52,106.36,215.96,382.56,8.90;52,106.36,227.92,361.86,8.90">To support this research direction, we recommend exploiting knowledge and experiences belonging to the field of Human-Computer Interaction and its advances for the development of interactive explanatory interfaces <ref type="bibr" coords="52,444.15,227.92,20.06,8.90" target="#b358">[356]</ref>.</s><s coords="52,471.21,227.92,17.71,8.90;52,106.36,239.87,382.56,8.90;52,106.36,251.83,360.77,8.90">This should always take into consideration the existing trade-off between the dimensions of models accuracy and their interpretability/explainability which are currently inversely correlated.</s><s coords="52,472.32,251.83,16.60,8.90;52,106.36,263.78,382.56,8.90;52,106.36,275.74,364.04,8.90">One possible suggestion is the use of methods that take advantage of modern learning techniques, to maximise the former dimension, and reasoning approaches to optimise the latter dimension.</s><s coords="52,473.43,275.74,15.49,8.90;52,106.36,287.70,382.56,8.90;52,106.36,299.65,184.38,8.90">The assumption is that integrating connectionist and symbolic paradigms is the most efficient way to produce meaningful and precise explanations.</s><s coords="52,295.06,299.65,193.86,8.90;52,106.36,311.61,191.37,8.90">Advances on these two paradigms are immense, however, their intersection is under exploration.</s><s coords="52,301.60,311.61,187.32,8.90;52,106.36,323.56,382.56,8.90;52,106.36,335.52,24.07,8.90">For example, on one hand, a school of thought suggests to firstly train accurate models from data and then wrap them with a reasoning layer <ref type="bibr" coords="52,106.36,335.52,20.06,8.90" target="#b165">[165]</ref>.</s><s coords="52,134.56,335.52,354.36,8.90;52,106.36,347.47,382.56,8.90;52,106.36,359.43,74.63,8.90">This layer, for instance, can be produced by exploiting advances in defeasible reasoning and argumentation <ref type="bibr" coords="52,182.28,347.47,20.75,8.90" target="#b143">[143,</ref><ref type="bibr" coords="52,205.10,347.47,17.43,8.90" target="#b142">142,</ref><ref type="bibr" coords="52,224.62,347.47,18.26,8.90" target="#b280">279]</ref> making use of knowledge-bases constructed with a human-inthe-loop approach.</s><s coords="52,184.04,359.43,304.88,8.90;52,106.36,371.38,382.56,8.90;52,106.36,383.34,97.79,8.90">On the other hand, another direction is to promote the use of neuro-symbolic learning and reasoning in parallel, each one informing the other at all stages of model construction and evaluation <ref type="bibr" coords="52,185.06,383.34,15.27,8.90" target="#b44">[45]</ref>.</s><s coords="52,207.58,383.34,281.34,8.90;52,106.36,395.29,321.01,8.90">Eventually, another interesting, novel and under-explored direction for future scholars concerns the development of structured formats of explanations.</s><s coords="52,431.76,395.29,57.16,8.90;52,106.36,407.25,382.56,8.90;52,106.36,419.20,140.56,8.90">These formats should consider all the elements and notions related to explainability, that can be trained with connectionist paradigms from data.</s><s coords="53,121.30,144.23,367.62,8.90;53,106.36,156.19,70.01,8.90">In summary, an high-level structure of a the current state-of-the-are in XAI is depicted in figure <ref type="figure" coords="53,132.19,156.19,9.96,8.90" target="#fig_9">18</ref> (part a).</s><s coords="53,179.73,156.19,309.19,8.90;53,106.36,168.14,359.03,8.90">On one hand, here, emphasis has been placed on the sequence of research activities currently and often performed by several scholars, their dependencies and order.</s><s coords="53,471.21,168.14,17.71,8.90;53,106.36,180.10,382.56,8.90;53,106.36,192.05,354.48,8.90">This sequence usually starts from input data that is then used for modeling purposes, employing connectionist data-driven learning or symbolic reasoning knowledge-based paradigms.</s><s coords="53,467.90,192.05,21.02,8.90;53,106.36,204.01,382.56,8.90;53,106.36,215.96,117.26,8.90">After a model has been formed, then an XAI methods is applied for its analysis, knowledge discovery, supporting its interpretability.</s><s coords="53,226.56,215.96,262.36,8.90;53,106.36,227.92,188.68,8.90">This phase provide the end-users of these models with one or more explanators for the purpose of its explainability.</s><s coords="53,298.05,227.92,190.87,8.90;53,106.36,239.87,382.56,8.90;53,106.36,251.83,306.70,8.90">Eventually, very few scholars have proposed approaches for evaluating such layer of explainability, either proposing formal, objective metrics or involving human-centred evaluation with model designers and end-users.</s><s coords="53,417.95,251.83,70.97,8.90;53,106.36,263.78,319.31,8.90">On the other end, what we believe is an ideal framework for XAI is depicted in figure <ref type="figure" coords="53,380.65,263.78,37.83,8.90" target="#fig_9">18 (part b</ref>).</s><s coords="53,429.46,263.78,59.46,8.90;53,106.36,275.74,362.30,8.90">Here, the main focus should be on the explanators, which is what end-users will ultimately interact with.</s><s coords="53,473.43,275.74,15.49,8.90;53,106.36,287.70,382.56,8.90;53,106.36,299.65,233.43,8.90">The development of explanators should be designed by taking into account the multiple attributes that are linked to the psychological construct of explainability.</s><s coords="53,343.92,299.65,145.00,8.90;53,106.36,311.61,382.56,8.90;53,106.36,323.56,49.52,8.90">Subsequently, scholars can focus on the modeling phase, preferrably using both connectionist and symbolic paradigms from Artificial Intelligence.</s><s coords="53,158.93,323.56,329.99,8.90;53,106.36,335.52,249.51,8.90">This will allow to develop models that are both robust in terms of accuracy but also intrinsically interpretable during all the stages of development.</s><s coords="53,358.95,335.52,129.97,8.90;53,106.36,347.47,382.56,8.90;53,106.36,359.43,382.56,8.90;53,106.36,371.38,202.02,8.90">Eventually, the last phase should focus on the evaluation of explainability of such models with a human-in-the-loop approach, involving both designers and end-users, and the development of interactive interface for supporting model interpretability and inference explainability.</s><s coords="80,147.58,190.34,341.34,7.12;80,106.36,199.80,382.56,7.12;80,106.36,209.26,179.09,7.12">List and classification of the scientific articles proposing human-centered approaches to evaluate methods for explainability, classified according to the construction approach, the type of measurement employed (qualitative or quantitative), and the format of their output explanation.</s></p><p><s coords="81,106.36,190.01,382.56,7.12;81,106.36,199.48,222.65,7.12">Table <ref type="table" coords="81,126.26,190.01,3.59,7.12" target="#tab_1">A</ref>.16: List of the methods for explainability generating visual explanations, such as heat-maps, whose degree of explainability is evaluated by comparison (listed in the fourth column).</s><s coords="81,331.35,199.48,157.57,7.12;81,106.36,208.94,148.55,7.12">These comparisons were carried out over different types of input data (listed in the third column).</s></p><p><s coords="81,112.33,229.14,227.76,8.01;81,357.96,229.14,100.59,8.01">Method for explainability (references) Acronym Input type Compared with (references)</s></p><p><s coords="81,112.33,242.89,123.79,8.01;81,263.18,242.89,18.43,8.01;81,303.49,242.89,30.39,8.01;81,357.96,242.89,90.17,8.01;81,112.33,259.72,59.51,8.01;81,263.18,259.72,16.61,8.01;81,303.49,259.72,30.39,8.01;81,357.96,254.25,61.27,8.01;81,357.96,265.20,56.28,8.01;81,112.33,287.52,76.45,8.01;81,263.18,287.52,9.46,8.01;81,303.49,287.52,30.39,8.01;81,357.96,276.56,84.93,8.01;81,357.96,287.52,84.93,8.01;81,357.96,298.48,123.04,8.01;81,112.33,315.32,61.01,8.01;81,263.18,315.32,12.45,8.01;81,303.49,315.32,30.39,8.01;81,357.96,309.84,67.99,8.01;81,357.96,320.80,81.93,8.01;81,112.33,343.11,85.66,8.01;81,263.18,343.11,17.44,8.01;81,303.49,343.11,30.39,8.01;81,357.96,332.15,64.00,8.01;81,357.96,343.11,77.94,8.01;81,357.96,354.07,92.15,8.01;81,112.33,387.35,90.50,8.01;81,263.18,387.35,9.46,8.01;81,303.49,387.35,30.39,8.01;81,357.96,365.43,84.93,8.01;81,357.96,376.39,119.81,8.01;81,357.96,387.35,81.36,8.01;81,357.96,398.30,62.76,8.01;81,357.96,409.26,99.13,8.01;81,112.33,431.58,138.90,8.01;81,112.33,442.54,19.42,8.01;81,263.19,437.06,16.44,8.01;81,303.49,437.06,30.39,8.01;81,357.96,420.62,123.04,8.01;81,357.96,431.58,60.77,8.01;81,357.96,442.54,78.20,8.01;81,357.96,453.50,42.34,8.01;81,112.33,470.33,138.90,8.01;81,112.33,481.29,68.49,8.01;81,263.19,475.81,21.91,8.01;81,303.49,464.86,30.39,8.01;81,303.49,475.14,42.52,8.97;81,303.49,486.77,41.20,8.01;81,357.96,475.81,110.58,8.01;81,112.33,503.61,98.41,8.01;81,263.18,503.61,11.46,8.01;81,303.49,503.61,30.39,8.01;81,357.96,498.13,121.04,8.01;81,357.96,509.09,47.32,8.01;81,112.33,536.89,93.43,8.01;81,263.18,536.89,11.46,8.01;81,303.49,536.89,30.39,8.01;81,357.96,520.45,47.32,8.01;81,357.96,531.41,60.77,8.01;81,357.96,542.36,97.13,8.01;81,357.96,553.32,65.26,8.01;81,112.33,564.68,59.88,8.01;81,263.18,564.68,12.96,8.01;81,303.49,564.68,30.39,8.01;81,357.96,564.68,95.64,8.01;81,112.33,576.04,70.09,8.01;81,263.18,576.04,12.96,8.01;81,303.49,576.04,30.39,8.01;81,357.96,576.04,119.54,8.01;81,112.33,587.40,133.61,8.01;81,263.18,587.40,22.92,8.01;81,303.49,587.40,30.39,8.01;81,357.96,587.40,109.58,8.01;81,112.33,604.23,67.49,8.01;81,263.18,604.23,11.46,8.01;81,303.49,604.23,30.39,8.01;81,357.96,598.75,69.98,8.01;81,357.96,609.71,96.87,8.01;81,112.33,621.07,138.90,8.01;81,112.33,632.03,19.42,8.01;81,263.19,626.55,16.44,8.01;81,303.49,626.55,26.63,8.01;81,357.96,626.55,60.27,8.01;81,112.33,643.39,126.06,8.01;81,263.18,643.39,11.46,8.01;81,303.49,643.39,26.63,8.01;81,357.96,643.39,65.26,8.01">Deep-Taylor Decomposition <ref type="bibr" coords="81,216.70,242.89,19.42,8.01" target="#b209">[208]</ref> DTD Pictorial GBP, IG, SA, PM in <ref type="bibr" coords="81,433.19,242.89,14.94,8.01" target="#b91">[92]</ref> DeepLIFT <ref type="bibr" coords="81,152.42,259.72,19.42,8.01" target="#b99">[100]</ref> DLT Pictorial IG, LRP in <ref type="bibr" coords="81,399.81,254.25,19.42,8.01" target="#b328">[326]</ref> IG, SA in <ref type="bibr" coords="81,394.82,265.20,19.42,8.01" target="#b334">[332]</ref> Gradient*Input <ref type="bibr" coords="81,169.36,287.52,19.42,8.01" target="#b99">[100]</ref> GI Pictorial GC, GBP and SG <ref type="bibr" coords="81,423.47,276.56,19.42,8.01" target="#b326">[324]</ref> GC, GBP, IG, SG <ref type="bibr" coords="81,423.47,287.52,19.42,8.01" target="#b327">[325]</ref> IG, LIME, OS, SM, SHAP in <ref type="bibr" coords="81,466.06,298.48,14.94,8.01" target="#b89">[90]</ref> GrandCam <ref type="bibr" coords="81,153.92,315.32,19.42,8.01" target="#b197">[196]</ref> GC Pictorial GI, GBP, SG <ref type="bibr" coords="81,406.53,309.84,19.42,8.01" target="#b326">[324]</ref> GI, GBP, IG, SG <ref type="bibr" coords="81,420.48,320.80,19.42,8.01" target="#b327">[325]</ref> Guided BackProp <ref type="bibr" coords="81,178.58,343.11,19.42,8.01" target="#b199">[198]</ref> GBP Pictorial GI, GC, SG <ref type="bibr" coords="81,402.54,332.15,19.42,8.01" target="#b326">[324]</ref> GI, GC, IG, SG <ref type="bibr" coords="81,416.49,343.11,19.42,8.01" target="#b327">[325]</ref> DTD, IG, SA, PM in <ref type="bibr" coords="81,435.17,354.07,14.94,8.01" target="#b91">[92]</ref> Integrated Gradients <ref type="bibr" coords="81,187.89,387.35,14.94,8.01" target="#b92">[93]</ref> IG Pictorial GI, GC, GBP, SG <ref type="bibr" coords="81,423.47,365.43,19.42,8.01" target="#b327">[325]</ref> LRP, LIME, OS, SM, SHAP <ref type="bibr" coords="81,462.83,376.39,14.94,8.01" target="#b89">[90]</ref> DLT and LRP in <ref type="bibr" coords="81,419.90,387.35,19.42,8.01" target="#b328">[326]</ref> DLT, SA in <ref type="bibr" coords="81,401.31,398.30,19.42,8.01" target="#b334">[332]</ref> DTD, GBP, SA, PM in <ref type="bibr" coords="81,442.16,409.26,14.94,8.01" target="#b91">[92]</ref> Layer-wise Relevance Propagation <ref type="bibr" coords="81,112.33,442.54,19.42,8.01" target="#b100">[101]</ref> LRP Pictorial IG, LIME, OS, SM, SHAP in <ref type="bibr" coords="81,466.06,420.62,14.94,8.01" target="#b89">[90]</ref> DLT, IG in <ref type="bibr" coords="81,399.31,431.58,19.42,8.01" target="#b328">[326]</ref> SA in <ref type="bibr" coords="81,380.88,442.54,18.68,8.01" target="#b333">[331,</ref><ref type="bibr" coords="81,401.80,442.54,15.69,8.01" target="#b331">329,</ref><ref type="bibr" coords="81,419.73,442.54,16.44,8.01" target="#b332">330]</ref> OS in <ref type="bibr" coords="81,380.88,453.50,19.42,8.01" target="#b333">[331]</ref> Local Interpretable Model-Agnostic Explanations <ref type="bibr" coords="81,161.40,481.29,19.42,8.01" target="#b134">[134]</ref> LIME Pictorial Numerical / Categorical GI, IG, OS, SM, SHAP in <ref type="bibr" coords="81,453.61,475.81,14.94,8.01" target="#b89">[90]</ref> Occlusion Sensitivity <ref type="bibr" coords="81,191.32,503.61,19.42,8.01" target="#b200">[199]</ref> OS Pictorial GI, IG, LIME, SM, SHAP in <ref type="bibr" coords="81,464.06,498.13,14.94,8.01" target="#b89">[90]</ref> LRP in <ref type="bibr" coords="81,385.87,509.09,19.42,8.01" target="#b333">[331]</ref> Sensitivity Analysis <ref type="bibr" coords="81,186.34,536.89,19.42,8.01" target="#b175">[174]</ref> SA Pictorial LRP in <ref type="bibr" coords="81,385.87,520.45,19.42,8.01" target="#b333">[331]</ref> DLT, IG in <ref type="bibr" coords="81,399.31,531.41,19.42,8.01" target="#b334">[332]</ref> DTD, GBP, IG, PM in <ref type="bibr" coords="81,440.16,542.36,14.94,8.01" target="#b91">[92]</ref> LRP in <ref type="bibr" coords="81,385.87,553.32,18.68,8.01" target="#b331">[329,</ref><ref type="bibr" coords="81,406.78,553.32,16.44,8.01" target="#b332">330]</ref> PatternNet <ref type="bibr" coords="81,152.79,564.68,19.42,8.01" target="#b211">[210]</ref> PM Pictorial DTD, GBP, IG, SA in [92] Saliency Maps <ref type="bibr" coords="81,167.49,576.04,14.94,8.01" target="#b97">[98]</ref> SM Pictorial GI, IG, LIME, OS, SHAP in [90] SHapley Additive exPlanations <ref type="bibr" coords="81,226.52,587.40,19.42,8.01" target="#b149">[149]</ref> SHAP Pictorial GI, IG, LIME, OS, SM in <ref type="bibr" coords="81,452.60,587.40,14.94,8.01" target="#b89">[90]</ref> SmoothGrad <ref type="bibr" coords="81,160.40,604.23,19.42,8.01" target="#b204">[203]</ref> SG Pictorial GI, GC, GBP <ref type="bibr" coords="81,408.52,598.75,19.42,8.01" target="#b326">[324]</ref> GI, GC, GBP and IG <ref type="bibr" coords="81,435.41,609.71,19.42,8.01" target="#b327">[325]</ref> Layer-wise Relevance Propagation <ref type="bibr" coords="81,112.33,632.03,19.42,8.01" target="#b100">[101]</ref> LRP Textual SA in <ref type="bibr" coords="81,380.88,626.55,18.68,8.01" target="#b329">[327,</ref><ref type="bibr" coords="81,401.80,626.55,16.44,8.01" target="#b332">330]</ref> Sensitivity Analysis methods <ref type="bibr" coords="81,218.97,643.39,19.42,8.01" target="#b175">[174]</ref> SA Textual LRP in <ref type="bibr" coords="81,385.86,643.39,18.68,8.01" target="#b329">[327,</ref><ref type="bibr" coords="81,406.78,643.39,16.44,8.01" target="#b332">330]</ref></s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,106.36,283.30,382.56,7.12;3,106.36,292.77,128.60,7.12;3,122.14,141.00,351.00,131.10"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="3,106.36,283.30,382.56,7.12;3,106.36,292.77,128.60,7.12">Figure 1: Diagrammatic view of Explainable Artificial Intelligence as a sub-field at the intersection of Artificial Intelligence and Human-Computer Interaction</s></p></div></figDesc><graphic coords="3,122.14,141.00,351.00,131.10" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,106.36,497.02,382.56,7.12;4,106.36,506.48,382.56,7.12;4,106.36,515.95,258.19,7.12;4,109.86,381.27,105.79,89.10"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s coords="4,106.36,497.02,382.56,7.12;4,106.36,506.48,382.56,7.12;4,106.36,515.95,258.19,7.12">Figure 2: Proposed classification of the XAI literature with (a) the distribution of published scientific articles over time, (b) the root of our hierarchical classification system representing the main four categories and the percentage of articles in each , and (c) the salient relations between these categories that have emerged.</s></p></div></figDesc><graphic coords="4,109.86,381.27,105.79,89.10" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,106.36,301.24,382.56,7.12;6,106.36,310.71,266.49,7.12;6,106.36,171.60,195.72,118.44"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s coords="6,106.36,301.24,382.56,7.12;6,106.36,310.71,266.49,7.12">Figure 3: Hierarchical classification of the review articles on explainable artificial intelligence and machine learning interpretability (left) and distribution of the review articles across categories (right).</s></p></div></figDesc><graphic coords="6,106.36,171.60,195.72,118.44" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,120.84,307.16,353.60,7.12;16,289.70,140.99,196.03,154.96"><head>Figure 4 :</head><label>4</label><figDesc><div><p><s coords="16,120.84,307.16,353.60,7.12">Figure 4: Classification of methods for explainability (left) and distribution of articles across categories (right).</s></p></div></figDesc><graphic coords="16,289.70,140.99,196.03,154.96" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="19,106.36,606.52,382.56,7.12;19,106.36,615.98,241.25,7.12;19,112.00,464.95,114.16,114.91"><head>Figure 6 :</head><label>6</label><figDesc><div><p><s coords="19,106.36,606.52,382.56,7.12;19,106.36,615.98,241.25,7.12">Figure 6: Examples of rule-based explanations generated by model-agnostic methods which can be visualized as (a) a decision tree (b) a list of rules accompanied by textual and visual examples.</s></p></div></figDesc><graphic coords="19,112.00,464.95,114.16,114.91" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="21,106.36,448.13,382.56,7.12;21,106.36,457.59,111.88,7.12;21,219.93,347.82,82.57,73.66"><head>Figure 7 :</head><label>7</label><figDesc><div><p><s coords="21,106.36,448.13,382.56,7.12;21,106.36,457.59,111.88,7.12">Figure 7: Examples of visual explanations generated by model-agnostic methods as (a) graphs, (b) restricted support regions, (c) heat-maps, or (e) plots.</s></p></div></figDesc><graphic coords="21,219.93,347.82,82.57,73.66" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="29,106.36,350.80,125.43,8.71;29,121.30,363.68,367.62,8.90;29,106.36,375.64,382.56,8.90;29,106.36,387.59,382.56,8.90;29,106.36,399.55,382.57,8.90;29,106.36,411.50,382.56,8.90;29,106.36,423.46,382.56,8.90;29,106.36,435.41,309.63,8.90"><head>6. 3 . 4 .</head><label>34</label><figDesc><div><p><s coords="29,133.75,350.80,98.03,8.71;29,121.30,363.68,367.62,8.90;29,106.36,375.64,264.16,8.90">Rule-based explanations Several methods for explainability are focused on rule-based explanations of the inferential process of neural networks, usually in the form of IF-THEN rules.</s><s coords="29,373.69,375.64,115.23,8.90;29,106.36,387.59,382.56,8.90;29,106.36,399.55,382.57,8.90;29,106.36,411.50,382.56,8.90;29,106.36,423.46,382.56,8.90;29,106.36,435.41,309.63,8.90">Scholars divided these methods into three classes [41, 132]: (I) decompositional methods work by extracting rules at the level of hidden and output neurons by analysing the values of their weights, (II) pedagogical methods treat an underlying neural network as a black-box and the rule extraction consists of mimicking the function computed by the network; weights are not subjected to analysis, and (III) eclectic methods that are a combination of the decompositional and pedagogical ones.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="32,106.36,245.51,382.56,7.12;32,106.36,254.98,264.58,7.12;32,253.05,144.82,112.91,74.05"><head>Figure 11 :</head><label>11</label><figDesc><div><p><s coords="32,106.36,245.51,382.56,7.12;32,106.36,254.98,264.58,7.12">Figure 11: Examples of rule-based explanations generated by methods for explainability for neural networks and visualized as (a) decision trees, (b) list of rules or (c) by showing the most relevant input.</s></p></div></figDesc><graphic coords="32,253.05,144.82,112.91,74.05" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="44,106.36,500.00,382.56,7.12;44,106.36,509.46,135.81,7.12;44,120.73,271.02,353.81,217.78"><head>Figure 17 :</head><label>17</label><figDesc><div><p><s coords="44,106.36,500.00,382.56,7.12;44,106.36,509.46,135.81,7.12">Figure 17: Classification of the approaches to evaluate methods for explainability (up) and distribution of the relative scientific studies across categories (down).</s></p></div></figDesc><graphic coords="44,120.73,271.02,353.81,217.78" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="52,138.66,631.36,317.95,7.12;52,109.12,449.83,187.60,154.87"><head>Figure 18 :</head><label>18</label><figDesc><div><p><s coords="52,138.66,631.36,317.95,7.12">Figure 18: State of the art (a) and envisioned (b) frameworks for eXplainable Artificial Intelligence.</s></p></div></figDesc><graphic coords="52,109.12,449.83,187.60,154.87" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,112.33,142.24,300.83,46.27"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="9,210.29,142.24,202.88,7.12">Definition of the notions related to the concept of explainability</s></p></div></figDesc><table coords="9,112.33,162.07,139.82,26.44"><row><cell>Notion</cell><cell>Description &amp; Reference</cell></row><row><cell>Algorithmic</cell><cell/></row><row><cell>transparency</cell><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="72,106.36,403.32,382.56,284.24"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="72,133.72,403.32,355.20,7.12;72,106.36,412.79,382.56,7.12;72,106.36,421.65,380.57,7.97">3: Model agnostic methods for explainability generating numerical explanations, classified according to the output format, stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical / Categorical, P: Pictorials, T: Textual, TS: Time Series) of the model.</s></p></div></figDesc><table coords="72,112.33,442.08,372.63,245.49"><row><cell cols="10">Table A.4: Model agnostic methods for explainability generating mixed explanations, classified according to the output</cell></row><row><cell cols="10">format, stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L:</cell></row><row><cell cols="10">Local) and input data (NC: Numerical / Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.</cell></row><row><cell cols="3">Method for explainability</cell><cell cols="2">Authors</cell><cell/><cell/><cell>Ref</cell><cell cols="2">Year Stage Scope Problem Input</cell></row><row><cell cols="2">Bayesian Teaching</cell><cell/><cell cols="3">Yang and Shafto</cell><cell/><cell cols="3">[186] 2017 PH</cell><cell>L</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="3">Evasion-Prone Samples Selection</cell><cell cols="2">Liu et al.</cell><cell/><cell/><cell cols="3">[190] 2018 PH</cell><cell>G</cell><cell>C</cell><cell>T</cell></row><row><cell>ExplAIner</cell><cell/><cell/><cell cols="3">Spinner et al.</cell><cell/><cell cols="3">[133] 2019 PH</cell><cell>G</cell><cell>C / R</cell><cell>P; NC; TS</cell></row><row><cell cols="3">Functional ANOVA decomposition, Variable Interaction Network graph</cell><cell cols="2">Hooker</cell><cell/><cell/><cell cols="3">[183] 2004 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="2">Justification Narratives</cell><cell/><cell cols="4">Biran and McKeown</cell><cell cols="3">[184] 2014 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="3">Local Interpretable Model-Agnostic Explanations (LIME)</cell><cell cols="3">Ribeiro et al.</cell><cell/><cell>[193, 134]</cell><cell cols="2">2016 PH</cell><cell>L</cell><cell>C</cell><cell>P; T</cell></row><row><cell>Maximum (MMD)-critic</cell><cell>Mean</cell><cell>Discrepancy</cell><cell cols="2">Kim et al.</cell><cell/><cell/><cell cols="3">[191] 2016 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell cols="3">Neighborhood-Based Explanations</cell><cell cols="3">Caruana et al.</cell><cell/><cell cols="3">[189] 1999 PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Pertinent negatives</cell><cell/><cell cols="3">Dhurandhar et al.</cell><cell/><cell cols="3">[192] 2018 PH</cell><cell>L</cell><cell>C</cell><cell>P; NC</cell></row><row><cell>Rivelo</cell><cell/><cell/><cell cols="3">Tamagnini et al.</cell><cell/><cell cols="3">[185] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>T</cell></row><row><cell cols="3">Sequential Bayesian Quadrature</cell><cell cols="3">Khanna et al.</cell><cell/><cell cols="3">[187] 2019 PH</cell><cell>L</cell><cell>C</cell><cell>P; NC</cell></row><row><cell cols="3">Set Cover Optimization (SCO)</cell><cell cols="2">Bien et al.</cell><cell/><cell/><cell cols="3">[188] 2011 PH</cell><cell>L</cell><cell>C</cell><cell>P; NC</cell></row><row><cell cols="3">Method for explainability</cell><cell cols="2">Authors</cell><cell/><cell/><cell>Ref</cell><cell cols="2">Year Stage Scope Problem Input</cell></row><row><cell cols="2">Distill-and-Compare</cell><cell/><cell cols="2">Tan et al.</cell><cell/><cell/><cell cols="3">[148] 2018 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="2">Explain and Ime</cell><cell/><cell cols="4">Robnik-Šikonja and Kononenko, Robnik-Šikonja</cell><cell>[150, 151]</cell><cell>2008, 2018</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell/><cell/><cell/><cell cols="2">Strumbelj</cell><cell/><cell>and</cell><cell/><cell/></row><row><cell/><cell/><cell/><cell cols="2">Kononenko,</cell><cell/><cell/><cell>[154,</cell><cell>2010,</cell></row><row><cell cols="2">Feature contribution</cell><cell/><cell cols="2">Kononenko</cell><cell/><cell/><cell>155,</cell><cell>2013,</cell><cell>PH</cell><cell>L</cell><cell>C / R</cell><cell>NC</cell></row><row><cell/><cell/><cell/><cell>et</cell><cell>al.,</cell><cell cols="2">Štrumbelj</cell><cell>156]</cell><cell>2009</cell></row><row><cell/><cell/><cell/><cell>et al.</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Feature contribution</cell><cell/><cell cols="3">Štrumbelj Kononenko, Štrumbelj et al.</cell><cell>and</cell><cell>[157, 158]</cell><cell>2008, 2010</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="2">Feature Importance</cell><cell/><cell cols="3">Henelius et al.</cell><cell/><cell cols="3">[163] 2014 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Feature perturbation</cell><cell/><cell cols="2">Štrumbelj Kononenko</cell><cell/><cell>and</cell><cell cols="3">[164] 2014 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="3">Global Sensitivity Analysis (GSA)</cell><cell cols="4">Cortez brechts, Cortez and and Em-Embrechts</cell><cell>[152, 153]</cell><cell>2011, 2013</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="3">Gradient Feature Auditing (GFA)</cell><cell cols="2">Adler et al.</cell><cell/><cell/><cell cols="3">[160] 2018 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="2">Influence functions</cell><cell/><cell cols="3">Koh and Liang</cell><cell/><cell cols="3">[161] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell cols="3">Monotone Influence Measures</cell><cell cols="3">Sliwinski et al.</cell><cell/><cell cols="3">[162] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell cols="3">Quantitative Input Influence (QII) functions</cell><cell cols="2">Datta et al.</cell><cell/><cell/><cell cols="3">[159] 2016 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="3">SHapley Additive exPlanations (SHAP)</cell><cell cols="3">Lundberg and Lee</cell><cell/><cell cols="3">[149] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="73,106.36,355.09,382.56,103.31"><head>Table A .</head><label>A</label><figDesc/><table coords="73,112.33,393.85,369.80,64.56"><row><cell>Method for explainability</cell><cell>Authors</cell><cell>Ref</cell><cell>Year</cell><cell cols="4">Stage Scope Problem Input</cell></row><row><cell>Anchors</cell><cell>Ribeiro et al.</cell><cell cols="2">[140] 2018</cell><cell>PH</cell><cell>G / L</cell><cell>C</cell><cell>T</cell></row><row><cell>Automated Reasoning</cell><cell>Bride et al.</cell><cell cols="2">[165] 2018</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Genetic Rule EXtraction (G-REX)</cell><cell>Johansson et al.</cell><cell>[166, 167]</cell><cell>2004</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Model Extraction</cell><cell>Bastani et al.</cell><cell cols="2">[168] 2017</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Partition Aware Local Model (PALM)</cell><cell>Krishnan and Wu</cell><cell cols="2">[169] 2017</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row></table><note coords="73,133.63,355.09,355.29,7.12;73,106.36,364.56,382.56,7.12;73,106.36,374.02,376.59,7.12"><p><s coords="73,133.63,355.09,355.29,7.12;73,106.36,364.56,382.56,7.12;73,106.36,374.02,376.59,7.12">5: Model agnostic methods for explainability generating rule-based explanations, classified according to the output format, stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), the scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the model.</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="73,106.36,471.82,382.56,226.10"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="73,133.87,471.82,355.04,7.12;73,106.36,481.29,382.56,7.12;73,106.36,490.75,382.56,7.12">6: Model agnostic methods for explainability generating visual explanations, classified according to output format, stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), the scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.</s></p></div></figDesc><table coords="73,112.33,510.58,369.85,187.34"><row><cell>Method for explainability</cell><cell>Authors</cell><cell>Ref</cell><cell cols="4">Year Stage Scope Problem Input</cell></row><row><cell>Class Signatures</cell><cell>Krause et al.</cell><cell cols="2">[179] 2016 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>ExplainD</cell><cell>Poulin et al.</cell><cell cols="2">[180] 2006 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Explanation Graph based on perturbed input element order</cell><cell>Alvarez-Melis and Jaakkola</cell><cell cols="2">[177] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>T</cell></row><row><cell>Image Perturbation</cell><cell>Fong and Vedaldi</cell><cell cols="2">[170] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Individual Conditional Expectation</cell><cell>Goldstein et al.</cell><cell cols="2">[175] 2015 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>iVisClassifier</cell><cell>Choo et al.</cell><cell cols="2">[172] 2010 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Layer-wise Relevance Propagation</cell><cell>Bach et al.</cell><cell cols="2">[101] 2015 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Manifold</cell><cell>Zhang et al.</cell><cell cols="2">[181] 2019 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>MLCube Explorer</cell><cell>Kahng et al.</cell><cell cols="2">[182] 2016 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Partial Importance and Individual Con-</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>ditional Importance plots based on</cell><cell>Casalicchio et al.</cell><cell cols="2">[176] 2018 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Shapley feature importance</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Restricted Support Region Set (RSRS) Detection</cell><cell>Liu and Wang</cell><cell cols="2">[171] 2012 PH</cell><cell>L</cell><cell>C</cell><cell>T</cell></row><row><cell>Saliency Detection</cell><cell cols="3">Dabkowski and Gal [173] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Sensitivity analysis</cell><cell>Baehrens et al.</cell><cell cols="2">[174] 2010 PH</cell><cell>L</cell><cell>C</cell><cell>P; NC</cell></row><row><cell>Spectral Relevance Analysis (SpRAy)</cell><cell>Lapuschkin et al.</cell><cell>[8]</cell><cell>2019 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Worst-case perturbations</cell><cell>Goodfellow et al.</cell><cell cols="2">[178] 2015 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="74,106.36,142.24,387.74,557.62"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="74,133.59,142.24,355.33,7.12;74,106.36,151.71,382.56,7.12;74,106.36,161.17,276.75,7.12">7: Methods for explainability for neural networks generating visual explanations, classified according to the stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series).</s></p></div></figDesc><table coords="74,112.33,181.00,381.76,518.87"><row><cell>Method for explainability</cell><cell>Authors</cell><cell>Ref</cell><cell cols="5">Year Stage Scope Problem Input</cell></row><row><cell>Activation maps</cell><cell>Hamidi-Haines et al.</cell><cell cols="3">[225] 2019 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Activation Maximization</cell><cell>Erhan et al., Nguyen et al.</cell><cell>[222, 223, 224]</cell><cell>2010, 2016</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>ActiVis</cell><cell>Kahng et al.</cell><cell cols="3">[231] 2018 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>And-Or Graph (AOG)</cell><cell>Zhang et al.</cell><cell cols="3">[229] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Cell Activation Values</cell><cell>Karpathy et al.</cell><cell cols="3">[226] 2016 PH</cell><cell>G / L</cell><cell>C</cell><cell>T</cell></row><row><cell>CLass-Enhanced Attentive Re-sponse (CLEAR)</cell><cell>Kumar et al.</cell><cell cols="3">[194] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell>Cnn-Inte</cell><cell>Liu et al.</cell><cell cols="3">[215] 2018 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Compositionality</cell><cell>Li et al.</cell><cell cols="3">[213] 2016 PH</cell><cell>L</cell><cell>C</cell><cell>T</cell></row><row><cell>Data-flow graphs</cell><cell>Wongsuphasawat et al.</cell><cell cols="3">[136] 2018 PH</cell><cell>G</cell><cell>C / R</cell><cell>P; NC; T</cell></row><row><cell>Deep Learning Important Fea-Tures (DeepLIFT)</cell><cell>Shrikumar et al.</cell><cell cols="3">[100] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>P; NC</cell></row><row><cell>Deep View (DV)</cell><cell>Zhong et al.</cell><cell cols="3">[233] 2017 PH</cell><cell>G</cell><cell>C / R</cell><cell>P</cell></row><row><cell>Deep Visualization Toolbox</cell><cell>Yosinski et al.</cell><cell cols="3">[232] 2015 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Deep-Taylor Decomposition</cell><cell>Montavon et al.</cell><cell cols="3">[208] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>DeepResolve</cell><cell>Liu and Gifford</cell><cell cols="3">[195] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Explanatory Graph</cell><cell>Zhang et al.</cell><cell cols="3">[227] 2018 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Feature maps</cell><cell>Zhang et al.</cell><cell cols="3">[204] 2018 AH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Generative Adversarial Network (GAN) Dissection</cell><cell>Bau et al.</cell><cell cols="3">[220] 2019 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>GradCam</cell><cell>Selvaraju et al.</cell><cell cols="3">[196] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Guided BackProp and Occlusion</cell><cell>Goyal et al.</cell><cell cols="3">[198] 2016 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Guided Feature Inversion</cell><cell>Du et al.</cell><cell cols="3">[202] 2018 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Hidden Activity Visualization</cell><cell>Rauber et al.</cell><cell cols="3">[218] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Important Neurons and Patches</cell><cell>Lengerich et al.</cell><cell cols="3">[221] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>iNNvestigate</cell><cell>Alber et al.</cell><cell cols="3">[234] 2019 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Integrated Gradients</cell><cell>Sundararajan et al.</cell><cell>[93]</cell><cell cols="2">2017 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Inverting Representations</cell><cell>Mahendran and Vedaldi</cell><cell cols="3">[201] 2015 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>LRP w/ Relevance Conservation</cell><cell>Arras et al.</cell><cell cols="3">[206] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>T</cell></row><row><cell>LRP w/ Local Renormalization Layers</cell><cell>Binder et al.</cell><cell cols="3">[207] 2016 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>LSTMVis</cell><cell>Strobelt et al.</cell><cell cols="3">[135] 2018 PH</cell><cell>G / L</cell><cell>C</cell><cell>T</cell></row><row><cell>N 2 VIS</cell><cell>Streeter et al.</cell><cell cols="3">[236] 2001 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Net2Vec</cell><cell>Fong and Vedaldi</cell><cell cols="3">[200] 2018 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Neural Network and CBR Twin-systems</cell><cell>Kenny and Keane</cell><cell cols="3">[212] 2019 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>OcclusionSensitivity</cell><cell>Zeiler and Fergus</cell><cell cols="3">[199] 2014 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>OpenBox</cell><cell>Chu et al.</cell><cell cols="3">[214] 2018 PH</cell><cell>G</cell><cell>C</cell><cell>P; NC</cell></row><row><cell>PatternNet, PatternAttribution</cell><cell>Kindermans et al.</cell><cell cols="3">[210] 2018 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Prediction Difference Analysis</cell><cell>Zintgraf et al.</cell><cell cols="3">[205] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Principal Component Analysis</cell><cell>Aubry and Russell</cell><cell cols="3">[216] 2015 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Receptive Fields</cell><cell>He and Pugeault</cell><cell cols="3">[209] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Relevant Features Selection</cell><cell>Mogrovejo et al.</cell><cell cols="3">[211] 2019 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Saliency maps</cell><cell>Olah et al.</cell><cell cols="3">[230] 2018 PH</cell><cell>G / L</cell><cell>C</cell><cell>P</cell></row><row><cell>Saliency maps</cell><cell>Simonyan et al.</cell><cell>[98]</cell><cell cols="2">2014 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Seq2seq-Vis</cell><cell>Strobelt et al.</cell><cell cols="3">[235] 2018 PH</cell><cell>L</cell><cell>C</cell><cell>T</cell></row><row><cell>SmoothGrad</cell><cell>Smilkov et al.</cell><cell cols="3">[203] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Stacking w/ Auxiliary Features (SWAF)</cell><cell>Rajani and Mooney</cell><cell cols="3">[197] 2017 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell cols="2">Symbolic Graph Reasoning (SGR) Liang et al.</cell><cell cols="3">[228] 2018 AH</cell><cell>G</cell><cell>C / R</cell><cell>P</cell></row><row><cell>t-SNE maps</cell><cell>Zahavy et al.</cell><cell cols="3">[217] 2016 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>TreeView</cell><cell>Thiagarajan et al.</cell><cell cols="3">[219] 2016 PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="75,106.36,250.98,382.56,339.67"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="75,132.64,250.98,356.28,7.12;75,106.36,260.45,382.56,7.12;75,106.36,269.91,355.56,7.12">8: Methods for explainability for neural networks generating rule-based explanations, classified according to the stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.</s></p></div></figDesc><table coords="75,112.33,289.74,370.42,300.92"><row><cell>Method for explainability</cell><cell>Authors</cell><cell/><cell>Ref</cell><cell cols="5">Year Stage Scope Problem Input</cell></row><row><cell>C4.5Rule-PANE</cell><cell cols="2">Zhou and Jiang</cell><cell cols="3">[248] 2003 PH</cell><cell>L</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>DecText</cell><cell>Boz</cell><cell/><cell cols="3">[249] 2002 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Discretized Interpretable Multi Layer Perceptrons (DIMLP)</cell><cell cols="2">Bologna Hayashi, Bologna and</cell><cell>[139, 240, 132] 241,</cell><cell>2017, 2018 1998,</cell><cell>PH</cell><cell>G / L</cell><cell>C</cell><cell>P; NC; T</cell></row><row><cell>Discretizing Hidden Unit Activation Values by Clustering</cell><cell cols="2">Setiono and Liu</cell><cell cols="3">[237] 1995 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>DT extraction</cell><cell cols="2">Frosst and Hin-ton, Zhang et al.</cell><cell>[245, 246]</cell><cell>2017, 2019</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Interval Propagation</cell><cell>Palade et al.</cell><cell/><cell cols="3">[141] 2001 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Iterative Rule Knowledge Distillation Hu et al.</cell><cell/><cell cols="3">[254] 2016 AH</cell><cell>G</cell><cell>C</cell><cell>T</cell></row><row><cell>Neural Network Knowledge eXtrac-tion (NNKX)</cell><cell cols="2">Bondarenko et al.</cell><cell cols="3">[238] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Rule Extraction From Neural net-work Ensemble (REFNE)</cell><cell>Zhou et al.</cell><cell/><cell cols="3">[247] 2003 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Rule Extraction from Neural Net-</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>work using Classified and Misclassi-</cell><cell>Biswas et al.</cell><cell/><cell cols="3">[243] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>fied data (RxNCM)</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Rule Extraction by Reverse Engineer-ing (RxREN)</cell><cell cols="2">Augasta mar Kathirvalavaku-and</cell><cell cols="3">[242] 2012 AH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Symbolic logic integration</cell><cell>Tran</cell><cell/><cell cols="3">[255] 2017 AH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Symbolic rules</cell><cell>Garcez et al.</cell><cell/><cell cols="3">[244] 2001 PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Tree Regularization</cell><cell>Wu et al.</cell><cell/><cell cols="3">[252] 2018 AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Trepan</cell><cell cols="2">Craven and Shav-lik, Craven and Shavlik</cell><cell>[250, 251]</cell><cell>1994, 1996</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell>Validity Interval Analysis (VIA)</cell><cell>Thrun</cell><cell/><cell cols="3">[239] 1995 PH</cell><cell>G</cell><cell>C / R</cell><cell>TS</cell></row><row><cell>Word Importance Scores</cell><cell>Murdoch Szlam</cell><cell>and</cell><cell cols="3">[253] 2017 PH</cell><cell>G</cell><cell>C</cell><cell>T</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="76,106.36,196.56,389.15,207.17"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="76,133.89,196.56,355.03,7.12;76,106.36,206.03,382.56,7.12;76,106.36,215.49,382.56,7.12;76,106.36,224.96,162.64,7.12">9: Methods for explainability for neural networks generating textual and numerical explanations, classified according to the output format (N: Numerical, T: Textual), stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.</s></p></div></figDesc><table coords="76,112.33,244.78,383.18,158.95"><row><cell>Method for explainability</cell><cell>Authors</cell><cell>Ref</cell><cell>Year</cell><cell>Output Format</cell><cell cols="4">Stage Scope Problem Input</cell></row><row><cell>Causal Importance</cell><cell>Féraud and Clérot</cell><cell cols="3">[261] 2002 N</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Concept Activation Vectors</cell><cell>Kim et al.</cell><cell cols="3">[145] 2018 N</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Contextual Importance, Utility</cell><cell>Främling</cell><cell cols="3">[262] 1996 N</cell><cell>PH</cell><cell>G / L</cell><cell>C</cell><cell>NC</cell></row><row><cell>InterpNET</cell><cell>Barratt</cell><cell cols="3">[256] 2017 T</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Most-Weighted-Path, Weighted-Combination, Maximum-Frequency-Difference Most-</cell><cell>García-Magariño et al.</cell><cell cols="3">[257] 2019 T</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>TS</cell></row><row><cell>Probes</cell><cell>Alain and Bengio</cell><cell cols="3">[144] 2017 N</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>P</cell></row><row><cell>Rationales</cell><cell>Lei et al.</cell><cell cols="3">[258] 2016 T</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>T</cell></row><row><cell>REcurrent LEXicon NETwork (RELEXNET)</cell><cell>Clos et al.</cell><cell cols="3">[263] 2017 N</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>T</cell></row><row><cell>Relevance, Discriminative Loss</cell><cell>Hendricks et al.</cell><cell>[259, 137]</cell><cell>2016, 2018</cell><cell>T</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Singular Vector Canonical Corre-lation Analysis (SVCCA)</cell><cell>Raghu et al.</cell><cell cols="3">[260] 2017 N</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="76,106.36,523.09,382.56,121.99"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="76,133.48,523.09,355.44,7.12;76,106.36,532.55,382.56,7.12;76,106.36,542.02,355.56,7.12">10: Methods for explainability for neural networks generating mixed explanations, classified according to the stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.</s></p></div></figDesc><table coords="76,112.33,561.84,366.97,83.23"><row><cell>Method for explainability</cell><cell>Authors</cell><cell>Ref</cell><cell cols="4">Year Stage Scope Problem Input</cell></row><row><cell>Activation values of hidden neurons</cell><cell>Tamajka et al.</cell><cell cols="2">[268] 2019 AH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Attention Alignment</cell><cell>Kim et al.</cell><cell cols="2">[264] 2018 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Deterministic Finite Automaton</cell><cell>Mayr and Yovine</cell><cell cols="2">[266] 2018 PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell>Deterministic Finite-state Automata (DFAs)</cell><cell>Omlin and Giles</cell><cell cols="2">[267] 1996 PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Image Caption Generation with Atten-tion Mechanism</cell><cell>Xu et al.</cell><cell cols="2">[147] 2015 PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell>Pointing and Justification Model (PJ-X)</cell><cell>Park et al.</cell><cell cols="2">[265] 2018 AH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="77,106.36,236.79,382.57,368.06"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="77,132.99,236.79,355.93,7.12;77,106.36,246.25,382.56,7.12;77,106.36,255.72,382.57,7.12;77,106.36,265.18,171.32,7.12">11: Methods for explainability for rule-based construction approaches, classified according to the output format (M: Mixed, N: Numerical, R: Rules, T: Textual, N: Numerical), stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorials, T: Textual, TS: Time Series) of the underlying model.</s></p></div></figDesc><table coords="77,112.33,285.01,374.67,319.84"><row><cell cols="2">Method for explainability</cell><cell>Authors</cell><cell>Ref</cell><cell>Year</cell><cell>Output Format</cell><cell cols="4">Stage Scope Problem Input</cell></row><row><cell cols="2">Ant Colony Optimization (ACO)</cell><cell>Otero and Freitas</cell><cell cols="3">[269] 2016 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">AntMiner+ and ALBA</cell><cell>Verbeke et al.</cell><cell cols="3">[270] 2011 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Argumentation</cell><cell/><cell>Rizzo and Longo</cell><cell>[142, 143]</cell><cell cols="2">2018 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Argumentation</cell><cell/><cell>Zeng et al.</cell><cell cols="3">[279] 2018 R</cell><cell>AH</cell><cell>G</cell><cell>C / R</cell><cell>P</cell></row><row><cell/><cell/><cell/><cell>[273,</cell><cell>2012,</cell><cell/><cell/><cell/><cell/></row><row><cell cols="2">Bayesian Rule Lists (BRL)</cell><cell>Letham et al.</cell><cell>274,</cell><cell>2013,</cell><cell>R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell/><cell/><cell/><cell>275]</cell><cell>2015</cell><cell/><cell/><cell/><cell/></row><row><cell cols="2">Bayesian Rule Sets (BRS)</cell><cell>Wang et al.</cell><cell>[276, 277]</cell><cell>2016, 2017</cell><cell>R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Interpretable Decision Set</cell><cell>Lakkaraju et al.</cell><cell cols="3">[272] 2016 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Exception Directed Acyclic Graphs (EDAGs)</cell><cell>Gaines</cell><cell cols="3">[271] 1996 M</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>ExpliClas</cell><cell/><cell>Alonso</cell><cell cols="3">[289] 2019 M</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">First Order Combined Learner (FOCL)</cell><cell>Pazzani</cell><cell cols="3">[278] 1997 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Fuzzy logic</cell><cell/><cell>Pierrard et al.</cell><cell cols="3">[282] 2018 R</cell><cell>AH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell>Fuzzy system</cell><cell/><cell>Jin</cell><cell cols="3">[281] 2000 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Fuzzy Inference-Grams (Fin-grams)</cell><cell>Pancho et al.</cell><cell cols="3">[288] 2013 V</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Fuzzy Inference Systems</cell><cell>Keneni et al.</cell><cell cols="3">[287] 2019 T</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>TS</cell></row><row><cell>Genetics-Based Learning (GBML)</cell><cell>Machine</cell><cell>Ishibuchi and No-jima</cell><cell cols="3">[280] 2007 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Interpretable Rule Mining (ICRM) Classification</cell><cell>Cano et al.</cell><cell cols="3">[284] 2013 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Linear Programming Relax-ation</cell><cell>Malioutov et al., Su et al.</cell><cell>[285, 286]</cell><cell>2017, 2016</cell><cell>R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Multi-Objective Evolutionary</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Algorithms based Interpretable</cell><cell>Wang and Palade</cell><cell cols="3">[283] 2011 R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Fuzzy (MOEAIF)</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Mycin</cell><cell/><cell>Shortliffe et al.</cell><cell>[3]</cell><cell cols="2">1975 T</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="78,106.36,175.27,386.99,491.10"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="78,132.99,175.27,355.93,7.12;78,106.36,184.73,382.56,7.12;78,106.36,194.20,382.56,7.12;78,106.36,203.66,56.17,7.12">12: Methods for explainability for data-driven approaches, classified according to the output format (M: Mixed, N: Numerical, R: Rules, T: Textual, N: Numerical), stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorial, T: Textual, TS: Time Series).</s></p></div></figDesc><table coords="78,112.33,223.49,381.01,442.88"><row><cell cols="2">Method for explainability</cell><cell/><cell>Authors</cell><cell/><cell>Ref</cell><cell>Year</cell><cell>Construction approach</cell><cell>Output Format</cell><cell cols="2">Stage Scope Problem Input</cell></row><row><cell cols="3">Contribution Propa-gation</cell><cell cols="2">Landecker et al.</cell><cell cols="2">[308] 2013</cell><cell>Hierarchical networks</cell><cell>V</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>P</cell></row><row><cell cols="2">DT extraction</cell><cell/><cell cols="2">Andrzejak et al.</cell><cell cols="2">[294] 2013</cell><cell>Distributed DTs</cell><cell>R</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell/><cell/><cell/><cell cols="2">Ferri et al.,</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">DT extraction</cell><cell/><cell cols="2">Van Assche and Block-</cell><cell>[292, 293]</cell><cell>2002, 2007</cell><cell>Ensembles</cell><cell>R</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell/><cell/><cell/><cell>eel</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">DT extraction</cell><cell/><cell cols="5">Alonso et al. [291] 2018 Ensembles</cell><cell>T</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="3">Discriminative Pat-terns</cell><cell cols="2">Gao et al.</cell><cell cols="3">[297] 2017 Ensembles</cell><cell>T</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>T</cell></row><row><cell cols="2">Explaining</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Bayesian work</cell><cell cols="2">net-Inferences</cell><cell cols="2">Yap et al.</cell><cell cols="2">[305] 2008</cell><cell>Bayesian networks</cell><cell>R</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>(EBI)</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell>Hyperplane-</cell><cell/><cell/></row><row><cell cols="2">ExtractRule</cell><cell/><cell cols="2">Fung et al.</cell><cell cols="2">[138] 2005</cell><cell>Based Linear</cell><cell>R</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>P; NC</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell>Classifiers</cell><cell/><cell/></row><row><cell cols="3">Factorized Asymp-totic Bayesian (FAB) inference</cell><cell>Hara Hayashi</cell><cell>and</cell><cell cols="3">[295] 2018 Ensembles</cell><cell>R</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="3">Feature Tweaking</cell><cell>Tolomei et al.</cell><cell/><cell cols="3">[290] 2017 Ensembles</cell><cell>N</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="3">Important Support Vectors and Border Classification</cell><cell>Barbella et al.</cell><cell/><cell cols="3">[146] 2009 SVM</cell><cell>N</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell>inTrees</cell><cell/><cell/><cell>Deng</cell><cell/><cell cols="3">[296] 2018 Ensembles</cell><cell>R</cell><cell>PH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="2">Nomograms</cell><cell/><cell cols="5">Jakulin et al. [302] 2005 SVM</cell><cell>V</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Nomograms</cell><cell/><cell>Možina et al.</cell><cell/><cell cols="3">[304] 2004 Naïve Bayes</cell><cell>V</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="3">Probabilistically Supported Argu-ments</cell><cell>Timmer et al.</cell><cell/><cell cols="2">[307] 2017</cell><cell>Bayesian networks</cell><cell>M</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Scenarios</cell><cell/><cell/><cell cols="2">Vlek et al.</cell><cell cols="2">[306] 2016</cell><cell>Bayesian networks</cell><cell>T</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Self-Organizing Maps</cell><cell/><cell>Hamel</cell><cell/><cell cols="3">[301] 2006 SVM</cell><cell>V</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="3">SVM+Prototypes</cell><cell cols="2">Núñez et al.</cell><cell cols="3">[299] 2002 SVM</cell><cell>M</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="3">Tree Space Proto-types</cell><cell cols="2">Tan et al.</cell><cell cols="3">[298] 2016 Ensembles</cell><cell>M</cell><cell>PH</cell><cell>L</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Visualization</cell><cell>for</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="3">RIsk Factor Analy-</cell><cell cols="2">Cho et al.</cell><cell cols="3">[303] 2008 SVM</cell><cell>V</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">sis (VRIFA)</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Weighted Classifier</cell><cell cols="2">Linear</cell><cell>Caragea et al.</cell><cell/><cell cols="3">[300] 2003 SVM</cell><cell>N</cell><cell>PH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="79,106.36,217.86,382.57,405.92"><head>Table A .</head><label>A</label><figDesc><div><p><s coords="79,134.10,217.86,354.82,7.12;79,106.36,227.32,382.56,7.12;79,106.36,236.79,382.57,7.12;79,106.36,246.25,162.64,7.12">13: Methods for explainability generating white-box models, classified according to the output format (M: Mixed, N: Numerical, R: Rules, T: Textual, N: Numerical), stage (AH: Ante-hoc, PH: Post-hoc), type of problems (C: Classification, R: Regression), scope (G: Global, L: Local) and input data (NC: Numerical/Categorical, P: Pictorial, T: Textual, TS: Time Series) of the underlying model.</s></p></div></figDesc><table coords="79,112.33,266.08,372.46,357.70"><row><cell>Table A.14:</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Method for explainability</cell><cell>Authors</cell><cell/><cell>Ref</cell><cell>Year</cell><cell>Construction approach</cell><cell>Output Format</cell><cell cols="3">Stage Scope Problem Input</cell></row><row><cell cols="2">Bayesian Model (BCM) Case</cell><cell cols="2">Kim et al.</cell><cell cols="2">[309] 2014</cell><cell>Bayesian reasoning case-based</cell><cell>M</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>P; T</cell></row><row><cell cols="2">Gaussian cess Regression Pro-(GPR)</cell><cell>Caywood et al.</cell><cell/><cell cols="2">[310] 2017</cell><cell>Gaussian Regression Process</cell><cell>N</cell><cell>AH</cell><cell>G</cell><cell>R</cell><cell>TS</cell></row><row><cell cols="2">General Additive Models (GAMs)</cell><cell cols="2">Lou et al., Lou et al.</cell><cell>[99, 131]</cell><cell>2012</cell><cell>Additive models</cell><cell>M</cell><cell>AH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="2">GAMs with pair-wise interactions (GA 2 Ms)</cell><cell cols="2">Caruana et al., Lou et al.</cell><cell cols="2">[311] 2015</cell><cell>Additive models</cell><cell>M</cell><cell>AH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="2">Mind the Gap Model (MGM)</cell><cell cols="2">Kim et al.</cell><cell cols="2">[316] 2015</cell><cell>Mind Gap Model the (MGM)</cell><cell>M</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>P; NC; T</cell></row><row><cell cols="2">Multi-Run Subtree Encapsu-lation</cell><cell cols="2">Howard and Edwards</cell><cell cols="2">[314] 2018</cell><cell>Tree-based gramming Genetic Pro-</cell><cell>M</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell>Oblique</cell><cell>Treed</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Sparse tive</cell><cell>Addi-Models</cell><cell cols="2">Wang et al.</cell><cell cols="2">[312] 2015</cell><cell>Additive models</cell><cell>N</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">(OT-SpAMs)</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Probabilistic Sentential Deci-sion Diagrams (PSDD)</cell><cell>Liang Van Broeck</cell><cell>and den</cell><cell cols="2">[315] 2017</cell><cell>Probabilistic diagrams sentential decision</cell><cell>R</cell><cell>AH</cell><cell>G</cell><cell>C / R</cell><cell>NC</cell></row><row><cell cols="2">Supersparse Lin-ear Integer Model (SLIM)</cell><cell cols="2">Ustun et al.</cell><cell cols="2">[317] 2014</cell><cell>Scoring sys-tems</cell><cell>N</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">Unsupervised</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Interpretable Word Sense</cell><cell cols="2">Panchenko et al.</cell><cell cols="3">[318] 2017 UIWSD</cell><cell>V</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>T</cell></row><row><cell cols="2">Disambiguation</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Transparent Gen-</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">eralized Additive Model Tree</cell><cell>Fahner</cell><cell/><cell cols="2">[313] 2018</cell><cell>Additive models</cell><cell>R</cell><cell>AH</cell><cell>G</cell><cell>C</cell><cell>NC</cell></row><row><cell cols="2">(TGAMT)</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="72,106.36,144.18,102.20,8.82">Appendix A. Appendix</head><p><s coords="80,112.33,609.79,78.94,8.01;80,294.37,609.79,36.34,8.01;80,112.33,621.14,114.11,8.01;80,294.37,621.14,170.08,8.01;80,112.33,632.10,164.15,8.01;80,294.37,632.10,55.29,8.01;80,112.33,643.06,93.89,8.01;80,294.37,643.06,19.42,8.01">Comparison approach Reference Sensitivity to input perturbation <ref type="bibr" coords="80,294.37,621.14,18.68,8.01" target="#b326">[324,</ref><ref type="bibr" coords="80,315.09,621.14,15.69,8.01" target="#b327">325,</ref><ref type="bibr" coords="80,332.83,621.14,11.21,8.01" target="#b89">90,</ref><ref type="bibr" coords="80,346.08,621.14,15.69,8.01" target="#b328">326,</ref><ref type="bibr" coords="80,363.81,621.14,15.69,8.01" target="#b329">327,</ref><ref type="bibr" coords="80,381.55,621.14,15.69,8.01" target="#b333">331,</ref><ref type="bibr" coords="80,399.28,621.14,15.69,8.01" target="#b334">332,</ref><ref type="bibr" coords="80,417.02,621.14,11.21,8.01" target="#b91">92,</ref><ref type="bibr" coords="80,430.27,621.14,15.69,8.01" target="#b332">330,</ref><ref type="bibr" coords="80,448.01,621.14,12.33,8.01" target="#b331">329</ref>] Sensitivity to model parameter randomization <ref type="bibr" coords="80,294.37,632.10,18.68,8.01" target="#b326">[324,</ref><ref type="bibr" coords="80,315.29,632.10,15.69,8.01" target="#b327">325,</ref><ref type="bibr" coords="80,333.22,632.10,16.44,8.01" target="#b357">355]</ref> Explanation completeness <ref type="bibr" coords="80,294.37,643.06,19.42,8.01" target="#b335">[333]</ref></s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="54,128.60,164.01,360.32,7.12;54,128.60,173.47,282.87,7.12" xml:id="b0">
	<analytic>
		<title level="a" type="main">Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)</title>
		<author>
			<persName coords=""><forename type="first">Amina</forename><surname>Adadi</surname></persName>
			<idno type="ORCID">0000-0002-9697-666X</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammed</forename><surname>Berrada</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2018.2870052</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<title level="j" type="abbrev">IEEE Access</title>
		<idno type="ISSNe">2169-3536</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160"/>
			<date type="published" when="2018">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Amina Adadi and Mohammed Berrada. Peeking inside the black-box: A survey on explainable artificial intelli- gence (xai). IEEE Access, 6:52138-52160, 2018. doi: 10.1109/ACCESS.2018.2870052.</note>
</biblStruct>

<biblStruct coords="54,128.60,182.94,360.32,7.12;54,128.60,192.40,301.41,7.12" xml:id="b1">
	<analytic>
		<title level="a" type="main">Asking ‘Why’ in AI: Explainability of intelligent systems – perspectives and challenges</title>
		<author>
			<persName coords=""><forename type="first">Alun</forename><surname>Preece</surname></persName>
			<idno type="ORCID">0000-0003-0349-9057</idno>
		</author>
		<idno type="DOI">10.1002/isaf.1422</idno>
	</analytic>
	<monogr>
		<title level="j">Intelligent Systems in Accounting, Finance and Management</title>
		<title level="j" type="abbrev">Intell Sys Acc Fin Mgmt</title>
		<idno type="ISSN">1055-615X</idno>
		<idno type="ISSNe">1099-1174</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="72"/>
			<date type="published" when="2018-04">2018</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alun Preece. Asking "why" in ai: Explainability of intelligent systems-perspectives and challenges. Intelligent Systems in Accounting, Finance and Management, 25(2):63-72, 2018. doi: 10.1002/isaf.1422.</note>
</biblStruct>

<biblStruct coords="54,128.60,201.87,360.32,7.12;54,128.60,211.33,360.32,7.12;54,128.60,220.80,356.90,7.12" xml:id="b2">
	<analytic>
		<title level="a" type="main">Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the mycin system</title>
		<author>
			<persName coords=""><forename type="first">Randall</forename><surname>Edward H Shortliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><forename type="middle">G</forename><surname>Stanton G Axline</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buchanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanley</forename><forename type="middle">N</forename><surname>Cordell Green</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-4809(75)90009-9</idno>
	</analytic>
	<monogr>
		<title level="j">Computers and biomedical research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="320"/>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Edward H Shortliffe, Randall Davis, Stanton G Axline, Bruce G Buchanan, C Cordell Green, and Stanley N Cohen. Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the mycin system. Computers and biomedical research, 8(4):303-320, 1975. doi: 10.1016/0010-4809(75)90009-9.</note>
</biblStruct>

<biblStruct coords="54,128.60,230.26,360.32,7.12;54,128.60,239.73,332.19,7.12" xml:id="b3">
	<analytic>
		<title level="a" type="main">Research directions in interpretable machine learning models</title>
		<author>
			<persName coords=""><forename type="first">Vanya</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Belle</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Lisboa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st European Symposium on Artificial Neural Networks, ESANN</title>
		<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="533" to="541"/>
		</imprint>
	</monogr>
	<note>i6doc</note>
	<note type="raw_reference">Vanya Van Belle and Paulo Lisboa. Research directions in interpretable machine learning models. In 21st Euro- pean Symposium on Artificial Neural Networks, ESANN, pages 533-541, Bruges, Belgium, 2013. i6doc.</note>
</biblStruct>

<biblStruct coords="54,128.60,249.19,360.32,7.12;54,128.60,258.65,348.26,7.12" xml:id="b4">
	<analytic>
		<title level="a" type="main">What should be in an xai explanation? what ift reveals</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Penney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Burnett</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">IUI Workshop</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Explainable Smart Systems -ExSS</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Jonathan Dodge, Sean Penney, Andrew Anderson, and Margaret M Burnett. What should be in an xai explanation? what ift reveals. In IUI Workshop 7: Explainable Smart Systems -ExSS, Tokyo, Japan, 2018. CEUR-WS.org.</note>
</biblStruct>

<biblStruct coords="54,128.60,268.12,360.32,7.12;54,128.60,277.58,360.32,7.12;54,128.60,287.05,71.07,7.12" xml:id="b5">
	<analytic>
		<title level="a" type="main">Making machine learning models interpretable</title>
		<author>
			<persName coords=""><forename type="first">Alfredo</forename><surname>Vellido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>David Martín-Guerrero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><forename type="middle">Jg</forename><surname>Lisboa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks</title>
		<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. i6doc</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="163" to="172"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alfredo Vellido, José David Martín-Guerrero, and Paulo JG Lisboa. Making machine learning models inter- pretable. In European Symposium on Artificial Neural Networks, ESANN, volume 12, pages 163-172, Bruges, Belgium, 2012. i6doc.</note>
</biblStruct>

<biblStruct coords="54,128.60,296.51,360.32,7.12;54,128.60,305.98,360.25,7.12;54,128.60,315.44,72.45,7.12" xml:id="b6">
	<analytic>
		<title level="a" type="main">Recommendation Agents for Electronic Commerce: Effects of Explanation Facilities on Trusting Beliefs</title>
		<author>
			<persName coords=""><forename type="first">Weiquan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izak</forename><surname>Benbasat</surname></persName>
		</author>
		<idno type="DOI">10.2753/mis0742-1222230410</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Management Information Systems</title>
		<title level="j" type="abbrev">Journal of Management Information Systems</title>
		<idno type="ISSN">0742-1222</idno>
		<idno type="ISSNe">1557-928X</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="217" to="246"/>
			<date type="published" when="2007-05">2007</date>
			<publisher>Informa UK Limited</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Weiquan Wang and Izak Benbasat. Recommendation agents for electronic commerce: Effects of explanation facilities on trusting beliefs. Journal of Management Information Systems, 23(4):217-246, 2007. doi: 10.2753/ mis0742-1222230410.</note>
</biblStruct>

<biblStruct coords="54,128.60,324.91,360.32,7.12;54,128.60,334.37,360.32,7.12;54,128.60,343.83,193.06,7.12" xml:id="b7">
	<analytic>
		<title level="a" type="main">Unmasking Clever Hans predictors and assessing what machines really learn</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephan</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-019-08987-4</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<title level="j" type="abbrev">Nat Commun</title>
		<idno type="ISSNe">2041-1723</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1096</biblScope>
			<date type="published" when="2019-03-11">2019</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sebastian Lapuschkin, Stephan Wäldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek, and Klaus- Robert Müller. Unmasking clever hans predictors and assessing what machines really learn. Nature communica- tions, 10(1):1096, 2019. doi: 10.1038/s41467-019-08987-4.</note>
</biblStruct>

<biblStruct coords="54,128.60,353.30,360.32,7.12;54,128.60,362.76,360.32,7.12;54,128.60,372.23,145.42,7.12" xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms for interpretable machine learning</title>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2630823</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-08-24">2014</date>
			<biblScope unit="page" from="1519" to="1519"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Cynthia Rudin. Algorithms for interpretable machine learning. In Proceedings of the 20th ACM SIGKDD inter- national conference on Knowledge discovery and data mining, pages 1519-1519, New York, New York, USA, 2014. ACM. doi: 10.1145/2623330.2630823.</note>
</biblStruct>

<biblStruct coords="54,128.60,381.69,360.32,7.12;54,128.60,391.16,309.20,7.12" xml:id="b9">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
			<idno type="ORCID">0000-0003-4283-2780</idno>
		</author>
		<idno type="DOI">10.1038/s42256-019-0048-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<title level="j" type="abbrev">Nat Mach Intell</title>
		<idno type="ISSNe">2522-5839</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215"/>
			<date type="published" when="2019-05-13">2019</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206, 2019. doi: 10.1038/s42256-019-0048-x.</note>
</biblStruct>

<biblStruct coords="54,128.60,400.62,360.32,7.12;54,128.60,410.09,360.32,7.12;54,128.60,419.55,100.81,7.12" xml:id="b10">
	<analytic>
		<title level="a" type="main">Explainable Artificial Intelligence for Neuroscience: Behavioral Neurostimulation</title>
		<author>
			<persName coords=""><forename type="first">Jean-Marc</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helen</forename><forename type="middle">S</forename><surname>Mayberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Ferrante</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2019.01346</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<title level="j" type="abbrev">Front. Neurosci.</title>
		<idno type="ISSNe">1662-453X</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1346</biblScope>
			<date type="published" when="2019-12-13">2019</date>
			<publisher>Frontiers Media SA</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Jean-Marc Fellous, Guillermo Sapiro, Andrew Rossi, Helen S Mayberg, and Michele Ferrante. Explainable artificial intelligence for neuroscience: Behavioral neurostimulation. Frontiers in Neuroscience, 13:1346, 2019. doi: 10.3389/fnins.2019.01346.</note>
</biblStruct>

<biblStruct coords="54,128.60,429.02,360.33,7.12;54,128.60,438.48,360.32,7.12;54,128.60,447.94,84.55,7.12" xml:id="b11">
	<analytic>
		<title level="a" type="main">Explainable planning</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniele</forename><surname>Magazzeni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="24" to="30"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Maria Fox, Derek Long, and Daniele Magazzeni. Explainable planning. In IJCAI 2017 Workshop on Explain- able Artificial Intelligence (XAI), pages 24-30, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="54,128.60,457.41,360.32,7.12;54,128.60,466.93,360.32,6.97;54,128.60,476.34,272.80,7.12" xml:id="b12">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence: A survey</title>
		<author>
			<persName coords=""><forename type="first">Filip</forename><surname>Karlo Došilović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Brčić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikica</forename><surname>Hlupić</surname></persName>
		</author>
		<idno type="DOI">10.23919/mipro.2018.8400040</idno>
	</analytic>
	<monogr>
		<title level="m">41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)</title>
		<meeting><address><addrLine>Opatija, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="210" to="0215"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Filip Karlo Došilović, Mario Brčić, and Nikica Hlupić. Explainable artificial intelligence: A survey. In 41st Inter- national Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pages 0210-0215, Opatija, Croatia, 2018. IEEE. doi: 10.23919/mipro.2018.8400040.</note>
</biblStruct>

<biblStruct coords="54,128.60,485.80,360.32,7.12;54,128.60,495.27,360.32,7.12;54,128.60,504.73,114.21,7.12" xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards Trust, Transparency and Liability in AI / AS systems</title>
		<author>
			<persName coords=""><forename type="first">Eva</forename><surname>Thelisson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kirtan</forename><surname>Padh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elisa</forename><surname>Celis</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/767</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="53" to="57"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Eva Thelisson, Kirtan Padh, and L Elisa Celis. Regulatory mechanisms and algorithms towards trust in ai/ml. In IJCAI Workshop on Explainable AI (XAI), pages 53-57, Melbourne, Australia, 2017. International Joint Confer- ences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="54,128.60,514.20,360.32,7.12;54,128.60,523.66,360.32,7.12;54,128.60,533.12,239.83,7.12" xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards Trust, Transparency and Liability in AI / AS systems</title>
		<author>
			<persName coords=""><forename type="first">Eva</forename><surname>Thelisson</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/767</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="5215" to="5216"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Eva Thelisson. Towards trust, transparency, and liability in ai/as systems. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 5215-5216, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc. doi: 10.24963/IJCAI.2017/767.</note>
</biblStruct>

<biblStruct coords="54,128.60,542.59,360.32,7.12;54,128.60,552.05,204.82,7.12" xml:id="b15">
	<analytic>
		<title level="a" type="main">Transparent, explainable, and accountable AI for robotics</title>
		<author>
			<persName coords=""><forename type="first">Sandra</forename><surname>Wachter</surname></persName>
			<idno type="ORCID">0000-0003-3800-0113</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Brent</forename><surname>Mittelstadt</surname></persName>
			<idno type="ORCID">0000-0002-4709-6404</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Luciano</forename><surname>Floridi</surname></persName>
			<idno type="ORCID">0000-0002-5444-2280</idno>
		</author>
		<idno type="DOI">10.1126/scirobotics.aan6080</idno>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<title level="j" type="abbrev">Sci. Robot.</title>
		<idno type="ISSNe">2470-9476</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2017</biblScope>
			<date type="published" when="2017-05-31"/>
			<publisher>American Association for the Advancement of Science (AAAS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sandra Wachter, Brent Mittelstadt, and Luciano Floridi. Transparent, explainable, and accountable ai for robotics. Science Robotics, 2(6), 2017. doi: 10.1126/scirobotics.aan6080.</note>
</biblStruct>

<biblStruct coords="54,128.60,561.52,360.32,7.12;54,128.60,570.98,360.32,7.12;54,128.60,580.45,104.93,7.12" xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards explainable artificial intelligence</title>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-6\1</idno>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5" to="22"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Wojciech Samek and Klaus-Robert Müller. Towards explainable artificial intelligence. In Explainable AI: In- terpreting, Explaining and Visualizing Deep Learning, pages 5-22. Springer, Cham, Switzerland, 2019. doi: 10.1007/978-3-030-28954-6\ 1.</note>
</biblStruct>

<biblStruct coords="54,128.60,589.91,360.32,7.12;54,128.60,599.38,252.20,7.12" xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of explanation methods for Bayesian networks</title>
		<author>
			<persName coords=""><forename type="first">Carmen</forename><surname>Lacave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Díez</surname></persName>
		</author>
		<idno type="DOI">10.1017/s026988890200019x</idno>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<title level="j" type="abbrev">The Knowledge Engineering Review</title>
		<idno type="ISSN">0269-8889</idno>
		<idno type="ISSNe">1469-8005</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="127"/>
			<date type="published" when="2002-06">2002</date>
			<publisher>Cambridge University Press (CUP)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Carmen Lacave and Francisco J Díez. A review of explanation methods for bayesian networks. The Knowledge Engineering Review, 17(2):107-127, 2002. doi: 10.1017/s026988890200019x.</note>
</biblStruct>

<biblStruct coords="54,128.60,608.84,360.32,7.12;54,128.60,618.31,360.32,7.12;54,128.60,627.77,123.82,7.12" xml:id="b18">
	<analytic>
		<title level="a" type="main">Comprehensible credit scoring models using rule extraction from support vector machines</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Vanthienen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ejor.2006.04.051</idno>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<title level="j" type="abbrev">European Journal of Operational Research</title>
		<idno type="ISSN">0377-2217</idno>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1466" to="1476"/>
			<date type="published" when="2007-12">2007</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">David Martens, Bart Baesens, Tony Van Gestel, and Jan Vanthienen. Comprehensible credit scoring models using rule extraction from support vector machines. European journal of operational research, 183(3):1466-1476, 2007. doi: 10.1016/j.ejor.2006.04.051.</note>
</biblStruct>

<biblStruct coords="54,128.60,637.23,360.32,7.12;54,128.60,646.70,223.00,7.12" xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual Analytics for Explainable Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/mcg.2018.042731661</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<title level="j" type="abbrev">IEEE Comput. Grap. Appl.</title>
		<idno type="ISSN">0272-1716</idno>
		<idno type="ISSNe">1558-1756</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="84" to="92"/>
			<date type="published" when="2018-07">2018</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Jaegul Choo and Shixia Liu. Visual analytics for explainable deep learning. IEEE Computer Graphics and Applications, 38(4):84-92, 2018. doi: 10.1109/mcg.2018.042731661.</note>
</biblStruct>

<biblStruct coords="54,128.60,656.16,360.32,7.12;54,128.60,665.63,360.32,7.12;54,128.60,675.09,72.91,7.12" xml:id="b20">
	<analytic>
		<title level="a" type="main">A Survey of Methods for Explaining Black Box Models</title>
		<author>
			<persName coords=""><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
			<idno type="ORCID">0000-0002-2827-7613</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3236009</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<title level="j" type="abbrev">ACM Comput. Surv.</title>
		<idno type="ISSN">0360-0300</idno>
		<idno type="ISSNe">1557-7341</idno>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="42"/>
			<date type="published" when="2018-08-22">2018</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. A survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5):93:1-93:42, 2018. doi: 10.1145/3236009.</note>
</biblStruct>

<biblStruct coords="54,128.60,684.56,360.32,7.12;54,128.60,694.02,148.62,7.12" xml:id="b21">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2018.07.007</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<title level="j" type="abbrev">Artificial Intelligence</title>
		<idno type="ISSN">0004-3702</idno>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38"/>
			<date type="published" when="2019-02">2019</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Tim Miller. Explanation in artificial intelligence: insights from the social sciences. Artificial Intelligence, 267: 1-38, 2019. doi: 10.1016/j.artint.2018.07.007.</note>
</biblStruct>

<biblStruct coords="55,128.60,145.58,360.32,7.12;55,128.60,155.04,360.32,7.12;55,128.60,164.51,332.70,7.12" xml:id="b22">
	<analytic>
		<title level="a" type="main">Report on the 2019 International Joint Conferences on Artificial Intelligence Explainable Artificial Intelligence Workshop</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosina</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Magazzeni</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v41i1.5302</idno>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<title level="j" type="abbrev">AI Magazine</title>
		<idno type="ISSN">0738-4602</idno>
		<idno type="ISSNe">2371-9621</idno>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="105"/>
			<date type="published" when="2017">2017</date>
			<publisher>Wiley</publisher>
			<pubPlace>Melbourne, Australia</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Tim Miller, Piers Howe, and Liz Sonenberg. Explainable ai: Beware of inmates running the asylum or: How i learnt to stop worrying and love the social and behavioural sciences. In IJCAI Workshop on Explainable AI (XAI), pages 36-42, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="55,128.60,173.97,360.32,7.12;55,128.60,183.44,188.91,7.12" xml:id="b23">
	<analytic>
		<title level="a" type="main">Rule Extraction from Recurrent Neural Networks: ATaxonomy and Review</title>
		<author>
			<persName coords=""><forename type="first">Henrik</forename><surname>Jacobsson</surname></persName>
		</author>
		<idno type="DOI">10.1162/0899766053630350</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<title level="j" type="abbrev">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<idno type="ISSNe">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1223" to="1263"/>
			<date type="published" when="2005-06-01">2005</date>
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Henrik Jacobsson. Rule extraction from recurrent neural networks: A taxonomy and review. Neural Computation, 17(6):1223-1263, 2005. doi: 10.1162/0899766053630350.</note>
</biblStruct>

<biblStruct coords="55,128.60,192.90,360.32,7.12;55,128.60,202.37,360.32,7.12;55,128.60,211.83,360.32,7.12;55,128.60,221.29,78.93,7.12" xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating Recurrent Neural Network Explanations</title>
		<author>
			<persName coords=""><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w19-4813</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="126"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Leila Arras, Ahmed Osman, Klaus-Robert Müller, and Wojciech Samek. Evaluating recurrent neural network explanations. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 113-126, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4813.</note>
</biblStruct>

<biblStruct coords="55,128.60,230.76,360.32,7.12;55,128.60,240.22,360.32,7.12;55,128.60,249.69,174.64,7.12" xml:id="b25">
	<analytic>
		<title level="a" type="main">Explainable software analytics</title>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><surname>Khanh Dam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Ghose</surname></persName>
		</author>
		<idno type="DOI">10.1145/3183399.3183424</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results</title>
		<meeting>the 40th International Conference on Software Engineering: New Ideas and Emerging Results<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="53" to="56"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Hoa Khanh Dam, Truyen Tran, and Aditya Ghose. Explainable software analytics. In Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results, pages 53-56, Gothenburg, Sweden, 2018. ACM. doi: 10.1145/3183399.3183424.</note>
</biblStruct>

<biblStruct coords="55,128.60,259.15,360.32,7.12;55,128.60,268.62,360.32,7.12;55,128.60,278.08,351.55,7.12" xml:id="b26">
	<analytic>
		<title level="a" type="main">Explaining explanation for "explainable ai</title>
		<author>
			<persName coords=""><forename type="first">Gary</forename><surname>Robert R Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shane</forename><forename type="middle">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mueller</surname></persName>
		</author>
		<idno type="DOI">10.1177/1541931218621047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting<address><addrLine>Philadelphia, Pennsylvania, USA; Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Los</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="197" to="201"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Robert R Hoffman, Gary Klein, and Shane T Mueller. Explaining explanation for "explainable ai". In Proceed- ings of the Human Factors and Ergonomics Society Annual Meeting, volume 62, pages 197-201, Philadelphia, Pennsylvania, USA, 2018. SAGE Publications Sage CA: Los Angeles, CA. doi: 10.1177/1541931218621047.</note>
</biblStruct>

<biblStruct coords="55,128.60,287.55,360.32,7.12;55,128.60,297.01,360.32,7.12;55,128.60,306.47,262.14,7.12" xml:id="b27">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence applications in nlp, biomedical, and malware classification: A literature review</title>
		<author>
			<persName coords=""><forename type="first">Mary</forename><surname>Sherin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mathews</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22868-2\90</idno>
	</analytic>
	<monogr>
		<title level="m">Intelligent Computing-Proceedings of the Computing Conference</title>
		<meeting><address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1269" to="1292"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Sherin Mary Mathews. Explainable artificial intelligence applications in nlp, biomedical, and malware classifica- tion: A literature review. In Intelligent Computing-Proceedings of the Computing Conference, pages 1269-1292, London, United Kingdom, 2019. Springer. doi: 10.1007/978-3-030-22868-2\ 90.</note>
</biblStruct>

<biblStruct coords="55,128.60,315.94,360.32,7.12;55,128.60,325.40,294.95,7.12" xml:id="b28">
	<analytic>
		<title level="a" type="main">Methods for interpreting and understanding deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dsp.2017.10.011</idno>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<title level="j" type="abbrev">Digital Signal Processing</title>
		<idno type="ISSN">1051-2004</idno>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="15"/>
			<date type="published" when="2017">2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Methods for interpreting and understanding deep neural networks. Digital Signal Processing, 73:1-15, 2017. doi: 10.1016/j.dsp.2017.10.011.</note>
</biblStruct>

<biblStruct coords="55,128.60,334.87,360.32,7.12;55,128.60,343.73,185.62,7.97" xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual interpretability for deep learning: a survey</title>
		<author>
			<persName coords=""><forename type="first">Quan-Shi</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0002-6108-2738</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1631/fitee.1700808</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<title level="j" type="abbrev">Frontiers Inf Technol Electronic Eng</title>
		<idno type="ISSN">2095-9184</idno>
		<idno type="ISSNe">2095-9230</idno>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="39"/>
			<date type="published" when="2018-01">2018</date>
			<publisher>Zhejiang University Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Quan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of Information Technology &amp; Electronic Engineering, 19(1):27-39, 2018.</note>
</biblStruct>

<biblStruct coords="55,128.60,353.80,360.32,7.12;55,128.60,363.26,282.63,7.12" xml:id="b30">
	<analytic>
		<title level="a" type="main">Classification in high-dimensional spectral data: Accuracy vs. interpretability vs. model size</title>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Backhaus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Udo</forename><surname>Seiffert</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2013.09.048</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<title level="j" type="abbrev">Neurocomputing</title>
		<idno type="ISSN">0925-2312</idno>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="15" to="22"/>
			<date type="published" when="2014-05">2014</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Andreas Backhaus and Udo Seiffert. Classification in high-dimensional spectral data: Accuracy vs. interpretability vs. model size. Neurocomputing, 131:15-22, 2014. doi: 10.1016/j.neucom.2013.09.048.</note>
</biblStruct>

<biblStruct coords="55,128.60,372.73,360.32,7.12;55,128.60,382.19,349.11,7.12" xml:id="b31">
	<analytic>
		<title level="a" type="main">Explanation in second generation expert systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johanna</forename><forename type="middle">D</forename><surname>Swartout</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-77927-5\24</idno>
	</analytic>
	<monogr>
		<title level="m">Second generation expert systems</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="543" to="585"/>
		</imprint>
	</monogr>
	<note type="raw_reference">William R Swartout and Johanna D Moore. Explanation in second generation expert systems. In Second genera- tion expert systems, pages 543-585. Springer, Berlin, Germany, 1993. doi: 10.1007/978-3-642-77927-5\ 24.</note>
</biblStruct>

<biblStruct coords="55,128.60,391.66,360.32,7.12;55,128.60,401.12,248.87,7.12" xml:id="b32">
	<analytic>
		<title level="a" type="main">Explanations from Intelligent Systems: Theoretical Foundations and Implications for Practice</title>
		<author>
			<persName coords=""><forename type="first">Shirley</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izak</forename><surname>Benbasat</surname></persName>
		</author>
		<idno type="DOI">10.2307/249487</idno>
	</analytic>
	<monogr>
		<title level="j">MIS Quarterly</title>
		<title level="j" type="abbrev">MIS Quarterly</title>
		<idno type="ISSN">0276-7783</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">497</biblScope>
			<date type="published" when="1999-12">1999</date>
			<publisher>JSTOR</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Shirley Gregor and Izak Benbasat. Explanations from intelligent systems: Theoretical foundations and implica- tions for practice. MIS quarterly, 23(4):497-530, 1999. doi: 10.2307/249487.</note>
</biblStruct>

<biblStruct coords="55,128.60,410.58,360.32,7.12;55,128.60,420.05,360.32,7.12;55,128.60,429.51,130.18,7.12" xml:id="b33">
	<analytic>
		<title level="a" type="main">A generalized taxonomy of explanations styles for traditional and social recommender systems</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Panagiotis</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Manolopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-011-0215-0</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<title level="j" type="abbrev">Data Min Knowl Disc</title>
		<idno type="ISSN">1384-5810</idno>
		<idno type="ISSNe">1573-756X</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="583"/>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexis Papadimitriou, Panagiotis Symeonidis, and Yannis Manolopoulos. A generalized taxonomy of explanations styles for traditional and social recommender systems. Data Mining and Knowledge Discovery, 24(3):555-583, 2012. doi: 10.1007/s10618-011-0215-0.</note>
</biblStruct>

<biblStruct coords="55,128.60,438.98,360.32,7.12;55,128.60,448.44,279.51,7.12" xml:id="b34">
	<analytic>
		<title level="a" type="main">Explanation in Case-Based Reasoning–Perspectives and Goals</title>
		<author>
			<persName coords=""><forename type="first">Frode</forename><surname>Sørmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jörg</forename><surname>Cassens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Agnar</forename><surname>Aamodt</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-005-4607-7</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<title level="j" type="abbrev">Artif Intell Rev</title>
		<idno type="ISSN">0269-2821</idno>
		<idno type="ISSNe">1573-7462</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="143"/>
			<date type="published" when="2005-10">2005</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Frode Sørmo, Jörg Cassens, and Agnar Aamodt. Explanation in case-based reasoning-perspectives and goals. Artificial Intelligence Review, 24(2):109-143, 2005. doi: 10.1007/S10462-005-4607-7.</note>
</biblStruct>

<biblStruct coords="55,128.60,457.91,360.32,7.12;55,128.60,467.37,360.32,7.12;55,128.60,476.84,192.25,7.12" xml:id="b35">
	<analytic>
		<title level="a" type="main">Counterfactuals in explainable artificial intelligence (xai): evidence from human reasoning</title>
		<author>
			<persName coords=""><surname>Byrne</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/199</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on AI (IJCAI)</title>
		<meeting><address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1438" to="1444"/>
		</imprint>
	</monogr>
	<note type="raw_reference">R Byrne. Counterfactuals in explainable artificial intelligence (xai): evidence from human reasoning. In Inter- national Joint Conference on AI (IJCAI), pages 1438-1444, Macao, China, 2019. International Joint Conferences on Artificial Intelligence, Inc. doi: 10.24963/ijcai.2019/199.</note>
</biblStruct>

<biblStruct coords="55,128.60,486.30,360.32,7.12;55,128.60,495.76,360.32,7.12;55,128.60,505.23,155.66,7.12" xml:id="b36">
	<analytic>
		<title level="a" type="main">Paving the way to explainable artificial intelligence with fuzzy modeling</title>
		<author>
			<persName coords=""><forename type="first">Corrado</forename><surname>Mencar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Alonso</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-12544-8\17</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Fuzzy Logic and Applications</title>
		<meeting><address><addrLine>Santa Margherita Ligure, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="215" to="227"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Corrado Mencar and José M Alonso. Paving the way to explainable artificial intelligence with fuzzy modeling. In International Workshop on Fuzzy Logic and Applications, pages 215-227, Santa Margherita Ligure, Italy, 2018. Springer. doi: 10.1007/978-3-030-12544-8\ 17.</note>
</biblStruct>

<biblStruct coords="55,128.60,514.69,360.32,7.12;55,128.60,524.16,360.32,7.12;55,128.60,533.62,314.21,7.12" xml:id="b37">
	<analytic>
		<title level="a" type="main">Logic meets probability: towards explainable ai systems for uncertain worlds</title>
		<author>
			<persName coords=""><forename type="first">Belle</forename><surname>Vaishak</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/733</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="19" to="25"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Vaishak Belle. Logic meets probability: towards explainable ai systems for uncertain worlds. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pages 19-25, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc. doi: 10.24963/ijcai.2017/733.</note>
</biblStruct>

<biblStruct coords="55,128.60,543.09,360.32,7.12;55,128.60,552.55,336.07,7.12" xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing learning and computation in artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shavlik</surname></persName>
		</author>
		<idno type="DOI">10.1142/s0218213092000260</idno>
	</analytic>
	<monogr>
		<title level="j">International journal on artificial intelligence tools</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="399" to="425"/>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mark W Craven and Jude W Shavlik. Visualizing learning and computation in artificial neural networks. Inter- national journal on artificial intelligence tools, 1(03):399-425, 1992. doi: 10.1142/s0218213092000260.</note>
</biblStruct>

<biblStruct coords="55,128.60,562.02,360.32,7.12;55,128.60,571.48,357.40,7.12" xml:id="b39">
	<analytic>
		<title level="a" type="main">Interpretability in machine learning-principles and practice</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Paulo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lisboa</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-03200-9\2</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Fuzzy Logic and Applications</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="15" to="21"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Paulo JG Lisboa. Interpretability in machine learning-principles and practice. In International Workshop on Fuzzy Logic and Applications, pages 15-21, Genoa, Italy, 2013. Springer. doi: 10.1007/978-3-319-03200-9\ 2.</note>
</biblStruct>

<biblStruct coords="55,128.60,580.95,360.33,7.12;55,128.60,590.41,248.40,7.12" xml:id="b40">
	<analytic>
		<title level="a" type="main">Rule extraction algorithm for deep neural networks: A review</title>
		<author>
			<persName coords=""><forename type="first">Tameru</forename><surname>Hailesilassie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCSIS) International Journal of Computer Science and Information Security</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="376" to="381"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tameru Hailesilassie. Rule extraction algorithm for deep neural networks: A review. (IJCSIS) International Journal of Computer Science and Information Security, 14(7):376-381, 2016.</note>
</biblStruct>

<biblStruct coords="55,128.60,599.87,360.32,7.12;55,128.60,609.34,360.32,7.12;55,128.60,618.80,136.20,7.12" xml:id="b41">
	<analytic>
		<title level="a" type="main">Evolutionary Fuzzy Systems for Explainable Artificial Intelligence: Why, When, What for, and Where to?</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oscar</forename><surname>Cordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Jose Del Jesus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francesco</forename><surname>Marcelloni</surname></persName>
		</author>
		<idno type="DOI">10.1109/mci.2018.2881645</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<title level="j" type="abbrev">IEEE Comput. Intell. Mag.</title>
		<idno type="ISSN">1556-603X</idno>
		<idno type="ISSNe">1556-6048</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="81"/>
			<date type="published" when="2019-02">2019</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alberto Fernandez, Francisco Herrera, Oscar Cordon, Maria Jose del Jesus, and Francesco Marcelloni. Evolution- ary fuzzy systems for explainable artificial intelligence: Why, when, what for, and where to? IEEE Computational Intelligence Magazine, 14(1):69-81, 2019.</note>
</biblStruct>

<biblStruct coords="55,128.60,628.27,360.32,7.12;55,128.60,637.73,228.31,7.12" xml:id="b42">
	<analytic>
		<title level="a" type="main">Designing fuzzy inference systems from data: An interpretability-oriented review</title>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Guillaume</surname></persName>
		</author>
		<idno type="DOI">10.1109/91.928739</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Fuzzy Syst.</title>
		<idno type="ISSN">1063-6706</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="426" to="443"/>
			<date type="published" when="2001-06">2001</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Serge Guillaume. Designing fuzzy inference systems from data: An interpretability-oriented review. IEEE trans- actions on fuzzy systems, 9(3):426-443, 2001. doi: 10.1109/91.928739.</note>
</biblStruct>

<biblStruct coords="55,128.60,647.20,360.32,7.12;55,128.60,656.66,360.32,7.12;55,128.60,666.13,104.34,7.12" xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards integrated neural-symbolic systems for human-level ai: Two research programs helping to bridge the gaps</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai-Uwe</forename><surname>Besold</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kühnberger</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bica.2015.09.003</idno>
	</analytic>
	<monogr>
		<title level="j">Biologically Inspired Cognitive Architectures</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="97" to="110"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tarek R Besold and Kai-Uwe Kühnberger. Towards integrated neural-symbolic systems for human-level ai: Two research programs helping to bridge the gaps. Biologically Inspired Cognitive Architectures, 14:97-110, 2015. doi: 10.1016/j.bica.2015.09.003.</note>
</biblStruct>

<biblStruct coords="55,128.60,675.59,360.32,7.12;55,128.60,685.06,360.32,7.12;55,128.60,694.52,360.32,7.12;56,128.60,145.58,40.73,7.12" xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural-symbolic learning and reasoning: contributions and challenges</title>
		<author>
			<persName coords=""><forename type="first">Artur D'avila</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luc</forename><surname>Tarek R Besold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Földiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai-Uwe</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><forename type="middle">C</forename><surname>Kühnberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Risto</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">L</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium Series</title>
		<meeting><address><addrLine>Palo Alto, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="20" to="23"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Artur d'Avila Garcez, Tarek R Besold, Luc De Raedt, Peter Földiak, Pascal Hitzler, Thomas Icard, Kai-Uwe Kühnberger, Luis C Lamb, Risto Miikkulainen, and Daniel L Silver. Neural-symbolic learning and reasoning: contributions and challenges. In AAAI Spring Symposium Series, pages 20-23, Palo Alto, California, USA, 2015. AAAI Press.</note>
</biblStruct>

<biblStruct coords="56,128.60,155.04,360.32,7.12;56,128.60,164.56,360.32,6.97;56,128.60,173.97,360.32,7.12" xml:id="b45">
	<analytic>
		<title level="a" type="main">Automated reasoning for explainable artificial intelligence</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Paola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonacina</forename></persName>
		</author>
		<idno type="DOI">10.29007/4b7h</idno>
	</analytic>
	<monogr>
		<title level="m">The First International ARCADE (Automated Reasoning: Challenges, Applications, Directions, Exemplary Achievements) Workshop (in association with CADE-26)</title>
		<meeting><address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="24" to="28"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Maria Paola Bonacina. Automated reasoning for explainable artificial intelligence. In The First International ARCADE (Automated Reasoning: Challenges, Applications, Directions, Exemplary Achievements) Workshop (in association with CADE-26), pages 24-28, Gothenburg, Sweden, 2017. ARCADE@ CADE. doi: 10.29007/4b7h.</note>
</biblStruct>

<biblStruct coords="56,128.60,183.44,360.32,7.12;56,128.60,192.90,360.32,7.12" xml:id="b46">
	<analytic>
		<title level="a" type="main">Safe and interpretable machine learning: a methodological review</title>
		<author>
			<persName coords=""><forename type="first">Clemens</forename><surname>Otte</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-32378-2\8</idno>
	</analytic>
	<monogr>
		<title level="m">Computational intelligence in intelligent data analysis</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="111" to="122"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Clemens Otte. Safe and interpretable machine learning: a methodological review. In Computational intelligence in intelligent data analysis, pages 111-122. Springer, Berlin, Germany, 2013. doi: 10.1007/978-3-642-32378-2\ 8.</note>
</biblStruct>

<biblStruct coords="56,128.60,202.37,360.32,7.12;56,128.60,211.83,360.32,7.12;56,128.60,221.29,154.49,7.12" xml:id="b47">
	<analytic>
		<title level="a" type="main">On the Importance of Comprehensible Classification Models for Protein Function Prediction</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniela</forename><forename type="middle">C</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rolf</forename><surname>Apweiler</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcbb.2008.47</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<title level="j" type="abbrev">IEEE/ACM Trans. Comput. Biol. and Bioinf.</title>
		<idno type="ISSN">1545-5963</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="182"/>
			<date type="published" when="2010-01">2010</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex A Freitas, Daniela C Wieser, and Rolf Apweiler. On the importance of comprehensible classification models for protein function prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB), 7(1):172-182, 2010. doi: 10.1109/tcbb.2008.47.</note>
</biblStruct>

<biblStruct coords="56,128.60,230.76,360.32,7.12;56,128.60,240.22,316.30,7.12" xml:id="b48">
	<analytic>
		<title level="a" type="main">Performance of classification models from a user perspective</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wouter</forename><surname>Verbeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Baesens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dss.2011.01.013</idno>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<title level="j" type="abbrev">Decision Support Systems</title>
		<idno type="ISSN">0167-9236</idno>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="782" to="793"/>
			<date type="published" when="2011-11">2011</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">David Martens, Jan Vanthienen, Wouter Verbeke, and Bart Baesens. Performance of classification models from a user perspective. Decision Support Systems, 51(4):782-793, 2011. doi: 10.1016/j.dss.2011.01.013.</note>
</biblStruct>

<biblStruct coords="56,128.60,249.69,360.32,7.12;56,128.60,259.15,360.32,7.12;56,128.60,268.62,153.84,7.12" xml:id="b49">
	<analytic>
		<title level="a" type="main">Human-Centric Justification of Machine Learning Predictions</title>
		<author>
			<persName coords=""><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2017-08">2017</date>
			<biblScope unit="page" from="8" to="13"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Or Biran and Courtenay Cotton. Explanation and justification in machine learning: A survey. In IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI), pages 8-13, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="56,128.60,278.08,360.32,7.12;56,128.60,286.95,360.32,7.97;56,128.60,297.01,211.75,7.12" xml:id="b50">
	<analytic>
		<title level="a" type="main">Explainable AI in Industry</title>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahin</forename><forename type="middle">Cem</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Mithal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3332281</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-07-25">2019</date>
			<biblScope unit="page" from="3203" to="3204"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Krishna Gade, Sahin Cem Geyik, Krishnaram Kenthapadi, Varun Mithal, and Ankur Taly. Explainable ai in industry. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 3203-3204, Anchorage, Alaska, USA, 2019. ACM.</note>
</biblStruct>

<biblStruct coords="56,128.60,306.47,360.32,7.12;56,128.60,315.94,360.32,7.12;56,128.60,325.40,360.32,7.12;56,128.60,334.87,108.91,7.12" xml:id="b51">
	<analytic>
		<title level="a" type="main">Explainable ai: a brief survey on history, research areas, approaches and challenges</title>
		<author>
			<persName coords=""><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangzhou</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32236-6\51</idno>
	</analytic>
	<monogr>
		<title level="m">CCF International Conference on Natural Language Processing and Chinese Computing</title>
		<meeting><address><addrLine>Switzerland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="563" to="574"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Feiyu Xu, Hans Uszkoreit, Yangzhou Du, Wei Fan, Dongyan Zhao, and Jun Zhu. Explainable ai: a brief survey on history, research areas, approaches and challenges. In CCF International Conference on Natu- ral Language Processing and Chinese Computing, pages 563-574, Switzerland, 2019. Springer, Cham. doi: 10.1007/978-3-030-32236-6\ 51.</note>
</biblStruct>

<biblStruct coords="56,128.60,344.33,360.32,7.12;56,128.60,353.80,360.32,7.12;56,128.60,363.26,91.89,7.12" xml:id="b52">
	<analytic>
		<title level="a" type="main">An integrative 3c evaluation framework for explainable artificial intelligence</title>
		<author>
			<persName coords=""><forename type="first">Xiaocong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jung</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI and semantic technologies for intelligent information systems (SIGODIS)</title>
		<meeting><address><addrLine>Cancún, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10"/>
		</imprint>
	</monogr>
	<note>AIS eLibrary</note>
	<note type="raw_reference">Xiaocong Cui, Jung Min Lee, and J Hsieh. An integrative 3c evaluation framework for explainable artificial intelligence. In AI and semantic technologies for intelligent information systems (SIGODIS), pages 1-10, Cancún, Mexico, 2019. AIS eLibrary.</note>
</biblStruct>

<biblStruct coords="56,128.60,372.73,360.32,7.12;56,128.60,382.19,360.25,7.12;56,128.60,391.66,80.47,7.12" xml:id="b53">
	<analytic>
		<title level="a" type="main">The role of trust in automation reliance</title>
		<author>
			<persName coords=""><forename type="first">Mary</forename><forename type="middle">T</forename><surname>Dzindolet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Regina</forename><forename type="middle">A</forename><surname>Pomranky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Linda</forename><forename type="middle">G</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hall</forename><forename type="middle">P</forename><surname>Beck</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1071-5819(03</idno>
	</analytic>
	<monogr>
		<title level="j">International journal of human-computer studies</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="38" to="45"/>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mary T Dzindolet, Scott A Peterson, Regina A Pomranky, Linda G Pierce, and Hall P Beck. The role of trust in automation reliance. International journal of human-computer studies, 58(6):697-718, 2003. doi: 10.1016/ s1071-5819(03)00038-7.</note>
</biblStruct>

<biblStruct coords="56,128.60,401.12,360.32,7.12;56,128.60,410.58,360.32,7.12;56,128.60,420.05,47.82,7.12" xml:id="b54">
	<analytic>
		<title level="a" type="main">A survey of explanations in recommender systems</title>
		<author>
			<persName coords=""><forename type="first">Nava</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judith</forename><surname>Masthoff</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdew.2007.4401070</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 23rd international conference on data engineering workshop</title>
		<meeting><address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="801" to="810"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Nava Tintarev and Judith Masthoff. A survey of explanations in recommender systems. In IEEE 23rd international conference on data engineering workshop, pages 801-810, Istanbul, Turkey, 2007. IEEE. doi: 10.1109/icdew. 2007.4401070.</note>
</biblStruct>

<biblStruct coords="56,128.60,429.51,302.29,7.12" xml:id="b55">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability</title>
		<author>
			<persName coords=""><surname>Zachary C Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="36" to="43"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zachary C Lipton. The mythos of model interpretability. Commun. ACM, 61(10):36-43, 2018.</note>
</biblStruct>

<biblStruct coords="56,128.60,438.98,360.32,7.12;56,128.60,448.44,360.32,7.12;56,128.60,457.91,125.50,7.12" xml:id="b56">
	<analytic>
		<title level="a" type="main">Designing explainability of an artificial intelligence system</title>
		<author>
			<persName coords=""><forename type="first">Taehyun</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sangwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sangyeon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1145/3183654.3183683</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Technology, Mind, and Society</title>
		<meeting>the Technology, Mind, and Society<address><addrLine>Washington, District of Columbia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Taehyun Ha, Sangwon Lee, and Sangyeon Kim. Designing explainability of an artificial intelligence system. In Proceedings of the Technology, Mind, and Society, page 14:1, Washington, District of Columbia, USA, 2018. ACM. doi: 10.1145/3183654.3183683.</note>
</biblStruct>

<biblStruct coords="56,128.60,467.37,360.32,7.12;56,128.60,476.84,360.32,7.12;56,128.60,486.30,129.98,7.12" xml:id="b57">
	<analytic>
		<title level="a" type="main">Defining explanation in probabilistic systems</title>
		<author>
			<persName coords=""><forename type="first">Urszula</forename><surname>Chajewska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Thirteenth conference on Uncertainty in artificial intelligence<address><addrLine>Providence, Rhode Island, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="62" to="71"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Urszula Chajewska and Joseph Y Halpern. Defining explanation in probabilistic systems. In Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence, pages 62-71, Providence, Rhode Island, USA, 1997. Morgan Kaufmann Publishers Inc.</note>
</biblStruct>

<biblStruct coords="56,128.60,495.76,360.32,7.12;56,128.60,505.23,360.32,7.12;56,128.60,514.69,125.37,7.12" xml:id="b58">
	<analytic>
		<title level="a" type="main">Causability and explainability of artificial intelligence in medicine</title>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helmut</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kurt</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heimo</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.1312</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1312">1312. 2019</date>
			<publisher>Wiley Interdisciplinary Reviews</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Andreas Holzinger, Georg Langs, Helmut Denk, Kurt Zatloukal, and Heimo Müller. Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9: e1312, 2019. doi: 10.1002/widm.1312.</note>
</biblStruct>

<biblStruct coords="56,128.60,524.16,360.32,7.12;56,128.60,533.62,360.32,7.12;56,128.60,543.09,309.75,7.12" xml:id="b59">
	<analytic>
		<title level="a" type="main">Principles of explanatory debugging to personalize interactive machine learning</title>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<idno type="DOI">10.1145/2678025.2701399</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on intelligent user interfaces</title>
		<meeting>the 20th international conference on intelligent user interfaces<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="126" to="137"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. Principles of explanatory debugging to personalize interactive machine learning. In Proceedings of the 20th international conference on intelligent user interfaces, pages 126-137, Atlanta, Georgia, USA, 2015. ACM. doi: 10.1145/2678025.2701399.</note>
</biblStruct>

<biblStruct coords="56,128.60,552.55,360.32,7.12;56,128.60,562.02,360.32,7.12;56,128.60,571.48,360.32,7.12;56,128.60,580.95,85.26,7.12" xml:id="b60">
	<analytic>
		<title level="a" type="main">Too much, too little, or just right? ways explanations impact end users' mental models</title>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irwin</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1109/vlhcc.2013.6645235</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)</title>
		<meeting><address><addrLine>Raleigh, NC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="10"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng-Keen Wong. Too much, too little, or just right? ways explanations impact end users' mental models. In IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), pages 3-10, Raleigh, NC, USA, 2013. IEEE. doi: 10. 1109/vlhcc.2013.6645235.</note>
</biblStruct>

<biblStruct coords="56,128.60,590.41,360.32,7.12;56,128.60,599.87,360.32,7.12;56,128.60,609.34,79.96,7.12" xml:id="b61">
	<analytic>
		<title level="a" type="main">Knowledge discovery: comprehensibility of the results</title>
		<author>
			<persName coords=""><forename type="first">Irit</forename><surname>Askira-Gelman</surname></persName>
		</author>
		<idno type="DOI">10.1109/hicss.1998.648319</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Hawaii International Conference on System Sciences</title>
		<meeting>the Thirty-First Hawaii International Conference on System Sciences<address><addrLine>Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="247" to="255"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Irit Askira-Gelman. Knowledge discovery: comprehensibility of the results. In Proceedings of the Thirty-First Hawaii International Conference on System Sciences, volume 5, pages 247-255, Hawaii, 1998. IEEE. doi: 10. 1109/hicss.1998.648319.</note>
</biblStruct>

<biblStruct coords="56,128.60,618.80,360.32,7.12;56,128.60,628.27,360.32,7.12;56,128.60,637.73,222.84,7.12" xml:id="b62">
	<analytic>
		<title level="a" type="main">A bibliometric analysis of the explainable artificial intelligence research field</title>
		<author>
			<persName coords=""><forename type="first">Ciro</forename><surname>Jose M Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corrado</forename><surname>Castiello</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mencar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems</title>
		<meeting><address><addrLine>Cádiz, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="15"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jose M Alonso, Ciro Castiello, and Corrado Mencar. A bibliometric analysis of the explainable artificial intelli- gence research field. In International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, pages 3-15, Cádiz, Spain, 2018. Springer.</note>
</biblStruct>

<biblStruct coords="56,128.60,647.20,360.32,7.12;56,128.60,656.66,360.32,7.12;56,128.60,666.13,71.07,7.12" xml:id="b63">
	<analytic>
		<title level="a" type="main">Interpretability of machine learning models and representations: an introduction</title>
		<author>
			<persName coords=""><forename type="first">Adrien</forename><surname>Bibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoît</forename><surname>Frénay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on the European Symposium on Artificial Neural Networks, ESANN</title>
		<meeting>on the European Symposium on Artificial Neural Networks, ESANN<address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. i6doc</date>
			<biblScope unit="page" from="77" to="82"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Adrien Bibal and Benoît Frénay. Interpretability of machine learning models and representations: an introduc- tion. In Proceedings on the European Symposium on Artificial Neural Networks, ESANN, pages 77-82, Bruges, Belgium, 2016. i6doc.</note>
</biblStruct>

<biblStruct coords="56,128.60,675.59,360.32,7.12;56,128.60,685.06,261.55,7.12" xml:id="b64">
	<analytic>
		<title level="a" type="main">Machine learning: Between accuracy and interpretability</title>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Bratko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-7091-2668-4\10</idno>
	</analytic>
	<monogr>
		<title level="m">Learning, networks and statistics</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="163" to="177"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ivan Bratko. Machine learning: Between accuracy and interpretability. In Learning, networks and statistics, pages 163-177. Springer, Vienna, Austria, 1997. doi: 10.1007/978-3-7091-2668-4\ 10.</note>
</biblStruct>

<biblStruct coords="56,128.60,694.52,360.32,7.12;57,128.60,145.58,360.32,7.12;57,128.60,155.04,360.32,7.12;57,128.60,164.51,60.23,7.12" xml:id="b65">
	<analytic>
		<title level="a" type="main">What does explainable ai really mean? a new conceptualiza-tion of perspectives</title>
		<author>
			<persName coords=""><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tarek</forename><forename type="middle">R</forename><surname>Besold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference of the italian Association of Artificial Intelligence</title>
		<meeting><address><addrLine>Bari, Italy; Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="8"/>
		</imprint>
		<respStmt>
			<orgName>University of Bremen</orgName>
		</respStmt>
	</monogr>
	<note>Workshop on Comprehensibility and Explanation in AI and ML</note>
	<note type="raw_reference">Derek Doran, Sarah Schulz, and Tarek R Besold. What does explainable ai really mean? a new conceptualiza- tion of perspectives. In 16th International Conference of the italian Association of Artificial Intelligence, 2017. Workshop on Comprehensibility and Explanation in AI and ML, pages 1-8, Bari, Italy, 2017. Cex, University of Bremen, Germany.</note>
</biblStruct>

<biblStruct coords="57,128.60,173.97,360.32,7.12;57,128.60,183.44,94.44,7.12" xml:id="b66">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
		<title level="m">Are we really discovering interesting knowledge from data. Expert Update (the BCS-SGAI magazine)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="41" to="47"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex A Freitas. Are we really discovering interesting knowledge from data. Expert Update (the BCS-SGAI magazine), 9(1):41-47, 2006.</note>
</biblStruct>

<biblStruct coords="57,128.60,192.90,360.32,7.12;57,128.60,202.37,360.32,7.12;57,128.60,211.83,360.32,7.12;57,128.60,221.29,108.91,7.12" xml:id="b67">
	<analytic>
		<title level="a" type="main">Explainable ai: the new 42</title>
		<author>
			<persName coords=""><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ajay</forename><surname>Chander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katharina</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Freddy</forename><surname>Lecue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-99740-7\21</idno>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<meeting><address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="295" to="303"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep Akata, Simone Stumpf, Peter Kiese- berg, and Andreas Holzinger. Explainable ai: the new 42? In International Cross-Domain Conference for Machine Learning and Knowledge Extraction, pages 295-303, Hamburg, Germany, 2018. Springer. doi: 10.1007/978-3-319-99740-7\ 21.</note>
</biblStruct>

<biblStruct coords="57,128.60,230.76,360.32,7.12;57,128.60,240.22,360.32,7.12;57,128.60,249.69,94.38,7.12" xml:id="b68">
	<analytic>
		<title level="a" type="main">Clinical applications of machine learning algorithms: beyond the black box</title>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><surname>David S Watson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">N</forename><surname>Krutzinna</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iain</forename><forename type="middle">B</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luciano</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Floridi</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.l886</idno>
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David S Watson, Jenny Krutzinna, Ian N Bruce, Christopher EM Griffiths, Iain B McInnes, Michael R Barnes, and Luciano Floridi. Clinical applications of machine learning algorithms: beyond the black box. BMJ, 364:l886, 2019. doi: 10.1136/bmj.l886.</note>
</biblStruct>

<biblStruct coords="57,128.60,259.15,360.32,7.12;57,128.60,268.62,360.25,7.12;57,128.60,278.08,80.76,7.12" xml:id="b69">
	<analytic>
		<title level="a" type="main">Designing and evaluating explanations for recommender systems</title>
		<author>
			<persName coords=""><forename type="first">Nava</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judith</forename><surname>Masthoff</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-85820-3\15</idno>
	</analytic>
	<monogr>
		<title level="m">Recommender systems handbook</title>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="479" to="510"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Nava Tintarev and Judith Masthoff. Designing and evaluating explanations for recommender systems. In Rec- ommender systems handbook, pages 479-510. Springer, Boston, Massachusetts, USA, 2011. doi: 10.1007/ 978-0-387-85820-3\ 15.</note>
</biblStruct>

<biblStruct coords="57,128.60,287.55,360.32,7.12;57,128.60,297.01,360.32,7.12;57,131.47,306.47,9.96,7.12" xml:id="b70">
	<analytic>
		<title level="a" type="main">Explaining recommendations: Design and evaluation</title>
		<author>
			<persName coords=""><forename type="first">Nava</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judith</forename><surname>Masthoff</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4899-7637-6\10</idno>
	</analytic>
	<monogr>
		<title level="m">Recommender systems handbook</title>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="353" to="382"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Nava Tintarev and Judith Masthoff. Explaining recommendations: Design and evaluation. In Recommender sys- tems handbook, pages 353-382. Springer, Boston, Massachusetts, USA, 2015. doi: 10.1007/978-1-4899-7637-6\ 10.</note>
</biblStruct>

<biblStruct coords="57,128.60,315.94,360.32,7.12;57,128.60,325.40,360.32,7.12;57,128.60,334.87,29.00,7.12" xml:id="b71">
	<analytic>
		<title level="a" type="main">Evaluating explanations by cognitive value</title>
		<author>
			<persName coords=""><forename type="first">Ajay</forename><surname>Chander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramya</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<meeting><address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="314" to="328"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ajay Chander and Ramya Srinivasan. Evaluating explanations by cognitive value. In International Cross- Domain Conference for Machine Learning and Knowledge Extraction, pages 314-328, Hamburg, Germany, 2018. Springer.</note>
</biblStruct>

<biblStruct coords="57,128.60,344.33,360.32,7.12;57,128.60,353.80,360.32,7.12;57,128.60,363.26,352.96,7.12" xml:id="b72">
	<analytic>
		<title level="a" type="main">Plan explicability and predictability for robot task planning</title>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarath</forename><surname>Sreedharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anagha</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tathagata</forename><surname>Chakraborti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hankz</forename><surname>Hankui Zhuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
		<idno type="DOI">10.1109/icra.2017.7989155</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1313" to="1320"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, Hankz Hankui Zhuo, and Subbarao Kambhampati. Plan explicability and predictability for robot task planning. In IEEE International Conference on Robotics and Automation (ICRA), pages 1313-1320, Singapore, 2017. IEEE. doi: 10.1109/icra.2017.7989155.</note>
</biblStruct>

<biblStruct coords="57,128.60,372.73,360.32,7.12;57,128.60,382.19,360.32,7.12;57,128.60,391.66,360.32,7.12;57,128.60,401.12,69.40,7.12" xml:id="b73">
	<analytic>
		<title level="a" type="main">Towards robust interpretability with self-explaining neural networks</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7786" to="7795"/>
		</imprint>
	</monogr>
	<note type="raw_reference">David Alvarez-Melis and Tommi S. Jaakkola. Towards robust interpretability with self-explaining neural net- works. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems (NeurIPS), pages 7786-7795, Montréal, Canada, 2018. Neural Information Processing Sys- tems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="57,128.60,410.58,360.32,7.12;57,128.60,420.05,360.32,7.12;57,128.60,429.51,360.32,7.12;57,128.60,438.98,29.89,7.12" xml:id="b74">
	<analytic>
		<title level="a" type="main">Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda</title>
		<author>
			<persName coords=""><forename type="first">Ashraf</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jo</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174156</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the CHI Conference on Human Factors in Computing Systems<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="582" to="599"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y Lim, and Mohan Kankanhalli. Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 582-599, Montréal, Canada, 2018. ACM. doi: 10.1145/3173574. 3174156.</note>
</biblStruct>

<biblStruct coords="57,128.60,448.44,360.32,7.12;57,128.60,457.91,360.32,7.12;57,128.60,467.37,360.32,7.12;57,128.60,476.84,126.49,7.12" xml:id="b75">
	<analytic>
		<title level="a" type="main">Dark patterns of explainability, transparency, and user control for intelligent systems</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Chromik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Malin</forename><surname>Eiband</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><forename type="middle">Theres</forename><surname>Völkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Buschek</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2327</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael Chromik, Malin Eiband, Sarah Theres Völkel, and Daniel Buschek. Dark patterns of explainability, transparency, and user control for intelligent systems. In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR-WS.org.</note>
</biblStruct>

<biblStruct coords="57,128.60,486.30,360.32,7.12;57,128.60,495.76,360.32,7.12;57,128.60,505.23,331.16,7.12" xml:id="b76">
	<analytic>
		<title level="a" type="main">Why and why not explanations improve the intelligibility of context-aware intelligent systems</title>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Avrahami</surname></persName>
		</author>
		<idno type="DOI">10.1145/1518701.1519023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2119" to="2128"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Brian Y Lim, Anind K Dey, and Daniel Avrahami. Why and why not explanations improve the intelligibility of context-aware intelligent systems. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 2119-2128, Boston, Massachusetts, USA, 2009. ACM. doi: 10.1145/1518701.1519023.</note>
</biblStruct>

<biblStruct coords="57,128.60,514.69,360.32,7.12;57,128.60,524.16,360.32,7.12;57,128.60,533.62,360.32,7.12;57,128.60,543.09,50.33,7.12" xml:id="b77">
	<analytic>
		<title level="a" type="main">Why these explanations? selecting intelligibility types for explanation goals</title>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danding</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wang</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2327</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Brian Y Lim, Qian Yang, Ashraf M Abdul, and Danding Wang. Why these explanations? selecting intelligibility types for explanation goals. In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR-WS.org.</note>
</biblStruct>

<biblStruct coords="57,128.60,552.55,360.32,7.12;57,128.60,562.02,198.37,7.12" xml:id="b78">
	<analytic>
		<title level="a" type="main">Planning text for advisory dialogues: Capturing intentional and rhetorical information</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johanna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cécile L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="651" to="694"/>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Johanna D Moore and Cécile L Paris. Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational linguistics, 19(4):651-694, 1993.</note>
</biblStruct>

<biblStruct coords="57,128.60,571.48,360.32,7.12;57,128.60,580.95,360.32,7.12;57,128.60,590.41,360.32,7.12;57,128.60,599.87,65.54,7.12" xml:id="b79">
	<analytic>
		<title level="a" type="main">A grounded interaction protocol for explainable artificial intelligence</title>
		<author>
			<persName coords=""><forename type="first">Prashan</forename><surname>Madumal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liz</forename><surname>Sonenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Vetere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1033" to="1041"/>
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
	<note type="raw_reference">Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. A grounded interaction protocol for explainable artificial intelligence. In Proceedings of the 18th International Conference on Autonomous Agents and MultiA- gent Systems, pages 1033-1041, Montréal, Canada, 2019. International Foundation for Autonomous Agents and Multiagent Systems.</note>
</biblStruct>

<biblStruct coords="57,128.60,609.34,360.32,7.12;57,128.60,618.80,265.07,7.12" xml:id="b80">
	<analytic>
		<title level="a" type="main">On rule interestingness measures</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0950-7051(99)00019-2</idno>
	</analytic>
	<monogr>
		<title level="m">Research and Development in Expert Systems XV</title>
		<meeting><address><addrLine>United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="147" to="158"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex A Freitas. On rule interestingness measures. In Research and Development in Expert Systems XV, pages 147-158. Springer, United Kingdom, 1999. doi: 10.1016/s0950-7051(99)00019-2.</note>
</biblStruct>

<biblStruct coords="57,128.60,628.27,360.32,7.12;57,128.60,637.73,360.32,7.12;57,128.60,647.20,360.32,7.12;57,128.60,656.66,26.42,7.12" xml:id="b81">
	<analytic>
		<title level="a" type="main">Interestingness elements for explainable reinforcement learning through introspection</title>
		<author>
			<persName coords=""><forename type="first">Pedro</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melinda</forename><forename type="middle">T</forename><surname>Gervasio</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2327</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Pedro Sequeira, Eric Yeh, and Melinda T Gervasio. Interestingness elements for explainable reinforcement learn- ing through introspection. In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR- WS.org.</note>
</biblStruct>

<biblStruct coords="57,128.60,666.13,360.32,7.12;57,128.60,675.59,360.32,7.12;57,128.60,685.06,81.93,7.12" xml:id="b82">
	<analytic>
		<title level="a" type="main">Measures of model interpretability for model selection</title>
		<author>
			<persName coords=""><forename type="first">André</forename><surname>Carrington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Helen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<meeting><address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="329" to="349"/>
		</imprint>
	</monogr>
	<note type="raw_reference">André Carrington, Paul Fieguth, and Helen Chen. Measures of model interpretability for model selection. In Inter- national Cross-Domain Conference for Machine Learning and Knowledge Extraction, pages 329-349, Hamburg, Germany, 2018. Springer.</note>
</biblStruct>

<biblStruct coords="57,128.60,694.52,360.32,7.12;58,128.60,145.58,360.32,7.12;58,128.60,155.04,184.75,7.12" xml:id="b83">
	<analytic>
		<title level="a" type="main">Explainable argumentation for wellness consultation</title>
		<author>
			<persName coords=""><forename type="first">Isabel</forename><surname>Sassoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nadin</forename><surname>Kökciyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elizabeth</forename><surname>Sklar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Explainable, Transparent Autonomous Agents and Multi-Agent Systems</title>
		<meeting><address><addrLine>Switzerland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="186" to="202"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Isabel Sassoon, Nadin Kökciyan, Elizabeth Sklar, and Simon Parsons. Explainable argumentation for wellness consultation. In International Workshop on Explainable, Transparent Autonomous Agents and Multi-Agent Sys- tems, pages 186-202, Switzerland, 2019. Springer, Cham.</note>
</biblStruct>

<biblStruct coords="58,128.60,164.51,360.32,7.12;58,128.60,173.97,360.32,7.12;58,128.60,183.44,360.32,7.12;58,128.60,192.90,50.33,7.12" xml:id="b84">
	<analytic>
		<title level="a" type="main">Exploring principled visualizations for deep network attributions</title>
		<author>
			<persName coords=""><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinhua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rory</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Najmi</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2327</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Mukund Sundararajan, Jinhua Xu, Ankur Taly, Rory Sayres, and Amir Najmi. Exploring principled visualizations for deep network attributions. In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR-WS.org.</note>
</biblStruct>

<biblStruct coords="58,128.60,202.37,360.32,7.12;58,128.60,211.83,360.32,7.12;58,128.60,221.29,85.05,7.12" xml:id="b85">
	<analytic>
		<title level="a" type="main">Low-level interpretability and high-level interpretability: a unified view of data-driven interpretable fuzzy system modelling</title>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.fss.2008.05.016</idno>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3091" to="3131"/>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shang-Ming Zhou and John Q Gan. Low-level interpretability and high-level interpretability: a unified view of data-driven interpretable fuzzy system modelling. Fuzzy Sets and Systems, 159(23):3091-3131, 2008. doi: 10.1016/j.fss.2008.05.016.</note>
</biblStruct>

<biblStruct coords="58,128.60,230.76,360.32,7.12;58,128.60,240.22,360.32,7.12;58,128.60,249.69,108.91,7.12" xml:id="b86">
	<analytic>
		<title level="a" type="main">Combining mental fit and data fit for classification rule selection</title>
		<author>
			<persName coords=""><forename type="first">Claus</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sondhauss</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-55721-7\21</idno>
	</analytic>
	<monogr>
		<title level="m">Exploratory Data Analysis in Empirical Research</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="188" to="203"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Claus Weihs and UM Sondhauss. Combining mental fit and data fit for classification rule selection. In Exploratory Data Analysis in Empirical Research, pages 188-203. Springer, Munich, Germany, 2003. doi: 10.1007/978-3-642-55721-7\ 21.</note>
</biblStruct>

<biblStruct coords="58,128.60,259.15,360.32,7.12;58,128.60,268.62,163.00,7.12" xml:id="b87">
	<analytic>
		<title level="a" type="main">Comprehensible classification models: a position paper</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1145/2594473.2594475</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex A Freitas. Comprehensible classification models: a position paper. ACM SIGKDD explorations newsletter, 15(1):1-10, 2014. doi: 10.1145/2594473.2594475.</note>
</biblStruct>

<biblStruct coords="58,128.60,278.08,360.32,7.12;58,128.60,287.55,324.14,7.12" xml:id="b88">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName coords=""><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visinf.2017.01.006</idno>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="56"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shixia Liu, Xiting Wang, Mengchen Liu, and Jun Zhu. Towards better analysis of machine learning models: A visual analytics perspective. Visual Informatics, 1(1):48-56, 2017. doi: 10.1016/j.visinf.2017.01.006.</note>
</biblStruct>

<biblStruct coords="58,128.60,297.01,360.32,7.12;58,128.60,306.47,360.32,7.12;58,128.60,315.94,21.92,7.12" xml:id="b89">
	<analytic>
		<title level="a" type="main">On the robustness of interpretability methods</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ICML Workshop in Human Interpretability in Machine Learning</title>
		<meeting>the 2018 ICML Workshop in Human Interpretability in Machine Learning<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71"/>
		</imprint>
	</monogr>
	<note>ICML</note>
	<note type="raw_reference">David Alvarez-Melis and Tommi S. Jaakkola. On the robustness of interpretability methods. In Proceedings of the 2018 ICML Workshop in Human Interpretability in Machine Learning, pages 66-71, Stockholm, Sweden, 2018. ICML.</note>
</biblStruct>

<biblStruct coords="58,128.60,325.40,360.32,7.12;58,128.60,334.87,360.32,7.12;58,128.60,344.33,360.32,7.12;58,128.60,353.80,347.42,7.12" xml:id="b90">
	<analytic>
		<title level="a" type="main">Concrete problems for autonomous vehicle safety: Advantages of bayesian deep learning</title>
		<author>
			<persName coords=""><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amar</forename><surname>Wilk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><forename type="middle">Vivian</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weller</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/661</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4745" to="4753"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Rowan McAllister, Yarin Gal, Alex Kendall, Mark Van Der Wilk, Amar Shah, Roberto Cipolla, and Adrian Vivian Weller. Concrete problems for autonomous vehicle safety: Advantages of bayesian deep learning. In Proceed- ings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pages 4745-4753, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc. doi: 10.24963/ijcai.2017/661.</note>
</biblStruct>

<biblStruct coords="58,128.60,363.26,360.32,7.12;58,128.60,372.73,360.32,7.12;58,128.60,382.19,238.19,7.12" xml:id="b91">
	<analytic>
		<title level="a" type="main">The (un) reliability of saliency methods</title>
		<author>
			<persName coords=""><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Explaining and Visualizing Deep Learning</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="93" to="101"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. In NIPS workshop on Explaining and Visualizing Deep Learning, pages 93-101, Long Beach, California, USA, 2017. NIPS.</note>
</biblStruct>

<biblStruct coords="58,128.60,391.66,360.32,7.12;58,128.60,401.12,360.32,7.12;58,128.60,410.58,34.84,7.12" xml:id="b92">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName coords=""><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3319" to="3328"/>
		</imprint>
	</monogr>
	<note type="report_type">JMLR.org</note>
	<note type="raw_reference">Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3319-3328, Sydney, Australia, 2017. JMLR.org.</note>
</biblStruct>

<biblStruct coords="58,128.60,420.05,360.32,7.12;58,128.60,429.51,288.91,7.12" xml:id="b93">
	<analytic>
		<title level="a" type="main">i know it when i see it". visualization and intuitive interpretability</title>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Offert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Symposium on Interpretable Machine Learning</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="43" to="46"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Fabian Offert. "i know it when i see it". visualization and intuitive interpretability. In NIPS Symposium on Interpretable Machine Learning, pages 43-46, Long Beach, California, USA, 2017. NIPS.</note>
</biblStruct>

<biblStruct coords="58,128.60,438.98,360.32,7.12;58,128.60,448.44,116.88,7.12" xml:id="b94">
	<analytic>
		<title level="a" type="main">Being transparent about transparency</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium</title>
		<meeting><address><addrLine>Palo Alto, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="48" to="53"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Joseph B Lyons. Being transparent about transparency. In AAAI Spring Symposium, pages 48-53, Palo Alto, California, USA, 2013. AAAI Press.</note>
</biblStruct>

<biblStruct coords="58,128.60,457.91,360.32,7.12;58,128.60,467.37,209.33,7.12" xml:id="b95">
	<analytic>
		<title level="a" type="main">Challenges for transparency</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML Workshop on Human Interpretability in Machine Learning</title>
		<meeting>the ICML Workshop on Human Interpretability in Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="62"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Adrian Weller. Challenges for transparency. In Proceedings of the ICML Workshop on Human Interpretability in Machine Learning, pages 55-62, Sydney, Australia, 2017. ICML.</note>
</biblStruct>

<biblStruct coords="58,128.60,476.84,360.32,7.12;58,128.60,486.30,115.14,7.12" xml:id="b96">
	<analytic>
		<title level="a" type="main">The pragmatic turn in explainable artificial intelligence (xai)</title>
		<author>
			<persName coords=""><forename type="first">Andrés</forename><surname>Páez</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11023-019-09502-w</idno>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="19"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Andrés Páez. The pragmatic turn in explainable artificial intelligence (xai). Minds and Machines, 29:1-19, 2019. doi: 10.1007/s11023-019-09502-w.</note>
</biblStruct>

<biblStruct coords="58,128.60,495.76,360.32,7.12;58,128.60,505.23,355.98,7.12" xml:id="b97">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
	<note type="raw_reference">Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Proceedings of ICLR Workshop, Banff, Canada, 2014. ICLR.</note>
</biblStruct>

<biblStruct coords="58,128.60,514.69,360.32,7.12;58,128.60,524.16,360.32,7.12;58,128.60,533.62,195.90,7.12" xml:id="b98">
	<analytic>
		<title level="a" type="main">Intelligible models for classification and regression</title>
		<author>
			<persName coords=""><forename type="first">Yin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<idno type="DOI">10.1145/2339530.2339556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="150" to="158"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Yin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150-158, Beijing, China, 2012. ACM. doi: 10.1145/2339530.2339556.</note>
</biblStruct>

<biblStruct coords="58,128.60,543.09,360.32,7.12;58,128.60,552.55,360.32,7.12;58,128.60,562.02,175.21,7.12" xml:id="b99">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName coords=""><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3145" to="3153"/>
		</imprint>
	</monogr>
	<note type="report_type">JMLR.org</note>
	<note type="raw_reference">Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3145-3153, Sydney, Australia, 2017. JMLR.org.</note>
</biblStruct>

<biblStruct coords="58,128.60,571.48,360.32,7.12;58,128.60,580.95,360.32,7.12;58,128.60,590.41,88.32,7.12" xml:id="b100">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.</note>
</biblStruct>

<biblStruct coords="58,128.60,599.87,360.32,7.12;58,128.60,609.34,360.32,7.12;58,128.60,618.80,203.67,7.12" xml:id="b101">
	<analytic>
		<title level="a" type="main">Explaining collaborative filtering recommendations</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
		<idno type="DOI">10.1145/358916.358995</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ACM conference on Computer supported cooperative work</title>
		<meeting>the 2000 ACM conference on Computer supported cooperative work<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="241" to="250"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jonathan L Herlocker, Joseph A Konstan, and John Riedl. Explaining collaborative filtering recommendations. In Proceedings of the 2000 ACM conference on Computer supported cooperative work, pages 241-250, Philadelphia, Pennsylvania, USA, 2000. ACM. doi: 10.1145/358916.358995.</note>
</biblStruct>

<biblStruct coords="58,128.60,628.27,360.32,7.12;58,128.60,637.73,360.32,7.12;58,128.60,647.20,273.16,7.12" xml:id="b102">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName coords=""><forename type="first">Josua</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenney</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858529</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5686" to="5697"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Josua Krause, Adam Perer, and Kenney Ng. Interacting with predictions: Visual inspection of black-box machine learning models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 5686-5697, San Jose, California, USA, 2016. ACM. doi: 10.1145/2858036.2858529.</note>
</biblStruct>

<biblStruct coords="58,128.60,654.92,360.32,8.86;58,128.60,666.13,360.32,7.12;58,128.60,675.59,303.84,7.12" xml:id="b103">
	<analytic>
		<title level="a" type="main">Can we do better explanations? a proposal of user-centered explainable ai</title>
		<author>
			<persName coords=""><forename type="first">Mireia</forename><surname>Ribera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2327</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Mireia Ribera and Àgata Lapedriza. Can we do better explanations? a proposal of user-centered explainable ai. In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR-WS.org.</note>
</biblStruct>

<biblStruct coords="58,128.60,685.05,360.32,7.12;58,128.60,694.52,360.32,7.12;58,292.66,706.13,9.96,8.90" xml:id="b104">
	<analytic>
		<title level="a" type="main">How people explain action (and autonomous intelligent systems should too)</title>
		<author>
			<persName coords=""><forename type="first">Graaf</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bertram</forename><forename type="middle">F</forename><surname>Malle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium on Artificial Intelligence for Human-Robot Interaction</title>
		<meeting><address><addrLine>Arlington</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">MM de Graaf and Bertram F Malle. How people explain action (and autonomous intelligent systems should too). In AAAI Fall Symposium on Artificial Intelligence for Human-Robot Interaction, pages 19-26, Arlington, 58</note>
</biblStruct>

<biblStruct coords="59,128.60,145.58,110.51,7.12" xml:id="b105">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Usa</forename><surname>Virginia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Virginia, USA, 2017. AAAI Press.</note>
</biblStruct>

<biblStruct coords="59,128.60,155.04,360.32,7.12;59,128.60,164.51,360.32,7.12;59,128.60,173.97,175.58,7.12" xml:id="b106">
	<analytic>
		<title level="a" type="main">A study into preferred explanations of virtual agent behavior</title>
		<author>
			<persName coords=""><forename type="first">Maaike</forename><surname>Harbers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karel</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John-Jules Ch</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Meyer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04380-2\17</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Intelligent Virtual Agents</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="132" to="145"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Maaike Harbers, Karel van den Bosch, and John-Jules Ch Meyer. A study into preferred explanations of virtual agent behavior. In International Workshop on Intelligent Virtual Agents, pages 132-145, Amsterdam, Netherlands, 2009. Springer. doi: 10.1007/978-3-642-04380-2\ 17.</note>
</biblStruct>

<biblStruct coords="59,128.60,183.44,360.32,7.12;59,128.60,192.90,360.32,7.12;59,128.60,202.37,270.90,7.12" xml:id="b107">
	<analytic>
		<title level="a" type="main">Reconstructive explanation: Explanation as complex problem solving</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">B</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Eleventh International Joint Conference on Artificial Intelligence<address><addrLine>Detroit, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="135" to="140"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael R Wick and William B Thompson. Reconstructive explanation: Explanation as complex problem solving. In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pages 135-140, Detroit, Michigan, USA, 1989. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="59,128.60,211.83,360.32,7.12;59,128.60,221.29,264.50,7.12" xml:id="b108">
	<analytic>
		<title level="a" type="main">Second generation expert system explanation</title>
		<author>
			<persName coords=""><surname>Michael R Wick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-77927-5\26</idno>
	</analytic>
	<monogr>
		<title level="m">Second Generation Expert Systems</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="614" to="640"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael R Wick. Second generation expert system explanation. In Second Generation Expert Systems, pages 614-640. Springer, Berlin, Germany, 1993. doi: 10.1007/978-3-642-77927-5\ 26.</note>
</biblStruct>

<biblStruct coords="59,128.60,230.76,360.32,7.12;59,128.60,240.22,296.08,7.12" xml:id="b109">
	<analytic>
		<title level="a" type="main">Designs for explaining intelligent agents</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Steven R Haynes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><forename type="middle">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ritter</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2008.09.008</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="110"/>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Steven R Haynes, Mark A Cohen, and Frank E Ritter. Designs for explaining intelligent agents. International Journal of Human-Computer Studies, 67(1):90-110, 2009. doi: 10.1016/j.ijhcs.2008.09.008.</note>
</biblStruct>

<biblStruct coords="59,128.60,249.69,360.32,7.12;59,128.60,259.15,360.32,7.12;59,128.60,268.62,50.47,7.12" xml:id="b110">
	<analytic>
		<title level="a" type="main">Introspectively assessing failures through explainable artificial intelligence</title>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><surname>Sheh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isaac</forename><surname>Monteath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS Workshop on Introspective Methods for Reliable Autonomy</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="40" to="47"/>
		</imprint>
	</monogr>
	<note>iliad-project.eu</note>
	<note type="raw_reference">Raymond Sheh and Isaac Monteath. Introspectively assessing failures through explainable artificial intelligence. In IROS Workshop on Introspective Methods for Reliable Autonomy, pages 40-47, Vancouver, Canada, 2017. iliad-project.eu.</note>
</biblStruct>

<biblStruct coords="59,128.60,278.08,360.32,7.12;59,128.60,287.55,360.32,7.12;59,128.60,297.01,360.32,7.12;59,128.60,306.47,37.42,7.12" xml:id="b111">
	<analytic>
		<title level="a" type="main">A new approach to expert system explanations</title>
		<author>
			<persName coords=""><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daryl</forename><surname>Mccullough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Decristofaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tanya</forename><surname>Korelsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Lavoie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Natural Language Generation</title>
		<meeting>the Ninth International Workshop on Natural Language Generation<address><addrLine>Niagara-on-the-Lake, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="78" to="87"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Regina Barzilay, Daryl McCullough, Owen Rambow, Jonathan DeCristofaro, Tanya Korelsky, and Benoit Lavoie. A new approach to expert system explanations. In Proceedings of the Ninth International Workshop on Natural Language Generation, pages 78-87, Niagara-on-the-Lake, Ontario, Canada, 1998. Association for Computational Linguistics.</note>
</biblStruct>

<biblStruct coords="59,128.60,315.94,360.32,7.12;59,128.60,325.40,102.14,7.12" xml:id="b112">
	<analytic>
		<title level="a" type="main">The structure and function of explanations</title>
		<author>
			<persName coords=""><forename type="first">Tania</forename><surname>Lombrozo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2006.08.004</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="464" to="470"/>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tania Lombrozo. The structure and function of explanations. Trends in cognitive sciences, 10(10):464-470, 2006. doi: 10.1016/j.tics.2006.08.004.</note>
</biblStruct>

<biblStruct coords="59,128.60,334.87,360.32,7.12;59,128.60,344.33,95.55,7.12" xml:id="b113">
	<analytic>
		<title level="a" type="main">Blah, a system which explains its reasoning</title>
		<author>
			<persName coords=""><surname>Weiner</surname></persName>
		</author>
		<idno type="DOI">10.1016/0004-3702</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="90021" to="90021"/>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
	<note type="raw_reference">JL Weiner. Blah, a system which explains its reasoning. Artificial intelligence, 15(1-2):19-48, 1980. doi: 10. 1016/0004-3702(80)90021-1.</note>
</biblStruct>

<biblStruct coords="59,128.60,353.80,318.21,7.12" xml:id="b114">
	<analytic>
		<title level="a" type="main">A dialogue system specification for explanation</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Walton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="374"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Douglas Walton. A dialogue system specification for explanation. Synthese, 182(3):349-374, 2011.</note>
</biblStruct>

<biblStruct coords="59,128.60,363.26,360.32,7.12;59,128.60,372.73,256.33,7.12" xml:id="b115">
	<analytic>
		<title level="a" type="main">Generating interactive explanations</title>
		<author>
			<persName coords=""><forename type="first">Alison</forename><surname>Cawsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th National Conference on Artificial Intelligence</title>
		<meeting>the 9th National Conference on Artificial Intelligence<address><addrLine>Anaheim, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="91"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alison Cawsey. Generating interactive explanations. In Proceedings of the 9th National Conference on Artificial Intelligence, volume 1, pages 86-91, Anaheim, California, USA, 1991. Citeseer.</note>
</biblStruct>

<biblStruct coords="59,128.60,382.19,360.32,7.12;59,128.60,391.66,119.40,7.12" xml:id="b116">
	<analytic>
		<title level="a" type="main">Planning interactive explanations</title>
		<author>
			<persName coords=""><forename type="first">Alison</forename><surname>Cawsey</surname></persName>
		</author>
		<idno type="DOI">10.1006/imms.1993.1009</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="199"/>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alison Cawsey. Planning interactive explanations. International Journal of Man-Machine Studies, 38(2):169-199, 1993. doi: 10.1006/imms.1993.1009.</note>
</biblStruct>

<biblStruct coords="59,128.60,401.12,360.32,7.12;59,128.60,410.58,136.22,7.12" xml:id="b117">
	<analytic>
		<title level="a" type="main">User modelling in interactive explanations</title>
		<author>
			<persName coords=""><forename type="first">Alison</forename><surname>Cawsey</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf01257890</idno>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="247"/>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alison Cawsey. User modelling in interactive explanations. User Modeling and User-Adapted Interaction, 3(3): 221-247, 1993. doi: 10.1007/bf01257890.</note>
</biblStruct>

<biblStruct coords="59,128.60,420.05,360.32,7.12;59,128.60,429.51,360.32,7.12;59,128.60,438.98,201.01,7.12" xml:id="b118">
	<analytic>
		<title level="a" type="main">User participation in the reasoning processes of expert systems</title>
		<author>
			<persName coords=""><forename type="first">Martha</forename><forename type="middle">E</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Second National Conference Artificial Intelligence</title>
		<meeting>Second National Conference Artificial Intelligence<address><addrLine>Menlo Park, California, USA; Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="358" to="361"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Martha E Pollack, Julia Hirschberg, and Bonnie Webber. User participation in the reasoning processes of expert systems. In Proceedings of Second National Conference Artificial Intelligence, pages 358-361, Menlo Park, California, USA, 1982. MIT Press, Cambridge, Massachusetts.</note>
</biblStruct>

<biblStruct coords="59,128.60,448.44,360.32,7.12;59,128.60,457.91,360.32,7.12;59,128.60,467.37,79.96,7.12" xml:id="b119">
	<analytic>
		<title level="a" type="main">Explanation facilities and interactive systems</title>
		<author>
			<persName coords=""><forename type="first">Hilary</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1145/169891.169951</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international conference on Intelligent user interfaces</title>
		<meeting>the 1st international conference on Intelligent user interfaces<address><addrLine>Orlando, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="159" to="166"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Hilary Johnson and Peter Johnson. Explanation facilities and interactive systems. In Proceedings of the 1st international conference on Intelligent user interfaces, pages 159-166, Orlando, Florida, USA, 1993. ACM. doi: 10.1145/169891.169951.</note>
</biblStruct>

<biblStruct coords="59,128.60,476.84,360.32,7.12;59,128.60,486.30,360.32,7.12;59,128.60,495.76,255.33,7.12" xml:id="b120">
	<analytic>
		<title level="a" type="main">Planning text for advisory dialogues</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johanna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecile</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Paris</surname></persName>
		</author>
		<idno type="DOI">10.3115/981623.981648</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 27th annual meeting on Association for Computational Linguistics<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="203" to="211"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Johanna D Moore and Cecile L Paris. Planning text for advisory dialogues. In Proceedings of the 27th annual meeting on Association for Computational Linguistics, pages 203-211, Vancouver, British Columbia, Canada, 1989. Association for Computational Linguistics. doi: 10.3115/981623.981648.</note>
</biblStruct>

<biblStruct coords="59,128.60,505.23,360.32,7.12;59,128.60,514.69,297.47,7.12" xml:id="b121">
	<analytic>
		<title level="a" type="main">A reactive approach to explanation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johanna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Swartout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<meeting><address><addrLine>Detroit, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="1504" to="1510"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Johanna D Moore and William R Swartout. A reactive approach to explanation. In IJCAI, pages 1504-1510, Detroit, Michigan, USA, 1989. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="59,128.60,524.16,360.32,7.12;59,128.60,533.62,360.32,7.12;59,128.60,543.09,68.97,7.12" xml:id="b122">
	<analytic>
		<title level="a" type="main">A reactive approach to explanation: Taking the user's feedback into account</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johanna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Swartout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language generation in artificial intelligence and computational linguistics</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="3" to="48"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Johanna D Moore and William R Swartout. A reactive approach to explanation: Taking the user's feedback into account. In Natural language generation in artificial intelligence and computational linguistics, pages 3-48. Springer, USA, 1991.</note>
</biblStruct>

<biblStruct coords="59,128.60,552.55,360.32,7.12;59,128.60,562.02,360.32,7.12;59,128.60,571.48,304.01,7.12" xml:id="b123">
	<analytic>
		<title level="a" type="main">Building explainable artificial intelligence systems</title>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Mark G Core</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dave</forename><surname>Van Lent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Gomboc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Milton</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rosenberg</surname></persName>
		</author>
		<idno type="DOI">10.21236/ada459166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st national conference on Artificial intelligence</title>
		<meeting>the 21st national conference on Artificial intelligence<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1766" to="1773"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mark G Core, H Chad Lane, Michael Van Lent, Dave Gomboc, Steve Solomon, and Milton Rosenberg. Building explainable artificial intelligence systems. In Proceedings of the 21st national conference on Artificial intelligence, pages 1766-1773, Boston, Massachusetts, USA, 2006. AAAI Press. doi: 10.21236/ada459166.</note>
</biblStruct>

<biblStruct coords="59,128.60,580.95,360.32,7.12;59,128.60,590.41,360.32,7.12;59,128.60,599.87,360.32,7.12;59,128.60,609.34,152.99,7.12" xml:id="b124">
	<analytic>
		<title level="a" type="main">Design recommendations to support automated explanation and tutoring</title>
		<author>
			<persName coords=""><forename type="first">Dave</forename><surname>Gomboc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Mark G Core</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Van Lent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Behavior Representation in Modelling and Simulation</title>
		<meeting>the Fourteenth Conference on Behavior Representation in Modelling and Simulation<address><addrLine>Universal City, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="331" to="340"/>
		</imprint>
	</monogr>
	<note>Simulation Interoperability Standards Organization (SISO</note>
	<note type="raw_reference">Dave Gomboc, Steve Solomon, Mark G Core, H Chad Lane, and Michael Van Lent. Design recommendations to support automated explanation and tutoring. In Proceedings of the Fourteenth Conference on Behavior Rep- resentation in Modelling and Simulation, pages 331-340, Universal City, California, USA, 2005. Simulation Interoperability Standards Organization (SISO).</note>
</biblStruct>

<biblStruct coords="59,128.60,618.80,360.32,7.12;59,128.60,628.27,360.32,7.12;59,128.60,637.73,262.53,7.12" xml:id="b125">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence for training and tutoring</title>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mark G Core</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Van Lent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dave</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gomboc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence in Education, AIED</title>
		<meeting>the 12th International Conference on Artificial Intelligence in Education, AIED<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="762" to="764"/>
		</imprint>
	</monogr>
	<note type="raw_reference">H Chad Lane, Mark G Core, Michael Van Lent, Steve Solomon, and Dave Gomboc. Explainable artificial intelli- gence for training and tutoring. In Proceedings of the 12th International Conference on Artificial Intelligence in Education, AIED, pages 762-764, Amsterdam, The Netherlands, 2005. IOS Press.</note>
</biblStruct>

<biblStruct coords="59,128.60,647.20,360.32,7.12;59,128.60,656.66,360.32,7.12;59,128.60,666.13,334.38,7.12" xml:id="b126">
	<analytic>
		<title level="a" type="main">An explainable artificial intelligence system for smallunit tactical behavior</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Van Lent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mancuso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>San Jose, California, USA; Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press; MIT Press</publisher>
			<date type="published" when="1999">2004. 1999</date>
			<biblScope unit="page" from="900" to="907"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael Van Lent, William Fisher, and Michael Mancuso. An explainable artificial intelligence system for small- unit tactical behavior. In Proceedings of the National Conference on Artificial Intelligence, pages 900-907, San Jose, California, USA, 2004. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</note>
</biblStruct>

<biblStruct coords="59,128.60,675.59,360.32,7.12;59,128.60,685.06,360.32,7.12;59,128.60,694.52,43.84,7.12" xml:id="b127">
	<analytic>
		<title level="a" type="main">Autotutor: An intelligent tutoring system with mixed-initiative dialogue</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Arthur C Graesser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Olney</surname></persName>
		</author>
		<idno type="DOI">10.1109/te.2005.856149</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Education</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="612" to="618"/>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Arthur C Graesser, Patrick Chipman, Brian C Haynes, and Andrew Olney. Autotutor: An intelligent tutoring system with mixed-initiative dialogue. IEEE Transactions on Education, 48(4):612-618, 2005. doi: 10.1109/te. 2005.856149.</note>
</biblStruct>

<biblStruct coords="60,128.60,145.58,360.32,7.12;60,128.60,155.04,360.32,7.12;60,128.60,164.51,116.88,7.12" xml:id="b128">
	<analytic>
		<title level="a" type="main">Explainable agency for intelligent autonomous systems</title>
		<author>
			<persName coords=""><forename type="first">Pat</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Meadows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohan</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongkyu</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4762" to="4764"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Pat Langley, Ben Meadows, Mohan Sridharan, and Dongkyu Choi. Explainable agency for intelligent autonomous systems. In Proceedings of the Thirty-First Conference on Artificial Intelligence, pages 4762-4764, San Francisco, California, USA, 2017. AAAI Press.</note>
</biblStruct>

<biblStruct coords="60,128.60,173.97,360.32,7.12;60,128.60,183.44,360.32,7.12;60,128.60,192.90,116.88,7.12" xml:id="b129">
	<analytic>
		<title level="a" type="main">Preferred explanations: Theory and generation via planning</title>
		<author>
			<persName coords=""><forename type="first">Shirin</forename><surname>Sohrabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="261" to="267"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Shirin Sohrabi, Jorge A Baier, and Sheila A McIlraith. Preferred explanations: Theory and generation via plan- ning. In Proceedings of the Twenty-Fifth Conference on Artificial Intelligence, pages 261-267, San Francisco, California, USA, 2011. AAAI Press.</note>
</biblStruct>

<biblStruct coords="60,128.60,202.37,360.32,7.12;60,128.60,211.83,360.32,7.12;60,128.60,221.29,27.90,7.12" xml:id="b130">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2013.50</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828"/>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828, 2013. doi: 10.1109/tpami. 2013.50.</note>
</biblStruct>

<biblStruct coords="60,128.60,230.76,360.32,7.12;60,128.60,240.22,360.32,7.12;60,128.60,249.69,300.62,7.12" xml:id="b131">
	<analytic>
		<title level="a" type="main">Accurate intelligible models with pairwise interactions</title>
		<author>
			<persName coords=""><forename type="first">Yin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giles</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2487579</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="623" to="631"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intelligible models with pairwise inter- actions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 623-631, Chicago, Illinois, USA, 2013. ACM. doi: 10.1145/2487575.2487579.</note>
</biblStruct>

<biblStruct coords="60,128.60,259.15,360.32,7.12;60,128.60,268.62,360.32,7.12;60,128.60,278.08,76.23,7.12" xml:id="b132">
	<analytic>
		<title level="a" type="main">A comparison study on rule extraction from neural network ensembles, boosted shallow trees, and svms</title>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Bologna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoichi</forename><surname>Hayashi</surname></persName>
		</author>
		<idno type="DOI">10.1155/2018/4084850</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Computational Intelligence and Soft Computing</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Guido Bologna and Yoichi Hayashi. A comparison study on rule extraction from neural network ensembles, boosted shallow trees, and svms. Applied Computational Intelligence and Soft Computing, 2018, 2018. doi: 10.1155/2018/4084850.</note>
</biblStruct>

<biblStruct coords="60,128.60,287.55,360.32,7.12;60,128.60,297.01,360.32,7.12;60,128.60,306.47,175.18,7.12" xml:id="b133">
	<analytic>
		<title level="a" type="main">explainer: A visual analytics framework for interactive and explainable machine learning</title>
		<author>
			<persName coords=""><forename type="first">Thilo</forename><surname>Spinner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Udo</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanna</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mennatallah</forename><surname>El-Assady</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934629</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1064" to="1074"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thilo Spinner, Udo Schlegel, Hanna Schäfer, and Mennatallah El-Assady. explainer: A visual analytics framework for interactive and explainable machine learning. IEEE transactions on visualization and computer graphics, 26: 1064-1074, 2019. doi: 10.1109/TVCG.2019.2934629.</note>
</biblStruct>

<biblStruct coords="60,128.60,315.94,360.32,7.12;60,128.60,325.40,360.32,7.12;60,128.60,334.87,331.60,7.12" xml:id="b134">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135-1144, San Francisco, CA, USA, 2016. ACM. doi: 10.1145/2939672.2939778.</note>
</biblStruct>

<biblStruct coords="60,128.60,344.33,360.32,7.12;60,128.60,353.80,360.32,7.12;60,128.60,363.26,210.48,7.12" xml:id="b135">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvcg.2017.2744158</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, and Alexander M Rush. Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks. IEEE transactions on visualization and computer graphics, 24(1):667-676, 2018. doi: 10.1109/tvcg.2017.2744158.</note>
</biblStruct>

<biblStruct coords="60,128.60,372.73,360.32,7.12;60,128.60,382.19,360.32,7.12;60,128.60,391.66,360.32,7.12;60,128.60,401.12,47.82,7.12" xml:id="b136">
	<analytic>
		<title level="a" type="main">Visualizing dataflow graphs of deep learning models in tensorflow</title>
		<author>
			<persName coords=""><forename type="first">Kanit</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimbo</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dandelion</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doug</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvcg.2017.2744878</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kanit Wongsuphasawat, Daniel Smilkov, James Wexler, Jimbo Wilson, Dandelion Mané, Doug Fritz, Dilip Kr- ishnan, Fernanda B Viégas, and Martin Wattenberg. Visualizing dataflow graphs of deep learning models in tensorflow. IEEE transactions on visualization and computer graphics, 24(1):1-12, 2018. doi: 10.1109/tvcg. 2017.2744878.</note>
</biblStruct>

<biblStruct coords="60,128.60,410.58,360.32,7.12;60,128.60,420.05,360.32,7.12;60,128.60,429.51,175.58,7.12" xml:id="b137">
	<analytic>
		<title level="a" type="main">Grounding visual explanations</title>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronghang</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeynep</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Akata</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01216-8\17</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference, Proceedings, Part II</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="269" to="286"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, and Zeynep Akata. Grounding visual explanations. In Com- puter Vision -ECCV 2018 -15th European Conference, Proceedings, Part II, pages 269-286, Munich, Germany, 2018. Springer. doi: 10.1007/978-3-030-01216-8\ 17.</note>
</biblStruct>

<biblStruct coords="60,128.60,438.98,360.32,7.12;60,128.60,448.50,360.32,6.97;60,128.60,457.91,266.52,7.12" xml:id="b138">
	<analytic>
		<title level="a" type="main">Rule extraction from linear support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sathyakama</forename><surname>Sandilya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bharat</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1145/1081870.1081878</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="32" to="40"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Glenn Fung, Sathyakama Sandilya, and R Bharat Rao. Rule extraction from linear support vector machines. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 32-40, Chicago, Illinois, USA, 2005. ACM. doi: 10.1145/1081870.1081878.</note>
</biblStruct>

<biblStruct coords="60,128.60,467.37,360.32,7.12;60,128.60,476.84,360.32,7.12;60,128.60,486.30,152.99,7.12" xml:id="b139">
	<analytic>
		<title level="a" type="main">Characterization of symbolic rules embedded in deep dimlp networks: a challenge to transparency of deep learning</title>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Bologna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoichi</forename><surname>Hayashi</surname></persName>
		</author>
		<idno type="DOI">10.1515/jaiscr-2017-0019</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence and Soft Computing Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="265" to="286"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Guido Bologna and Yoichi Hayashi. Characterization of symbolic rules embedded in deep dimlp networks: a challenge to transparency of deep learning. Journal of Artificial Intelligence and Soft Computing Research, 7(4): 265-286, 2017. doi: 10.1515/jaiscr-2017-0019.</note>
</biblStruct>

<biblStruct coords="60,128.60,495.76,360.32,7.12;60,128.60,505.23,360.32,7.12;60,128.60,514.69,60.65,7.12" xml:id="b140">
	<analytic>
		<title level="a" type="main">Anchors: High-precision model-agnostic explanations</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1527" to="1535"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations. In Thirty-Second AAAI Conference on Artificial Intelligence, pages 1527-1535, New Orleans, Louisiana, USA, 2018. AAAI Press.</note>
</biblStruct>

<biblStruct coords="60,128.60,524.16,360.32,7.12;60,128.60,533.62,360.32,7.12;60,128.60,543.09,140.55,7.12" xml:id="b141">
	<analytic>
		<title level="a" type="main">Interpretation of trained neural networks by rule extraction</title>
		<author>
			<persName coords=""><surname>Vasile Palade</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Daniel-Ciprian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Neagu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Patton</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45493-4\20</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Intelligence</title>
		<meeting><address><addrLine>Dortmund, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="152" to="161"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Vasile Palade, Daniel-Ciprian Neagu, and Ron J Patton. Interpretation of trained neural networks by rule extrac- tion. In International Conference on Computational Intelligence, pages 152-161, Dortmund, Germany, 2001. Springer. doi: 10.1007/3-540-45493-4\ 20.</note>
</biblStruct>

<biblStruct coords="60,128.60,552.55,360.32,7.12;60,128.60,562.02,360.32,7.12;60,128.60,571.48,198.87,7.12" xml:id="b142">
	<monogr>
		<title level="m" type="main">Inferential models of mental workload with defeasible argumentation and nonmonotonic fuzzy reasoning: a comparative study. In 2nd Workshop on Advances In Argumentation In Artificial Intelligence</title>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="11" to="26"/>
			<pubPlace>Trento, Italy</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Lucas Rizzo and Luca Longo. Inferential models of mental workload with defeasible argumentation and non- monotonic fuzzy reasoning: a comparative study. In 2nd Workshop on Advances In Argumentation In Artificial Intelligence, pages 11-26, Trento, Italy, 2018. CEUR-WS.org.</note>
</biblStruct>

<biblStruct coords="60,128.60,580.95,360.32,7.12;60,128.60,590.41,360.32,7.12;60,128.60,599.87,309.97,7.12" xml:id="b143">
	<analytic>
		<title level="a" type="main">A qualitative investigation of the explainability of defeasible argumentation and non-monotonic fuzzy reasoning</title>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">Proceedings for the 26th AIAI Irish Conference on Artificial Intelligence and Cognitive Science Trinity College Dublin</title>
		<meeting>for the 26th AIAI Irish Conference on Artificial Intelligence and Cognitive Science Trinity College Dublin<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="138" to="149"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lucas Rizzo and Luca Longo. A qualitative investigation of the explainability of defeasible argumentation and non-monotonic fuzzy reasoning. In Proceedings for the 26th AIAI Irish Conference on Artificial Intelligence and Cognitive Science Trinity College Dublin, pages 138-149, Dublin, Ireland, 2018. CEUR-WS.org.</note>
</biblStruct>

<biblStruct coords="60,128.60,609.34,360.32,7.12;60,128.60,618.80,360.32,7.12;60,128.60,628.27,40.07,7.12" xml:id="b144">
	<analytic>
		<title level="a" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, Workshop Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">68</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. In 5th International Conference on Learning Representations, Workshop Track Proceedings, page 68, Toulon, France, 2017. ICLR.</note>
</biblStruct>

<biblStruct coords="60,128.60,637.73,360.32,7.12;60,128.60,647.20,360.32,7.12;60,128.60,656.66,242.51,7.12" xml:id="b145">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</title>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernanda</forename><surname>Viegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2673" to="2682"/>
		</imprint>
	</monogr>
	<note>ICML</note>
	<note type="raw_reference">Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International Conference on Machine Learning, pages 2673-2682, Stockholm, Sweden, 2018. ICML.</note>
</biblStruct>

<biblStruct coords="60,128.60,666.13,360.32,7.12;60,128.60,675.59,360.32,7.12;60,128.60,685.06,182.51,7.12" xml:id="b146">
	<analytic>
		<title level="a" type="main">Understanding support vector machine classifications via a recommender system-like approach</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Barbella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sami</forename><surname>Benzaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bret</forename><surname>Janara M Christensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">R</forename><surname>Victor Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Musicant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DMIN</title>
		<meeting><address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CSREA Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="305" to="311"/>
		</imprint>
	</monogr>
	<note type="raw_reference">David Barbella, Sami Benzaid, Janara M Christensen, Bret Jackson, X Victor Qin, and David R Musicant. Un- derstanding support vector machine classifications via a recommender system-like approach. In DMIN, pages 305-311, Las Vegas, Nevada, USA, 2009. CSREA Press.</note>
</biblStruct>

<biblStruct coords="60,128.60,694.52,360.32,7.12;61,128.60,145.58,360.32,7.12;61,128.60,155.04,75.94,7.12" xml:id="b147">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2048. 2015</date>
			<biblScope unit="page" from="2057" to="2088"/>
		</imprint>
	</monogr>
	<note type="raw_reference">K Xu, J Ba, R Kiros, A Courville, R Salakhutdinov, R Zemel, and Y Bengio. Show, attend and tell: Neural image caption generation with visual attention. Proceedings of the International Conference on Machine Learning, 2048:2057-2088, 2015.</note>
</biblStruct>

<biblStruct coords="61,128.60,164.51,360.32,7.12;61,128.60,173.97,360.32,7.12;61,128.60,183.44,278.26,7.12" xml:id="b148">
	<analytic>
		<title level="a" type="main">Distill-and-compare: Auditing black-box models using transparent model distillation</title>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giles</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yin</forename><surname>Lou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3278721.3278725</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the AAAI/ACM Conference on AI, Ethics, and Society<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="303" to="310"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. Distill-and-compare: Auditing black-box models using transparent model distillation. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 303-310, New Orleans, Louisiana, USA, 2018. ACM. doi: 10.1145/3278721.3278725.</note>
</biblStruct>

<biblStruct coords="61,128.60,192.90,360.33,7.12;61,128.60,202.37,360.32,7.12;61,128.60,211.83,117.44,7.12" xml:id="b149">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pages 4765-4774, Long Beach, California, USA, 2017. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="61,128.60,219.55,360.32,8.86;61,128.60,230.76,315.24,7.12" xml:id="b150">
	<analytic>
		<title level="a" type="main">Explaining classifications for individual instances</title>
		<author>
			<persName coords=""><forename type="first">Marko</forename><surname>Robnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Šikonja</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
		<idno type="DOI">10.1109/tkde.2007.190734</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="589" to="600"/>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marko Robnik-Šikonja and Igor Kononenko. Explaining classifications for individual instances. IEEE Transac- tions on Knowledge and Data Engineering, 20(5):589-600, 2008. doi: 10.1109/tkde.2007.190734.</note>
</biblStruct>

<biblStruct coords="61,128.60,238.48,360.32,8.86;61,128.60,249.69,17.93,7.12" xml:id="b151">
	<analytic>
		<title level="a" type="main">Explanation of prediction models with explain prediction</title>
		<author>
			<persName coords=""><forename type="first">Marko</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="22"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marko Robnik-Šikonja. Explanation of prediction models with explain prediction. Informatica, 42(1):13-22, 2018.</note>
</biblStruct>

<biblStruct coords="61,128.60,259.15,360.32,7.12;61,128.60,268.62,360.32,7.12;61,128.60,278.08,108.78,7.12" xml:id="b152">
	<analytic>
		<title level="a" type="main">Opening black box data mining models using sensitivity analysis</title>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Embrechts</surname></persName>
		</author>
		<idno type="DOI">10.1109/cidm.2011.5949423</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Computational Intelligence and Data Mining (CIDM)</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="341" to="348"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Paulo Cortez and Mark J Embrechts. Opening black box data mining models using sensitivity analysis. In IEEE Symposium on Computational Intelligence and Data Mining (CIDM), pages 341-348, Paris, France, 2011. IEEE. doi: 10.1109/cidm.2011.5949423.</note>
</biblStruct>

<biblStruct coords="61,128.60,287.55,360.32,7.12;61,128.60,297.01,294.50,7.12" xml:id="b153">
	<analytic>
		<title level="a" type="main">Using sensitivity analysis and visualization techniques to open black box data mining models</title>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Embrechts</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2012.10.039</idno>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="page" from="1" to="17"/>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Paulo Cortez and Mark J Embrechts. Using sensitivity analysis and visualization techniques to open black box data mining models. Information Sciences, 225:1-17, 2013. doi: 10.1016/j.ins.2012.10.039.</note>
</biblStruct>

<biblStruct coords="61,128.60,306.47,360.32,7.12;61,128.60,315.94,258.49,7.12" xml:id="b154">
	<analytic>
		<title level="a" type="main">An efficient explanation of individual classifications using game theory</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Strumbelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="18"/>
			<date type="published" when="2010-03">March 2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Erik Strumbelj and Igor Kononenko. An efficient explanation of individual classifications using game theory. Journal of Machine Learning Research, 11:1-18, March 2010. ISSN 1532-4435.</note>
</biblStruct>

<biblStruct coords="61,128.60,323.66,360.32,8.86;61,128.60,334.87,254.38,7.12" xml:id="b155">
	<analytic>
		<title level="a" type="main">Explanation and reliability of individual predictions</title>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Štrumbelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zoran</forename><surname>Bosnić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Darko</forename><surname>Pevec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matjaž</forename><surname>Kukar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marko</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="48"/>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Igor Kononenko, Erik Štrumbelj, Zoran Bosnić, Darko Pevec, Matjaž Kukar, and Marko Robnik-Šikonja. Expla- nation and reliability of individual predictions. Informatica, 37(1):41-48, 2013.</note>
</biblStruct>

<biblStruct coords="61,128.60,342.59,360.32,8.86;61,128.60,353.20,360.32,7.97;61,128.60,363.26,13.95,7.12" xml:id="b156">
	<analytic>
		<title level="a" type="main">Explaining instance classifications with interactions of subsets of feature values</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Štrumbelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M Robnik</forename><surname>Šikonja</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.datak.2009.01.004</idno>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="886" to="904"/>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Erik Štrumbelj, Igor Kononenko, and M Robnik Šikonja. Explaining instance classifications with interactions of subsets of feature values. Data &amp; Knowledge Engineering, 68(10):886-904, 2009. doi: 10.1016/j.datak.2009.01. 004.</note>
</biblStruct>

<biblStruct coords="61,128.60,370.98,360.32,8.86;61,128.60,382.19,360.32,7.12;61,128.60,391.66,230.93,7.12" xml:id="b157">
	<analytic>
		<title level="a" type="main">Towards a model independent method for explaining classification for individual instances</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Štrumbelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-85836-2\26</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Warehousing and Knowledge Discovery</title>
		<meeting><address><addrLine>Turin, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="273" to="282"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Erik Štrumbelj and Igor Kononenko. Towards a model independent method for explaining classification for individual instances. In International Conference on Data Warehousing and Knowledge Discovery, pages 273- 282, Turin, Italy, 2008. Springer. doi: 10.1007/978-3-540-85836-2\ 26.</note>
</biblStruct>

<biblStruct coords="61,128.60,399.37,360.32,8.86;61,128.60,410.58,360.32,7.12;61,128.60,420.05,162.06,7.12" xml:id="b158">
	<analytic>
		<title level="a" type="main">Explanation and reliability of prediction models: the case of breast cancer recurrence</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Štrumbelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zoran</forename><surname>Bosnić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Branko</forename><surname>Zakotnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cvetka</forename><surname>Grašič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kuhar</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10115-009-0244-9</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="324"/>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Erik Štrumbelj, Zoran Bosnić, Igor Kononenko, Branko Zakotnik, and Cvetka Grašič Kuhar. Explanation and reliability of prediction models: the case of breast cancer recurrence. Knowledge and information systems, 24(2): 305-324, 2010. doi: 10.1007/s10115-009-0244-9.</note>
</biblStruct>

<biblStruct coords="61,128.60,429.51,360.32,7.12;61,128.60,438.98,360.32,7.12;61,128.60,448.44,178.27,7.12" xml:id="b159">
	<analytic>
		<title level="a" type="main">Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems</title>
		<author>
			<persName coords=""><forename type="first">Anupam</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shayak</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yair</forename><surname>Zick</surname></persName>
		</author>
		<idno type="DOI">10.1109/sp.2016.42</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE symposium on security and privacy (SP)</title>
		<meeting><address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="598" to="617"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In IEEE symposium on security and privacy (SP), pages 598-617, San Jose, California, USA, 2016. IEEE. doi: 10.1109/sp.2016.42.</note>
</biblStruct>

<biblStruct coords="61,128.60,457.91,360.32,7.12;61,128.60,467.37,360.32,7.12;61,128.60,476.84,206.78,7.12" xml:id="b160">
	<analytic>
		<title level="a" type="main">Auditing black-box models for indirect influence</title>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Casey</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sorelle</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tionney</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Rybeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suresh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-017-1116-3</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="122"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Philip Adler, Casey Falk, Sorelle A Friedler, Tionney Nix, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. Auditing black-box models for indirect influence. Knowledge and Information Systems, 54(1):95-122, 2018. doi: 10.1007/s10115-017-1116-3.</note>
</biblStruct>

<biblStruct coords="61,128.60,486.30,360.32,7.12;61,128.60,495.76,360.32,7.12;61,128.60,505.23,34.84,7.12" xml:id="b161">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName coords=""><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1885" to="1894"/>
		</imprint>
	</monogr>
	<note type="report_type">JMLR.org</note>
	<note type="raw_reference">Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1885-1894, Sydney, Australia, 2017. JMLR.org.</note>
</biblStruct>

<biblStruct coords="61,128.60,514.69,360.32,7.12;61,128.60,524.16,360.32,7.12;61,128.60,533.62,153.84,7.12" xml:id="b162">
	<analytic>
		<title level="a" type="main">A characterization of monotone influence measures for data classification</title>
		<author>
			<persName coords=""><forename type="first">Jakub</forename><surname>Sliwinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Strobel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yair</forename><surname>Zick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-17 Workshop on Explainable AI (XAI)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="48" to="52"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jakub Sliwinski, Martin Strobel, and Yair Zick. A characterization of monotone influence measures for data classi- fication. In IJCAI-17 Workshop on Explainable AI (XAI), pages 48-52, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="61,128.60,543.09,360.32,7.12;61,128.60,552.55,360.32,7.12;61,128.60,562.02,109.38,7.12" xml:id="b163">
	<analytic>
		<title level="a" type="main">A peek into the black box: exploring classifiers by randomization</title>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Henelius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Puolamäki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henrik</forename><surname>Boström</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lars</forename><surname>Asker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Panagiotis</forename><surname>Papapetrou</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-014-0368-8</idno>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="1503" to="1529"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Andreas Henelius, Kai Puolamäki, Henrik Boström, Lars Asker, and Panagiotis Papapetrou. A peek into the black box: exploring classifiers by randomization. Data mining and knowledge discovery, 28(5-6):1503-1529, 2014. doi: 10.1007/s10618-014-0368-8.</note>
</biblStruct>

<biblStruct coords="61,128.60,569.74,360.32,8.86;61,128.60,580.95,336.49,7.12" xml:id="b164">
	<analytic>
		<title level="a" type="main">Explaining prediction models and individual predictions with feature contributions</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Štrumbelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-013-0679-x</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="647" to="665"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Erik Štrumbelj and Igor Kononenko. Explaining prediction models and individual predictions with feature con- tributions. Knowledge and information systems, 41(3):647-665, 2014. doi: 10.1007/s10115-013-0679-x.</note>
</biblStruct>

<biblStruct coords="61,128.60,590.41,360.32,7.12;61,128.60,599.87,360.32,7.12;61,128.60,609.34,230.93,7.12" xml:id="b165">
	<analytic>
		<title level="a" type="main">Towards dependable and explainable machine learning using automated reasoning</title>
		<author>
			<persName coords=""><forename type="first">Hadrien</forename><surname>Bride</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jin</forename><forename type="middle">Song</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhé</forename><surname>Hóu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-02450-5\25</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Formal Engineering Methods</title>
		<meeting><address><addrLine>Gold Coast, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="412" to="416"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Hadrien Bride, Jie Dong, Jin Song Dong, and Zhé Hóu. Towards dependable and explainable machine learning using automated reasoning. In International Conference on Formal Engineering Methods, pages 412-416, Gold Coast, Australia, 2018. Springer. doi: 10.1007/978-3-030-02450-5\ 25.</note>
</biblStruct>

<biblStruct coords="61,128.60,618.80,360.32,7.12;61,128.60,628.27,360.32,7.12;61,128.60,637.73,76.62,7.12" xml:id="b166">
	<analytic>
		<title level="a" type="main">Accuracy vs. comprehensibility in data mining models</title>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lars</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rikard</forename><surname>König</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international conference on information fusion</title>
		<meeting>the seventh international conference on information fusion<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="295" to="300"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ulf Johansson, Lars Niklasson, and Rikard König. Accuracy vs. comprehensibility in data mining models. In Proceedings of the seventh international conference on information fusion, volume 1, pages 295-300, Stockholm, Sweden, 2004. Elsevier.</note>
</biblStruct>

<biblStruct coords="61,128.60,647.20,360.32,7.12;61,128.60,656.66,356.76,7.12" xml:id="b167">
	<analytic>
		<title level="a" type="main">The truth is in there-rule extraction from opaque models using genetic programming</title>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rikard</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lars</forename><surname>Niklasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
		<meeting><address><addrLine>Miami Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="658" to="663"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ulf Johansson, Rikard König, and Lars Niklasson. The truth is in there-rule extraction from opaque models using genetic programming. In FLAIRS Conference, pages 658-663, Miami Beach, Florida, USA, 2004. AAAI Press.</note>
</biblStruct>

<biblStruct coords="61,128.60,666.13,360.32,7.12;61,128.60,675.59,360.32,7.12" xml:id="b168">
	<analytic>
		<title level="a" type="main">Interpretability via model extraction</title>
		<author>
			<persName coords=""><forename type="first">Osbert</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carolyn</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hamsa</forename><surname>Bastani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fairness, Accountability, and Transparency in Machine Learning Workshop</title>
		<meeting><address><addrLine>Halifax, Nova Scotia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="57" to="61"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Osbert Bastani, Carolyn Kim, and Hamsa Bastani. Interpretability via model extraction. In Fairness, Account- ability, and Transparency in Machine Learning Workshop, pages 57-61, Halifax, Nova Scotia, Canada, 2017.</note>
</biblStruct>

<biblStruct coords="61,128.60,685.06,29.70,7.12" xml:id="b169">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Fat/Ml</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">FAT/ML.</note>
</biblStruct>

<biblStruct coords="61,128.60,694.52,360.32,7.12;62,128.60,145.58,360.32,7.12;62,128.60,155.04,87.93,7.12" xml:id="b170">
	<analytic>
		<title level="a" type="main">Palm: Machine learning explanations for iterative debugging</title>
		<author>
			<persName coords=""><forename type="first">Sanjay</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugene</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077257.3077271</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics</title>
		<meeting>the 2nd Workshop on Human-In-the-Loop Data Analytics<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Sanjay Krishnan and Eugene Wu. Palm: Machine learning explanations for iterative debugging. In Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics, page 4, Chicago, Illinois, USA, 2017. ACM. doi: 10.1145/3077257.3077271.</note>
</biblStruct>

<biblStruct coords="62,128.60,164.51,360.32,7.12;62,128.60,173.97,360.32,7.12;62,128.60,183.44,111.78,7.12" xml:id="b171">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3429" to="3437"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE International Conference on Computer Vision, pages 3429-3437, Venice, Italy, 2017. IEEE. doi: 10.1109/iccv.2017.371.</note>
</biblStruct>

<biblStruct coords="62,128.60,192.90,360.32,7.12;62,128.60,202.37,360.32,7.12;62,128.60,211.83,294.38,7.12" xml:id="b172">
	<analytic>
		<title level="a" type="main">What has my classifier learned? visualizing the classification rules of bag-of-feature model by support region detection</title>
		<author>
			<persName coords=""><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2012.6248103</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Providence, Rhode Island, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3586" to="3593"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lingqiao Liu and Lei Wang. What has my classifier learned? visualizing the classification rules of bag-of-feature model by support region detection. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3586-3593, Providence, Rhode Island, USA, 2012. IEEE. doi: 10.1109/cvpr.2012.6248103.</note>
</biblStruct>

<biblStruct coords="62,128.60,221.29,360.32,7.12;62,128.60,230.76,360.32,7.12;62,128.60,240.22,320.66,7.12" xml:id="b173">
	<analytic>
		<title level="a" type="main">ivisclassifier: An interactive visual analytics system for classification based on supervised dimension reduction</title>
		<author>
			<persName coords=""><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanseung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaeyeon</forename><surname>Kihm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haesun</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1109/vast.2010.5652443</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on visual analytics; Science and Technology</title>
		<meeting><address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="27" to="34"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jaegul Choo, Hanseung Lee, Jaeyeon Kihm, and Haesun Park. ivisclassifier: An interactive visual analytics system for classification based on supervised dimension reduction. In IEEE Symposium on visual analytics; Science and Technology, pages 27-34, Salt Lake City, Utah, USA, 2010. IEEE. doi: 10.1109/vast.2010.5652443.</note>
</biblStruct>

<biblStruct coords="62,128.60,249.69,360.32,7.12;62,128.60,259.15,360.32,7.12;62,128.60,268.62,117.44,7.12" xml:id="b174">
	<analytic>
		<title level="a" type="main">Real time image saliency for black box classifiers</title>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dabkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6967" to="6976"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances in Neural Information Processing Systems, pages 6967-6976, Long Beach, California, USA, 2017. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="62,128.60,278.08,360.32,7.12;62,128.60,285.80,360.32,8.86;62,128.60,297.01,57.78,7.12" xml:id="b175">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timon</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert M</forename><surname>Ãžller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831"/>
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert M Ãžller. How to explain individual classification decisions. Journal of Machine Learning Research, 11(Jun): 1803-1831, 2010.</note>
</biblStruct>

<biblStruct coords="62,128.60,306.47,360.32,7.12;62,128.60,315.94,360.32,7.12;62,128.60,325.40,217.70,7.12" xml:id="b176">
	<analytic>
		<title level="a" type="main">Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Kapelner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Bleich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emil</forename><surname>Pitkin</surname></persName>
		</author>
		<idno type="DOI">10.1080/10618600.2014.907095</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="65"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics, 24(1):44-65, 2015. doi: 10.1080/10618600.2014.907095.</note>
</biblStruct>

<biblStruct coords="62,128.60,334.87,360.32,7.12;62,128.60,344.33,360.32,7.12;62,128.60,353.80,260.14,7.12" xml:id="b177">
	<analytic>
		<title level="a" type="main">Visualizing the feature importance for black box models</title>
		<author>
			<persName coords=""><forename type="first">Giuseppe</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernd</forename><surname>Bischl</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-10925-7\40</idno>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="655" to="670"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Giuseppe Casalicchio, Christoph Molnar, and Bernd Bischl. Visualizing the feature importance for black box models. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 655-670, Dublin, Ireland, 2018. Springer. doi: 10.1007/978-3-030-10925-7\ 40.</note>
</biblStruct>

<biblStruct coords="62,128.60,363.26,360.32,7.12;62,128.60,372.73,360.32,7.12;62,128.60,382.19,360.32,7.12;62,128.60,391.66,65.43,7.12" xml:id="b178">
	<analytic>
		<title level="a" type="main">A causal framework for explaining the predictions of black-box sequence-to-sequence models</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>doi: 10. 18653</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="17" to="1042"/>
		</imprint>
	</monogr>
	<note type="raw_reference">David Alvarez-Melis and Tommi S Jaakkola. A causal framework for explaining the predictions of black-box sequence-to-sequence models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 412-421, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10. 18653/v1/d17-1042.</note>
</biblStruct>

<biblStruct coords="62,128.60,401.12,360.32,7.12;62,128.60,410.58,360.32,7.12;62,128.60,420.05,20.15,7.12" xml:id="b179">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
	<note type="raw_reference">Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Proceedings of the 3rd International Conference on Learning Representations, San Diego, California, USA, 2015. ICLR.</note>
</biblStruct>

<biblStruct coords="62,128.60,429.51,360.32,7.12;62,128.60,438.98,360.32,7.12;62,128.60,448.44,98.10,7.12" xml:id="b180">
	<analytic>
		<title level="a" type="main">Using visual analytics to interpret predictive machine learning models</title>
		<author>
			<persName coords=""><forename type="first">Josua</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrico</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Human Interpretability in Machine Learning</title>
		<meeting><address><addrLine>New York City, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="106" to="110"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Josua Krause, Adam Perer, and Enrico Bertini. Using visual analytics to interpret predictive machine learning models. In ICML Workshop on Human Interpretability in Machine Learning, pages 106-110, New York City, New York, USA, 2016. ICML.</note>
</biblStruct>

<biblStruct coords="62,128.60,457.91,360.32,7.12;62,128.60,467.37,360.32,7.12;62,128.60,476.84,360.32,7.12;62,128.60,486.30,280.37,7.12" xml:id="b181">
	<analytic>
		<title level="a" type="main">Visual explanation of evidence with additive classifiers</title>
		<author>
			<persName coords=""><forename type="first">Brett</forename><surname>Poulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roman</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duane</forename><surname>Szafron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Russell</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">S</forename><surname>Wishart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alona</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Pearcy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cam</forename><surname>Macdonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Anvik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The National Conference On Artificial Intelligence</title>
		<meeting>Of The National Conference On Artificial Intelligence<address><addrLine>Boston, Massachusetts, USA; Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">2006. 1999</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1822" to="1829"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Brett Poulin, Roman Eisner, Duane Szafron, Paul Lu, Russell Greiner, David S Wishart, Alona Fyshe, Brandon Pearcy, Cam MacDonell, and John Anvik. Visual explanation of evidence with additive classifiers. In Proceedings Of The National Conference On Artificial Intelligence, volume 21, pages 1822-1829, Boston, Massachusetts, USA, 2006. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</note>
</biblStruct>

<biblStruct coords="62,128.60,495.76,360.32,7.12;62,128.60,505.23,360.32,7.12;62,128.60,514.69,101.05,7.12" xml:id="b182">
	<analytic>
		<title level="a" type="main">Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models</title>
		<author>
			<persName coords=""><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lezhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, and David S Ebert. Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models. IEEE transactions on visualization and computer graphics, 25(1):364-373, 2019.</note>
</biblStruct>

<biblStruct coords="62,128.60,524.16,360.32,7.12;62,128.60,533.62,360.32,7.12;62,128.60,543.09,201.65,7.12" xml:id="b183">
	<analytic>
		<title level="a" type="main">Visual exploration of machine learning results using data cube analysis</title>
		<author>
			<persName coords=""><forename type="first">Minsuk</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dezhi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duen</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Polo</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939502.2939503</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human-In-the-Loop Data Analytics</title>
		<meeting>the Workshop on Human-In-the-Loop Data Analytics<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Minsuk Kahng, Dezhi Fang, and Duen Horng Polo Chau. Visual exploration of machine learning results using data cube analysis. In Proceedings of the Workshop on Human-In-the-Loop Data Analytics, page 1, San Francisco, California, USA, 2016. ACM. doi: 10.1145/2939502.2939503.</note>
</biblStruct>

<biblStruct coords="62,128.60,552.55,360.32,7.12;62,128.60,562.02,360.32,7.12;62,128.60,571.48,145.42,7.12" xml:id="b184">
	<analytic>
		<title level="a" type="main">Discovering additive structure in black box functions</title>
		<author>
			<persName coords=""><forename type="first">Giles</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="DOI">10.1145/1014052.1014122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="575" to="580"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Giles Hooker. Discovering additive structure in black box functions. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 575-580, Seattle, Washington, USA, 2004. ACM. doi: 10.1145/1014052.1014122.</note>
</biblStruct>

<biblStruct coords="62,128.60,580.95,360.32,7.12;62,128.60,590.41,266.24,7.12" xml:id="b185">
	<analytic>
		<title level="a" type="main">Justification narratives for individual classifications</title>
		<author>
			<persName coords=""><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AutoML workshop at ICML</title>
		<meeting>the AutoML workshop at ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="1" to="7"/>
		</imprint>
	</monogr>
	<note>ICML</note>
	<note type="raw_reference">Or Biran and Kathleen McKeown. Justification narratives for individual classifications. In Proceedings of the AutoML workshop at ICML, volume 2014, pages 1-7, Beijing, China, 2014. ICML.</note>
</biblStruct>

<biblStruct coords="62,128.60,599.87,360.32,7.12;62,128.60,609.34,360.32,7.12;62,128.60,618.80,299.05,7.12" xml:id="b186">
	<analytic>
		<title level="a" type="main">Interpreting black-box classifiers using instance-level visual explanations</title>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Tamagnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josua</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aritra</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrico</forename><surname>Bertini</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077257.3077260</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics</title>
		<meeting>the 2nd Workshop on Human-In-the-Loop Data Analytics<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Paolo Tamagnini, Josua Krause, Aritra Dasgupta, and Enrico Bertini. Interpreting black-box classifiers using instance-level visual explanations. In Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics, pages 6:1-6:6, San Francisco, California, USA, 2017. ACM. doi: 10.1145/3077257.3077260.</note>
</biblStruct>

<biblStruct coords="62,128.60,628.27,360.32,7.12;62,128.60,637.73,360.32,7.12;62,128.60,647.20,39.19,7.12" xml:id="b187">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence via bayesian teaching</title>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Cheng-Hsin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Shafto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 workshop on Teaching Machines, Robots, and Humans</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="127" to="137"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Scott Cheng-Hsin Yang and Patrick Shafto. Explainable artificial intelligence via bayesian teaching. In NIPS 2017 workshop on Teaching Machines, Robots, and Humans, pages 127-137, Long Beach, California, USA, 2017. NIPS.</note>
</biblStruct>

<biblStruct coords="62,128.60,656.66,360.32,7.12;62,128.60,666.13,360.32,7.12;62,128.60,675.59,360.32,7.12;62,128.60,685.06,100.48,7.12" xml:id="b188">
	<analytic>
		<title level="a" type="main">Interpreting black box predictions using fisher kernels</title>
		<author>
			<persName coords=""><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanmi</forename><surname>Koyejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</editor>
		<meeting><address><addrLine>Naha, Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04-18">16-18 Apr 2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="3382" to="3390"/>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
	<note type="raw_reference">Rajiv Khanna, Been Kim, Joydeep Ghosh, and Sanmi Koyejo. Interpreting black box predictions using fisher kernels. In Kamalika Chaudhuri and Masashi Sugiyama, editors, The 22nd International Conference on Artificial Intelligence and Statistics, volume 89, pages 3382-3390, Naha, Okinawa, Japan, 16-18 Apr 2019. Proceedings of Machine Learning Research.</note>
</biblStruct>

<biblStruct coords="62,128.60,694.52,360.32,7.12;63,128.60,145.58,191.17,7.12" xml:id="b189">
	<analytic>
		<title level="a" type="main">Prototype selection for interpretable classification</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Bien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="DOI">10.1214/11-aoas495</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2403" to="2424"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob Bien, Robert Tibshirani, et al. Prototype selection for interpretable classification. The Annals of Applied Statistics, 5(4):2403-2424, 2011. doi: 10.1214/11-aoas495.</note>
</biblStruct>

<biblStruct coords="63,128.60,155.04,360.32,7.12;63,128.60,164.51,360.32,7.12;63,128.60,173.97,215.17,7.12" xml:id="b190">
	<analytic>
		<title level="a" type="main">Case-based explanation of non-case-based learning methods</title>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hooshang</forename><surname>Kangarloo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Usha</forename><surname>Dionisio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Symposium</title>
		<meeting>the AMIA Symposium<address><addrLine>Washington, District of Columbia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">212</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Rich Caruana, Hooshang Kangarloo, JD Dionisio, Usha Sinha, and David Johnson. Case-based explanation of non-case-based learning methods. In Proceedings of the AMIA Symposium, page 212, Washington, District of Columbia, USA, 1999. American Medical Informatics Association.</note>
</biblStruct>

<biblStruct coords="63,128.60,183.44,360.32,7.12;63,128.60,192.90,360.32,7.12;63,128.60,202.37,231.99,7.12" xml:id="b191">
	<analytic>
		<title level="a" type="main">Adversarial detection with model interpretation</title>
		<author>
			<persName coords=""><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3220027</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1803" to="1811"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ninghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1803-1811, London, United Kingdom, 2018. ACM. doi: 10.1145/3219819.3220027.</note>
</biblStruct>

<biblStruct coords="63,128.60,211.83,360.32,7.12;63,128.60,221.29,360.32,7.12;63,128.60,230.76,257.34,7.12" xml:id="b192">
	<analytic>
		<title level="a" type="main">Examples are not enough, learn to criticize! criticism for interpretability</title>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oluwasanmi</forename><forename type="middle">O</forename><surname>Koyejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2280" to="2288"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism for interpretability. In Advances in Neural Information Processing Systems, pages 2280-2288, Long Beach, California, USA, 2016. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="63,128.60,240.22,360.32,7.12;63,128.60,249.69,360.32,7.12;63,128.60,259.15,360.32,7.12;63,128.60,268.62,12.17,7.12" xml:id="b193">
	<analytic>
		<title level="a" type="main">Explanations based on the missing: Towards contrastive explanations with pertinent negatives</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronny</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chun-Chen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paishun</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Payel</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31 (NIPS)</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="592" to="603"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, and Payel Das. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. In Advances in Neural Information Processing Systems 31 (NIPS), pages 592-603, Montréal, Canada, 2018. Curran Associates, Inc.</note>
</biblStruct>

<biblStruct coords="63,128.60,278.08,360.32,7.12;63,128.60,287.55,360.32,7.12;63,128.60,297.01,144.90,7.12" xml:id="b194">
	<analytic>
		<title level="a" type="main">Model-agnostic interpretability of machine learning</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability of machine learning. In Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining, pages 1135-1144, San Francisco, California, USA, 2016. ACM.</note>
</biblStruct>

<biblStruct coords="63,128.60,306.47,360.32,7.12;63,128.60,315.94,360.32,7.12;63,128.60,325.40,360.32,7.12;63,128.60,334.87,81.42,7.12" xml:id="b195">
	<analytic>
		<title level="a" type="main">Explaining the unexplained: A class-enhanced attentive response (clear) approach to understanding deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Devinder</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2017.215</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="36" to="44"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Devinder Kumar, Alexander Wong, and Graham W Taylor. Explaining the unexplained: A class-enhanced atten- tive response (clear) approach to understanding deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 36-44, Honolulu, Hawaii, USA, 2017. IEEE. doi: 10.1109/cvprw.2017.215.</note>
</biblStruct>

<biblStruct coords="63,128.60,344.33,360.32,7.12;63,128.60,353.80,360.32,7.12;63,128.60,363.26,146.35,7.12" xml:id="b196">
	<analytic>
		<title level="a" type="main">Visualizing feature maps in deep neural networks using deepresolve. a genomics case study</title>
		<author>
			<persName coords=""><forename type="first">Ge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning 2017 -Workshop on Visualization for Deep Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="32" to="41"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ge Liu and David Gifford. Visualizing feature maps in deep neural networks using deepresolve. a genomics case study. In International Conference on Machine Learning 2017 -Workshop on Visualization for Deep Learning, pages 32-41, Sydney, Australia, 2017. ICML.</note>
</biblStruct>

<biblStruct coords="63,128.60,372.73,360.32,7.12;63,128.60,382.19,360.32,7.12;63,128.60,391.66,360.25,7.12;63,128.60,401.12,47.90,7.12" xml:id="b197">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.74</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, pages 618-626, Venice, Italy, 2017. IEEE. doi: 10.1109/ ICCV.2017.74.</note>
</biblStruct>

<biblStruct coords="63,128.60,410.58,360.32,7.12;63,128.60,420.05,156.18,7.12" xml:id="b198">
	<analytic>
		<title level="a" type="main">Using explanations to improve ensembling of visual question answering systems</title>
		<author>
			<persName coords=""><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajani</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Training</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="248" to="349"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nazneen Fatema Rajani and Raymond J Mooney. Using explanations to improve ensembling of visual question answering systems. Training, 82:248-349, 2017.</note>
</biblStruct>

<biblStruct coords="63,128.60,429.51,360.32,7.12;63,128.60,438.98,360.32,7.12;63,128.60,448.44,61.77,7.12" xml:id="b199">
	<analytic>
		<title level="a" type="main">Towards transparent ai systems: Interpreting visual question answering models</title>
		<author>
			<persName coords=""><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akrit</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Visualization for Deep Learning</title>
		<meeting><address><addrLine>New York City, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Batra. Towards transparent ai systems: Interpreting visual question answering models. In ICML Workshop on Visualization for Deep Learning, New York City, New York, USA, 2016. ICML.</note>
</biblStruct>

<biblStruct coords="63,128.60,457.91,360.33,7.12;63,128.60,467.37,249.92,7.12" xml:id="b200">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European confer- ence on computer vision, pages 818-833, Zurich, Switzerland, 2014. Springer.</note>
</biblStruct>

<biblStruct coords="63,128.60,476.84,360.32,7.12;63,128.60,486.30,360.32,7.12;63,128.60,495.76,276.43,7.12" xml:id="b201">
	<analytic>
		<title level="a" type="main">Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00910</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8730" to="8738"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ruth Fong and Andrea Vedaldi. Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8730-8738, Salt Lake City, Utah, USA, 2018. IEEE. doi: 10.1109/CVPR.2018.00910.</note>
</biblStruct>

<biblStruct coords="63,128.60,505.23,360.32,7.12;63,128.60,514.69,360.32,7.12;63,128.60,524.16,218.56,7.12" xml:id="b202">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName coords=""><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7299155</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5188" to="5196"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5188-5196, Boston, Massachusetts, USA, 2015. IEEE. doi: 10.1109/cvpr.2015.7299155.</note>
</biblStruct>

<biblStruct coords="63,128.60,533.62,360.32,7.12;63,128.60,543.09,360.32,7.12;63,128.60,551.95,360.32,7.97;63,128.60,562.02,29.89,7.12" xml:id="b203">
	<analytic>
		<title level="a" type="main">Towards explanation of dnn-based prediction with guided feature inversion</title>
		<author>
			<persName coords=""><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3220099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1358" to="1367"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mengnan Du, Ninghao Liu, Qingquan Song, and Xia Hu. Towards explanation of dnn-based prediction with guided feature inversion. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 1358-1367, London, United Kingdom, 2018. ACM. doi: 10.1145/3219819. 3220099.</note>
</biblStruct>

<biblStruct coords="63,128.60,571.48,360.32,7.12;63,128.60,580.95,360.32,7.12;63,128.60,590.41,198.38,7.12" xml:id="b204">
	<analytic>
		<title level="a" type="main">Smoothgrad: removing noise by adding noise</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning 2017 -Workshop on Visualization for Deep Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="24"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. In International Conference on Machine Learning 2017 -Workshop on Visualization for Deep Learning, pages 15-24, Sydney, Australia, 2017. ICML.</note>
</biblStruct>

<biblStruct coords="63,128.60,599.87,360.32,7.12;63,128.60,609.34,360.32,7.12;63,128.60,618.80,160.56,7.12" xml:id="b205">
	<analytic>
		<title level="a" type="main">Interpretable convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00920</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8827" to="8836"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8827-8836, Salt Lake City, Utah, USA, 2018. IEEE. doi: 10.1109/cvpr.2018.00920.</note>
</biblStruct>

<biblStruct coords="63,128.60,628.27,360.32,7.12;63,128.60,637.73,360.32,7.12;63,128.60,647.20,40.07,7.12" xml:id="b206">
	<analytic>
		<title level="a" type="main">Visualizing deep neural network decisions: Prediction difference analysis</title>
		<author>
			<persName coords=""><forename type="first">Luisa</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tameem</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions: Prediction difference analysis. In 5th International Conference on Learning Representations, Toulon, France, 2017. ICLR.</note>
</biblStruct>

<biblStruct coords="63,128.60,656.66,360.32,7.12;63,128.60,666.13,360.32,7.12;63,128.60,675.59,360.32,7.12;63,128.60,685.06,181.69,7.12" xml:id="b207">
	<analytic>
		<title level="a" type="main">Explaining recurrent neural network predictions in sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-5221</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="159" to="168"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Leila Arras, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. Explaining recurrent neural net- work predictions in sentiment analysis. In Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 159-168, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/w17-5221.</note>
</biblStruct>

<biblStruct coords="63,128.60,694.52,360.32,7.12;64,128.60,145.58,360.32,7.12;64,128.60,155.04,360.32,7.12" xml:id="b208">
	<analytic>
		<title level="a" type="main">Layer-wise relevance propagation for neural networks with local renormalization layers</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44781-0\8</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="63" to="71"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, Klaus-Robert Müller, and Wojciech Samek. Layer- wise relevance propagation for neural networks with local renormalization layers. In International Conference on Artificial Neural Networks, pages 63-71, Barcelona, Spain, 2016. Springer. doi: 10.1007/978-3-319-44781-0\ 8.</note>
</biblStruct>

<biblStruct coords="64,128.60,164.51,360.32,7.12;64,128.60,173.97,360.32,7.12;64,128.60,183.44,33.87,7.12" xml:id="b209">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep taylor decomposition</title>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222"/>
			<date type="published" when="2017-05">May. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert Müller. Ex- plaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition, 65(May):211- 222, 2017.</note>
</biblStruct>

<biblStruct coords="64,128.60,192.90,360.32,7.12;64,128.60,202.37,360.32,7.12;64,128.60,211.83,41.84,7.12" xml:id="b210">
	<analytic>
		<title level="a" type="main">Deep saliency: What is learnt by a deep network about saliency</title>
		<author>
			<persName coords=""><forename type="first">Sen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Pugeault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning -Workshop on Visualization for Deep Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Sen He and Nicolas Pugeault. Deep saliency: What is learnt by a deep network about saliency? In International Conference on Machine Learning -Workshop on Visualization for Deep Learning, pages 1-5, Sydney, Australia, 2017. ICML.</note>
</biblStruct>

<biblStruct coords="64,128.60,221.29,360.32,7.12;64,128.60,230.76,360.32,7.12;64,128.60,240.22,240.26,7.12" xml:id="b211">
	<analytic>
		<title level="a" type="main">Learning how to explain neural networks: Patternnet and patternattribution</title>
		<author>
			<persName coords=""><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristof</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Dähne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
	<note type="raw_reference">Pieter-Jan Kindermans, Kristof T. Schütt, Maximilian Alber, Klaus-Robert Müller, Dumitru Erhan, Been Kim, and Sven Dähne. Learning how to explain neural networks: Patternnet and patternattribution. In 6th International Conference on Learning Representations, Vancouver, Canada, 2018. ICLR.</note>
</biblStruct>

<biblStruct coords="64,128.60,249.69,360.32,7.12;64,128.60,259.15,360.32,7.12;64,128.60,268.62,183.69,7.12" xml:id="b212">
	<analytic>
		<title level="a" type="main">Visual explanation by interpretation: Improving visual feedback capabilities of deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Oramas</forename><surname>Mogrovejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
	<note type="raw_reference">Oramas Mogrovejo, José Antonio, Kaili Wang, and Tinne Tuytelaars. Visual explanation by interpretation: Im- proving visual feedback capabilities of deep neural networks. In 7th International Conference on Learning Rep- resentations, New Orleans, Louisiana, USA, 2019. ICLR.</note>
</biblStruct>

<biblStruct coords="64,128.60,278.08,360.32,7.12;64,128.60,287.55,360.32,7.12;64,128.60,297.01,360.32,7.12;64,128.60,306.47,263.33,7.12" xml:id="b213">
	<analytic>
		<title level="a" type="main">Twin-systems to explain artificial neural networks using case-based reasoning: comparative tests of feature-weighting methods in ann-cbr twins for xai</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eoin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">T</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Keane</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/376</idno>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth International Joint Conferences on Artifical Intelligence (IJCAI)</title>
		<meeting><address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2708" to="2715"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Eoin M Kenny and Mark T Keane. Twin-systems to explain artificial neural networks using case-based reason- ing: comparative tests of feature-weighting methods in ann-cbr twins for xai. In Twenty-Eighth International Joint Conferences on Artifical Intelligence (IJCAI), pages 2708-2715, Macao, China, 2019. International Joint Conferences on Artificial Intelligence Organization. doi: 10.24963/ijcai.2019/376.</note>
</biblStruct>

<biblStruct coords="64,128.60,315.94,360.32,7.12;64,128.60,325.46,360.32,6.97;64,128.60,334.87,360.32,7.12;64,128.60,344.33,130.55,7.12" xml:id="b214">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName coords=""><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1082</idno>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="681" to="691"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp. In The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 681-691, San Diego California, USA, 2016. The Association for Computational Linguistics. doi: 10.18653/v1/n16-1082.</note>
</biblStruct>

<biblStruct coords="64,128.60,353.80,360.32,7.12;64,128.60,363.26,360.32,7.12;64,128.60,372.73,360.32,7.12;64,128.60,382.19,87.93,7.12" xml:id="b215">
	<analytic>
		<title level="a" type="main">Exact and consistent interpretation for piecewise linear neural networks: A closed form solution</title>
		<author>
			<persName coords=""><forename type="first">Lingyang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juhua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lanjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3220063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1244" to="1253"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lingyang Chu, Xia Hu, Juhua Hu, Lanjun Wang, and Jian Pei. Exact and consistent interpretation for piecewise linear neural networks: A closed form solution. In Proceedings of the 24th ACM SIGKDD International Confer- ence on Knowledge Discovery and Data Mining, pages 1244-1253, London, United Kingdom, 2018. ACM. doi: 10.1145/3219819.3220063.</note>
</biblStruct>

<biblStruct coords="64,128.60,391.66,360.32,7.12;64,128.60,401.12,360.32,7.12;64,128.60,410.58,150.83,7.12" xml:id="b216">
	<analytic>
		<title level="a" type="main">Interpretable deep convolutional neural networks via metalearning</title>
		<author>
			<persName coords=""><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.2018.8489172</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Xuan Liu, Xiaoguang Wang, and Stan Matwin. Interpretable deep convolutional neural networks via meta- learning. In International Joint Conference on Neural Networks (IJCNN), pages 1-9, Rio de Janeiro, Brazil, 2018. IEEE. doi: 10.1109/ijcnn.2018.8489172.</note>
</biblStruct>

<biblStruct coords="64,128.60,420.05,360.32,7.12;64,128.60,429.51,360.32,7.12;64,128.60,438.98,89.65,7.12" xml:id="b217">
	<analytic>
		<title level="a" type="main">Understanding deep features with computer-generated imagery</title>
		<author>
			<persName coords=""><forename type="first">Aubry</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.329</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2875" to="2883"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mathieu Aubry and Bryan C Russell. Understanding deep features with computer-generated imagery. In Proceed- ings of the IEEE International Conference on Computer Vision, pages 2875-2883, Santiago, Chile, 2015. IEEE. doi: 10.1109/iccv.2015.329.</note>
</biblStruct>

<biblStruct coords="64,128.60,448.44,360.32,7.12;64,128.60,457.91,322.77,7.12" xml:id="b218">
	<analytic>
		<title level="a" type="main">Graying the black box: Understanding dqns</title>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nir</forename><surname>Ben-Zrihem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>New York City, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1899" to="1908"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Tom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding dqns. In International Conference on Machine Learning, pages 1899-1908, New York City, New York, USA, 2016. PMLR.</note>
</biblStruct>

<biblStruct coords="64,128.60,467.37,360.32,7.12;64,128.60,476.84,360.32,7.12;64,128.60,486.30,91.69,7.12" xml:id="b219">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandre</forename><forename type="middle">X</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><forename type="middle">C</forename><surname>Falcao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Telea</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvcg.2016.2598838</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Paulo E Rauber, Samuel G Fadel, Alexandre X Falcao, and Alexandru C Telea. Visualizing the hidden activity of artificial neural networks. IEEE transactions on visualization and computer graphics, 23(1):101-110, 2017. doi: 10.1109/tvcg.2016.2598838.</note>
</biblStruct>

<biblStruct coords="64,128.60,495.76,360.32,7.12;64,128.60,505.23,360.32,7.12;64,128.60,514.69,97.63,7.12" xml:id="b220">
	<analytic>
		<title level="a" type="main">Treeview: Peeking into deep neural networks via feature-space partitioning</title>
		<author>
			<persName coords=""><forename type="first">Bhavya</forename><surname>Jayaraman J Thiagarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasanna</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthikeyan</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramamurthy</forename><surname>Natesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Interpretability Workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jayaraman J Thiagarajan, Bhavya Kailkhura, Prasanna Sattigeri, and Karthikeyan Natesan Ramamurthy. Tree- view: Peeking into deep neural networks via feature-space partitioning. In NIPS Interpretability Workshop, Barcelona, Spain, 2016. NIPS.</note>
</biblStruct>

<biblStruct coords="64,128.60,524.16,360.32,7.12;64,128.60,533.62,360.32,7.12;64,128.60,543.09,316.66,7.12" xml:id="b221">
	<analytic>
		<title level="a" type="main">Gan dissection: Visualizing and understanding generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhou</forename><surname>Bolei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation</title>
		<meeting>the International Conference on Learning Representation<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
	<note type="raw_reference">David Bau, Jun-Yan Zhu, Hendrik Strobelt, Zhou Bolei, Joshua B. Tenenbaum, William T. Freeman, and Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In Proceedings of the International Conference on Learning Representation, New Orleans, Louisiana, USA, 2019. ICLR.</note>
</biblStruct>

<biblStruct coords="64,128.60,552.55,360.32,7.12;64,128.60,562.02,360.32,7.12;64,128.60,571.48,350.48,7.12" xml:id="b222">
	<analytic>
		<title level="a" type="main">Towards visual explanations for convolutional neural networks via input resampling</title>
		<author>
			<persName coords=""><forename type="first">Sandeep</forename><surname>Benjamin J Lengerich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Konam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephanie</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuela</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning 2017 -Workshop on Visualization for Deep Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="25" to="31"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Benjamin J Lengerich, Sandeep Konam, Eric P Xing, Stephanie Rosenthal, and Manuela Veloso. Towards visual explanations for convolutional neural networks via input resampling. In International Conference on Machine Learning 2017 -Workshop on Visualization for Deep Learning, pages 25-31, Sydney, Australia, 2017. ICML.</note>
</biblStruct>

<biblStruct coords="64,128.60,580.95,360.32,7.12;64,128.60,590.41,360.32,7.12;64,128.60,599.87,44.06,7.12" xml:id="b223">
	<monogr>
		<title level="m" type="main">Understanding representations learned in deep architectures</title>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1355</biblScope>
		</imprint>
		<respStmt>
			<orgName>Department d'Informatique et Recherche Operationnelle, University of Montreal, QC, Canada, Tech. Rep</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Dumitru Erhan, Aaron Courville, and Yoshua Bengio. Understanding representations learned in deep architec- tures. Department d'Informatique et Recherche Operationnelle, University of Montreal, QC, Canada, Tech. Rep, 1355:1, 2010.</note>
</biblStruct>

<biblStruct coords="64,128.60,609.34,360.32,7.12;64,128.60,618.80,360.32,7.12;64,128.60,628.27,289.57,7.12" xml:id="b224">
	<analytic>
		<title level="a" type="main">Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization for Deep Learning workshop. International Conference on Machine Learning</title>
		<meeting><address><addrLine>New York City; New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Anh Nguyen, Jason Yosinski, and Jeff Clune. Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks. In Visualization for Deep Learning workshop. Inter- national Conference on Machine Learning, New York City, New York, USA, 2016. ICML.</note>
</biblStruct>

<biblStruct coords="64,128.60,637.73,360.32,7.12;64,128.60,647.20,360.32,7.12;64,128.60,656.66,348.76,7.12" xml:id="b225">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName coords=""><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3387" to="3395"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In Advances in Neural Information Processing Systems, pages 3387-3395, Barcelona, Spain, 2016. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="64,128.60,666.13,360.32,7.12;64,128.60,675.59,360.32,7.12;64,128.60,685.06,360.32,7.12;64,128.60,694.52,126.49,7.12" xml:id="b226">
	<analytic>
		<title level="a" type="main">Interactive naming for explaining deep neural networks: A formative study</title>
		<author>
			<persName coords=""><forename type="first">Mandana</forename><surname>Hamidi-Haines</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of the ACM IUI 2019 Workshops colocated with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2327</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Mandana Hamidi-Haines, Zhongang Qi, Alan Fern, Fuxin Li, and Prasad Tadepalli. Interactive naming for ex- plaining deep neural networks: A formative study. In Joint Proceedings of the ACM IUI 2019 Workshops co- located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR-WS.org.</note>
</biblStruct>

<biblStruct coords="65,128.60,145.58,360.32,7.12;65,128.60,155.04,153.13,7.12" xml:id="b227">
	<analytic>
		<title level="a" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName coords=""><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshops</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. In ICLR Workshops, San Juan, Puerto Rico, 2016. ICLR.</note>
</biblStruct>

<biblStruct coords="65,128.60,164.51,360.32,7.12;65,128.60,173.97,360.32,7.12;65,128.60,183.44,116.00,7.12" xml:id="b228">
	<analytic>
		<title level="a" type="main">Interpreting cnn knowledge via an explanatory graph</title>
		<author>
			<persName coords=""><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruiming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2124" to="2132"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, and Song-Chun Zhu. Interpreting cnn knowledge via an explanatory graph. In Thirty-Second AAAI Conference on Artificial Intelligence, pages 2124-2132, New Orleans, Louisiana, USA, 2018. AAAI Press.</note>
</biblStruct>

<biblStruct coords="65,128.60,192.90,360.32,7.12;65,128.60,202.37,360.32,7.12;65,128.60,211.83,157.50,7.12" xml:id="b229">
	<analytic>
		<title level="a" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName coords=""><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1853" to="1863"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, and Eric P Xing. Symbolic graph reasoning meets convolu- tions. In Advances in Neural Information Processing Systems, pages 1853-1863, Montréal, Canada, 2018. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="65,128.60,221.29,360.32,7.12;65,128.60,230.76,360.32,7.12;65,128.60,240.22,151.85,7.12" xml:id="b230">
	<analytic>
		<title level="a" type="main">Growing interpretable part graphs on convnets via multi-shot learning</title>
		<author>
			<persName coords=""><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruiming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2898" to="2906"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu. Growing interpretable part graphs on con- vnets via multi-shot learning. In Thirty-First AAAI Conference on Artificial Intelligence, pages 2898-2906, San Francisco, California, USA, 2017. AAAI Press.</note>
</biblStruct>

<biblStruct coords="65,128.60,249.69,360.32,7.12;65,128.60,259.15,335.61,7.12" xml:id="b231">
	<analytic>
		<title level="a" type="main">The building blocks of interpretability</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katherine</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00010</idno>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. The building blocks of interpretability. Distill, 3(3):e10, 2018. doi: 10.23915/distill.00010.</note>
</biblStruct>

<biblStruct coords="65,128.60,268.62,360.32,7.12;65,128.60,278.08,360.32,7.12;65,128.60,287.55,41.84,7.12" xml:id="b232">
	<analytic>
		<title level="a" type="main">A cti v is: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName coords=""><forename type="first">Minsuk</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duen</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Polo</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Minsuk Kahng, Pierre Y Andrews, Aditya Kalro, and Duen Horng Polo Chau. A cti v is: Visual exploration of industry-scale deep neural network models. IEEE transactions on visualization and computer graphics, 24(1): 88-97, 2018.</note>
</biblStruct>

<biblStruct coords="65,128.60,297.01,360.32,7.12;65,128.60,306.47,348.43,7.12" xml:id="b233">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning, poster presentation</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ICML</note>
	<note type="raw_reference">Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. In In ICML Workshop on Deep Learning, poster presentation, Lille, France, 2015. ICML.</note>
</biblStruct>

<biblStruct coords="65,128.60,315.94,360.32,7.12;65,128.60,325.40,360.32,7.12;65,128.60,334.87,249.16,7.12" xml:id="b234">
	<analytic>
		<title level="a" type="main">Evolutionary visual analysis of deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Wen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shenghui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning 2017 -Workshop on Visualization for Deep Learning</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6" to="14"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Wen Zhong, Cong Xie, Yuan Zhong, Yang Wang, Wei Xu, Shenghui Cheng, and Klaus Mueller. Evolutionary visual analysis of deep neural networks. In International Conference on Machine Learning 2017 -Workshop on Visualization for Deep Learning, pages 6-14, Sydney, Australia, 2017. ICML.</note>
</biblStruct>

<biblStruct coords="65,128.60,344.33,360.32,7.12;65,128.60,353.80,360.32,7.12;65,128.60,363.26,197.31,7.12" xml:id="b235">
	<analytic>
		<title level="a" type="main">neural networks</title>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Seegerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miriam</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter-Jan</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kindermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="8"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>innvestigate</note>
	<note type="raw_reference">Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam Hägele, Kristof T. Schütt, Grégoire Mon- tavon, Wojciech Samek, Klaus-Robert Müller, Sven Dähne, and Pieter-Jan Kindermans. innvestigate neural net- works. Journal of Machine Learning Research, 20:1-8, 2019.</note>
</biblStruct>

<biblStruct coords="65,128.60,372.73,360.32,7.12;65,128.60,382.19,360.32,7.12;65,128.60,391.66,250.55,7.12" xml:id="b236">
	<analytic>
		<title level="a" type="main">Seq2seq-vis: A visual debugging tool for sequence-to-sequence models</title>
		<author>
			<persName coords=""><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2865044</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="353" to="363"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hendrik Strobelt, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pfister, and Alexander M Rush. Seq2seq-vis: A visual debugging tool for sequence-to-sequence models. IEEE transactions on visualization and computer graphics, 25(1):353-363, 2018. doi: 10.1109/TVCG.2018.2865044.</note>
</biblStruct>

<biblStruct coords="65,128.60,401.12,360.32,7.12;65,128.60,410.58,360.32,7.12;65,128.60,420.05,189.50,7.12" xml:id="b237">
	<analytic>
		<title level="a" type="main">Nvis: An interactive visualization tool for neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">O</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><forename type="middle">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Data Exploration and Analysis VIII</title>
		<meeting><address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">4302</biblScope>
			<biblScope unit="page" from="234" to="242"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew J Streeter, Matthew O Ward, and Sergio A Alvarez. Nvis: An interactive visualization tool for neural networks. In Visual Data Exploration and Analysis VIII, volume 4302, pages 234-242, San Jose, California, USA, 2001. International Society for Optics and Photonics.</note>
</biblStruct>

<biblStruct coords="65,128.60,429.51,360.32,7.12;65,128.60,438.98,360.25,7.12;65,128.60,448.44,70.18,7.12" xml:id="b238">
	<analytic>
		<title level="a" type="main">Understanding neural networks via rule extraction</title>
		<author>
			<persName coords=""><forename type="first">Rudy</forename><surname>Setiono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1142/S0129065797000380</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<meeting><address><addrLine>Montréal, Québec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="480" to="485"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Rudy Setiono and Huan Liu. Understanding neural networks via rule extraction. In IJCAI, volume 1, pages 480-485, Montréal, Québec, 1995. International Joint Conferences on Artificial Intelligence, Inc. doi: 10.1142/ S0129065797000380.</note>
</biblStruct>

<biblStruct coords="65,128.60,457.91,360.32,7.12;65,128.60,467.37,360.32,7.12;65,128.60,476.84,23.91,7.12" xml:id="b239">
	<analytic>
		<title level="a" type="main">Classification tree extraction from trained artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">Andrey</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ludmila</forename><surname>Aleksejeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vilen</forename><surname>Jumutc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arkady</forename><surname>Borisov</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2017.01.172</idno>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="556" to="563"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Andrey Bondarenko, Ludmila Aleksejeva, Vilen Jumutc, and Arkady Borisov. Classification tree extraction from trained artificial neural networks. Procedia Computer Science, 104:556-563, 2017. doi: 10.1016/j.procs.2017. 01.172.</note>
</biblStruct>

<biblStruct coords="65,128.60,486.30,360.32,7.12;65,128.60,495.76,314.44,7.12" xml:id="b240">
	<analytic>
		<title level="a" type="main">Extracting rules from artificial neural networks with distributed representations</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="505" to="512"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Sebastian Thrun. Extracting rules from artificial neural networks with distributed representations. In Advances in neural information processing systems, pages 505-512, Denver, Colorado, USA, 1995. MIT Press.</note>
</biblStruct>

<biblStruct coords="65,128.60,505.23,360.32,7.12;65,128.60,514.69,328.56,7.12" xml:id="b241">
	<analytic>
		<title level="a" type="main">Symbolic rule extraction from the dimlp neural network</title>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Bologna</surname></persName>
		</author>
		<idno type="DOI">10.1007/10719871\17</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Hybrid Neural Systems</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="240" to="254"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Guido Bologna. Symbolic rule extraction from the dimlp neural network. In International Workshop on Hybrid Neural Systems, pages 240-254, Denver, Colorado, USA, 1998. Springer. doi: 10.1007/10719871\ 17.</note>
</biblStruct>

<biblStruct coords="65,128.60,524.16,360.32,7.12;65,128.60,533.62,360.32,7.12;65,128.60,543.09,155.66,7.12" xml:id="b242">
	<analytic>
		<title level="a" type="main">A rule extraction study based on a convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Bologna</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-99740-7\22</idno>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<meeting><address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="304" to="313"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Guido Bologna. A rule extraction study based on a convolutional neural network. In International Cross- Domain Conference for Machine Learning and Knowledge Extraction, pages 304-313, Hamburg, Germany, 2018. Springer. doi: 10.1007/978-3-319-99740-7\ 22.</note>
</biblStruct>

<biblStruct coords="65,128.60,552.55,360.32,7.12;65,128.60,562.02,360.25,7.12;65,128.60,571.48,66.35,7.12" xml:id="b243">
	<analytic>
		<title level="a" type="main">Reverse engineering the neural networks for rule extraction in classification problems</title>
		<author>
			<persName coords=""><forename type="first">Augasta</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thangairulappan</forename><surname>Kathirvalavakumar</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11063-011-9207-8</idno>
	</analytic>
	<monogr>
		<title level="j">Neural processing letters</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="150"/>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M Gethsiyal Augasta and Thangairulappan Kathirvalavakumar. Reverse engineering the neural networks for rule extraction in classification problems. Neural processing letters, 35(2):131-150, 2012. doi: 10.1007/ s11063-011-9207-8.</note>
</biblStruct>

<biblStruct coords="65,128.60,580.95,360.32,7.12;65,128.60,590.41,360.32,7.12;65,128.60,599.87,181.38,7.12" xml:id="b244">
	<analytic>
		<title level="a" type="main">Rule extraction from training data using neural network</title>
		<author>
			<persName coords=""><forename type="first">Saroj</forename><surname>Kumar Biswas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manomita</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Biswajit</forename><surname>Purkayastha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pinki</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dalton</forename><surname>Meitei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thounaojam</forename></persName>
		</author>
		<idno type="DOI">10.1142/S0218213017500063</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal on Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page">1750006</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Saroj Kumar Biswas, Manomita Chakraborty, Biswajit Purkayastha, Pinki Roy, and Dalton Meitei Thounaojam. Rule extraction from training data using neural network. International Journal on Artificial Intelligence Tools, 26 (03):1750006, 2017. doi: 10.1142/S0218213017500063.</note>
</biblStruct>

<biblStruct coords="65,128.60,609.34,360.32,7.12;65,128.60,618.80,360.32,7.12;65,128.60,628.27,29.05,7.12" xml:id="b245">
	<analytic>
		<title level="a" type="main">Symbolic knowledge extraction from trained neural networks: A sound approach</title>
		<author>
			<persName coords=""><surname>Arthur D'avila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krysia</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dov</forename><forename type="middle">M</forename><surname>Broda</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gabbay</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0004-3702(00)00077-1</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="155" to="207"/>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Arthur d'Avila Garcez, Krysia Broda, and Dov M Gabbay. Symbolic knowledge extraction from trained neural networks: A sound approach. Artificial Intelligence, 125(1-2):155-207, 2001. doi: 10.1016/s0004-3702(00) 00077-1.</note>
</biblStruct>

<biblStruct coords="65,128.60,637.73,360.32,7.12;65,128.60,647.25,360.32,6.97;65,128.60,656.66,300.35,7.12" xml:id="b246">
	<analytic>
		<title level="a" type="main">Distilling a neural network into a soft decision tree</title>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference of the italian Association of Artificial Intelligence</title>
		<meeting><address><addrLine>Bari, Italy; Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="8"/>
		</imprint>
		<respStmt>
			<orgName>University of Bremen</orgName>
		</respStmt>
	</monogr>
	<note>Workshop on Comprehensibility and Explanation in AI and ML</note>
	<note type="raw_reference">Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. In 16th Interna- tional Conference of the italian Association of Artificial Intelligence, 2017. Workshop on Comprehensibility and Explanation in AI and ML, pages 1-8, Bari, Italy, 2017. Cex, University of Bremen, Germany.</note>
</biblStruct>

<biblStruct coords="65,128.60,666.13,360.32,7.12;65,128.60,675.59,360.32,7.12;65,128.60,685.06,59.11,7.12" xml:id="b247">
	<analytic>
		<title level="a" type="main">Interpreting cnns via decision trees</title>
		<author>
			<persName coords=""><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6261" to="6270"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Quanshi Zhang, Yu Yang, Haotian Ma, and Ying Nian Wu. Interpreting cnns via decision trees. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6261-6270, Long Beach, California, USA, 2019. IEEE.</note>
</biblStruct>

<biblStruct coords="65,128.60,694.52,360.32,7.12;66,128.60,145.58,124.42,7.12" xml:id="b248">
	<analytic>
		<title level="a" type="main">Extracting symbolic rules from trained neural network ensembles</title>
		<author>
			<persName coords=""><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shi-Fu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ai Communications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15"/>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhi-Hua Zhou, Yuan Jiang, and Shi-Fu Chen. Extracting symbolic rules from trained neural network ensembles. Ai Communications, 16(1):3-15, 2003.</note>
</biblStruct>

<biblStruct coords="66,128.60,155.04,360.32,7.12;66,128.60,164.51,360.32,7.12" xml:id="b249">
	<analytic>
		<title level="a" type="main">Medical diagnosis with c4. 5 rule preceded by artificial neural network ensemble</title>
		<author>
			<persName coords=""><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/titb.2003.808498</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="42"/>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhi-Hua Zhou and Yuan Jiang. Medical diagnosis with c4. 5 rule preceded by artificial neural network ensemble. IEEE Transactions on information Technology in Biomedicine, 7(1):37-42, 2003. doi: 10.1109/titb.2003.808498.</note>
</biblStruct>

<biblStruct coords="66,128.60,173.97,360.32,7.12;66,128.60,183.44,360.32,7.12;66,128.60,192.90,137.45,7.12" xml:id="b250">
	<analytic>
		<title level="a" type="main">Extracting decision trees from trained neural networks</title>
		<author>
			<persName coords=""><forename type="first">Olcay</forename><surname>Boz</surname></persName>
		</author>
		<idno type="DOI">10.1145/775047.775113</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="456" to="461"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Olcay Boz. Extracting decision trees from trained neural networks. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 456-461, Edmonton, Alberta, Canada, 2002. ACM. doi: 10.1145/775047.775113.</note>
</biblStruct>

<biblStruct coords="66,128.60,202.37,360.32,7.12;66,128.60,211.83,360.32,7.12;66,128.60,221.29,117.64,7.12" xml:id="b251">
	<analytic>
		<title level="a" type="main">Using sampling and queries to extract rules from trained neural networks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shavlik</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-1-55860-335-6.50013-1</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Proceedings</title>
		<meeting><address><addrLine>New Brunswick, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="37" to="45"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mark W Craven and Jude W Shavlik. Using sampling and queries to extract rules from trained neural networks. In Machine Learning Proceedings, pages 37-45. Elsevier, New Brunswick, New Jersey, USA, 1994. doi: 10. 1016/b978-1-55860-335-6.50013-1.</note>
</biblStruct>

<biblStruct coords="66,128.60,230.76,360.32,7.12;66,128.60,240.22,306.47,7.12" xml:id="b252">
	<analytic>
		<title level="a" type="main">Extracting tree-structured representations of trained networks</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="24" to="30"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mark Craven and Jude W Shavlik. Extracting tree-structured representations of trained networks. In Advances in neural information processing systems, pages 24-30, Denver, Colorado, USA, 1996. MIT Press.</note>
</biblStruct>

<biblStruct coords="66,128.60,249.69,230.14,7.12;66,381.53,249.69,107.39,7.12;66,128.60,259.15,360.32,7.12;66,128.60,268.62,263.13,7.12" xml:id="b253">
	<analytic>
		<title level="a" type="main">Beyond sparsity: Tree regularization of deep models for interpretability</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sonali</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maurizio</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zazzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finale</forename><surname>Volker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1670" to="1678"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mike Wu, Michael C Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker and Finale Doshi-Velez. Beyond sparsity: Tree regularization of deep models for interpretability. In Thirty-Second AAAI Conference on Artificial Intelligence, pages 1670-1678, New Orleans, Louisiana, USA, 2018. AAAI Press.</note>
</biblStruct>

<biblStruct coords="66,128.60,278.08,360.32,7.12;66,128.60,287.55,360.32,7.12;66,128.60,297.01,20.15,7.12" xml:id="b254">
	<analytic>
		<title level="a" type="main">Automatic rule extraction from long short term memory networks</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W James Murdoch and Arthur Szlam. Automatic rule extraction from long short term memory networks. In 5th International Conference on Learning Representations, Conference Track Proceedings, Toulon, France, 2017. ICLR.</note>
</biblStruct>

<biblStruct coords="66,128.60,306.47,360.32,7.12;66,128.60,315.94,360.32,7.12;66,128.60,325.40,360.32,7.12;66,128.60,334.87,90.27,7.12" xml:id="b255">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName coords=""><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2410" to="2420"/>
		</imprint>
	</monogr>
	<note>Long Papers</note>
	<note type="raw_reference">Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with logic rules. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL, Volume 1: Long Papers, pages 2410-2420, Berlin, Germany, 2016. Association for Computational Linguistics. doi: 10.18653/v1/p16-1228.</note>
</biblStruct>

<biblStruct coords="66,128.60,344.33,360.32,7.12;66,128.60,353.80,312.56,7.12" xml:id="b256">
	<analytic>
		<title level="a" type="main">Unsupervised neural-symbolic integration</title>
		<author>
			<persName coords=""><surname>Son N Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-17 Workshop on Explainable AI (XAI)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="58" to="62"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Son N Tran. Unsupervised neural-symbolic integration. In IJCAI-17 Workshop on Explainable AI (XAI), pages 58-62, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="66,128.60,363.26,360.32,7.12;66,128.60,372.73,273.42,7.12" xml:id="b257">
	<analytic>
		<title level="a" type="main">Interpnet: Neural introspection for interpretable deep learning</title>
		<author>
			<persName coords=""><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Symposium on Interpretable Machine Learning</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="47" to="53"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Shane Barratt. Interpnet: Neural introspection for interpretable deep learning. In NIPS Symposium on Inter- pretable Machine Learning, pages 47-53, Long Beach, California, USA, 2017. NIPS.</note>
</biblStruct>

<biblStruct coords="66,128.60,382.19,360.32,7.12;66,128.60,391.66,360.32,7.12;66,128.60,401.12,29.89,7.12" xml:id="b258">
	<analytic>
		<title level="a" type="main">Human-centric ai for trustworthy iot systems with explainable multilayer perceptrons</title>
		<author>
			<persName coords=""><forename type="first">Iván</forename><surname>García-Magariño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajarajan</forename><surname>Muttukrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaime</forename><surname>Lloret</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2937521</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="125562" to="125574"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Iván García-Magariño, Rajarajan Muttukrishnan, and Jaime Lloret. Human-centric ai for trustworthy iot systems with explainable multilayer perceptrons. IEEE Access, 7:125562-125574, 2019. doi: 10.1109/ACCESS.2019. 2937521.</note>
</biblStruct>

<biblStruct coords="66,128.60,410.58,360.32,7.12;66,128.60,420.05,360.32,7.12;66,128.60,429.51,137.71,7.12" xml:id="b259">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="107" to="117"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107-117, Austin, Texas, USA, 2016. Association for Computational Linguistics.</note>
</biblStruct>

<biblStruct coords="66,128.60,438.98,360.32,7.12;66,128.60,448.44,360.32,7.12;66,128.60,457.91,171.60,7.12" xml:id="b260">
	<analytic>
		<title level="a" type="main">Generating visual explanations</title>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeynep</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcus</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernt</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0\1</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="19"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell. Generat- ing visual explanations. In European Conference on Computer Vision, pages 3-19, Amsterdam, The Netherlands, 2016. Springer. doi: 10.1007/978-3-319-46493-0\ 1.</note>
</biblStruct>

<biblStruct coords="66,128.60,467.37,360.32,7.12;66,128.60,476.84,360.32,7.12;66,128.60,486.30,360.32,7.12;66,128.60,495.76,36.08,7.12" xml:id="b261">
	<analytic>
		<title level="a" type="main">Singular vector canonical correlation analysis for deep learning dynamics and interpretability</title>
		<author>
			<persName coords=""><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Svcca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6076" to="6085"/>
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems Foundation</note>
	<note type="raw_reference">Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical cor- relation analysis for deep learning dynamics and interpretability. In Advances in Neural Information Processing Systems, pages 6076-6085, Long Beach, California, USA, 2017. Neural Information Processing Systems Foun- dation, Inc.</note>
</biblStruct>

<biblStruct coords="66,128.60,505.23,360.32,7.12;66,128.60,514.69,195.65,7.12" xml:id="b262">
	<analytic>
		<title level="a" type="main">A methodology to explain neural network classification</title>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Féraud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabrice</forename><surname>Clérot</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0893-6080(01)00127-7</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="246"/>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Raphael Féraud and Fabrice Clérot. A methodology to explain neural network classification. Neural Networks, 15(2):237-246, 2002. doi: 10.1016/s0893-6080(01)00127-7.</note>
</biblStruct>

<biblStruct coords="66,128.60,524.16,360.33,7.12;66,128.60,533.62,357.65,7.12" xml:id="b263">
	<analytic>
		<title level="a" type="main">Explaining results of neural networks by contextual importance and utility</title>
		<author>
			<persName coords=""><forename type="first">Kary</forename><surname>Främling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Rule Extraction From Trained Artificial Neural Networks Workshop</title>
		<meeting>Rule Extraction From Trained Artificial Neural Networks Workshop<address><addrLine>Brighton, England</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="43" to="56"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Kary Främling. Explaining results of neural networks by contextual importance and utility. In Proceedings of Rule Extraction From Trained Artificial Neural Networks Workshop, pages 43-56, Brighton, England, 1996. Citeseer.</note>
</biblStruct>

<biblStruct coords="66,128.60,543.09,360.32,7.12;66,128.60,552.55,360.32,7.12;66,128.60,562.02,216.48,7.12" xml:id="b264">
	<analytic>
		<title level="a" type="main">Towards explainable text classification by jointly learning lexicon and modifier terms</title>
		<author>
			<persName coords=""><forename type="first">Jérémie</forename><surname>Clos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nirmalie</forename><surname>Wiratunga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stewart</forename><surname>Massie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-17 Workshop on Explainable AI (XAI)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="19" to="23"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jérémie Clos, Nirmalie Wiratunga, and Stewart Massie. Towards explainable text classification by jointly learning lexicon and modifier terms. In IJCAI-17 Workshop on Explainable AI (XAI), pages 19-23, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="66,128.60,571.48,360.32,7.12;66,128.60,580.95,360.32,7.12;66,128.60,590.41,75.15,7.12" xml:id="b265">
	<analytic>
		<title level="a" type="main">Textual explanations for self-driving vehicles</title>
		<author>
			<persName coords=""><forename type="first">Jinkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="563" to="578"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for self-driving vehicles. In Proceedings of the European conference on computer vision (ECCV), pages 563-578, Munich, Germany, 2018. ECCV.</note>
</biblStruct>

<biblStruct coords="66,128.60,599.87,360.32,7.12;66,128.60,609.34,360.32,7.12;66,128.60,618.80,335.26,7.12" xml:id="b266">
	<analytic>
		<title level="a" type="main">Multimodal explanations: Justifying decisions and pointing to the evidence</title>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Huk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8779" to="8788"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Mul- timodal explanations: Justifying decisions and pointing to the evidence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8779-8788, Salt Lake City, Utah, USA, 2018. IEEE.</note>
</biblStruct>

<biblStruct coords="66,128.60,628.27,360.32,7.12;66,128.60,637.73,360.32,7.12;66,128.60,647.20,155.66,7.12" xml:id="b267">
	<analytic>
		<title level="a" type="main">Regular inference on artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">Franz</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><surname>Yovine</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-99740-7\25</idno>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<meeting><address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="350" to="369"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Franz Mayr and Sergio Yovine. Regular inference on artificial neural networks. In International Cross- Domain Conference for Machine Learning and Knowledge Extraction, pages 350-369, Hamburg, Germany, 2018. Springer. doi: 10.1007/978-3-319-99740-7\ 25.</note>
</biblStruct>

<biblStruct coords="66,128.60,656.66,360.32,7.12;66,128.60,666.13,213.36,7.12" xml:id="b268">
	<analytic>
		<title level="a" type="main">Extraction of rules from discrete-time recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Omlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giles</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(95)00086-0</idno>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="52"/>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Christian W Omlin and C Lee Giles. Extraction of rules from discrete-time recurrent neural networks. Neural networks, 9(1):41-52, 1996. doi: 10.1016/0893-6080(95)00086-0.</note>
</biblStruct>

<biblStruct coords="66,128.60,675.59,360.32,7.12;66,128.60,685.06,360.32,7.12;66,128.60,694.52,123.30,7.12" xml:id="b269">
	<analytic>
		<title level="a" type="main">Transforming convolutional neural network to an interpretable classifier</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Tamajka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wanda</forename><surname>Benesova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matej</forename><surname>Kompanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Systems, Signals and Image Processing (IWSSIP)</title>
		<meeting><address><addrLine>Osijek, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="255" to="259"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Martin Tamajka, Wanda Benesova, and Matej Kompanek. Transforming convolutional neural network to an interpretable classifier. In International Conference on Systems, Signals and Image Processing (IWSSIP), pages 255-259, Osijek, Croatia, 2019. IEEE.</note>
</biblStruct>

<biblStruct coords="67,128.60,145.58,360.32,7.12;67,128.60,155.04,360.32,7.12;67,131.47,164.51,21.92,7.12" xml:id="b270">
	<analytic>
		<title level="a" type="main">Improving the interpretability of classification rules discovered by an ant colony algorithm: Extended results</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Otero</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1162/evco\a\00155</idno>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="409"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fernando EB Otero and Alex A Freitas. Improving the interpretability of classification rules discovered by an ant colony algorithm: Extended results. Evolutionary computation, 24(3):385-409, 2016. doi: 10.1162/evco\ a\ 00155.</note>
</biblStruct>

<biblStruct coords="67,128.60,173.97,360.32,7.12;67,128.60,183.44,360.32,7.12;67,128.60,192.90,127.72,7.12" xml:id="b271">
	<analytic>
		<title level="a" type="main">Building comprehensible customer churn prediction models with advanced rule induction techniques</title>
		<author>
			<persName coords=""><forename type="first">Wouter</forename><surname>Verbeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Baesens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2010.08.023</idno>
	</analytic>
	<monogr>
		<title level="j">Expert systems with applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2354" to="2364"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wouter Verbeke, David Martens, Christophe Mues, and Bart Baesens. Building comprehensible customer churn prediction models with advanced rule induction techniques. Expert systems with applications, 38(3):2354-2364, 2011. doi: 10.1016/j.eswa.2010.08.023.</note>
</biblStruct>

<biblStruct coords="67,128.60,202.37,360.32,7.12;67,128.60,211.83,360.32,7.12;67,128.60,221.29,360.32,7.12;67,128.60,230.76,90.76,7.12" xml:id="b272">
	<analytic>
		<title level="a" type="main">Transforming rules and trees into comprehensible knowledge structures</title>
		<author>
			<persName coords=""><forename type="first">Brian</forename><forename type="middle">R</forename><surname>Gaines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Usama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gregory</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Padhraic</forename><surname>Piatetsky-Shapiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ramasamy</forename><surname>Smyth</surname></persName>
		</editor>
		<editor>
			<persName><surname>Uthurusamy</surname></persName>
		</editor>
		<meeting><address><addrLine>Menlo Park, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>American Association for Artificial Intelligence</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="205" to="226"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Brian R. Gaines. Transforming rules and trees into comprehensible knowledge structures. In Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors, Advances in Knowledge Dis- covery and Data Mining, pages 205-226. American Association for Artificial Intelligence, Menlo Park, CA, USA, 1996. ISBN 0-262-56097-6.</note>
</biblStruct>

<biblStruct coords="67,128.60,240.22,360.32,7.12;67,128.60,249.69,360.32,7.12;67,128.60,259.15,360.32,7.12;67,128.60,268.62,87.93,7.12" xml:id="b273">
	<analytic>
		<title level="a" type="main">Interpretable decision sets: A joint framework for description and prediction</title>
		<author>
			<persName coords=""><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939874</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1675" to="1684"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. Interpretable decision sets: A joint frame- work for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1675-1684, San Francisco, California, USA, 2016. ACM. doi: 10.1145/2939672.2939874.</note>
</biblStruct>

<biblStruct coords="67,128.60,278.08,360.32,7.12;67,128.60,287.55,360.32,7.12;67,128.60,297.01,69.29,7.12" xml:id="b274">
	<monogr>
		<title level="m" type="main">Building interpretable classifiers with rules using bayesian analysis</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tyler</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Madigan</surname></persName>
		</author>
		<idno>tr609</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1350" to="1371"/>
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Department of Statistics Technical Report</note>
	<note type="raw_reference">Benjamin Letham, Cynthia Rudin, Tyler H McCormick, and David Madigan. Building interpretable classifiers with rules using bayesian analysis. Department of Statistics Technical Report tr609, University of Washington, 9 (3):1350-1371, 2012.</note>
</biblStruct>

<biblStruct coords="67,128.60,306.47,360.32,7.12;67,128.60,315.94,360.32,7.12;67,128.60,325.40,360.32,7.12;67,128.60,334.87,40.73,7.12" xml:id="b275">
	<analytic>
		<title level="a" type="main">An interpretable stroke prediction model using rules and bayesian analysis</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tyler</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th AAAI Conference on Late-Breaking Developments in the Field of Artificial Intelligence, AAAIWS'13-17</title>
		<meeting>the 17th AAAI Conference on Late-Breaking Developments in the Field of Artificial Intelligence, AAAIWS'13-17<address><addrLine>Palo Alto, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="65" to="67"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. An interpretable stroke prediction model using rules and bayesian analysis. In Proceedings of the 17th AAAI Conference on Late-Breaking Devel- opments in the Field of Artificial Intelligence, AAAIWS'13-17, pages 65-67, Palo Alto, California, USA, 2013. AAAI Press.</note>
</biblStruct>

<biblStruct coords="67,128.60,344.33,360.32,7.12;67,128.60,353.80,360.32,7.12;67,128.60,363.26,142.90,7.12" xml:id="b276">
	<analytic>
		<title level="a" type="main">Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model</title>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Madigan</surname></persName>
		</author>
		<idno type="DOI">10.1214/15-aoas848</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1350" to="1371"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Benjamin Letham, Cynthia Rudin, Tyler H McCormick, David Madigan, et al. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3): 1350-1371, 2015. doi: 10.1214/15-aoas848.</note>
</biblStruct>

<biblStruct coords="67,128.60,372.73,360.32,7.12;67,128.60,382.19,360.32,7.12;67,128.60,391.66,237.16,7.12" xml:id="b277">
	<analytic>
		<title level="a" type="main">Bayesian rule sets for interpretable classification</title>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finale</forename><surname>Velez-Doshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yimin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erica</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Macneille</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdm.2016.0171</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on Data Mining (ICDM)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1269" to="1274"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Tong Wang, Cynthia Rudin, Finale Velez-Doshi, Yimin Liu, Erica Klampfl, and Perry MacNeille. Bayesian rule sets for interpretable classification. In IEEE 16th International Conference on Data Mining (ICDM), pages 1269-1274, Barcelona, Spain, 2016. IEEE. doi: 10.1109/icdm.2016.0171.</note>
</biblStruct>

<biblStruct coords="67,128.60,401.12,360.32,7.12;67,128.60,410.58,360.32,7.12;67,128.60,420.05,69.29,7.12" xml:id="b278">
	<analytic>
		<title level="a" type="main">A bayesian framework for learning rule sets for interpretable classification</title>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yimin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erica</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Macneille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2357" to="2393"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. A bayesian framework for learning rule sets for interpretable classification. The Journal of Machine Learning Research, 18 (1):2357-2393, 2017.</note>
</biblStruct>

<biblStruct coords="67,128.60,429.51,360.32,7.12;67,128.60,438.98,260.84,7.12" xml:id="b279">
	<analytic>
		<title level="a" type="main">Comprehensible knowledge discovery: gaining insight from data</title>
		<author>
			<persName coords=""><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Federal Data Mining Conference and Exposition</title>
		<meeting><address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="73" to="82"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M Pazzani. Comprehensible knowledge discovery: gaining insight from data. In First Federal Data Mining Conference and Exposition, pages 73-82, London, United Kingdom, 1997. ACM.</note>
</biblStruct>

<biblStruct coords="67,128.60,448.44,360.32,7.12;67,128.60,457.91,360.32,7.12;67,128.60,467.43,360.32,6.97;67,128.60,476.84,360.32,7.12;67,128.60,486.30,18.82,7.12" xml:id="b280">
	<analytic>
		<title level="a" type="main">Building more explainable artificial intelligence with argumentation</title>
		<author>
			<persName coords=""><forename type="first">Zhiwei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cyril</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Jih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8044" to="8046"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhiwei Zeng, Chunyan Miao, Cyril Leung, and Jing Jih Chin. Building more explainable artificial intelligence with argumentation. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI- 18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th Symposium on Educational Advances in Artificial Intelligence (EAAI-18), pages 8044-8046, New Orleans, Louisiana, USA, 2018. AAAI Press.</note>
</biblStruct>

<biblStruct coords="67,128.60,495.76,360.32,7.12;67,128.60,505.23,360.32,7.12;67,128.60,514.69,122.05,7.12" xml:id="b281">
	<analytic>
		<title level="a" type="main">Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning</title>
		<author>
			<persName coords=""><forename type="first">Hisao</forename><surname>Ishibuchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yusuke</forename><surname>Nojima</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijar.2006.01.004</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="31"/>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hisao Ishibuchi and Yusuke Nojima. Analysis of interpretability-accuracy tradeoff of fuzzy systems by multi- objective fuzzy genetics-based machine learning. International Journal of Approximate Reasoning, 44(1):4-31, 2007. doi: 10.1016/j.ijar.2006.01.004.</note>
</biblStruct>

<biblStruct coords="67,128.60,524.16,360.32,7.12;67,128.60,533.62,289.95,7.12" xml:id="b282">
	<analytic>
		<title level="a" type="main">Fuzzy modeling of high-dimensional systems: complexity reduction and interpretability improvement</title>
		<author>
			<persName coords=""><forename type="first">Jin</forename><surname>Yaochu</surname></persName>
		</author>
		<idno type="DOI">10.1109/91.842154</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="212" to="221"/>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yaochu Jin. Fuzzy modeling of high-dimensional systems: complexity reduction and interpretability improve- ment. IEEE Transactions on Fuzzy Systems, 8(2):212-221, 2000. doi: 10.1109/91.842154.</note>
</biblStruct>

<biblStruct coords="67,128.60,543.09,360.32,7.12;67,128.60,552.55,360.32,7.12;67,128.60,562.02,225.69,7.12" xml:id="b283">
	<analytic>
		<title level="a" type="main">Learning fuzzy relations and properties for explainable artificial intelligence</title>
		<author>
			<persName coords=""><forename type="first">Régis</forename><surname>Pierrard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Philippe</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Céline</forename><surname>Hudelot</surname></persName>
		</author>
		<idno type="DOI">10.1109/FUZZ-IEEE.2018.8491538</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Régis Pierrard, Jean-Philippe Poli, and Céline Hudelot. Learning fuzzy relations and properties for explainable artificial intelligence. In IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), pages 1-8, Rio de Janeiro, Brazil, 2018. IEEE. doi: 10.1109/FUZZ-IEEE.2018.8491538.</note>
</biblStruct>

<biblStruct coords="67,128.60,571.48,360.32,7.12;67,128.60,580.95,280.91,7.12" xml:id="b284">
	<analytic>
		<title level="a" type="main">Building interpretable fuzzy models for high dimensional data analysis in cancer diagnosis</title>
		<author>
			<persName coords=""><forename type="first">Zhenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Palade</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2164-12-s2-s5</idno>
	</analytic>
	<monogr>
		<title level="m">S5:1-S</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhenyu Wang and Vasile Palade. Building interpretable fuzzy models for high dimensional data analysis in cancer diagnosis. BMC genomics, 12(2):S5:1-S5:11, 2011. doi: 10.1186/1471-2164-12-s2-s5.</note>
</biblStruct>

<biblStruct coords="67,128.60,590.41,360.32,7.12;67,128.60,599.87,210.82,7.12" xml:id="b285">
	<analytic>
		<title level="a" type="main">An interpretable classification rule mining algorithm</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amelia</forename><surname>Zafra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastián</forename><surname>Ventura</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2013.03.038</idno>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page" from="1" to="20"/>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alberto Cano, Amelia Zafra, and SebastiáN Ventura. An interpretable classification rule mining algorithm. Infor- mation Sciences, 240:1-20, 2013. doi: 10.1016/j.ins.2013.03.038.</note>
</biblStruct>

<biblStruct coords="67,128.60,609.34,360.32,7.12;67,128.60,618.80,360.32,7.12" xml:id="b286">
	<analytic>
		<title level="a" type="main">Learning interpretable classification rules with boolean compressed sensing</title>
		<author>
			<persName coords=""><surname>Dmitry M Malioutov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amin</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjeeb</forename><surname>Emad</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transparent Data Mining for Big and Small Data</title>
		<imprint>
			<biblScope unit="page" from="95" to="121"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Dmitry M Malioutov, Kush R Varshney, Amin Emad, and Sanjeeb Dash. Learning interpretable classification rules with boolean compressed sensing. In Transparent Data Mining for Big and Small Data, pages 95-121.</note>
</biblStruct>

<biblStruct coords="67,128.60,628.27,236.80,7.12" xml:id="b287">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-54024-5\5</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Cham, Switzerland</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Springer, Cham, Switzerland, 2017. doi: 10.1007/978-3-319-54024-5\ 5.</note>
</biblStruct>

<biblStruct coords="67,128.60,637.73,360.32,7.12;67,128.60,647.20,360.32,7.12;67,128.60,656.66,193.68,7.12" xml:id="b288">
	<analytic>
		<title level="a" type="main">Interpretable two-level boolean rule learning for classification</title>
		<author>
			<persName coords=""><forename type="first">Guolong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dennis</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kush</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><forename type="middle">M</forename><surname>Malioutov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop Human Interpretability in Machine Learning</title>
		<meeting>ICML Workshop Human Interpretability in Machine Learning<address><addrLine>New York City, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="66" to="70"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Guolong Su, Dennis Wei, Kush R Varshney, and Dmitry M Malioutov. Interpretable two-level boolean rule learning for classification. In Proceedings of ICML Workshop Human Interpretability in Machine Learning, pages 66-70, New York City, New York, USA, 2016. ICML.</note>
</biblStruct>

<biblStruct coords="67,128.60,666.13,360.32,7.12;67,128.60,675.59,360.32,7.12;67,128.60,685.06,223.13,7.12" xml:id="b289">
	<analytic>
		<title level="a" type="main">Evolving rule-based explainable artificial intelligence for unmanned aerial vehicles</title>
		<author>
			<persName coords=""><forename type="first">Devinder</forename><surname>Blen M Keneni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Al Bataineh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vijaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><forename type="middle">Y</forename><surname>Devabhaktuni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><forename type="middle">D</forename><surname>Javaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Zaientz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Marinier</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2893141</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="17001" to="17016"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Blen M Keneni, Devinder Kaur, Ali Al Bataineh, Vijaya K Devabhaktuni, Ahmad Y Javaid, Jack D Zaientz, and Robert P Marinier. Evolving rule-based explainable artificial intelligence for unmanned aerial vehicles. IEEE Access, 7:17001-17016, 2019. doi: 10.1109/ACCESS.2019.2893141.</note>
</biblStruct>

<biblStruct coords="67,128.60,694.52,360.32,7.12;68,128.60,145.58,360.32,7.12;68,128.60,155.04,218.12,7.12" xml:id="b290">
	<analytic>
		<title level="a" type="main">Fingrams: visual repre-sentations of fuzzy rule-based inference for expert analysis of comprehensibility</title>
		<author>
			<persName coords=""><forename type="first">Jose</forename><forename type="middle">M</forename><surname>David P Pancho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oscar</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnaud</forename><surname>Cordón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Quirin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Magdalena</surname></persName>
		</author>
		<idno type="DOI">10.1109/tfuzz.2013.2245130</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1133" to="1149"/>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David P Pancho, Jose M Alonso, Oscar Cordón, Arnaud Quirin, and Luis Magdalena. Fingrams: visual repre- sentations of fuzzy rule-based inference for expert analysis of comprehensibility. IEEE Transactions on Fuzzy Systems, 21(6):1133-1149, 2013. doi: 10.1109/tfuzz.2013.2245130.</note>
</biblStruct>

<biblStruct coords="68,128.60,164.51,360.32,7.12;68,128.60,173.97,360.32,7.12;68,128.60,183.44,173.69,7.12" xml:id="b291">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence for kids</title>
		<author>
			<persName coords=""><forename type="first">Alonso</forename><surname>Jose</surname></persName>
		</author>
		<idno type="DOI">10.2991/eusflat-19.2019.21</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Fuzzy Systems Association and the European Society for Fuzzy Logic and Technology (EUSFLAT)</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Atlantis Press</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jose M Alonso. Explainable artificial intelligence for kids. In Conference of the International Fuzzy Systems Association and the European Society for Fuzzy Logic and Technology (EUSFLAT), Prague, Czech Republic, 2019. Atlantis Press. doi: 10.2991/eusflat-19.2019.21.</note>
</biblStruct>

<biblStruct coords="68,128.60,192.90,360.32,7.12;68,128.60,202.37,360.32,7.12;68,128.60,211.83,360.32,7.12;68,128.60,221.29,87.93,7.12" xml:id="b292">
	<analytic>
		<title level="a" type="main">Interpretable predictions of tree-based ensembles via actionable feature tweaking</title>
		<author>
			<persName coords=""><forename type="first">Gabriele</forename><surname>Tolomei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3098039</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, Nova Scotia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="465" to="474"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Gabriele Tolomei, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. Interpretable predictions of tree-based ensembles via actionable feature tweaking. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 465-474, Halifax, Nova Scotia, Canada, 2017. ACM. doi: 10.1145/3097983.3098039.</note>
</biblStruct>

<biblStruct coords="68,128.60,230.76,360.32,7.12;68,128.60,240.22,360.32,7.12;68,128.60,249.69,73.28,7.12" xml:id="b293">
	<analytic>
		<title level="a" type="main">Explainable ai beer style classifier</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ciro</forename><surname>Ramos-Soto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corrado</forename><surname>Castiello</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mencar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SICSA Workshop on Reasoning, Learning and Explainability</title>
		<meeting>the SICSA Workshop on Reasoning, Learning and Explainability<address><addrLine>Aberdeen, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5"/>
		</imprint>
	</monogr>
	<note type="raw_reference">José M Alonso, Alejandro Ramos-Soto, Ciro Castiello, and Corrado Mencar. Explainable ai beer style classifier. In Proceedings of the SICSA Workshop on Reasoning, Learning and Explainability, pages 1-5, Aberdeen, Scotland, UK, 2018. CEUR-WS.</note>
</biblStruct>

<biblStruct coords="68,128.60,259.15,360.32,7.12;68,128.60,268.62,360.32,7.12;68,128.60,278.08,108.68,7.12" xml:id="b294">
	<analytic>
		<title level="a" type="main">From ensemble methods to comprehensible models</title>
		<author>
			<persName coords=""><forename type="first">César</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>José Ramírez-Quintana</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-36182-0\16</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Discovery Science</title>
		<meeting><address><addrLine>Lübeck, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="165" to="177"/>
		</imprint>
	</monogr>
	<note type="raw_reference">César Ferri, José Hernández-Orallo, and M José Ramírez-Quintana. From ensemble methods to comprehensible models. In International Conference on Discovery Science, pages 165-177, Lübeck, Germany, 2002. Springer. doi: 10.1007/3-540-36182-0\ 16.</note>
</biblStruct>

<biblStruct coords="68,128.60,287.55,360.32,7.12;68,128.60,297.01,360.32,7.12;68,128.60,306.47,155.66,7.12" xml:id="b295">
	<analytic>
		<title level="a" type="main">Seeing the forest through the trees: Learning a comprehensible model from an ensemble</title>
		<author>
			<persName coords=""><forename type="first">Anneleen</forename><surname>Van Assche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hendrik</forename><surname>Blockeel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-74958-5\39</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<meeting><address><addrLine>Warsaw, Poland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="418" to="429"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Anneleen Van Assche and Hendrik Blockeel. Seeing the forest through the trees: Learning a comprehensible model from an ensemble. In European Conference on Machine Learning, pages 418-429, Warsaw, Poland, 2007. Springer. doi: 10.1007/978-3-540-74958-5\ 39.</note>
</biblStruct>

<biblStruct coords="68,128.60,315.94,360.32,7.12;68,128.60,325.40,360.32,7.12;68,128.60,334.87,187.09,7.12" xml:id="b296">
	<analytic>
		<title level="a" type="main">Interpretable models from distributed data via merging of decision trees</title>
		<author>
			<persName coords=""><forename type="first">Artur</forename><surname>Andrzejak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Silvestre</forename><surname>Zabala</surname></persName>
		</author>
		<idno type="DOI">10.1109/cidm.2013.6597210</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Computational Intelligence and Data Mining (CIDM)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Artur Andrzejak, Felix Langner, and Silvestre Zabala. Interpretable models from distributed data via merging of decision trees. In IEEE Symposium on Computational Intelligence and Data Mining (CIDM), pages 1-9, Singapore, 2013. IEEE. doi: 10.1109/cidm.2013.6597210.</note>
</biblStruct>

<biblStruct coords="68,128.60,344.33,360.32,7.12;68,128.60,353.80,360.32,7.12;68,128.60,363.26,117.34,7.12" xml:id="b297">
	<analytic>
		<title level="a" type="main">Making tree ensembles interpretable: A bayesian model selection approach</title>
		<author>
			<persName coords=""><forename type="first">Satoshi</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kohei</forename><surname>Hayashi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics, AISTATS</title>
		<meeting><address><addrLine>Playa Blanca, Lanzarote, Canary Islands, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="77" to="85"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Satoshi Hara and Kohei Hayashi. Making tree ensembles interpretable: A bayesian model selection approach. In International Conference on Artificial Intelligence and Statistics, AISTATS, pages 77-85, Playa Blanca, Lanzarote, Canary Islands, Spain, 2018. PMLR.</note>
</biblStruct>

<biblStruct coords="68,128.60,372.73,360.32,7.12;68,128.60,382.19,161.62,7.12" xml:id="b298">
	<analytic>
		<title level="a" type="main">Interpreting tree ensembles with intrees</title>
		<author>
			<persName coords=""><forename type="first">Houtao</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41060-018-0144-8</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Houtao Deng. Interpreting tree ensembles with intrees. International Journal of Data Science and Analytics, 7 (4):1-11, 2018. doi: 10.1007/s41060-018-0144-8.</note>
</biblStruct>

<biblStruct coords="68,128.60,391.66,360.32,7.12;68,128.60,401.12,360.25,7.12;68,128.60,410.58,48.07,7.12" xml:id="b299">
	<analytic>
		<title level="a" type="main">An interpretable classification framework for information extraction from online healthcare forums</title>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Lawley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1155/2017/2460174</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of healthcare engineering</title>
		<imprint>
			<biblScope unit="page" from="798" to="809"/>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jun Gao, Ninghao Liu, Mark Lawley, and Xia Hu. An interpretable classification framework for information extraction from online healthcare forums. Journal of healthcare engineering, 2017:798-809, 2017. doi: 10.1155/ 2017/2460174.</note>
</biblStruct>

<biblStruct coords="68,128.60,420.05,360.32,7.12;68,128.60,429.51,256.19,7.12" xml:id="b300">
	<analytic>
		<title level="a" type="main">Tree space prototypes: Another look at making tree ensembles interpretable</title>
		<author>
			<persName coords=""><forename type="first">Giles</forename><surname>Hui Fen Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">T</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Interpretability Workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hui Fen Tan, Giles Hooker, and Martin T Wells. Tree space prototypes: Another look at making tree ensembles interpretable. In NIPS Interpretability Workshop, Barcelona, Spain, 2016. NIPS.</note>
</biblStruct>

<biblStruct coords="68,128.60,438.98,360.32,7.12;68,128.60,448.44,332.19,7.12" xml:id="b301">
	<analytic>
		<title level="a" type="main">Rule extraction from support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Haydemar</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cecilio</forename><surname>Angulo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreu</forename><surname>Català</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks, ESANN</title>
		<meeting><address><addrLine>Bruges, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002. i6doc</date>
			<biblScope unit="page" from="107" to="112"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Haydemar Núñez, Cecilio Angulo, and Andreu Català. Rule extraction from support vector machines. In Euro- pean Symposium on Artificial Neural Networks, ESANN, pages 107-112, Bruges, Belgium, 2002. i6doc.</note>
</biblStruct>

<biblStruct coords="68,128.60,457.91,360.32,7.12;68,128.60,467.37,360.32,7.12;68,128.60,476.84,130.90,7.12" xml:id="b302">
	<analytic>
		<title level="a" type="main">Towards simple, easy-to-understand, yet accurate classifiers</title>
		<author>
			<persName coords=""><forename type="first">Doina</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dianne</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdm.2003.1250961</idno>
	</analytic>
	<monogr>
		<title level="m">Third IEEE International Conference on Data Mining</title>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="497" to="500"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Doina Caragea, Dianne Cook, and Vasant Honavar. Towards simple, easy-to-understand, yet accurate classifiers. In Third IEEE International Conference on Data Mining, pages 497-500, San Francisco, California, USA, 2003. IEEE. doi: 10.1109/icdm.2003.1250961.</note>
</biblStruct>

<biblStruct coords="68,128.60,486.30,360.32,7.12;68,128.60,495.76,360.32,7.12;68,128.60,505.23,147.85,7.12" xml:id="b303">
	<analytic>
		<title level="a" type="main">Visualization of support vector machines with unsupervised learning</title>
		<author>
			<persName coords=""><forename type="first">Lutz</forename><surname>Hamel</surname></persName>
		</author>
		<idno type="DOI">10.1109/cibcb.2006.330984</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Computational Intelligence and Bioinformatics and Computational Biology</title>
		<meeting><address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="8"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lutz Hamel. Visualization of support vector machines with unsupervised learning. In IEEE Symposium on Computational Intelligence and Bioinformatics and Computational Biology, pages 1-8, Toronto, Ontario, Canada, 2006. IEEE. doi: 10.1109/cibcb.2006.330984.</note>
</biblStruct>

<biblStruct coords="68,128.60,514.69,360.32,7.12;68,128.60,524.16,360.32,7.12;68,128.60,533.62,324.97,7.12" xml:id="b304">
	<analytic>
		<title level="a" type="main">Nomograms for visualizing support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Aleks</forename><surname>Jakulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Možina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janez</forename><surname>Demšar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Blaž</forename><surname>Zupan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1081870.1081886</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="108" to="117"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Aleks Jakulin, Martin Možina, Janez Demšar, Ivan Bratko, and Blaž Zupan. Nomograms for visualizing support vector machines. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 108-117, Chicago, Illinois, USA, 2005. ACM. doi: 10.1145/1081870.1081886.</note>
</biblStruct>

<biblStruct coords="68,128.60,543.09,360.32,7.12;68,128.60,552.55,360.32,7.12;68,128.60,562.02,360.32,7.12;68,128.60,571.48,25.90,7.12" xml:id="b305">
	<analytic>
		<title level="a" type="main">Nonlinear support vector machine visualization for risk factor analysis using nomograms and localized radial basis function kernels</title>
		<author>
			<persName coords=""><forename type="first">Hwanjo</forename><surname>Baek Hwan Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jongshill</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Young</forename><forename type="middle">Joon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">In</forename><forename type="middle">Young</forename><surname>Chee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sun</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/titb.2007.902300</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="256"/>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Baek Hwan Cho, Hwanjo Yu, Jongshill Lee, Young Joon Chee, In Young Kim, and Sun I Kim. Nonlinear support vector machine visualization for risk factor analysis using nomograms and localized radial basis function kernels. IEEE Transactions on Information Technology in Biomedicine, 12(2):247-256, 2008. doi: 10.1109/titb.2007. 902300.</note>
</biblStruct>

<biblStruct coords="68,128.60,580.95,360.32,7.12;68,128.60,590.41,360.32,7.12;68,128.60,599.87,193.66,7.12" xml:id="b306">
	<analytic>
		<title level="a" type="main">Nomograms for visualization of naive bayesian classifier</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Možina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janez</forename><surname>Demšar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kattan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Blaž</forename><surname>Zupan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-30116-5\32</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Principles of Data Mining and Knowledge Discovery</title>
		<meeting><address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="337" to="348"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Martin Možina, Janez Demšar, Michael Kattan, and Blaž Zupan. Nomograms for visualization of naive bayesian classifier. In European Conference on Principles of Data Mining and Knowledge Discovery, pages 337-348, Pisa, Italy, 2004. Springer. doi: 10.1007/978-3-540-30116-5\ 32.</note>
</biblStruct>

<biblStruct coords="68,128.60,609.34,360.32,7.12;68,128.60,618.80,223.07,7.12" xml:id="b307">
	<analytic>
		<title level="a" type="main">Explaining inferences in bayesian networks</title>
		<author>
			<persName coords=""><forename type="first">Ghim-Eng</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hwee-Hwa</forename><surname>Pang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-007-0093-8</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="278"/>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ghim-Eng Yap, Ah-Hwee Tan, and Hwee-Hwa Pang. Explaining inferences in bayesian networks. Applied Intelligence, 29(3):263-278, 2008. doi: 10.1007/s10489-007-0093-8.</note>
</biblStruct>

<biblStruct coords="68,128.60,628.27,360.32,7.12;68,128.60,637.73,360.25,7.12;68,128.60,647.20,66.35,7.12" xml:id="b308">
	<analytic>
		<title level="a" type="main">A method for explaining bayesian networks for legal evidence with scenarios</title>
		<author>
			<persName coords=""><forename type="first">Charlotte</forename><forename type="middle">S</forename><surname>Vlek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henry</forename><surname>Prakken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Silja</forename><surname>Renooij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Verheij</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10506-016-9183-4</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="324"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Charlotte S Vlek, Henry Prakken, Silja Renooij, and Bart Verheij. A method for explaining bayesian net- works for legal evidence with scenarios. Artificial Intelligence and Law, 24(3):285-324, 2016. doi: 10.1007/ s10506-016-9183-4.</note>
</biblStruct>

<biblStruct coords="68,128.60,656.66,360.32,7.12;68,128.60,666.13,360.32,7.12;68,128.60,675.59,153.93,7.12" xml:id="b309">
	<analytic>
		<title level="a" type="main">A two-phase method for extracting explanatory arguments from bayesian networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sjoerd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John-Jules Ch</forename><surname>Timmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henry</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Silja</forename><surname>Prakken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Renooij</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Verheij</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijar.2016.09.002</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="475" to="494"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sjoerd T Timmer, John-Jules Ch Meyer, Henry Prakken, Silja Renooij, and Bart Verheij. A two-phase method for extracting explanatory arguments from bayesian networks. International Journal of Approximate Reasoning, 80: 475-494, 2017. doi: 10.1016/j.ijar.2016.09.002.</note>
</biblStruct>

<biblStruct coords="68,128.60,685.06,360.32,7.12;68,128.60,694.52,360.32,7.12;69,128.60,145.58,360.32,7.12;69,128.60,155.04,141.09,7.12" xml:id="b310">
	<analytic>
		<title level="a" type="main">Interpreting individual classifications of hierarchical networks</title>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Landecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Thomure</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Melanie</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Garrett</forename><forename type="middle">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">P</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brumby</surname></persName>
		</author>
		<idno type="DOI">10.1109/CIDM.2013.6597214</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Computational Intelligence and Data Mining (CIDM)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-04">apr 2013</date>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="32" to="38"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Will Landecker, Michael D. Thomure, Luis M. A. Bettencourt, Melanie Mitchell, Garrett T Kenyon, and Steven P. Brumby. Interpreting individual classifications of hierarchical networks. In IEEE Symposium on Computational Intelligence and Data Mining (CIDM), volume 165, pages 32-38, Singapore, apr 2013. IEEE. ISBN 978-1-4673- 5895-8. doi: 10.1109/CIDM.2013.6597214.</note>
</biblStruct>

<biblStruct coords="69,128.60,164.51,360.32,7.12;69,128.60,173.97,360.32,7.12;69,128.60,183.44,290.53,7.12" xml:id="b311">
	<analytic>
		<title level="a" type="main">The bayesian case model: A generative approach for case-based reasoning and prototype classification</title>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montréal, Québec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1952" to="1960"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Been Kim, Cynthia Rudin, and Julie A Shah. The bayesian case model: A generative approach for case-based reasoning and prototype classification. In Advances in Neural Information Processing Systems, pages 1952-1960, Montréal, Québec, Canada, 2014. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="69,128.60,192.90,360.32,7.12;69,128.60,202.37,360.32,7.12;69,128.60,211.83,352.05,7.12" xml:id="b312">
	<analytic>
		<title level="a" type="main">Gaussian process regression for predictive but interpretable machine learning models: An example of predicting mental workload across tasks</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Matthew S Caywood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">B</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hal</forename><forename type="middle">S</forename><surname>Colombe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Monica</forename><forename type="middle">Z</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weiland</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnhum.2016.00647</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in human neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="647" to="665"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew S Caywood, Daniel M Roberts, Jeffrey B Colombe, Hal S Greenwald, and Monica Z Weiland. Gaussian process regression for predictive but interpretable machine learning models: An example of predicting mental workload across tasks. Frontiers in human neuroscience, 10:647-665, 2017. doi: 10.3389/fnhum.2016.00647.</note>
</biblStruct>

<biblStruct coords="69,128.60,221.29,360.32,7.12;69,128.60,230.76,360.32,7.12;69,128.60,240.22,360.32,7.12;69,128.60,249.69,125.50,7.12" xml:id="b313">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2788613</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1721" to="1730"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1721-1730, Sydney, Australia, 2015. ACM. doi: 10.1145/2783258.2788613.</note>
</biblStruct>

<biblStruct coords="69,128.60,259.15,360.32,7.12;69,128.60,268.62,360.32,7.12;69,128.60,278.08,324.77,7.12" xml:id="b314">
	<analytic>
		<title level="a" type="main">Trading interpretability for accuracy: Oblique treed sparse additive models</title>
		<author>
			<persName coords=""><forename type="first">Jialei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryohei</forename><surname>Fujimaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yosuke</forename><surname>Motohashi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783407</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1245" to="1254"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jialei Wang, Ryohei Fujimaki, and Yosuke Motohashi. Trading interpretability for accuracy: Oblique treed sparse additive models. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1245-1254, Sydney, Australia, 2015. ACM. doi: 10.1145/2783258.2783407.</note>
</biblStruct>

<biblStruct coords="69,128.60,287.55,360.32,7.12;69,128.60,297.01,360.32,7.12;69,128.60,306.47,49.58,7.12" xml:id="b315">
	<analytic>
		<title level="a" type="main">Developing transparent credit risk scorecards more effectively: An explainable artificial intelligence approach</title>
		<author>
			<persName coords=""><forename type="first">Gerald</forename><surname>Fahner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Seventh International Conference on Data Analytics</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="17" to="24"/>
		</imprint>
	</monogr>
	<note>Data Analytics</note>
	<note type="raw_reference">Gerald Fahner. Developing transparent credit risk scorecards more effectively: An explainable artificial intelli- gence approach. In The Seventh International Conference on Data Analytics, pages 17-24, Athens, Greece, 2018. Data Analytics.</note>
</biblStruct>

<biblStruct coords="69,128.60,315.94,360.32,7.12;69,128.60,325.40,360.32,7.12;69,128.60,334.87,239.58,7.12" xml:id="b316">
	<analytic>
		<title level="a" type="main">Explainable ai: The promise of genetic programming multi-run subtree encapsulation</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="DOI">10.1109/iCMLDE.2018.00037</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Data Engineering (iCMLDE)</title>
		<meeting><address><addrLine>Dallas, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="158" to="159"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel Howard and Mark A Edwards. Explainable ai: The promise of genetic programming multi-run subtree encapsulation. In International Conference on Machine Learning and Data Engineering (iCMLDE), pages 158- 159, Dallas, Texas, USA, 2018. IEEE. doi: 10.1109/iCMLDE.2018.00037.</note>
</biblStruct>

<biblStruct coords="69,128.60,344.33,360.32,7.12;69,128.60,353.80,360.32,7.12;69,128.60,363.26,216.48,7.12" xml:id="b317">
	<analytic>
		<title level="a" type="main">Towards compact interpretable models: Shrinking of learned probabilistic sentential decision diagrams</title>
		<author>
			<persName coords=""><forename type="first">Yitao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guy</forename><surname>Van Den Broeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-17 Workshop on Explainable AI (XAI)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="31" to="35"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Yitao Liang and Guy Van den Broeck. Towards compact interpretable models: Shrinking of learned probabilistic sentential decision diagrams. In IJCAI-17 Workshop on Explainable AI (XAI), pages 31-35, Melbourne, Australia, 2017. International Joint Conferences on Artificial Intelligence, Inc.</note>
</biblStruct>

<biblStruct coords="69,128.60,372.73,360.32,7.12;69,128.60,382.19,360.32,7.12;69,128.60,391.66,257.32,7.12" xml:id="b318">
	<analytic>
		<title level="a" type="main">Mind the gap: A generative approach to interpretable feature selection and extraction</title>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montréal, Québec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2260" to="2268"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Been Kim, Julie A Shah, and Finale Doshi-Velez. Mind the gap: A generative approach to interpretable feature selection and extraction. In Advances in Neural Information Processing Systems, pages 2260-2268, Montréal, Québec, Canada, 2015. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="69,128.60,401.12,360.32,7.12;69,128.60,410.58,76.38,7.12" xml:id="b319">
	<analytic>
		<title level="a" type="main">Supersparse linear integer models for interpretable classification</title>
		<author>
			<persName coords=""><forename type="first">Berk</forename><surname>Ustun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Traca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page" from="11" to="47"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Berk Ustun, Stefano Traca, and Cynthia Rudin. Supersparse linear integer models for interpretable classification. Stat, 1050:11-47, 2014.</note>
</biblStruct>

<biblStruct coords="69,128.60,420.05,360.32,7.12;69,128.60,429.51,360.32,7.12;69,128.60,439.03,360.32,6.97;69,128.60,448.44,288.96,7.12" xml:id="b320">
	<analytic>
		<title level="a" type="main">Unsupervised does not mean uninterpretable: The case for word sense induction and disambiguation</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugen</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Paolo Ponzetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="98"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander Panchenko, Eugen Ruppert, Stefano Faralli, Simone Paolo Ponzetto, and Chris Biemann. Unsupervised does not mean uninterpretable: The case for word sense induction and disambiguation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, volume 1, pages 86-98, Valencia, Spain, 2017. Association for Computational Linguistics.</note>
</biblStruct>

<biblStruct coords="69,128.60,457.91,360.32,7.12;69,128.60,467.37,360.32,7.12;69,128.60,476.84,41.84,7.12" xml:id="b321">
	<analytic>
		<title level="a" type="main">Interpretability of linguistic fuzzy rule-based systems: An overview of interpretability measures</title>
		<author>
			<persName coords=""><forename type="first">Maria</forename><forename type="middle">Jose</forename><surname>Gacto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rafael</forename><surname>Alcalá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2011.02.021</idno>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="4340" to="4360"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Maria Jose Gacto, Rafael Alcalá, and Francisco Herrera. Interpretability of linguistic fuzzy rule-based systems: An overview of interpretability measures. Information Sciences, 181(20):4340-4360, 2011. doi: 10.1016/j.ins. 2011.02.021.</note>
</biblStruct>

<biblStruct coords="69,128.60,486.30,360.32,7.12;69,128.60,495.76,360.32,7.12;69,128.60,505.23,49.37,7.12" xml:id="b322">
	<analytic>
		<title level="a" type="main">A study of statistical techniques and performance measures for genetics-based machine learning: accuracy and interpretability</title>
		<author>
			<persName coords=""><forename type="first">Salvador</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julián</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">959</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Salvador García, Alberto Fernández, Julián Luengo, and Francisco Herrera. A study of statistical techniques and performance measures for genetics-based machine learning: accuracy and interpretability. Soft Computing, 13 (10):959, 2009.</note>
</biblStruct>

<biblStruct coords="69,128.60,512.95,360.32,8.86;69,128.60,523.56,360.32,7.97;69,128.60,533.62,193.94,7.12" xml:id="b323">
	<analytic>
		<title level="a" type="main">Decision-making framework with doubleloop learning through interpretable black-box machine learning models</title>
		<author>
			<persName coords=""><forename type="first">Marko</forename><surname>Bohanec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marko</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirjana</forename><surname>Kljajić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Borštnar</forename></persName>
		</author>
		<idno type="DOI">10.1108/imds-09-2016-0409</idno>
	</analytic>
	<monogr>
		<title level="j">Industrial Management &amp; Data Systems</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1389" to="1406"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marko Bohanec, Marko Robnik-Šikonja, and Mirjana Kljajić Borštnar. Decision-making framework with double- loop learning through interpretable black-box machine learning models. Industrial Management &amp; Data Systems, 117(7):1389-1406, 2017. doi: 10.1108/imds-09-2016-0409.</note>
</biblStruct>

<biblStruct coords="69,128.60,541.34,360.32,8.86;69,128.60,552.55,338.03,7.12" xml:id="b324">
	<analytic>
		<title level="a" type="main">Explaining machine learning models in sales predictions</title>
		<author>
			<persName coords=""><forename type="first">Marko</forename><surname>Bohanec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirjana</forename><surname>Kljajić Borštnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marko</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.11.010</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="416" to="428"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marko Bohanec, Mirjana Kljajić Borštnar, and Marko Robnik-Šikonja. Explaining machine learning models in sales predictions. Expert Systems with Applications, 71:416-428, 2017. doi: 10.1016/j.eswa.2016.11.010.</note>
</biblStruct>

<biblStruct coords="69,128.60,562.02,360.32,7.12;69,128.60,571.48,360.32,7.12;69,128.60,580.95,360.32,7.12;69,128.60,590.41,280.83,7.12" xml:id="b325">
	<analytic>
		<title level="a" type="main">The dangers of post-hoc interpretability: Unjustified counterfactual explanations</title>
		<author>
			<persName coords=""><forename type="first">Thibault</forename><surname>Laugel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marie-Jeanne</forename><surname>Lesot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/388</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, (IJCAI)</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, (IJCAI)<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2801" to="2807"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. The dangers of post-hoc interpretability: Unjustified counterfactual explanations. In Proceedings of the Twenty-Eighth Inter- national Joint Conference on Artificial Intelligence, (IJCAI), pages 2801-2807, Macao, China, 2019. International Joint Conferences on Artificial Intelligence Organization. doi: 10.24963/ijcai.2019/388.</note>
</biblStruct>

<biblStruct coords="69,128.60,599.87,360.32,7.12;69,128.60,609.34,360.32,7.12;69,128.60,618.80,105.59,7.12" xml:id="b326">
	<analytic>
		<title level="a" type="main">Local explanation methods for deep neural networks lack sensitivity to parameter values</title>
		<author>
			<persName coords=""><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
	<note type="raw_reference">Julius Adebayo, Justin Gilmer, Ian Goodfellow, and Been Kim. Local explanation methods for deep neural networks lack sensitivity to parameter values. In 6th International Conference on Learning Representations, Vancouver, Canada, 2018. ICLR.</note>
</biblStruct>

<biblStruct coords="69,128.60,628.27,360.32,7.12;69,128.60,637.73,360.32,7.12;69,128.60,647.20,360.32,7.12;69,128.60,656.66,117.44,7.12" xml:id="b327">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName coords=""><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="9505" to="9515"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS, pages 9505-9515, Montréal, Canada, 2018. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="69,128.60,666.13,360.32,7.12;69,128.60,675.59,360.32,7.12;69,128.60,685.06,105.59,7.12" xml:id="b328">
	<analytic>
		<title level="a" type="main">Towards better understanding of gradient-based attribution methods for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enea</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
	<note type="raw_reference">Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understanding of gradient-based attribution methods for deep neural networks. In 6th International Conference on Learning Representations, Vancouver, Canada, 2018. ICLR.</note>
</biblStruct>

<biblStruct coords="69,128.60,694.52,360.32,7.12;70,128.60,145.58,360.32,7.12;70,128.60,155.04,345.54,7.12" xml:id="b329">
	<analytic>
		<title level="a" type="main">Explaining predic-tions of non-linear classifiers in nlp</title>
		<author>
			<persName coords=""><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Franziska</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w16-1601</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="7"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. Explaining predic- tions of non-linear classifiers in nlp. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 1-7, Berlin, Germany, 2016. Association for Computational Linguistics. doi: 10.18653/v1/w16-1601.</note>
</biblStruct>

<biblStruct coords="70,128.60,164.51,360.32,7.12;70,128.60,173.97,360.32,7.12;70,128.60,183.44,90.13,7.12" xml:id="b330">
	<analytic>
		<title level="a" type="main">what is relevant in a text document?": An interpretable machine learning approach</title>
		<author>
			<persName coords=""><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Franziska</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0181142</idno>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">181142</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. "what is relevant in a text document?": An interpretable machine learning approach. PloS one, 12(8):e0181142, 2017. doi: 10. 1371/journal.pone.0181142.</note>
</biblStruct>

<biblStruct coords="70,128.60,192.90,360.33,7.12;70,128.60,202.37,360.32,7.12;70,128.60,211.83,249.79,7.12" xml:id="b331">
	<analytic>
		<title level="a" type="main">Evaluating the visualization of what a deep neural network has learned</title>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2016.2599820</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2660" to="2673"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Müller. Eval- uating the visualization of what a deep neural network has learned. IEEE transactions on neural networks and learning systems, 28(11):2660-2673, 2017. doi: 10.1109/tnnls.2016.2599820.</note>
</biblStruct>

<biblStruct coords="70,128.60,221.29,360.32,7.12;70,128.60,230.76,307.23,7.12" xml:id="b332">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models</title>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITU Journal: ICT Discoveries</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wojciech Samek, Thomas Wiegand, and Klaus-Robert Müller. Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. ITU Journal: ICT Discoveries, 1:1-10, 2017.</note>
</biblStruct>

<biblStruct coords="70,128.60,240.22,360.32,7.12;70,128.60,249.69,360.32,7.12;70,128.60,259.15,234.86,7.12" xml:id="b333">
	<analytic>
		<title level="a" type="main">Analyzing and validating neural networks predictions</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML Workshop on Visualization for Deep Learning</title>
		<meeting>the ICML Workshop on Visualization for Deep Learning<address><addrLine>New York City, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="118" to="121"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander Binder, Wojciech Samek, Grégoire Montavon, Sebastian Bach, and Klaus-Robert Müller. Analyzing and validating neural networks predictions. In Proceedings of the ICML Workshop on Visualization for Deep Learning, pages 118-121, New York City, New York, USA, 2016. ICML.</note>
</biblStruct>

<biblStruct coords="70,128.60,268.62,360.32,7.12;70,128.60,278.08,214.60,7.12" xml:id="b334">
	<analytic>
		<title level="a" type="main">Interpretation of neural networks is fragile</title>
		<author>
			<persName coords=""><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Machine Deception</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In NIPS workshop on Machine Deception, Long Beach, California, USA, 2017. NIPS.</note>
</biblStruct>

<biblStruct coords="70,128.60,287.55,360.32,7.12;70,128.60,297.01,360.25,7.12;70,128.60,306.47,80.47,7.12" xml:id="b335">
	<analytic>
		<title level="a" type="main">Review and comparison of methods to study the contribution of variables in artificial neural network models</title>
		<author>
			<persName coords=""><forename type="first">Muriel</forename><surname>Gevrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Dimopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sovan</forename><surname>Lek</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0304-3800(02)00257-0</idno>
	</analytic>
	<monogr>
		<title level="j">Ecological modelling</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="264"/>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Muriel Gevrey, Ioannis Dimopoulos, and Sovan Lek. Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological modelling, 160(3):249-264, 2003. doi: 10.1016/ s0304-3800(02)00257-0.</note>
</biblStruct>

<biblStruct coords="70,128.60,315.94,360.32,7.12;70,128.60,325.40,254.45,7.12" xml:id="b336">
	<analytic>
		<title level="a" type="main">An evaluation of explanations of probabilistic inference</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Henri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Suermondt</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cooper</surname></persName>
		</author>
		<idno type="DOI">10.1006/cbmr.1993.1017</idno>
	</analytic>
	<monogr>
		<title level="j">Computers and Biomedical Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="242" to="254"/>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Henri J Suermondt and Gregory F Cooper. An evaluation of explanations of probabilistic inference. Computers and Biomedical Research, 26(3):242-254, 1993. doi: 10.1006/cbmr.1993.1017.</note>
</biblStruct>

<biblStruct coords="70,128.60,334.87,360.32,7.12;70,128.60,344.33,216.12,7.12" xml:id="b337">
	<analytic>
		<title level="a" type="main">The impact of explanation facilities on user acceptance of expert systems advice</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.2307/249686</idno>
	</analytic>
	<monogr>
		<title level="j">Mis Quarterly</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="172"/>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L Richard Ye and Paul E Johnson. The impact of explanation facilities on user acceptance of expert systems advice. Mis Quarterly, 19(2):157-172, 1995. doi: 10.2307/249686.</note>
</biblStruct>

<biblStruct coords="70,128.60,353.80,360.32,7.12;70,128.60,363.26,360.25,7.12;70,128.60,372.73,80.47,7.12" xml:id="b338">
	<analytic>
		<title level="a" type="main">An effective metacognitive strategy: Learning by doing and explaining with a computer-based cognitive tutor</title>
		<author>
			<persName coords=""><forename type="first">Awmm</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenneth</forename><forename type="middle">R</forename><surname>Aleven</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Koedinger</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0364-0213(02</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="67"/>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vincent AWMM Aleven and Kenneth R Koedinger. An effective metacognitive strategy: Learning by doing and explaining with a computer-based cognitive tutor. Cognitive science, 26(2):147-179, 2002. doi: 10.1016/ s0364-0213(02)00061-7.</note>
</biblStruct>

<biblStruct coords="70,128.60,382.19,360.32,7.12;70,128.60,391.66,321.21,7.12" xml:id="b339">
	<analytic>
		<title level="a" type="main">Guidelines for developing explainable cognitive models</title>
		<author>
			<persName coords=""><forename type="first">Maaike</forename><surname>Harbers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joost</forename><surname>Broekens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karel</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John-Jules</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCM</title>
		<meeting>ICCM<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="85" to="90"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Maaike Harbers, Joost Broekens, Karel Van Den Bosch, and John-Jules Meyer. Guidelines for developing ex- plainable cognitive models. In Proceedings of ICCM, pages 85-90, Berlin, Germany, 2010. Citeseer.</note>
</biblStruct>

<biblStruct coords="70,128.60,401.12,360.32,7.12;70,128.60,410.58,360.32,7.12;70,128.60,420.05,246.98,7.12" xml:id="b340">
	<analytic>
		<title level="a" type="main">Design and evaluation of explainable bdi agents</title>
		<author>
			<persName coords=""><forename type="first">Maaike</forename><surname>Harbers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karel</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John-Jules</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Meyer</surname></persName>
		</author>
		<idno type="DOI">10.1109/wi-iat.2010.115</idno>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="132"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Maaike Harbers, Karel van den Bosch, and John-Jules Meyer. Design and evaluation of explainable bdi agents. In 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology, volume 2, pages 125-132, Toronto, Canada, 2010. IEEE. doi: 10.1109/wi-iat.2010.115.</note>
</biblStruct>

<biblStruct coords="70,128.60,429.51,360.32,7.12;70,128.60,438.98,360.32,7.12;70,128.60,448.44,119.32,7.12" xml:id="b341">
	<analytic>
		<title level="a" type="main">Manipulating and measuring model interpretability</title>
		<author>
			<persName coords=""><forename type="first">Forough</forename><surname>Poursabzi-Sangdeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jake</forename><forename type="middle">M</forename><surname>Daniel G Goldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanna</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Women in Machine Learning Workshop</title>
		<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Vaughan, and Hanna Wal- lach. Manipulating and measuring model interpretability. In NIPS Women in Machine Learning Workshop, Long Beach, California, USA, 2017. NIPS.</note>
</biblStruct>

<biblStruct coords="70,128.60,457.91,360.32,7.12;70,128.60,467.37,360.32,7.12;70,128.60,476.84,306.58,7.12" xml:id="b342">
	<analytic>
		<title level="a" type="main">How it works: a field study of non-technical users interacting with an intelligent system</title>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Tullio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Anind</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Chalecki</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fogarty</surname></persName>
		</author>
		<idno type="DOI">10.1145/1240624.1240630</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="31" to="40"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Joe Tullio, Anind K Dey, Jason Chalecki, and James Fogarty. How it works: a field study of non-technical users interacting with an intelligent system. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 31-40, San Jose, California, USA, 2007. ACM. doi: 10.1145/1240624.1240630.</note>
</biblStruct>

<biblStruct coords="70,128.60,486.30,360.32,7.12;70,128.60,495.76,360.32,7.12;70,128.60,505.23,360.32,7.12;70,128.60,514.69,117.44,7.12" xml:id="b343">
	<analytic>
		<title level="a" type="main">Human-in-the-loop interpretability prior</title>
		<author>
			<persName coords=""><forename type="first">Isaac</forename><surname>Lage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Been</forename><surname>Samuel J Gershman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Finale</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems Foundation, Inc</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="10180" to="10189"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Isaac Lage, Andrew Ross, Samuel J Gershman, Been Kim, and Finale Doshi-Velez. Human-in-the-loop inter- pretability prior. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In- formation Processing Systems 2018, NeurIPS, pages 10180-10189, Montréal, Canada, 2018. Neural Information Processing Systems Foundation, Inc.</note>
</biblStruct>

<biblStruct coords="70,128.60,524.16,360.32,7.12;70,128.60,533.62,360.32,7.12;70,128.60,543.09,105.68,7.12" xml:id="b344">
	<analytic>
		<title level="a" type="main">User-oriented assessment of classification model understandability</title>
		<author>
			<persName coords=""><forename type="first">Hiva</forename><surname>Allahyari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niklas</forename><surname>Lavesson</surname></persName>
		</author>
		<idno type="DOI">10.3233/978-1-60750-754-3-11</idno>
	</analytic>
	<monogr>
		<title level="m">11th scandinavian conference on Artificial intelligence</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="11" to="19"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Hiva Allahyari and Niklas Lavesson. User-oriented assessment of classification model understandability. In 11th scandinavian conference on Artificial intelligence, pages 11-19, Trondheim, Norway, 2011. IOS Press. doi: 10.3233/978-1-60750-754-3-11.</note>
</biblStruct>

<biblStruct coords="70,128.60,552.55,360.32,7.12;70,128.60,562.02,360.32,7.12;70,128.60,571.48,165.44,7.12" xml:id="b345">
	<analytic>
		<title level="a" type="main">An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Huysmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karel</forename><surname>Dejaeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Baesens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dss.2010.12.003</idno>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="154"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Johan Huysmans, Karel Dejaeger, Christophe Mues, Jan Vanthienen, and Bart Baesens. An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models. Decision Support Systems, 51 (1):141-154, 2011. doi: 10.1016/j.dss.2010.12.003.</note>
</biblStruct>

<biblStruct coords="70,128.60,580.95,360.32,7.12;70,128.60,590.41,360.32,7.12;70,128.60,598.13,152.87,8.86" xml:id="b346">
	<analytic>
		<title level="a" type="main">Comprehensibility of classification trees-survey design validation</title>
		<author>
			<persName coords=""><forename type="first">Mitja</forename><surname>Luštrek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matjaž</forename><surname>Gams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Martinčić-Ipšić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Information Technologies and Information Society-ITIS2014</title>
		<meeting><address><addrLine>Šmarješke toplice, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>ITIS</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="46" to="61"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mitja Luštrek, Matjaž Gams, Sanda Martinčić-Ipšić, et al. Comprehensibility of classification trees-survey design validation. In 6th International Conference on Information Technologies and Information Society-ITIS2014, pages 46-61, Šmarješke toplice, Slovenia, 2014. ITIS.</note>
</biblStruct>

<biblStruct coords="70,128.60,609.34,360.32,7.12;70,128.60,618.80,360.32,7.12;70,128.60,628.27,255.03,7.12" xml:id="b347">
	<analytic>
		<title level="a" type="main">Why-oriented end-user debugging of naive bayes text classification</title>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Oberst</surname></persName>
		</author>
		<idno type="DOI">10.1145/2030365.2030367</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems (TiiS)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="2"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Todd Kulesza, Simone Stumpf, Weng-Keen Wong, Margaret M Burnett, Stephen Perona, Andrew Ko, and Ian Oberst. Why-oriented end-user debugging of naive bayes text classification. ACM Transactions on Interactive Intelligent Systems (TiiS), 1(1):2:1-2:31, 2011. doi: 10.1145/2030365.2030367.</note>
</biblStruct>

<biblStruct coords="70,128.60,637.73,360.32,7.12;70,128.60,647.20,360.32,7.12;70,128.60,656.66,125.50,7.12" xml:id="b348">
	<analytic>
		<title level="a" type="main">Assessing demand for intelligibility in context-aware applications</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dey</surname></persName>
		</author>
		<idno type="DOI">10.1145/1620545.1620576</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on Ubiquitous computing</title>
		<meeting>the 11th international conference on Ubiquitous computing<address><addrLine>Orlando, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="195" to="204"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Brian Y Lim and Anind K Dey. Assessing demand for intelligibility in context-aware applications. In Proceedings of the 11th international conference on Ubiquitous computing, pages 195-204, Orlando, Florida, USA, 2009. ACM. doi: 10.1145/1620545.1620576.</note>
</biblStruct>

<biblStruct coords="70,128.60,666.13,360.32,7.12;70,128.60,675.59,360.32,7.12;70,128.60,685.06,360.32,7.12;70,128.60,694.52,26.42,7.12" xml:id="b349">
	<analytic>
		<title level="a" type="main">Exploring the need for explainable artificial intelligence (xai) in intelligent tutoring systems (its)</title>
		<author>
			<persName coords=""><forename type="first">Vanessa</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Conati</surname></persName>
		</author>
		<ptr target="CEUR-WS.org"/>
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2327</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Vanessa Putnam and Cristina Conati. Exploring the need for explainable artificial intelligence (xai) in intelligent tutoring systems (its). In Joint Proceedings of the ACM IUI 2019 Workshops co-located with the 24th ACM Conference on Intelligent User Interfaces (ACM IUI, volume 2327, Los Angeles, California, USA, 2019. CEUR- WS.org.</note>
</biblStruct>

<biblStruct coords="71,128.60,145.58,360.32,7.12;71,128.60,155.04,360.32,7.12;71,128.60,164.51,208.59,7.12" xml:id="b350">
	<analytic>
		<title level="a" type="main">Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases</title>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01231-1\31</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="498" to="512"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Pierre Stock and Moustapha Cisse. Convnets and imagenet beyond accuracy: Understanding mistakes and uncov- ering biases. In Proceedings of the European Conference on Computer Vision (ECCV), pages 498-512, Munich, Germany, 2018. Springer. doi: 10.1007/978-3-030-01231-1\ 31.</note>
</biblStruct>

<biblStruct coords="71,128.60,173.97,360.32,7.12;71,128.60,183.44,360.32,7.12;71,128.60,192.90,341.18,7.12" xml:id="b351">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.354</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6541" to="6549"/>
		</imprint>
	</monogr>
	<note type="raw_reference">David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6541-6549, Honolulu, Hawaii, USA, 2017. IEEE. doi: 10.1109/cvpr.2017.354.</note>
</biblStruct>

<biblStruct coords="71,128.60,202.37,360.32,7.12;71,128.60,211.83,360.32,7.12;71,128.60,221.29,360.32,7.12;71,128.60,230.76,75.75,7.12" xml:id="b352">
	<analytic>
		<title level="a" type="main">Analyzing classifiers: Fisher vectors and deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.318</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2912" to="2920"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Sebastian Lapuschkin, Alexander Binder, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. An- alyzing classifiers: Fisher vectors and deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2912-2920, Las Vegas, Nevada, USA, 2016. IEEE. doi: 10.1109/cvpr.2016.318.</note>
</biblStruct>

<biblStruct coords="71,128.60,240.22,360.32,7.12;71,128.60,249.69,360.32,7.12;71,128.60,259.15,360.32,7.12;71,128.60,268.62,104.10,7.12" xml:id="b353">
	<analytic>
		<title level="a" type="main">Interpretable human action recognition in compressed domain</title>
		<author>
			<persName coords=""><forename type="first">Vignesh</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelius</forename><surname>Hellge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2017.7952445</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1692" to="1696"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Vignesh Srinivasan, Sebastian Lapuschkin, Cornelius Hellge, Klaus-Robert Müller, and Wojciech Samek. Inter- pretable human action recognition in compressed domain. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1692-1696, New Orleans, Louisiana, USA, 2017. IEEE. doi: 10.1109/ICASSP.2017.7952445.</note>
</biblStruct>

<biblStruct coords="71,128.60,278.08,360.32,7.12;71,128.60,287.55,360.32,7.12;71,128.60,297.01,105.86,7.12" xml:id="b354">
	<analytic>
		<title level="a" type="main">Interpretable deep neural networks for single-trial eeg classification</title>
		<author>
			<persName coords=""><forename type="first">Irene</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2016.10.008</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="141" to="145"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Irene Sturm, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert Müller. Interpretable deep neu- ral networks for single-trial eeg classification. Journal of neuroscience methods, 274:141-145, 2016. doi: 10.1016/j.jneumeth.2016.10.008.</note>
</biblStruct>

<biblStruct coords="71,128.60,306.47,360.32,7.12;71,128.60,315.94,360.32,7.12;71,128.60,325.40,360.32,7.12;71,128.60,334.87,90.27,7.12" xml:id="b355">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural machine translation</title>
		<author>
			<persName coords=""><forename type="first">Yanzhuo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-1106</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1150" to="1159"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. Visualizing and understanding neural machine trans- lation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1150-1159, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/p17-1106.</note>
</biblStruct>

<biblStruct coords="71,128.60,344.33,360.32,7.12;71,128.60,353.80,360.32,7.12;71,128.60,363.26,360.25,7.12;71,128.60,372.73,13.95,7.12" xml:id="b356">
	<analytic>
		<title level="a" type="main">Explainable deep neural networks for multivariate time series predictions</title>
		<author>
			<persName coords=""><forename type="first">Roy</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anika</forename><surname>Schumann</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/932</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6488" to="6490"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Roy Assaf and Anika Schumann. Explainable deep neural networks for multivariate time series predictions. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 6488-6490, Macao, China, 2019. International Joint Conferences on Artificial Intelligence Organization. doi: 10.24963/ijcai.2019/ 932.</note>
</biblStruct>

<biblStruct coords="71,128.60,382.19,360.32,7.12;71,128.60,391.66,160.92,7.12" xml:id="b357">
	<analytic>
		<title level="a" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">University of Montreal</title>
		<imprint>
			<biblScope unit="volume">1341</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of a deep network. University of Montreal, 1341(3):1, 2009.</note>
</biblStruct>

<biblStruct coords="71,128.60,401.12,360.32,7.12;71,128.60,410.58,301.18,7.12" xml:id="b358">
	<analytic>
		<title level="a" type="main">Artificial intelligence, autonomy, and humanmachine teams: Interdependence, context, and explainable ai</title>
		<author>
			<persName coords=""><forename type="first">Ranjeev</forename><surname>William F Lawless</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Mittu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Sofge</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hiatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5" to="13"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">William F Lawless, Ranjeev Mittu, Donald Sofge, and Laura Hiatt. Artificial intelligence, autonomy, and human- machine teams: Interdependence, context, and explainable ai. AI Magazine, 40(3):5-13, 2019.</note>
</biblStruct>

<biblStruct coords="71,128.60,420.05,360.32,7.12;71,128.60,429.51,304.94,7.12" xml:id="b359">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence for safe intraoperative decision support</title>
		<author>
			<persName coords=""><forename type="first">Lauren</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Teodor</forename><surname>Grantcharov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Rudzicz</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamasurg.2019.2821</idno>
	</analytic>
	<monogr>
		<title level="j">JAMA surgery</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1064" to="1065"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lauren Gordon, Teodor Grantcharov, and Frank Rudzicz. Explainable artificial intelligence for safe intraoperative decision support. JAMA surgery, 154(11):1064-1065, 2019. doi: 10.1001/jamasurg.2019.2821.</note>
</biblStruct>

<biblStruct coords="71,128.60,438.98,360.32,7.12;71,128.60,448.44,238.31,7.12" xml:id="b360">
	<analytic>
		<title level="a" type="main">Interactive machine learning for health informatics: when do we need the human-in-the-loop?</title>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40708-016-0042-6</idno>
	</analytic>
	<monogr>
		<title level="j">Brain Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="131"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Andreas Holzinger. Interactive machine learning for health informatics: when do we need the human-in-the-loop? Brain Informatics, 3(2):119-131, 2016. doi: 10.1007/s40708-016-0042-6.</note>
</biblStruct>

<biblStruct coords="71,128.60,457.91,360.32,7.12;71,128.60,467.37,360.25,7.12;71,128.60,476.84,63.34,7.12" xml:id="b361">
	<analytic>
		<title level="a" type="main">The importance of ensuring artificial intelligence and machine learning can be understood at the human level: Sudha ram</title>
		<author>
			<persName coords=""><surname>Ram</surname></persName>
		</author>
		<idno type="DOI">10.1093/eurpub/ckz185.259</idno>
	</analytic>
	<monogr>
		<title level="j">European Journal of Public Health</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">Supplement 4</biblScope>
			<biblScope unit="page" from="185" to="259"/>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S Ram. The importance of ensuring artificial intelligence and machine learning can be understood at the human level: Sudha ram. European Journal of Public Health, 29(Supplement 4):ckz185-259, 2019. doi: 10.1093/ eurpub/ckz185.259.</note>
</biblStruct>

<biblStruct coords="71,128.60,486.30,360.32,7.12;71,128.60,495.76,360.25,7.12;71,128.60,505.23,77.37,7.12" xml:id="b362">
	<analytic>
		<title level="a" type="main">Survey and critique of techniques for extracting rules from trained artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joachim</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><forename type="middle">B</forename><surname>Tickle</surname></persName>
		</author>
		<idno type="DOI">10.1016/0950-7051</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge-based systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="81920" to="81924"/>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Robert Andrews, Joachim Diederich, and Alan B Tickle. Survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-based systems, 8(6):373-389, 1995. doi: 10.1016/ 0950-7051(96)81920-4.</note>
</biblStruct>

<biblStruct coords="71,128.60,514.69,360.32,7.12;71,128.60,524.16,359.48,7.12" xml:id="b363">
	<analytic>
		<title level="a" type="main">Visual interpretation of kernel-based prediction models</title>
		<author>
			<persName coords=""><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timon</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1002/minf.201100059</idno>
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="817" to="826"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Katja Hansen, David Baehrens, Timon Schroeter, Matthias Rupp, and Klaus-Robert Müller. Visual interpretation of kernel-based prediction models. Molecular Informatics, 30(9):817-826, 2011. doi: 10.1002/minf.201100059.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
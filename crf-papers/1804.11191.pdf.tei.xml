<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mathematical Foundations of Computing</title>
				<funder ref="#_f3FKdmn #_gCCPGsS">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-31">31 May 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,275.92,221.40,57.11,7.61"><forename type="first">Zhuwei</forename><surname>Qin</surname></persName>
							<email>zqin@gmu.edu</email>
						</author>
						<author>
							<persName coords="1,192.49,261.65,48.40,7.61"><forename type="first">Fuxun</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName coords="1,253.39,261.65,69.32,7.61"><forename type="first">Chenchen</forename><surname>Liu</surname></persName>
							<email>chliu@clarkson.edu</email>
						</author>
						<author>
							<persName coords="1,352.75,261.65,58.52,7.61"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">†George Mason University 4400 University Dr, Fairfax, VA 22030, USA</note>
								<orgName type="institution">†George Mason University</orgName>
								<address>
									<addrLine>4400 University Dr</addrLine>
									<postCode>22030</postCode>
									<settlement>Fairfax</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">‡Clarkson University 8 Clarkson Ave, Potsdam, NY 13699, USA</note>
								<orgName type="institution">‡Clarkson University</orgName>
								<address>
									<addrLine>8 Clarkson Ave</addrLine>
									<postCode>13699</postCode>
									<settlement>Potsdam</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mathematical Foundations of Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-31">31 May 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">7786FACFEB1B4DF2EDD039B0AF6D382C</idno>
					<idno type="DOI">10.3934/mfc.2018008</idno>
					<idno type="arXiv">arXiv:1804.11191v2[cs.CV]</idno>
					<note type="submission">Received October 2017; revised December 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>2010 Mathematics Subject Classification. Primary: 58F15</term>
					<term>58F17; Secondary: 53C35 Deep learning</term>
					<term>convolutional neural network</term>
					<term>CNN feature</term>
					<term>CNN visualization</term>
					<term>network interpretability</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,234.72,304.40,143.90,8.74">(Communicated by Zhipeng Cai)</head><p><s coords="1,162.54,334.75,40.95,6.09">Abstract.</s><s coords="1,208.47,333.41,242.34,6.99;1,162.54,343.37,288.27,6.99;1,162.54,353.33,212.74,6.99">Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive performance on many computer vision related tasks, such as object detection, image recognition, image retrieval, etc.</s><s coords="1,378.73,353.33,72.08,6.99;1,162.54,363.29,288.27,6.99;1,162.54,373.26,229.52,6.99">These achievements benefit from the CNNs' outstanding capability to learn the input features with deep layers of neuron structures and iterative training process.</s><s coords="1,395.73,373.26,55.08,6.99;1,162.54,383.22,288.27,6.99;1,162.54,393.18,288.27,6.99">However, these learned features are hard to identify and interpret from a human vision perspective, causing a lack of understanding of the CNNs' internal working mechanism.</s><s coords="1,162.54,403.14,288.27,6.99;1,162.54,413.11,288.27,6.99;1,162.54,423.07,76.36,6.99">To improve the CNN interpretability, the CNN visualization is well utilized as a qualitative analysis method, which translates the internal features into visually perceptible patterns.</s><s coords="1,242.61,423.07,208.19,6.99;1,162.54,433.03,288.27,6.99;1,162.54,442.99,122.37,6.99">And many CNN visualization works have been proposed in the literature to interpret the CNN in perspectives of network structure, operation, and semantic concept.</s></p><p><s coords="1,174.49,452.96,276.31,6.99;1,162.54,462.92,288.27,6.99;1,162.54,472.88,288.27,6.99;1,162.54,482.85,134.26,6.99">In this paper, we expect to provide a comprehensive survey of several representative CNN visualization methods, including Activation Maximization, Network Inversion, Deconvolutional Neural Networks (DeconvNet), and Network Dissection based visualization.</s><s coords="1,300.52,482.85,150.28,6.99;1,162.54,492.81,179.61,6.99">These methods are presented in terms of motivations, algorithms, and experiment results.</s><s coords="1,346.00,492.81,104.81,6.99;1,162.54,502.77,288.26,6.99;1,162.54,512.73,288.27,6.99;1,162.54,522.70,98.04,6.99">Based on these visualization methods, we also discuss their practical applications to demonstrate the significance of the CNN interpretability in areas of network design, optimization, security enhancement, etc.</s></p><p><s coords="1,126.67,544.92,79.82,8.77">1. Introduction.</s><s coords="1,211.48,544.95,275.20,8.74;1,126.67,556.91,360.00,8.74;1,126.67,568.86,360.00,8.74;1,126.67,580.82,120.74,8.74">The Convolutional Neural Networks (CNNs) have been widely investigated as one of the most promising solutions for various computer vision related tasks, such as object detection <ref type="bibr" coords="1,298.94,568.86,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="1,318.22,568.86,11.62,8.74" target="#b57">58]</ref>, image recognition <ref type="bibr" coords="1,418.85,568.86,15.50,8.74" target="#b35">[36,</ref><ref type="bibr" coords="1,438.13,568.86,12.73,8.74" target="#b62">63,</ref><ref type="bibr" coords="1,454.66,568.86,12.73,8.74" target="#b65">66,</ref><ref type="bibr" coords="1,471.17,568.86,11.62,8.74" target="#b26">27]</ref>, image retrieval <ref type="bibr" coords="1,194.53,580.82,15.50,8.74" target="#b21">[22,</ref><ref type="bibr" coords="1,213.34,580.82,11.62,8.74" target="#b24">25]</ref>, etc.</s></p><p><s coords="1,138.63,592.77,348.04,8.74;1,126.67,604.73,360.00,8.74;1,126.67,616.68,24.44,8.74">Inspired by the hierarchical organization of the human visual cortex <ref type="bibr" coords="1,449.88,592.77,14.61,8.74" target="#b33">[34]</ref>, the CNN is constructed with many intricately interconnected layers of neuron structures.</s><s coords="1,157.79,616.68,328.87,8.74;1,126.67,628.64,65.65,8.74">These neurons act as the basic units to learn and extract certain features from the input.</s><s coords="1,196.58,628.64,290.10,8.74;2,126.67,102.11,3.97,6.12;2,180.17,102.11,253.00,6.12;2,126.67,123.98,311.75,8.74">With the network complexity increment caused by the neuron layer 2 ZHUWEI QIN, FUNXUN YU, CHENCHEN LIU AND XIANG CHEN depth, the performance of the input feature extraction is also enhanced.</s><s coords="2,442.77,123.98,43.90,8.74;2,126.67,135.93,360.00,8.74;2,126.67,147.89,102.22,8.74">For example, AlexNet -one of the most representative CNNs, has 650K neurons and 60M related parameters <ref type="bibr" coords="2,210.63,147.89,14.61,8.74" target="#b35">[36]</ref>.</s><s coords="2,233.18,147.89,253.49,8.74;2,126.67,159.84,360.00,8.74;2,126.67,171.80,336.88,8.74">Also, sophisticated algorithms are proposed to support the training and testing of such a complex network, and the backpropagation method is widely applied to train the CNN parameters through multiple layers <ref type="bibr" coords="2,429.15,171.80,15.50,8.74" target="#b38">[39,</ref><ref type="bibr" coords="2,448.06,171.80,11.62,8.74" target="#b43">44]</ref>.</s><s coords="2,468.24,171.80,18.43,8.74;2,126.67,183.75,360.00,8.74;2,126.67,195.71,345.25,8.74">Furthermore, to fine-tune the network to specific functions, large pools of labeled data are required for iteratively training the massive neurons and connection weights.</s><s coords="2,476.15,195.71,10.52,8.74;2,126.67,207.66,360.00,8.74;2,126.67,219.62,191.58,8.74">So far, many high-performance CNN designs have been proposed, such as AlexNet <ref type="bibr" coords="2,468.41,207.66,14.61,8.74" target="#b35">[36]</ref>, VGG <ref type="bibr" coords="2,153.68,219.62,14.61,8.74" target="#b62">[63]</ref>, GoogleNet <ref type="bibr" coords="2,223.98,219.62,14.61,8.74" target="#b65">[66]</ref>, ResNet <ref type="bibr" coords="2,281.11,219.62,14.61,8.74" target="#b26">[27]</ref>, etc.</s><s coords="2,323.43,219.62,163.24,8.74;2,126.67,231.57,270.54,8.74">Some of the designs can even achieve beyond human-level accuracy on object recognition tasks <ref type="bibr" coords="2,378.95,231.57,14.61,8.74" target="#b60">[61]</ref>.</s></p><p><s coords="2,138.63,243.53,348.05,8.74;2,126.67,255.48,360.00,8.74;2,126.67,267.44,76.45,8.74">Although the CNNs can achieve competitive classification accuracy, the CNNs still suffer from high computational cost, slow training speed, and security vulnerability <ref type="bibr" coords="2,157.69,267.44,15.50,8.74" target="#b66">[67,</ref><ref type="bibr" coords="2,176.53,267.44,12.73,8.74" target="#b37">38,</ref><ref type="bibr" coords="2,192.61,267.44,7.01,8.74" target="#b3">4]</ref>.</s><s coords="2,207.63,267.44,279.04,8.74;2,126.67,279.39,360.00,8.74;2,126.67,291.35,360.00,8.74;2,126.67,303.30,360.00,8.74;2,126.67,315.26,360.00,8.74;2,126.67,327.21,76.91,8.74">One major reason causing these shortcomings is the lack of network interpretability, especially the limited understanding of the internal features learned by each convolutional layer: Mathematically, the convolutional layer neurons (namely the convolution filters) convolve with the input image or the outputs of the previous layer, the results are considered as learned features and recorded in the feature maps.</s><s coords="2,209.09,327.21,277.59,8.74;2,126.67,339.17,283.92,8.74">With deeper layers, the neurons are expected to extract higher level features, and eventually converge to the final classification.</s><s coords="2,416.54,339.17,70.13,8.74;2,126.67,351.12,360.00,8.74;2,126.67,363.08,360.00,8.74;2,126.67,375.03,309.18,8.74">However, as the CNN training is considered as a black-box process and the neurons are designed in the format of simple matrices, the formation of those neuron values are unpredictable and the neuron meanings are impossible to directly explained.</s><s coords="2,440.40,375.03,46.27,8.74;2,126.67,386.99,360.00,8.74;2,126.67,398.94,360.00,8.74;2,126.67,410.90,319.02,8.74">Hence, the poor network interpretability significantly hinders the robustness evaluation of each network layer, the further optimization on the network structure, as well as the network adaptability and transferability to different applications <ref type="bibr" coords="2,411.38,410.90,15.50,8.74" target="#b52">[53,</ref><ref type="bibr" coords="2,430.20,410.90,11.62,8.74" target="#b59">60]</ref>.</s></p><p><s coords="2,138.63,422.85,348.04,8.74;2,126.67,434.81,360.00,8.74;2,126.67,446.76,38.83,8.74">A qualitative way to improve the network interpretability is the network visualization, which translates the internal features into visually perceptible image patterns.</s><s coords="2,171.23,446.76,315.44,8.74;2,126.67,458.72,360.00,8.74;2,126.67,470.68,106.17,8.74">This visualization process is referred from the human visual cortex system analysis: In a human brain, the human visual cortex is embedded in multiple vision neuron areas <ref type="bibr" coords="2,214.58,470.68,14.61,8.74" target="#b56">[57]</ref>.</s><s coords="2,237.84,470.68,248.84,8.74;2,126.67,482.63,309.64,8.74">In each vision neuron area, numerous neurons selectively respond to different features, such as colors, edges, and shapes <ref type="bibr" coords="2,402.03,482.63,15.50,8.74" target="#b34">[35,</ref><ref type="bibr" coords="2,420.82,482.63,11.62,8.74" target="#b53">54]</ref>.</s><s coords="2,440.74,482.63,45.94,8.74;2,126.67,494.59,360.00,8.74;2,126.67,506.54,360.00,8.74;2,126.67,518.50,114.55,8.74">To explore the relationship between the neurons and features, researchers usually find the preferred stimulus to identify individual kind of the response and illustrate the response to certain visual patterns.</s><s coords="2,247.40,518.50,239.27,8.74;2,126.67,530.45,196.32,8.74">The CNN visualization also follows such an analytical approach to realize the CNN interpretability.</s></p><p><s coords="2,138.63,542.41,348.05,8.74;2,126.67,554.36,360.00,8.74;2,126.67,566.32,359.99,8.74;2,126.67,578.27,80.88,8.74">Up to now, many effective network visualization works have been proposed in the literature, and several representative methods are widely adopted: 1) Erhan et al. proposed the Activation Maximization to interpret traditional shallow networks <ref type="bibr" coords="2,155.71,578.27,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="2,175.27,578.27,12.73,8.74" target="#b27">28,</ref><ref type="bibr" coords="2,192.05,578.27,11.62,8.74" target="#b67">68]</ref>.</s><s coords="2,214.18,578.27,272.49,8.74;2,126.67,590.23,360.00,8.74;2,126.67,602.18,153.69,8.74">Later, this method was further improved by Simonyan et al., which synthesized an input image pattern with the maximum activation of a single CNN neuron for visualization <ref type="bibr" coords="2,262.10,602.18,14.61,8.74" target="#b61">[62]</ref>.</s><s coords="2,287.48,602.18,199.20,8.74;2,126.67,614.14,360.00,8.74;2,126.67,626.09,207.61,8.74">This fundamental method was also extended by many other works with different regularizers for interpretability improvement of the synthesized image patterns <ref type="bibr" coords="2,281.66,626.09,15.50,8.74" target="#b70">[71,</ref><ref type="bibr" coords="2,301.61,626.09,12.73,8.74" target="#b51">52,</ref><ref type="bibr" coords="2,318.78,626.09,11.62,8.74" target="#b49">50]</ref>.</s><s coords="2,342.09,626.09,144.58,8.74;2,126.67,638.05,360.00,8.74;2,126.67,650.00,57.33,8.74">2) Besides the visualization of a single neuron, Mahendran et al. revealed the CNN internal features in the layer level <ref type="bibr" coords="2,149.53,650.00,15.50,8.74" target="#b45">[46,</ref><ref type="bibr" coords="2,168.50,650.00,11.62,8.74" target="#b46">47]</ref>.</s><s coords="2,188.92,650.00,297.75,8.74;2,126.67,661.96,360.00,8.74;2,126.67,673.91,150.60,8.74">The Network Inversion was proposed to reconstruct an input image based on multiple neurons' activation to illustrate a comprehensive feature map learned by each single CNN layer.</s><s coords="2,283.33,673.91,203.33,8.74;2,126.67,685.87,359.99,8.74;2,126.67,697.82,360.00,8.74;3,126.67,123.98,247.49,8.74">3) Rather than reconstructing an input image for feature visualization, Zeiler et al. proposed the Deconvolutional Neural Network based Visualization (DeconvNet) <ref type="bibr" coords="2,273.00,697.82,14.61,8.74" target="#b71">[72]</ref>, which utilized the DeconvNet framework to project the feature map to an image dimension directly.</s><s coords="3,380.03,123.98,106.65,8.74;3,126.67,135.93,360.00,8.74;3,126.67,147.89,328.49,8.74">With the direct projecting, DeconvNet can highlight what patterns in the input image activate the specific neurons and hence link the neurons and the meaning of input data directly.</s><s coords="3,459.55,147.89,27.13,8.74;3,126.67,159.84,360.00,8.74;3,126.67,171.80,188.43,8.74">4) Recently, Zhou et al. <ref type="bibr" coords="3,209.99,159.84,10.52,8.74" target="#b2">[3]</ref> proposed the Network Dissection based Visualization, which interpreted the CNN in the semantic level.</s><s coords="3,320.72,171.80,165.95,8.74;3,126.67,183.75,360.00,8.74;3,126.67,195.71,243.85,8.74">By referencing a heterogeneous image dataset -Borden, the Network Dissection can effectively partition the input image into multiple sections with various semantic definitions.</s><s coords="3,375.78,195.71,110.89,8.74;3,126.67,207.66,360.00,8.74;3,126.67,219.62,33.21,8.74">As the semantics directly represent the feature meanings, the neuron interpretability can be significantly enhanced.</s></p><p><s coords="3,138.63,231.57,348.05,8.74;3,126.67,243.53,360.00,8.74;3,126.67,255.48,83.38,8.74">This survey paper is expected to provide a comprehensive review of these representative CNN visualization methods, in terms of motivations, algorithms, and experiment results.</s><s coords="3,215.89,255.48,270.78,8.74;3,126.67,267.44,360.00,8.74;3,126.67,279.39,247.32,8.74">We also discuss the practical applications of the CNN visualization, demonstrating the significance of the network interpretability in areas of network design, optimization, security enhancement, etc.</s></p><p><s coords="3,138.63,291.35,348.04,8.74;3,126.67,303.30,231.41,8.74">The rest of the paper is organized as follows: In Section 2, we introduce the background knowledge of the CNN and visualization.</s><s coords="3,362.44,303.30,124.22,8.74;3,126.67,315.26,360.00,8.74;3,126.67,327.21,360.00,8.74;3,126.67,339.17,57.34,8.74">In Sections 3∼6, we describe the four aforementioned representative visualization methods, namely the Activation Maximization, DeconvNet, Network Inversion, and Network Dissection based visualization.</s><s coords="3,189.70,339.17,282.15,8.74">In Section 7, we present several CNN visualization applications.</s><s coords="3,477.54,339.17,9.13,8.74;3,126.67,351.12,359.99,8.74;3,126.67,363.08,19.98,8.74">In Section 8, the CNN visualization research potentials are discussed with the conclusion.</s></p><p><s coords="3,126.67,385.36,76.47,8.77">2. Background.</s><s coords="3,208.12,385.39,278.55,8.74;3,126.67,397.34,221.30,8.74">In this section, we introduce the background knowledge of the CNN structure, algorithm, and CNN visualization.</s></p><p><s coords="3,126.67,416.67,15.50,8.74">2.1.</s><s coords="3,147.90,416.64,80.87,8.77">CNN structure.</s><s coords="3,233.75,416.67,252.92,8.74;3,126.67,428.62,360.00,8.74">In machine learning, the CNN is a type of deep neural networks (DNNs), which has been widely used for computer vision related tasks.</s><s coords="3,126.67,440.58,360.00,8.74;3,126.67,452.53,360.00,8.74;3,126.67,464.49,146.53,8.74">Fig. <ref type="figure" coords="3,145.97,440.58,4.98,8.74" target="#fig_0">1</ref> shows a representative CNN structure -CaffeNet <ref type="bibr" coords="3,365.92,440.58,14.61,8.74" target="#b32">[33]</ref>, which is a replication of AlexNet with 5 convolutional layers (CLs) and 2 max-pooling layers (PLs) followed by 3 fully-connected layers (FLs):</s></p><p><s coords="3,138.63,476.44,348.05,8.74;3,126.67,488.40,360.00,8.74;3,126.67,500.35,80.64,8.74">Convolutional Layer : Fig. <ref type="figure" coords="3,259.03,476.44,4.98,8.74" target="#fig_0">1</ref> demonstrates the CNN structure, and the yellow blocks represent the convolutional filters -neurons in the convolutional layers for feature extraction.</s><s coords="3,212.71,500.35,273.97,8.74;3,126.67,512.31,360.00,8.74;3,126.67,524.26,175.18,8.74">These filters perform the convolution process to transform the input images or previous layer feature maps into the output feature maps, which are denoted as the blue blocks in Fig. <ref type="figure" coords="3,294.10,524.26,3.87,8.74" target="#fig_0">1</ref>.</s></p><p><s coords="3,138.63,536.22,288.18,8.74">Fig. <ref type="figure" coords="3,158.88,536.22,4.98,8.74" target="#fig_2">2</ref> (a) shows the detailed convolutional process of the first CL.</s><s coords="3,430.04,536.22,56.63,8.74;3,126.67,548.17,360.00,8.74">The convolutional filters in the first layer have three channels corresponding to the three RGB</s></p><formula xml:id="formula_0" coords="4,165.47,170.01,165.44,43.03">… … … … … … … -0.3 -0.7 1 0 1.8 -1 0 1 1 -1 -1 1 0 1 -1 0 1 1 -1 -1 1 0 1 -1 0 1 1 Input Channel R Input Channel G Input Channel B</formula><p><s coords="4,165.07,216.70,166.62,6.81;4,126.67,300.04,179.71,8.74">Filter Channel #1 Filter Channel #2 Filter Channel #3  color dimensions of the raw input images.</s><s coords="4,310.70,300.04,175.98,8.74;4,126.67,312.00,360.00,8.74">Each filter channel performs dot production in a small region of the input data to compose a color specific feature channel.</s><s coords="4,126.67,323.95,360.00,8.74;4,126.67,335.91,293.85,8.74">This process is usually followed by an activation function F , usually relu <ref type="bibr" coords="4,445.07,323.95,9.96,8.74" target="#b4">[5]</ref>, which gives a summation of dot productions when positive or 0 otherwise.</s><s coords="4,424.91,335.91,61.77,8.74;4,126.67,347.86,314.77,9.65">Hence, we can get a color comprehensive element p ij of the final rectified feature map.</s><s coords="4,446.65,347.86,40.02,8.74;4,126.67,359.82,360.00,8.74">Based on the small region convolution, each filter is replicated across the entire input image.</s><s coords="4,126.67,371.77,360.00,9.65">Multiple p ij s would produce the final rectified feature map (or activation map) a i,l .</s><s coords="4,138.63,383.73,348.04,8.74;4,126.67,395.68,101.89,8.74">The rectified feature maps represent the extracted features and would act as the inputs for the next CL.</s><s coords="4,231.96,395.68,254.71,8.74;4,126.67,407.64,57.90,8.74">Mathematically, for the filter i in layer l, This process can be viewed as:</s></p><formula xml:id="formula_1" coords="4,244.26,420.37,242.41,9.65">a i,l+1 = F ( w i,l a i,l + b i,l ),<label>(1)</label></formula><p><s coords="4,126.67,436.64,292.23,8.74">where w, b represents the weights and bias parameters respectively.</s><s coords="4,138.63,448.60,348.04,8.74;4,126.67,460.55,221.97,8.74">With such a process, the filters act as feature extractors from the original input image to learn the useful features for classification.</s></p><p><s coords="4,138.63,472.51,348.05,8.74;4,126.67,484.46,126.07,8.74">Pooling Layer : As shown in Fig. <ref type="figure" coords="4,289.18,472.51,3.87,8.74" target="#fig_0">1</ref>, after each CL, it's optimal to apply a PL on the output feature maps.</s><s coords="4,259.72,484.46,226.95,8.74;4,126.67,496.42,360.00,8.74;4,126.67,508.37,85.10,9.65">As denoted by the green blocks, the pooling filters perform the down-sampling operation to reduce the data dimension of the input feature maps (a i,l ).</s><s coords="4,218.44,508.37,268.24,8.74;4,126.67,520.33,109.03,8.74">Fig. <ref type="figure" coords="4,239.52,508.37,4.98,8.74" target="#fig_2">2</ref> (b) shows the max-pooling process, which is a widely adopted pooling method.</s><s coords="4,240.08,520.33,246.60,8.74;4,126.67,532.28,360.00,8.74;4,126.67,544.24,360.00,8.74;4,126.67,556.19,235.70,8.74">The max-pooling is achieved by applying a 2 × 2 pooling window to select the maximal element of a 2 × 2 region of the input feature map with a stride of 2. This process aggressively reduces the spatial size of the feature maps and condense the extracted feature information.</s></p><p><s coords="4,138.63,568.15,348.04,8.74;4,126.67,580.10,173.01,8.74">Hence, the pooling layers contribute the CNN with fewer data redundancy and therefore less data processing workload.</s></p><p><s coords="4,138.63,592.06,348.04,8.74;4,126.67,604.01,279.25,8.74">Fully-connected Layer : In Fig. <ref type="figure" coords="4,275.31,592.06,3.87,8.74" target="#fig_0">1</ref>, each yellow circle represent one neuron in the FLs, which is connected to all the previous input feature maps.</s><s coords="4,411.24,604.01,75.42,8.74;4,126.67,615.97,360.00,8.74;4,126.67,627.92,360.00,8.74;4,126.67,639.88,91.46,8.74">The FLs perform a comprehensive feature evaluation based on the features extracted by the CLs, and generate an N-dimensional probability vector, where N is the number of the classification targets.</s><s coords="4,224.05,639.88,262.63,8.74;4,126.67,651.83,110.22,8.74">For example, in the digit classification task, N would be 10 for the digits of 0∼9 <ref type="bibr" coords="4,218.63,651.83,14.61,8.74" target="#b39">[40]</ref>.</s></p><p><s coords="4,138.63,663.79,348.04,8.74;4,126.67,675.74,199.31,8.74">After the final layer of FLs, a SoftMax function is used to generate the final classification probability as defined in the Eq.</s><s coords="4,329.30,675.74,7.75,8.74">2:</s></p><formula xml:id="formula_2" coords="4,270.51,688.69,216.16,27.87">P i = e ai 10 N =1 e an ,<label>(2)</label></formula><p><s coords="5,126.67,123.98,55.77,9.65;5,182.44,122.40,7.67,6.12;5,194.47,123.98,159.62,8.74">where a i is i th neuron output in the final FL layer.</s><s coords="5,360.12,123.98,126.55,8.74;5,126.67,135.93,360.00,8.74;5,126.67,147.89,130.57,8.74">This function normalizes the final FL layer output to a vector of values between zero and one, which gives a probability over all 10 classes.</s><s coords="5,138.63,159.84,348.04,8.74;5,126.67,171.80,360.01,8.74;5,126.67,183.75,295.98,9.65">By applying the above-mentioned hierarchical structured layers, CNNs transform the input image layer by layer from the original pixel values to the final probability vector P , in which the largest P i indicates the most predicted class.</s></p><p><s coords="5,126.67,202.17,15.50,8.74">2.2.</s><s coords="5,147.90,202.14,83.65,8.77">CNN algorithm.</s><s coords="5,236.53,202.17,250.15,8.74;5,126.67,214.12,248.47,8.74">The CNNs not only benefit from the deep hierarchical structure, but also the delicate learning algorithm <ref type="bibr" coords="5,356.88,214.12,14.61,8.74" target="#b38">[39]</ref>.</s><s coords="5,383.36,214.12,103.31,8.74;5,126.67,226.08,360.00,8.74;5,126.67,238.03,338.63,8.74">The learning algorithm aims to minimize the training error between the predicted values and actual labels by updating the network parameters, which are quantified by the loss function.</s><s coords="5,469.52,238.03,17.16,8.74;5,126.67,249.99,138.26,8.74">The training error can be viewed as:</s></p><formula xml:id="formula_3" coords="5,241.41,265.00,245.26,30.03">C(w, b) = 1 n i=1 n L(w, b, x i , y i ),<label>(3)</label></formula><p><s coords="5,126.67,300.76,360.00,9.65;5,126.67,312.72,42.40,8.74">where L(•) represents the loss function, and (x 1 , y 1 ), ...(x n , y n ) represent the training examples.</s><s coords="5,175.52,312.72,311.15,8.74;5,126.67,324.67,89.78,8.74">During the learning process, a square loss is usually applied, then the loss function will be:</s></p><formula xml:id="formula_4" coords="5,232.38,338.98,254.29,11.72">L(w, b, x i , y i ) = (y i -f (w, b, x i )) 2 ,<label>(4)</label></formula><p><s coords="5,126.67,357.44,311.92,8.74">where f (•) indicates the predicted values calculated by the whole CNN:</s></p><formula xml:id="formula_5" coords="5,137.50,375.81,349.17,9.65">f (w, b, x) = F ( w i,l F (w i,l-1 F (...F ( w i,1 x i,0 + b i,0 )...) + b i,l-1 ) + b i,l ).<label>(5)</label></formula><p><s coords="5,126.67,394.19,360.00,8.74;5,126.67,406.14,360.00,8.74;5,126.67,418.10,40.98,8.74">In order to minimize the C(w, b), a partial derivative ∂C/∂(w, b) with respect to each weight w and bias b is calculated by backpropagating through all the layers in the CNN.</s><s coords="5,169.89,418.10,316.78,8.74;5,126.67,430.05,29.11,8.74">The gradient descent method is utilized to iteratively update all parameter values.</s><s coords="5,160.21,430.05,311.67,8.74">The update procedure for w from iteration j to j + 1 can be viewed as:</s></p><formula xml:id="formula_6" coords="5,244.43,444.89,238.00,22.31">w j+1 = w j -η • ∂C(w; x, y) ∂w , (<label>6</label></formula><formula xml:id="formula_7" coords="5,482.43,451.63,4.24,8.74">)</formula><p><s coords="5,126.67,470.80,124.44,8.74">where η is the learning rate.</s><s coords="5,256.53,470.80,230.14,8.74;5,126.67,482.76,129.43,8.74">Before the learning process, the parameters are usually randomly initialized <ref type="bibr" coords="5,237.84,482.76,14.61,8.74" target="#b20">[21]</ref>.</s><s coords="5,262.08,482.76,224.59,8.74;5,126.67,494.71,222.09,8.74">With the learning process, the convolutional filters become well configured to extract certain features.</s><s coords="5,354.02,494.71,132.65,8.74;5,126.67,506.67,239.88,8.74">The features captured by convolutional filters can be demonstrated by visualization.</s><s coords="5,138.63,518.62,348.04,8.74;5,126.67,530.58,46.47,8.74">A lot of works have been proposed to optimize the structure and algorithm of the CNNs.</s><s coords="5,179.00,530.58,307.67,8.74;5,126.67,542.53,167.36,8.74">For example, much deeper network structures have been investigated, such as VGG, GoogleNet, and ResNet.</s><s coords="5,299.13,542.53,187.54,8.74;5,126.67,554.49,360.00,8.74;5,126.67,566.44,199.01,8.74">At the same time, some regularization and optimization techniques have been applied, such as dropout <ref type="bibr" coords="5,391.42,554.49,14.61,8.74" target="#b64">[65]</ref>, batch normalization <ref type="bibr" coords="5,148.26,566.44,14.60,8.74" target="#b30">[31]</ref>, momentum <ref type="bibr" coords="5,224.65,566.44,14.61,8.74" target="#b55">[56]</ref>, and adagrad <ref type="bibr" coords="5,307.42,566.44,14.61,8.74" target="#b12">[13]</ref>.</s><s coords="5,333.44,566.44,153.24,8.74;5,126.67,578.40,266.40,8.74">As a result, CNNs have been well optimized and widely used in computer vision related tasks.</s><s coords="5,399.23,578.40,87.44,8.74;5,126.67,590.35,360.00,8.74;5,126.67,602.31,360.00,8.74;5,126.67,614.26,61.33,8.74">However, the CNNs still suffer from high computational cost, slow training speed, and large training dataset requirement, which highly compromise the applicability and performance efficiency <ref type="bibr" coords="5,169.74,614.26,14.61,8.74" target="#b25">[26]</ref>.</s><s coords="5,192.95,614.26,293.72,8.74;5,126.67,626.22,217.10,8.74">Hence, these weakness require more understanding about the CNN working mechanism to further optimize the CNN.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3." coords="5,126.67,644.61,175.06,8.77">CNN visualization mechanism.</head><p><s coords="5,306.71,644.64,179.95,8.74;5,126.67,656.59,360.00,8.74;5,126.67,668.55,47.35,8.74">CNN visualization is a well utilized qualitative method to analysis the CNN working mechanism regarding the network interpretability.</s><s coords="5,178.24,668.55,308.43,8.74;5,126.67,680.50,360.00,8.74;5,126.67,692.46,41.97,8.74">The interpretability is related to the ability of the human to understand the CNNs, which can be improved by demonstrating the internal features learned by CNNs.</s><s coords="5,172.84,692.46,313.83,8.74;5,126.67,704.41,247.48,8.74">Visualization greatly helps to interpret the CNN's internal features, since it utilizes the human visual cortex system as a reference.</s><s coords="6,138.63,292.10,348.05,8.74;6,126.67,304.06,137.63,8.74">Fig. <ref type="figure" coords="6,158.54,292.10,4.98,8.74" target="#fig_3">3</ref> shows how human visual cortex system process the visual information and how CNNs extract the features.</s><s coords="6,268.68,304.06,217.99,8.74;6,126.67,316.01,360.00,8.74;6,126.67,327.97,166.29,8.74">As shown in the left part Fig. <ref type="figure" coords="6,400.71,304.06,3.87,8.74" target="#fig_3">3</ref>, the human visual system processes the object features in a feed-forward and hierarchical approach through multiple visual neuron areas.</s><s coords="6,299.50,327.97,187.18,8.74;6,126.67,339.92,317.36,8.74">When humans recognize a face, the visual neurons with small receptive fields in the lower visual neuron area (e.g.</s><s coords="6,450.21,339.92,36.46,8.74;6,126.67,351.88,298.96,8.74">V1), are sensitive to basic visual features <ref type="bibr" coords="6,269.47,351.88,15.50,8.74" target="#b47">[48,</ref><ref type="bibr" coords="6,288.22,351.88,12.73,8.74" target="#b29">30,</ref><ref type="bibr" coords="6,304.21,351.88,11.62,8.74" target="#b28">29]</ref>, such as edges and lines.</s><s coords="6,430.04,351.88,56.63,8.74;6,126.67,363.83,104.10,8.74">In the higher visual neuron areas (e.g.</s><s coords="6,234.93,363.83,251.74,8.74;6,126.67,375.79,296.20,8.74">V2 and V4), the visual neurons have larger receptive fields, and are sensitive to complex features, such as shapes and objects.</s><s coords="6,430.01,375.79,56.66,8.74;6,126.67,387.74,360.00,8.74;6,126.67,399.70,268.19,8.74">In the visual neuron area of IT, the visual neurons have the largest and most comprehensive receptive fields, therefore they are sensitive to the entire face.</s></p><p><s coords="6,138.63,411.65,348.04,8.74;6,126.67,423.61,360.00,8.74;6,126.67,435.56,360.00,8.74;6,126.67,447.52,162.92,8.74">For CNN interpretability study, researchers found the similar feature representation through the CNNs visualization as shown in the right part of Fig. <ref type="figure" coords="6,432.02,423.61,3.87,8.74" target="#fig_3">3</ref>. Typically, the CNN feature extraction starts with small features such as edges and colored blobs in the first convolutional layer.</s><s coords="6,295.70,447.52,190.97,8.74;6,126.67,459.47,360.00,8.74;6,126.67,471.43,169.42,8.74">Then the feature extraction progresses into general shapes and partial objects with deeper layers, and ends in a final classification with the fully-connected layers.</s><s coords="6,302.77,471.43,183.90,8.74;6,126.67,483.38,360.00,8.74;6,126.67,495.34,224.00,8.74">By comparing the functionalities of brain visual neurons' receptive fields to the CNN's neurons <ref type="bibr" coords="6,363.77,483.38,14.61,8.74" target="#b36">[37]</ref>, visualization illustrates the functionalities of each component in the CNNs.</s></p><p><s coords="6,126.67,516.09,15.50,8.74">2.4.</s><s coords="6,147.90,516.06,147.03,8.77">CNNs visualization methods.</s><s coords="6,299.90,516.09,186.77,8.74;6,126.67,528.05,360.00,8.74;6,126.67,540.00,360.00,8.74;6,126.67,551.96,42.78,8.74">The similarity between how the human vision system and CNN recognizes image inspired research to work on interpreting CNNs, and lots of CNN visualization works of the learned features have been widely discussed.</s><s coords="6,173.86,551.96,312.81,8.74;6,126.67,563.91,110.30,8.74;7,138.63,174.78,348.05,8.74;7,126.67,186.74,244.16,8.74">In the early research stage, the visualization mainly focused on the lowlevel features <ref type="bibr" coords="6,188.64,563.91,15.50,8.74" target="#b54">[55,</ref><ref type="bibr" coords="6,208.37,563.91,12.73,8.74" target="#b41">42,</ref><ref type="bibr" coords="6,225.35,563.91,11.62,8.74" target="#b42">43]</ref> • In Activation Maximization, a visualized input image pattern is synthesized to illustrate a specific neuron's max stimulus in each layer;</s></p><p><s coords="7,138.63,201.68,348.04,8.74;7,126.67,213.64,360.00,8.74;7,126.67,225.59,137.09,8.74">• DeconNet utilizes an inversed CNN structure, which is composed deconvolutional and unpooling layers, to find the image pattern in the original input image for a specific neuron activation;</s></p><p><s coords="7,138.63,240.54,348.05,8.74;7,126.67,252.49,360.00,8.74;7,126.67,264.45,87.29,8.74">• Network Inversion reconstructs an input image based on the original image from a specific layer's feature maps, which reveals what image information is preserved in that layer;</s></p><p><s coords="7,138.63,279.39,348.04,8.74;7,126.67,291.35,187.99,8.74">• Network Dissection describes neurons as visual semantic detectors, which can match six kinds of semantic concepts (e.g.</s><s coords="7,321.07,291.35,165.61,8.74;7,126.67,303.30,47.07,8.74">scene, object, part, material, texture, and color).</s></p><p><s coords="7,138.63,318.25,348.05,8.74;7,126.67,330.20,360.00,8.74;7,126.67,342.16,160.16,8.74">To compare these methods directly, we summarize the overview, algorithms, and visualization results of these methods: 1) The overview summarizes the history and represent works in this line of work.</s><s coords="7,293.17,342.16,193.50,8.74;7,126.67,354.11,149.67,8.74">2) The algorithms explain how this method works for the CNNs visualization.</s><s coords="7,282.44,354.11,204.23,8.74;7,126.67,366.07,221.42,8.74">3) The visualization results provide a comprehensive understanding how CNNs extract features.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." coords="7,126.67,393.16,221.74,8.77">Visualization by activation maximization.</head><p><s coords="7,140.85,407.98,345.82,8.74;7,126.67,419.93,102.86,8.74">Synthesize an input pattern image that can maximize a specific neuron's activation in arbitrary layers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." coords="7,126.67,442.59,360.00,8.77;7,126.67,454.58,360.00,8.74">The overview. Activation Maximization (AM) is proposed to visualize the preferred inputs of neurons in each layer. The preferred input can indicate what</head><p><s coords="7,126.67,466.53,138.28,8.74">features of a neuron has learned.</s><s coords="7,269.11,466.53,217.57,8.74;7,126.67,478.49,262.85,8.74">The learned feature is represented by a synthesized input pattern that can cause maximal activation of a neuron.</s><s coords="7,393.76,478.49,92.91,8.74;7,126.67,490.44,360.00,8.74;7,126.67,502.40,169.12,8.74">In order to synthesize such an input pattern, each pixel of the CNN's input is iteratively changed to maximize the activation of the neuron.</s></p><p><s coords="7,138.63,514.35,348.04,8.74;7,126.67,526.31,149.00,8.74">The idea behind the AM is intuitive, and the fundamental algorithm was proposed by <ref type="bibr" coords="7,167.77,526.31,104.25,8.74">Erhan et al. in 2009 [14]</ref>.</s><s coords="7,279.98,526.31,206.69,8.74;7,126.67,538.26,360.00,8.74;7,126.67,550.22,253.21,8.74">They visualized the preferred input patterns for the hidden neurons in the Deep Belief Net <ref type="bibr" coords="7,320.09,538.26,15.50,8.74" target="#b27">[28]</ref> and the Stacked Denoising Auto-Encoder <ref type="bibr" coords="7,166.72,550.22,15.50,8.74" target="#b67">[68]</ref> learned from the MNIST digit dataset <ref type="bibr" coords="7,361.62,550.22,14.61,8.74" target="#b39">[40]</ref>.</s><s coords="7,386.91,550.22,99.77,8.74;7,126.67,562.17,360.00,8.74;7,126.67,574.13,47.73,8.74">Later, Simonyan et al. utilized this method to maximize the activation of neurons in the last layer of CNNs <ref type="bibr" coords="7,156.14,574.13,14.61,8.74" target="#b61">[62]</ref>.</s><s coords="7,179.07,574.13,307.61,8.74;7,126.67,586.08,77.96,8.74">Google also has synthesized similar visualized patterns for their inception network <ref type="bibr" coords="7,186.37,586.08,14.61,8.74" target="#b48">[49]</ref>.</s><s coords="7,210.97,586.08,275.70,8.74;7,126.67,598.04,258.01,8.74">Yosinksi et al. further applied the AM in a large scale, which visualized the arbitrary neurons in all layers of a CNN <ref type="bibr" coords="7,366.42,598.04,14.61,8.74" target="#b70">[71]</ref>.</s><s coords="7,389.07,598.04,97.61,8.74;7,126.67,609.99,360.00,8.74;7,126.67,621.95,146.82,8.74">Recently, a lot of optimization works have followed this idea to improve the interpretability and diversity of the visualized patterns <ref type="bibr" coords="7,239.39,621.95,15.50,8.74" target="#b51">[52,</ref><ref type="bibr" coords="7,257.99,621.95,11.62,8.74" target="#b49">50]</ref>.</s><s coords="7,277.84,621.95,208.83,8.74;7,126.67,633.90,360.00,8.74;7,126.67,645.86,115.01,8.74">With all these works, the AM has demonstrated great capability to interpret the interests of neurons and identify the hierarchical features learned by CNNs.</s></p><p><s coords="7,126.67,668.55,15.50,8.74">3.2.</s><s coords="7,147.90,668.52,76.78,8.77">The algorithm.</s><s coords="7,229.67,668.55,257.01,8.74;7,126.67,680.50,44.08,8.74">In this section, the fundamental algorithm of the AM is presented.</s><s coords="7,175.01,680.50,311.67,8.74;7,126.67,692.46,360.00,8.74;7,126.67,704.41,56.21,8.74">Then, another optimized AM algorithm is discussed, which dramatically improves the interpretability of visualized patterns by utilizing a deep generator network <ref type="bibr" coords="7,164.62,704.41,14.61,8.74" target="#b49">[50]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1." coords="8,126.67,123.98,139.82,8.74">Activation maximization.</head><p><s coords="8,271.48,123.98,215.19,8.74;8,126.67,135.93,185.75,8.74;8,312.42,134.36,4.08,6.12;8,317.00,135.93,169.68,8.74;8,126.67,147.89,62.04,8.74">The fundamental algorithm of the AM can be viewed as synthesizing a pattern image x * , which maximizes the activation of a target neuron:</s></p><formula xml:id="formula_8" coords="8,258.31,158.63,228.36,18.14">x * = argmax x a i,l (θ, x),<label>(7)</label></formula><p><s coords="8,126.67,181.04,274.00,8.74">where θ denotes the network parameter sets (weight and bias).</s><s coords="8,138.63,193.00,189.90,8.74">This process can be divided into four steps:</s></p><p><s coords="8,138.63,204.95,348.04,9.65;8,126.67,216.91,103.78,8.74">(1) An image x = x 0 with random pixel values is set to be the input to the activation computation.</s></p><p><s coords="8,138.63,229.80,220.34,8.74">(2) The gradients with respect to the noise image</s></p><formula xml:id="formula_9" coords="8,364.03,226.72,16.34,6.79">∂a i,l</formula><p><s coords="8,367.85,235.27,9.20,6.12;8,385.92,229.80,100.76,8.74;8,126.67,241.75,269.22,8.74">∂x are computed by using backpropagation, while the parameters of this CNN are fixed.</s></p><p><s coords="8,138.63,253.71,348.04,8.74;8,126.67,267.15,275.55,8.74;8,407.07,264.08,16.34,6.79;8,410.88,272.63,9.20,6.12;8,425.10,267.15,2.77,8.74">(3) Each pixel of the noise image is changed iteratively to maximize the activation of the neuron, which is guided by the direction of the gradient ∂a i,l ∂x .</s><s coords="8,433.27,267.15,53.40,8.74;8,126.67,279.11,189.56,8.74">Every single iteration in this process applies the update:</s></p><formula xml:id="formula_10" coords="8,257.23,294.56,229.45,22.31">x ← x + η • ∂a i,l (θ, x) ∂x ,<label>(8)</label></formula><p><s coords="8,126.67,321.10,199.30,8.74">where η denotes the gradient ascent step size.</s></p><p><s coords="8,138.63,333.05,244.27,8.74;8,382.90,331.48,4.08,6.12;8,387.48,333.05,99.19,8.74;8,126.67,345.01,60.94,8.74">(4) This process terminates at a specific pattern image x * , when the image without any noise.</s><s coords="8,192.04,345.01,257.26,8.74">This pattern is seen as preferred input for this neuron <ref type="bibr" coords="8,431.03,345.01,14.61,8.74" target="#b70">[71]</ref>.</s></p><p><s coords="8,138.63,356.96,348.04,9.65;8,126.67,368.92,360.00,8.74;8,126.67,380.87,360.00,8.74;8,126.67,392.83,360.00,8.74;8,126.67,404.78,360.01,8.74">Typically, we are supposed to use the unnormalized activation a i (θ, x) of class c in the final CNN layer of this visualization network , rather than the probability returned by the SoftMax in Eq. 2. Because the SoftMax normalize the final layer output to a vector of values between zero and one, the maximization of the class probability can be achieved by minimizing the probability of other classes.</s><s coords="8,126.67,416.74,360.00,8.74;8,126.67,428.69,204.93,8.74">This method can be applied to any kinds of CNNs as long as we can compute the aforementioned gradients of the image pattern.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,126.67,448.36,23.25,8.74">3.2.2.</head><p><s coords="8,155.01,448.36,180.76,8.74">Activation maximization with regulation.</s><s coords="8,340.76,448.36,145.92,8.74;8,126.67,460.31,359.99,8.74;8,126.67,472.27,316.38,8.74">However, the AM method has a considerable shortcoming: as the CNN becoming deeper, the visualized patterns in higher layers are usually tend to be unrealistic and uninterpretable.</s><s coords="8,450.47,472.27,36.20,8.74;8,126.67,484.22,360.00,8.74;8,126.67,496.18,301.34,8.74">In order to find the human-interpretable patterns, many regularization methods have been experimentally shown to improve the interpretability of the patterns.</s></p><p><s coords="8,138.63,508.13,348.05,8.74;8,126.67,520.09,63.67,8.74">A regularization parameter of λ(x) is usually introduced to bias the visualized pattern image:</s></p><formula xml:id="formula_11" coords="8,239.55,530.84,247.12,18.14">x * = argmax x (a i,l (θ, x) -λ(x)).<label>(9)</label></formula><p><s coords="8,126.67,553.24,360.00,9.65;8,126.67,565.20,360.00,8.74;8,126.67,577.15,90.51,8.74">Different methods are adopted to implemented the λ(x), such as 2 decay, Gaussian blur, mean image initialization, and clipping pixels with very small absolute value <ref type="bibr" coords="8,152.00,577.15,15.49,8.74" target="#b61">[62,</ref><ref type="bibr" coords="8,170.41,577.15,12.73,8.74" target="#b70">71,</ref><ref type="bibr" coords="8,186.05,577.15,12.73,8.74" target="#b69">70,</ref><ref type="bibr" coords="8,201.69,577.15,11.62,8.74" target="#b51">52]</ref>.</s><s coords="8,221.47,577.15,265.20,9.65;8,126.67,589.11,272.46,8.74">For example, the 2 decay tends to prevent a small number of extreme pixel values from dominating the visualized patterns.</s><s coords="8,404.82,589.11,81.85,8.74;8,126.67,601.06,360.00,8.74;8,126.67,613.02,360.00,8.74;8,126.67,624.97,81.11,8.74">The Gaussian blur penalize high frequency information in the visualized patterns, and the contribution of a pixel is measured as how much the activation increases or decreases when the pixel is set to zero.</s><s coords="8,212.12,624.97,274.56,8.74;8,126.67,636.93,127.76,8.74">Each of these regularization methods can be applied to the AM individually or cooperatively.</s><s coords="8,260.31,636.93,89.16,8.74">In the Section 3.3.2,</s><s coords="8,353.40,636.93,133.28,8.74;8,126.67,648.88,304.38,8.74">we shows the bias patterns by applying these regularization methods to improve the interpretability.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,126.67,668.55,23.25,8.74">3.2.3.</head><p><s coords="8,155.01,668.55,218.27,8.74">Activation maximization with generator networks.</s><s coords="8,378.26,668.55,108.41,8.74;8,126.67,680.50,360.00,8.74;8,126.67,692.46,360.00,8.74;8,126.67,704.41,223.51,8.74">Instead of utilizing regularizer λ(x) to bias the visualized, Nguyen et al. <ref type="bibr" coords="8,349.93,680.50,15.50,8.74" target="#b49">[50]</ref> utilized a image generator network <ref type="bibr" coords="8,165.25,692.46,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="8,184.72,692.46,12.73,8.74" target="#b23">24]</ref> to to replace the iterative random pixel tuning, which maximize the activation of the neurons in the final CNN layer.</s><s coords="8,354.42,704.41,132.25,8.74;9,126.67,123.98,360.00,8.74;9,126.67,135.93,182.02,8.74">The synthesized pattern image by the generator is more close to the realistic image, which greatly improves the interpretability of the visualized patterns.</s></p><p><s coords="9,138.63,147.89,348.05,8.74;9,126.67,159.84,151.24,8.74">Recently, most of the generator networks related works are based on Generative Adversarial Networks (GAN) <ref type="bibr" coords="9,259.64,159.84,14.61,8.74" target="#b23">[24]</ref>.</s><s coords="9,284.70,159.84,201.97,8.74;9,126.67,171.80,360.00,8.74;9,126.67,183.75,360.00,8.74;9,126.67,195.71,318.71,8.74">GANs can learn to mimic any distribution of data and generate realistic data samples, such as image, music, and speech, which is featured with a complementary composition of two neural networks: One generative network takes noise as input and aim to generate realistic data samples.</s><s coords="9,450.95,195.71,35.73,8.74;9,126.67,207.66,360.00,8.74;9,126.67,219.62,360.00,8.74;9,126.67,231.57,203.75,8.74">Another discriminator network receives the generated data samples from the output of the generative network and the real data samples from the training data sets, which aim to distinguishes between the two sources.</s><s coords="9,337.37,231.57,149.30,8.74;9,126.67,243.53,360.00,8.74;9,126.67,255.48,87.40,8.74">The goal of generative network is to generate passable data samples, to lie without being distinguished by the discriminator network.</s><s coords="9,222.25,255.48,264.42,8.74;9,126.67,267.44,159.97,8.74">The goal of the discriminator is to identify images coming from the generative network as fake.</s><s coords="9,291.82,267.44,194.85,8.74;9,126.67,279.39,360.00,8.74;9,126.67,291.35,208.75,8.74">After fine training of both the networks, the GAN eventually achieves a balance, where the discriminator can hardly distinguish generated data samples from real data samples.</s><s coords="9,340.42,291.35,146.25,8.74;9,126.67,303.30,360.00,8.74;9,126.67,315.26,36.64,8.74">In such a case, we can claim that the generative network has achieved an optimal capability in generating realistic samples.</s><s coords="9,168.70,315.26,317.97,8.74;9,126.67,327.21,320.02,8.74">So far GANs have particularly produced excellent results in image data, and primarily been used to generate samples of realistic images <ref type="bibr" coords="9,406.29,327.21,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="9,420.13,327.21,12.73,8.74" target="#b40">41,</ref><ref type="bibr" coords="9,436.18,327.21,7.01,8.74" target="#b1">2]</ref>.</s></p><p><s coords="9,138.63,339.17,348.04,8.74;9,126.67,351.12,360.00,8.74;9,126.67,363.08,239.81,8.74">Benefit from the success of GANs, the generative network is utilized to overcome the aforementioned shortcoming of AM that the visualized patterns in higher layers are usually tend to be unrealistic and uninterpretable.</s><s coords="9,372.42,363.08,114.25,8.74;9,126.67,375.03,360.00,8.74;9,126.67,386.99,360.00,9.65;9,126.67,398.94,204.05,8.74">The generative network is utilized to generate or synthesize the pattern image that maximize the activation of the selected neuron a i,l in the final layer This method is called Deep Generative Network Activation Maximization (DGN-AM).</s></p><p><s coords="9,138.63,410.90,195.36,8.74">The DGN-AM implemention can be view as:</s></p><formula xml:id="formula_12" coords="9,230.93,427.59,255.74,18.14">x * = argmax x (a i,l (θ, G(x)) -λ(x)),<label>(10)</label></formula><p><s coords="9,126.67,453.31,346.58,8.74">where G indicates the generative network that takes the noise image as input.</s><s coords="9,479.20,453.31,7.47,8.74;9,126.67,465.26,360.00,8.74;9,126.67,477.22,16.28,9.65">It can synthesize the pattern image that causes high activation of the target neuron a i,l .</s><s coords="9,147.38,477.22,339.29,9.65;9,126.67,489.17,195.71,8.74">In <ref type="bibr" coords="9,159.83,477.22,14.61,8.74" target="#b49">[50]</ref>, the author found that the 2 regularization with small degree helps to generate more human-interpretable patterns.</s><s coords="9,326.95,489.17,87.87,8.74">In the Section 3.3.3,</s><s coords="9,418.20,489.17,68.47,8.74;9,126.67,501.13,232.81,8.74">we compare the pattern image synthesized by the AM and DGN-AM.</s><s coords="10,138.63,452.84,348.04,8.74;10,126.67,464.80,360.00,8.74;10,126.67,476.76,203.85,8.74">This interesting finding reveals that the CNNs attempt to imitate the human visual cortex system, which the neurons in the lower visual area are sensitive to basic patterns, such as colors, edges, and lines.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2." coords="10,126.67,500.92,150.25,8.74">Hidden layers visualization.</head><p><s coords="10,281.90,500.92,204.77,8.74;10,126.67,512.88,268.14,8.74">Beyond the first layer, the neurons in the following layers gradually learn to extract feature hierarchically.</s><s coords="10,138.63,524.83,348.05,8.74;10,126.67,536.79,288.22,8.74">Fig. <ref type="figure" coords="10,159.35,524.83,4.98,8.74" target="#fig_6">5</ref> shows the visualization of 6 hidden layers from the second convolutional layer (CL 2) to the second fully connected layer (FL 2) in each row.</s><s coords="10,419.11,536.79,67.57,8.74;10,126.67,548.74,261.21,8.74">Several neurons in each layer are randomly selected as our AM test targets.</s><s coords="10,393.10,548.74,93.57,8.74;10,126.67,560.70,360.00,8.74;10,126.67,572.65,360.00,8.74;10,126.67,584.61,161.75,8.74">We observed that: 1) Some important patterns are visualized, such as edges (CL 2-4), faces (CL 4-1), wheels (CL 4-2), bottles (CL 5-1), eyes (CL 5-2), etc., which demonstrate the abundant features learned by the neurons.</s><s coords="10,292.78,584.61,193.89,8.74;10,126.67,596.56,322.26,8.74">2) Meanwhile, not all the visualized patterns are interpretable even with multiple regularization methods are applied.</s><s coords="10,456.35,596.56,30.33,8.74;10,126.67,608.52,360.00,8.74;10,126.67,620.47,360.00,8.74;10,126.67,632.43,54.04,8.74">3) The complexity and variation of the visualized patterns are increasing from lower layers to higher layers, which indicates that increasingly invariant features are learned by the neurons.</s><s coords="10,185.94,632.43,300.73,8.74;10,126.67,644.38,360.00,8.74;10,126.67,656.34,47.32,8.74">4) From the CL 5 to FLs, we can find there is a large pattern variation increment, which could indicate the FLs provide a more comprehensive feature evaluation.</s><s coords="12,126.67,123.98,360.00,8.74;12,126.67,135.93,36.61,8.74">lipsticks in the third column (AM-3) and the images are far from being photorealistic.</s><s coords="12,171.10,135.93,315.58,8.74;12,126.67,147.89,360.00,8.74;12,126.67,159.84,91.12,8.74">For the DGN-AM shown in the second row of Fig. <ref type="figure" coords="12,405.52,135.93,3.87,8.74">6</ref>, by utilizing the generator networks, the DGN-AM greatly improves the images quality in terms of color and texture.</s><s coords="12,224.18,159.84,262.50,8.74;12,126.67,171.80,360.00,8.74;12,126.67,183.75,98.43,8.74">Because the fully connected layer contain information from all areas of the image and the generator network provides a strong biases toward realistic visualizations.</s><s coords="12,138.63,195.71,348.04,8.74;12,126.67,207.66,211.31,8.74">Through the final layer visualization, we can clearly see what objects combinations could affect the CNN classification decision.</s><s coords="12,342.15,207.66,144.52,8.74;12,126.67,219.62,360.00,8.74;12,126.67,231.57,236.32,8.74">For example, if the CNN classifies an image of a cell phone held in a human hand as a cell phone, it is unclear if the classification decision is affected by the human hand.</s><s coords="12,369.67,231.57,117.00,8.74;12,126.67,243.53,360.00,8.74;12,126.67,255.48,201.85,8.74">Through the visualization, we can see there is a cell phone and a human hand in the cell phone class, which are shown in the second row and third column.</s><s coords="12,332.80,255.48,153.88,8.74;12,126.67,267.44,303.14,8.74">In this case, the visualization shows the CNN has learned to detect both object information in one image.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,126.67,680.50,23.25,8.74">3.3.3.</head><p><s coords="12,126.67,288.38,15.50,8.74">3.4.</s><s coords="12,147.90,288.35,72.26,8.77">The summary.</s><s coords="12,225.14,288.38,261.53,8.74;12,126.67,300.33,360.00,8.74;12,126.67,312.29,110.06,8.74">As the most intuitive visualization method, the AM reveals that CNNs learn to detect the important features such as faces, wheels, and bottles without our specification.</s><s coords="12,240.99,312.29,245.68,8.74;12,126.67,324.24,360.00,8.74;12,126.67,336.20,79.88,8.74">At the same time, CNNs attempt to mimic the hierarchical organization of the visual cortex, and then successfully build up the hierarchical feature extraction.</s><s coords="12,210.84,336.20,275.83,8.74;12,126.67,348.15,360.00,8.74;12,126.67,360.11,196.43,8.74">In addition, this visualization method suggests that the individual neurons extract features in a more local manner rather than distributed, which each neuron correspond to a specific pattern.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." coords="12,126.67,384.70,223.44,8.77">Visualization by deconvolutional network.</head><p><s coords="12,140.85,399.52,345.83,8.74;12,126.67,411.47,117.75,8.74">Find the selective pattern from a given input image that activate a specific neuron in the convolutional layers.</s><s coords="12,138.63,563.92,348.04,8.74">The research related to the DeconvNet structure is mainly led by Zeiler et al.</s><s coords="12,126.67,575.88,360.00,8.74;12,126.67,587.83,360.00,8.74;12,126.67,599.79,236.69,8.74">In <ref type="bibr" coords="12,138.94,575.88,14.61,8.74" target="#b72">[73]</ref>, they first proposed the DeconvNet structure aiming to capture certain general features for reconstructing the natural image by projecting a highly diverse set of low-dimension feature maps to high dimension.</s><s coords="12,369.62,599.79,117.05,8.74;12,126.67,611.74,360.00,8.74;12,126.67,623.70,360.00,8.74;12,126.67,635.65,24.99,8.74">Later in <ref type="bibr" coords="12,409.20,599.79,14.61,8.74" target="#b73">[74]</ref>, they utilized the DeconvNet structure to decompose an image hierarchically, which could capture the image information at all scales, from low-level edges to high-level object parts.</s><s coords="12,156.55,635.65,330.12,8.74;12,126.67,647.61,360.00,8.74;12,126.67,659.56,98.35,8.74">Eventually, they applied the DeconvNet structure for CNN visualization by interpreting CNN hidden features <ref type="bibr" coords="12,277.63,647.61,14.61,8.74" target="#b71">[72]</ref>, which made it become an effective method to visualize the CNNs.</s><s coords="12,126.67,680.50,15.50,8.74">4.2.</s><s coords="12,147.90,680.47,74.02,8.77">The algorithm.</s><s coords="12,226.90,680.50,259.76,8.74;12,126.67,692.46,360.00,8.74;12,126.67,704.41,192.92,8.74">The DeconvNet is an effective method to visualize the CNNs, we will explain the DeconvNet based visualization in terms of DeconvNet structure and the visualization process in this section.</s><s coords="13,138.63,417.17,216.61,8.74">Each layer of the DeconvNet is defined as follows:</s></p><p><s coords="13,138.63,429.12,348.04,8.74;13,126.67,441.08,360.00,8.74">Reversed Convolution/Deconvolutional Layer : To explain the deconvolutional layer, we first take a look at the convolutional layers as shown in the top Fig. <ref type="figure" coords="13,463.18,441.08,19.62,8.74" target="#fig_9">7 (a)</ref>.</s><s coords="13,126.67,453.03,360.00,8.74;13,126.67,464.99,360.00,9.65;13,126.67,476.95,219.68,8.74">The convolutional layer transforms the input feature maps into the output feature maps described in Eq. 1: a i,l+1 = F ( w i,l a i,l + b i,l ), where the w, b is the filter parameters and F is the relu activation function.</s><s coords="13,353.14,476.95,133.53,8.74;13,126.67,488.90,360.00,9.65;13,126.67,500.86,248.39,9.65">We combine the convolutional and summing operations of layer l into a single filter matrix f l and convert the multiple feature maps a i,l into a single feature vector A l :</s></p><formula xml:id="formula_13" coords="13,274.40,517.45,212.27,9.65">A l+1 = A l * f l .<label>(11)</label></formula><p><s coords="13,126.67,534.04,263.09,8.74;13,389.77,532.47,3.70,6.12;13,389.77,538.90,2.52,6.12;13,397.50,534.04,52.94,8.74">After appling the relu function, the rectified feature maps A r l is produced.</s><s coords="13,138.63,546.00,348.04,8.74;13,126.67,557.95,360.00,8.74;13,126.67,569.91,205.81,8.74">While in the deconvolutional operation, the reversed convolutional layer, namely the deconvolutional layer, uses the transposed versions of the same convolutional filters to perform the convolutional operations.</s><s coords="13,338.05,569.91,148.62,8.74;13,126.67,581.86,44.34,8.74">The deconvolution process can be viewed as:</s></p><formula xml:id="formula_14" coords="13,272.16,592.20,210.08,12.69">R l = R l+1 * f T l . (<label>12</label></formula><formula xml:id="formula_15" coords="13,482.24,594.27,4.43,8.74">)</formula><p><s coords="13,126.67,608.77,25.12,8.74;13,152.86,607.20,4.71,6.12;13,151.79,613.63,2.52,6.12;13,162.22,608.77,324.45,8.74;13,126.67,620.73,169.36,8.74">The f T l is the transposed versions of the convolutional filters, which is flipped from the filters f horizontally and vertically.</s><s coords="13,300.39,620.73,186.29,9.65;13,126.67,632.68,210.02,8.74;13,337.77,631.11,4.71,6.12;13,336.70,637.54,2.52,6.12;13,344.05,632.68,2.77,8.74">The R l indicates the rectified feature maps in the DeconvNet, which is convolved with the f T l .</s><s coords="13,138.63,644.64,348.04,8.74;13,126.67,656.59,360.00,8.74;13,126.67,668.55,36.31,8.74">Reversed Rectification Layer : The CNNs usually use the relu activation function, which rectifies the feature maps thus ensuring the feature maps are always positive.</s><s coords="13,167.44,668.55,319.23,8.74;13,126.67,680.50,246.18,8.74;13,372.93,678.93,8.82,6.12;13,384.67,680.50,102.00,8.74">The feature maps of deconvolutional layer are also ensured to be positive in reconstruction by passing the unpooled feature maps R up through a relu function.</s></p><p><s coords="13,138.63,692.46,348.05,8.74;13,126.67,704.41,220.88,8.74">Reversed Max-pooling/Unpooling Layer : The reversed max-pooling process in a DeconvNet is implemented by the unpooling layer.</s><s coords="13,352.17,704.41,134.51,8.74;14,126.67,123.98,360.00,8.74;14,126.67,135.93,344.09,8.74;14,470.76,134.36,3.70,6.12;14,470.76,140.79,12.65,6.12;14,483.91,135.93,2.77,8.74;14,126.67,147.89,360.00,8.74;14,126.67,159.84,25.52,8.74">Fig. <ref type="figure" coords="13,372.58,704.41,4.98,8.74" target="#fig_9">7</ref> (b) shows the unpooling process in detail: In order to implement the reversed operation of max-pooling, which performs the downsampling operation on the rectified feature maps A r l+1 , the unpooling layer transform the pooled feature maps to the unpooled feature maps.</s></p><p><s coords="14,138.63,171.80,348.04,8.74;14,126.67,183.75,207.97,8.74">During the max-pooling operation, the positions of maximal values within each pooling window are recorded in switch variables.</s><s coords="14,338.85,183.75,147.82,8.74;14,126.67,195.71,360.00,8.74;14,126.67,207.66,195.50,8.74">The switches first specify the position of which elements in the rectified feature map are copied into the pooled feature map, then mark them as M in the switches.</s><s coords="14,327.76,207.66,158.90,8.74;14,126.67,219.62,360.00,8.74;14,126.67,231.57,37.36,8.74">These switches variables are used in the unpooling operation to place each maximal value back to its original pooled location.</s><s coords="14,168.34,231.57,318.33,8.74;14,126.67,243.53,360.00,8.74;14,126.67,255.48,98.64,8.74">Due to the dimension gap, certain amount of locations are inevitable constructed without certain information, therefore these locations are usually filled by zero for compensation.</s><s coords="14,126.67,273.70,23.25,8.74">4.2.2.</s><s coords="14,155.01,273.70,92.60,8.74">Visualization process.</s><s coords="14,252.59,273.70,234.08,8.74;14,126.67,285.65,247.37,8.74">Based on the reversed structure formed by those layers, the DeconvNet can be well utilized to visualize the CNNs.</s><s coords="14,378.25,285.65,108.42,8.74;14,126.67,297.61,120.30,8.74">The visualization process can be described as follows:</s></p><p><s coords="14,138.63,309.56,348.04,8.74;14,126.67,321.52,124.42,8.74">(1) All neurons' feature maps can be captured when a specific input image is processed through the CNN.</s></p><p><s coords="14,138.63,333.47,348.04,8.74;14,126.67,345.43,194.35,8.74">(2) The feature map of the target neuron for visualization is selected while all other neurons' feature maps are set to zeros.</s></p><p><s coords="14,138.63,357.38,348.04,8.74;14,126.67,369.34,276.33,8.74">(3) In order to obtain the visualized pattern, the target neuron's feature map is projected back to the image dimension through the DeconvNet.</s></p><p><s coords="14,138.63,381.29,348.04,8.74;14,126.67,393.25,315.82,8.74">(4) To visualize all the neurons, this process is applied to all neurons repeatedly and obtain a set of corresponding pattern images for CNN visualization.</s></p><p><s coords="14,138.63,405.20,348.04,8.74;14,126.67,417.16,360.00,8.74;14,126.67,429.11,117.68,8.74">These visualized patterns indicate which pixels or features in the input image contribute to the activation of the neuron, and it also can be used to examine the CNN design shortcomings.</s><s coords="14,251.49,429.11,235.18,8.74;14,126.67,441.07,185.89,8.74">In the next section, these visualized patterns will be demonstrated with practical experiements.</s><s coords="15,138.63,195.71,306.82,8.74">Lower layers (CL1, CL 2) capture the small edges, corners, and parts.</s><s coords="15,450.34,195.71,36.33,8.74;15,126.67,207.66,326.58,8.74">CL3 has more complex invariance, capturing similar textures such as mesh patterns.</s><s coords="15,457.58,207.66,29.09,8.74;15,126.67,219.62,360.00,8.74">Higher layers (CL4, CL5) are more class-specific, which show the almost entire objects.</s><s coords="15,126.67,231.57,360.00,8.74;15,126.67,243.53,250.45,8.74">Compared to the Activation Maximization, the DeconvNet based visualization can provide much more explicit and straightforward patterns.</s><s coords="15,126.67,268.08,23.25,8.74">4.3.2.</s><s coords="15,155.01,268.08,298.54,8.74">DeconvNet based visualization for network analysis and optimization.</s><s coords="15,458.53,268.08,28.14,8.74;15,126.67,280.04,360.00,8.74;15,126.67,291.99,273.39,8.74">Beside the convolutional layer visualization for interpretation analysis, DeconvNet can be also used to examine the CNN design for further optimization.</s></p><p><s coords="15,138.63,303.95,351.84,8.74">Fig. <ref type="figure" coords="15,157.86,303.95,4.98,8.74" target="#fig_11">9</ref> (a) and (c) show the visualization of the first and second layers from AlexNet.</s><s coords="15,126.67,315.90,360.00,8.74;15,126.67,327.86,360.00,8.74;15,126.67,339.81,84.76,8.74">We can find that: 1) There are some "dead" neurons without any specific patterns (indicated in pure gray color) in the first layer, which means they have no activation for the inputs.</s><s coords="15,217.80,339.81,268.88,8.74;15,126.67,351.77,94.18,8.74">This could be a symptom of high learning rates or not good weights initialization.</s><s coords="15,228.69,351.77,257.98,8.74;15,126.67,363.72,148.40,8.74">2) The second layer visualization shows aliasing artifacts, highlighting by the red rectangles.</s><s coords="15,279.43,363.72,207.24,8.74;15,126.67,375.68,118.61,8.74">This could be caused by the large stride used in the first-layer convolutions.</s></p><p><s coords="15,138.63,387.63,348.04,8.74;15,126.67,399.59,19.93,8.74">These findings from the visualization can be well applied to the CNN optimization.</s><s coords="15,152.02,399.59,334.65,8.74;15,126.67,411.54,360.00,8.74;15,126.67,423.50,128.02,8.74">Hence, Zeiler et al. proposed ZFNet, which reduced the first layer filter size and shrink the convolutional stride of AlexNet to retain much more features in the first two convolutional layers.</s></p><p><s coords="15,138.63,435.45,348.04,8.74;15,126.67,447.41,307.17,8.74">The improvement introduced by ZFNet is demonstrated in Fig. <ref type="figure" coords="15,424.60,435.45,4.98,8.74" target="#fig_11">9</ref> (b) and (d), which shows the visualizations of the first and second layers of ZFNet.</s><s coords="15,438.39,447.41,48.28,8.74;15,126.67,459.37,360.00,8.74;15,126.67,471.32,169.73,8.74">We can see that the patterns in the first layer become more distinctive, and the patterns in the second layer have no aliasing artifacts.</s><s coords="15,302.07,471.32,184.60,8.74;15,126.67,483.28,219.90,8.74">Hence, the visualization can be effectively applied in CNN analysis and further optimization.</s><s coords="16,165.54,358.94,321.13,8.74;16,126.67,370.90,149.66,8.74">2) The lower layers (CL1, CL2) converge quickly, since distinguished patterns appear within a few epochs.</s><s coords="16,282.28,370.90,204.39,8.74;16,126.67,382.85,360.00,8.74;16,126.67,394.81,228.78,8.74">3) However, the distinguished patterns appear after a considerable number of epochs in the upper layers (CL4, CL5), which means these layers need to be trained until fully converged.</s></p><p><s coords="16,138.63,406.76,348.04,8.74;16,126.67,418.72,360.00,8.74;16,126.67,430.67,176.93,8.74">Additionally, if noisy patterns are observed in the training process, that could indicate that the network hasn't been trained long enough, or low regularization strength that may result in overfitting.</s><s coords="16,312.62,430.67,174.06,8.74;16,126.67,442.63,360.00,8.74;16,126.67,454.58,85.75,8.74">By visualizing features at several time points during training, we can find the design shortcomings and adjust the network parameters in time.</s><s coords="16,217.45,454.58,269.22,8.74;16,126.67,466.54,216.58,8.74">In general, the visualization of training process is an effective way to monitor and evaluate the training statues.</s><s coords="16,126.67,487.82,15.50,8.74">4.4.</s><s coords="16,147.90,487.79,71.91,8.77">The summary.</s><s coords="16,224.79,487.82,261.88,8.74;16,126.67,499.77,360.00,8.74">The DeconvNet highlights which selected patterns in the input image contribute to the activation of a neuron in a more interpretable manner.</s><s coords="16,126.67,511.73,360.00,8.74;16,126.67,523.68,57.56,8.74">Additionally, this method can be used to examine the problems with the CNNs for optimization.</s><s coords="16,188.65,523.68,298.03,8.74;16,126.67,535.64,345.65,8.74">And the training monitoring could provide the CNN research with a better criteria when adjusting the training configuration and stopping training.</s></p><p><s coords="16,138.63,547.60,348.05,8.74;16,126.67,559.55,360.01,8.74;16,126.67,571.51,87.56,8.74">However, both methods of AM and DeconvNet visualize the CNN in the neuron level, lacking a comprehensive perspective from higher structure, such as layer and whole network.</s><s coords="16,222.52,571.51,264.16,8.74;16,126.67,583.46,360.00,8.74;16,126.67,595.42,282.83,8.74">In the following sections, we will further discuss high-level CNN visualization methods, which interpret each individual layer and visualize the information captured by the set of neurons in a layer as a whole.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." coords="16,126.67,620.49,190.19,8.77">Visualization by network inversion.</head><p><s coords="16,140.85,635.31,345.82,8.74;16,126.67,647.27,344.77,8.74">Reconstruct an image from all the neurons' feature maps in an arbitrary layer to highlight the comprehensive CNN layer-level feature for a given input image.</s></p><p><s coords="16,126.67,668.55,15.50,8.74">5.1.</s><s coords="16,147.90,668.52,71.05,8.77">The overview.</s><s coords="16,223.94,668.55,262.74,8.74;16,126.67,680.50,360.00,8.74;16,126.67,692.46,284.16,8.74">Different from the activation from a single network neuron, the layer-level activation will reveal a comprehensive feature representation, which is composed of the all neuron activation patterns inside a layer.</s><s coords="16,417.40,692.46,69.27,8.74;16,126.67,704.41,360.00,8.74;17,126.67,123.98,360.00,8.74;17,126.67,135.93,297.22,8.74">Hence, different form the aforementioned visualization methods, which visualize the CNN from a single neuron's activation, the Network Inversion based visualization can be used to analysis the activation information from a layer level perspective.</s></p><p><s coords="17,138.63,147.89,348.04,8.74;17,126.67,159.84,360.00,8.74;17,126.67,171.80,360.00,8.74;17,126.67,183.75,360.00,8.74;17,126.67,195.71,261.16,8.74">Before the Network Inversion is applied to visualize the CNNs, the fundamental idea of Network Inversion was proposed to study the traditional computer vision representation, such as the Histogram of Oriented Gradients (HOG) <ref type="bibr" coords="17,438.84,171.80,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="17,453.90,171.80,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="17,471.17,171.80,11.62,8.74" target="#b18">19]</ref>, the Scale Invariant Feature Transform (SIFT) <ref type="bibr" coords="17,335.27,183.75,14.61,8.74" target="#b44">[45]</ref>, the Local Binary Descriptors (LBD) <ref type="bibr" coords="17,158.69,195.71,9.96,8.74" target="#b7">[8]</ref>, and the Bag of Visual Words Descriptors <ref type="bibr" coords="17,358.44,195.71,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="17,372.33,195.71,11.62,8.74" target="#b63">64]</ref>.</s><s coords="17,392.42,195.71,94.25,8.74;17,126.67,207.66,315.62,8.74">Later, two variants of the Network Inversion were proposed for CNN visualization <ref type="bibr" coords="17,391.93,207.66,15.50,8.74" target="#b45">[46,</ref><ref type="bibr" coords="17,410.75,207.66,12.73,8.74" target="#b46">47,</ref><ref type="bibr" coords="17,426.80,207.66,11.62,8.74" target="#b10">11]</ref>:</s></p><p><s coords="17,138.63,219.62,348.04,8.74;17,126.67,231.57,360.00,8.74;17,126.67,243.53,148.97,8.74">(1) Regularizer based Network Inversion: It is proposed by Mahendran et al., which reconstructs the image from each layer by using gradient descent approach and a regularization term <ref type="bibr" coords="17,241.32,243.53,15.50,8.74" target="#b45">[46,</ref><ref type="bibr" coords="17,260.15,243.53,11.62,8.74" target="#b46">47]</ref>.</s></p><p><s coords="17,138.63,255.48,348.04,8.74;17,126.67,267.44,360.00,8.74;17,126.67,279.39,98.28,8.74">(2) UpconvNet based Network Inversion: It is proposed by Dosovitskiy et al. <ref type="bibr" coords="17,471.17,255.48,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="17,126.67,267.44,11.62,8.74" target="#b11">12]</ref>, which reconstructs the image by training a dedicated Up-convolutional Neural Network (UpconvNet).</s></p><p><s coords="17,138.63,291.35,348.04,8.74;17,126.67,303.30,281.19,8.74">Overall, the main goal of both algorithms is to reconstruct the original input image from one whole layer's feature maps' specific activation.</s><s coords="17,415.63,303.30,71.04,8.74;17,126.67,315.26,360.00,8.74;17,126.67,327.21,147.72,8.74">The Regularizer based Network Inversion is easier to be implemented, since it does not require to train an extra dedicated network.</s><s coords="17,279.96,327.21,206.71,8.74;17,126.67,339.17,360.00,8.74;17,126.67,351.12,223.49,8.74">While the UpconvNet based Network Inversion can visualize more existent information in higher layers with an extra dedicated network and significantly more computational cost.</s><s coords="17,138.63,470.45,348.05,8.74;17,126.67,482.41,277.41,8.74">The Regularizer based Network Inversion has the same architecture and parameters as the original CNN before the visualization target layer.</s><s coords="17,410.11,482.41,76.56,8.74;17,126.67,494.36,360.00,9.65;17,126.67,506.32,360.00,9.65;17,126.67,518.27,152.55,8.74">In this case, each pixel of the to be reconstructed image x 0 is adjusted to minimize the objective loss function error between the target feature map A(x 0 ) of x 0 and the feature map A(x) of the original input image x.</s></p><p><s coords="17,138.63,530.23,348.04,8.74;17,126.67,542.18,250.68,8.74">For the UpconvNet based Network Inversion, the UpconvNet provides a inverse path for the feature map back to the image dimension.</s><s coords="17,385.60,542.18,101.07,8.74;17,126.67,554.14,360.00,8.74;17,126.67,566.09,240.71,9.65">The parameters of the UpconvNet are adjusted to minimize the objective loss function error between the reconstructed image x 0 and the original input image x.</s></p><p><s coords="17,138.63,578.05,348.05,8.74;17,126.67,590.00,69.19,8.74">In the following sections, we will give detailed explanations of the two methods mathematically.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="17,126.67,613.69,23.25,8.74">5.2.1.</head><p><s coords="17,155.01,613.69,156.92,8.74">Regularizer based network inversion.</s><s coords="17,316.91,613.69,169.76,8.74;17,126.67,625.64,326.43,8.74;17,453.10,624.07,4.08,6.12;17,461.49,625.64,25.18,8.74;17,126.67,637.60,198.81,8.74">The fundamental algorithm of Regularizer based Network Inversion can be viewed as reconstructing an image x * which minimizes the objective function as following:</s></p><formula xml:id="formula_16" coords="17,218.50,654.54,268.17,18.14">x * = argmin x (C • L(A(x), A(x 0 )) -λ(x)),<label>(13)</label></formula><p><s coords="17,126.67,680.50,360.00,8.74;17,126.67,692.46,127.42,9.65">where the loss function L computes the difference between the two aforementioned feature maps A(x 0 ) and A(x).</s><s coords="17,258.20,692.46,228.47,8.74;17,126.67,704.41,338.41,8.74">The constant C trades off the loss and the regularizer, and the regularizer λ(x) restricts the reconstructed image to a natural image.</s><s coords="17,469.52,704.41,17.16,8.74">The</s></p><formula xml:id="formula_17" coords="18,228.69,347.53,253.55,12.62">L(A(x), A(x 0 )) = A(x) -A(x 0 ) 2 , (<label>14</label></formula><formula xml:id="formula_18" coords="18,482.24,350.50,4.43,8.74">)</formula><p><s coords="18,126.67,368.53,360.00,8.74;18,126.67,380.49,90.30,8.74">which is the most commonly used measurement to evaluate the similarity between different images <ref type="bibr" coords="18,198.71,380.49,14.61,8.74" target="#b68">[69]</ref>.</s></p><p><s coords="18,138.63,392.44,348.05,8.74;18,126.67,404.40,360.00,8.74;18,126.67,416.35,360.00,8.74;18,126.67,428.31,195.92,8.74">In order to make the reconstructed images look closer to the nature images, multiple regularization approaches have been experimentally studied to improve the reconstruction quality, such as α-norm, total variation norm (TV) , jittering, and texture or style regularizers <ref type="bibr" coords="18,271.44,428.31,15.50,8.74" target="#b58">[59,</ref><ref type="bibr" coords="18,290.65,428.31,12.73,8.74" target="#b48">49,</ref><ref type="bibr" coords="18,307.10,428.31,11.62,8.74" target="#b17">18]</ref>.</s><s coords="18,328.21,428.31,158.46,8.74;18,126.67,440.26,49.23,8.74;18,175.98,438.69,21.63,6.12;18,199.18,440.26,116.59,8.74">As an example, for a discrete image data x ⊂ R HXW , the TV norm is given by:</s></p><formula xml:id="formula_19" coords="18,203.69,456.14,278.56,23.84">λ(x) = i,j ((x i,j+1 -x i,j ) 2 + (x i+1,j -x i,j ) 2 ) β 2 , (<label>15</label></formula><formula xml:id="formula_20" coords="18,482.24,460.07,4.43,8.74">)</formula><p><s coords="18,126.67,488.18,360.00,8.74;18,126.67,500.13,84.19,8.74">where the regularizer β =1 stands for the standard TV norm that is mostly used in image denoising.</s><s coords="18,216.13,500.13,270.55,8.74;18,126.67,512.09,160.48,8.74">In this case, the TV norm penalizes the reconstructed images to encourage the spatial smoothness.</s></p><p><s coords="18,138.63,524.04,348.04,8.74;18,126.67,536.00,98.07,8.74">Based on such a Network Inversion framework, the visualization process can be divided into five steps:</s></p><p><s coords="18,138.63,547.95,348.04,8.74;18,126.67,559.91,360.00,9.65;18,126.67,571.86,90.61,8.74">(1) The visualization target layer's feature maps A(x) of the original input image x and the feature maps A(x 0 ) of the to be reconstructed x 0 (initialized with noise) are firstly computed.</s></p><p><s coords="18,138.63,583.82,348.04,9.65;18,126.67,595.78,158.96,8.74">(2) The error between the two feature map sets -L(A(x), A(x 0 )) is then computed by the objective loss function.</s></p><p><s coords="18,138.63,608.77,187.95,8.74;18,331.13,605.98,51.47,6.12">(3) Guided by the direction of the gradient L(A(x),A(x0))</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="18,352.26,614.25,9.20,6.12">∂x</head><p><s coords="18,383.80,608.77,102.88,8.74;18,126.67,620.73,320.75,8.74">, each pixel of the noise image is changed iteratively to minimize the objective loss function error.</s></p><p><s coords="18,138.63,632.68,278.14,8.74;18,416.77,631.11,4.08,6.12;18,421.35,632.68,65.32,8.74;18,126.67,644.64,336.63,8.74">(4) This process terminates at a specific reconstructed image x * , which is used to demonstrate what information is preserve in the visualization target layer.</s></p><p><s coords="18,138.63,656.59,348.05,8.74;18,126.67,668.55,360.00,8.74;18,126.67,680.50,360.00,8.74;18,126.67,692.46,23.88,8.74">The Regularizer based Network Inversion iteratively tweaks the input noise towards the direction that minimizes the difference between the two feature map sets, while the UpconvNet based Network Inversion minimizes the image reconstruction error.</s><s coords="18,156.15,692.46,330.52,8.74;18,126.67,704.41,74.02,8.74">In the next section, we will then discuss the UpconvNet based Network Inversion in detail.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2." coords="19,126.67,123.98,360.00,8.74">UpconvNet based network inversion. Although the Regularizer based Network</head><p><s coords="19,126.67,135.93,360.00,8.74;19,126.67,147.89,280.17,8.74">Inversion can reconstruct a image for CNN layer visualization, it still suffer from relatively slow computation speed due to gradient computation.</s><s coords="19,411.97,147.89,74.71,8.74;19,126.67,159.84,360.00,8.74;19,126.67,171.80,360.00,8.74;19,126.67,183.75,351.32,8.74">To overcome this shortcoming, Dosovitskiy et al. proposed another Network Inversion approach, which trained an extra dedicated Up-convolutional Neural Network (UpconvNet) to reconstruct the image with better image quality and computation efficiency <ref type="bibr" coords="19,459.73,183.75,14.61,8.74" target="#b10">[11]</ref>.</s></p><p><s coords="19,138.63,195.71,348.04,8.74;19,126.67,207.66,246.16,8.74">The UpconvNet can project the low-dimension feature maps back to the image dimension with similar reversed layers as in DeconvNet.</s><s coords="19,378.91,207.66,107.76,8.74;19,126.67,219.62,360.00,8.74;19,126.67,231.57,169.53,8.74">As shown in the bottom part of Fig. <ref type="figure" coords="19,178.60,219.62,8.49,8.74" target="#fig_14">11</ref>, the UpconvNet takes the feature maps A(x) as the input, and yields the reconstructed image as the output.</s></p><p><s coords="19,138.63,243.53,233.38,8.74">Each layer of the UpconvNet are described as follows:</s></p><p><s coords="19,138.63,255.48,352.48,8.74;19,126.67,267.44,330.23,8.74">Reversed Convolutional Layer : The filters are re-trained in the UpconvNet whereas the DeconvNet uses the transposed versions of the same convolutional filters.</s><s coords="19,461.14,267.44,25.54,8.74;19,126.67,279.39,359.99,9.65;19,126.67,291.35,76.16,8.74">Given a training set of images and their feature maps (x i , A(x i )), the training procedure can be viewed as:</s></p><formula xml:id="formula_21" coords="19,220.64,307.68,266.03,21.98">W * = argmin w i ||x i -D(A(x i ), W )|| 2 ,<label>(16)</label></formula><p><s coords="19,126.67,336.87,360.00,8.74;19,126.67,348.82,360.00,9.65">where the weight W of UpconvNet is optimized to minimize the squared Euclidean distance between the input image x i and the output of UpconvNet -D(A(x i ), W ).</s></p><p><s coords="19,138.63,360.78,348.05,8.74;19,126.67,372.73,303.82,8.74">Reversed Rectification Layer : The feature maps of UpconvNet are also ensured to be positive, with the leaky relu nonlinearity of slope 0.2 is applied:</s></p><formula xml:id="formula_22" coords="19,261.91,390.48,224.76,20.69">A(x) = x x 0 0.2 x &lt; 0<label>(17)</label></formula><p><s coords="19,138.63,420.39,348.05,8.74;19,126.67,432.34,29.34,8.74">Reversed Max-pooling Layer : The unpooling layers in UpconvNet are quite simplified.</s><s coords="19,160.32,432.34,326.35,8.74;19,126.67,444.30,360.00,8.74;19,126.67,456.25,58.98,8.74">The feature maps are upsampled by a factor of 2, which replaces each value by a 2 × 2 block with the original value in the top left corner and all other entries equal to zero.</s></p><p><s coords="19,138.63,468.21,348.05,8.74;19,126.67,480.16,177.53,8.74">After the training process, we can utilize this UpconvNet to reconstruct any input image without computing the gradients.</s><s coords="19,310.76,480.16,175.91,8.74;19,126.67,492.12,309.34,8.74">Therefore, it dramatically decreases the computational cost and can be applied to various kinds of deep networks.</s><s coords="19,440.09,492.12,46.58,8.74;19,126.67,504.07,351.90,8.74">In the next section, we will evaluate the visualization results based on these two approaches.</s><s coords="19,138.63,632.68,348.04,8.74;19,126.67,644.64,252.49,8.74">From Fig. <ref type="figure" coords="19,185.70,632.68,8.49,8.74" target="#fig_16">12</ref>, we can find that: 1) The visualization from the CLs look similar to the original image, although with increasing fuzziness.</s><s coords="19,385.26,644.64,101.41,8.74;19,126.67,656.59,360.00,8.74;19,126.67,668.55,45.06,8.74">This indicates that the lower layers preserve much more detailed information, such as colors and locations of objects.</s><s coords="19,176.68,668.55,309.99,8.74">2) The visualization quality has an obvious drop from the CLs to FLs.</s><s coords="19,126.67,680.50,360.00,8.74;19,126.67,692.46,245.63,8.74">However, the visualization from higher CLs and even FLs preserve color (Upcon-vNet) and the approximate object location information.</s><s coords="19,377.72,692.46,108.95,8.74;19,126.67,704.41,360.00,8.74;20,126.67,123.98,52.70,8.74">3) The UpconvNet based visualization quality is better than the Regularizer based visualization, especially for the FLs.</s><s coords="20,185.10,123.98,301.57,8.74;20,126.67,135.93,49.62,8.74">4) The unrelated information is gradually filtered from low layers to high layers.</s><s coords="20,138.63,433.94,348.04,8.74;20,126.67,445.89,230.74,8.74">(2) Dropout: 50% of the feature maps' values are set to be zeros and then normalized to keep their Euclidean norm unchanged.</s></p><p><s coords="20,138.63,457.85,348.05,8.74;20,126.67,469.80,81.35,8.74">Fig. <ref type="figure" coords="20,159.28,457.85,9.96,8.74" target="#fig_17">13</ref> shows the reconstructed images under the two perturbation approaches in different layers.</s><s coords="20,216.83,469.80,269.84,8.74;20,126.67,481.76,360.00,8.74;20,126.67,493.71,319.95,8.74">From Fig. <ref type="figure" coords="20,266.27,469.80,9.96,8.74" target="#fig_17">13</ref> we can see that: 1) In FL1, the binarization hardly changes the reconstruction quality, which means almost all information about the input image is contained in the pattern of non-zero feature maps.</s><s coords="20,455.77,493.71,30.91,8.74;20,126.67,505.67,213.32,8.74">2) The Dropout changes the reconstructed images a lot.</s><s coords="20,345.31,505.67,141.37,8.74;21,126.67,123.98,360.00,8.74;21,126.67,135.93,360.00,8.74;21,126.67,147.89,166.65,8.74">However, Dosovitskiy et al. also experimentally showed that by Dropouting the 50% least important feature maps could significantly reduce the reconstruction error, which is even better than not applying any Dropout for most layers.</s></p><p><s coords="21,138.63,159.84,348.05,8.74;21,126.67,171.80,360.00,8.74;21,126.67,183.75,274.76,8.74">These observations could be a proof that various CNN compression techniques could achieve optimal performance, such as quantization and filter pruning, due to the considerable amount of redundant information in each layer.</s><s coords="21,405.62,183.75,81.06,8.74;21,126.67,195.71,360.00,8.74;21,126.67,207.66,341.11,8.74">Hence the Network Inversion based Visualization can be used to evaluate the importance of feature maps, and pruning the least important feature maps for network compression.</s><s coords="21,126.67,226.18,15.50,8.74">5.4.</s><s coords="21,147.90,226.15,71.79,8.77">The summary.</s><s coords="21,224.67,226.18,262.00,8.74;21,126.67,238.14,360.00,8.74;21,126.67,250.09,174.15,8.74">The Network Inversion based Visualization projects a specific layer's feature maps back to the image dimension, which provides insights into what features a specific layer would preserve.</s><s coords="21,306.63,250.09,180.04,8.74;21,126.67,262.05,360.00,8.74;21,126.67,274.00,268.63,8.74">Additionally, by perturbing some feature maps for visualization, we can verify the CNN preserved a lot redundant information in each layer, and therefore further optimize the CNN design.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." coords="21,126.67,295.14,194.13,8.77">Visualization by network dissection.</head><p><s coords="21,140.85,309.96,345.83,8.74;21,126.67,321.91,141.31,8.74">Evaluate the correlation between each convolutional neuron or multiple neurons with a specific semantic concept.</s></p><p><s coords="21,126.67,340.44,15.50,8.74">6.1.</s><s coords="21,147.90,340.41,72.71,8.77">The overview.</s><s coords="21,225.58,340.44,261.09,8.74;21,126.67,352.39,360.00,8.74;21,126.67,364.35,62.02,8.74">In previous sections, multiple visualization methods were demonstrated to reveal the visual perceptible patterns that a single neuron or layer could capture.</s><s coords="21,193.15,364.35,293.52,8.74;21,126.67,376.30,239.94,8.74">However, there is still a missing link between the visual perceptible patterns and the clear interpretable semantic concepts.</s></p><p><s coords="21,138.63,388.26,348.04,8.74;21,126.67,400.21,360.00,8.74;21,126.67,412.17,161.10,8.74">Hence, Bau et al. proposed the Network Dissection, which directly associates each convolutional neuron with a specific semantic concept, such as color, textures, materials, parts, objects, and scenes.</s><s coords="21,292.88,412.17,193.79,8.74;21,126.67,424.12,360.00,8.74;21,126.67,436.08,252.18,8.74">The correlation between the neuron and the semantic concept is measured by seeking for the neuron that strongly responses to particular image content with specific semantic concepts.</s><s coords="21,384.98,436.08,101.69,8.74;21,126.67,448.03,360.00,8.74;21,126.67,459.99,137.44,8.74">A heterogeneous image dataset -Borden, provides the images with specific semantic concepts labeled corresponding to local content.</s><s coords="21,271.36,459.99,215.31,8.74;21,126.67,471.94,360.00,8.74;21,126.67,483.90,26.90,8.74">A set of Broden examples are shown in Fig. <ref type="figure" coords="21,473.95,459.99,8.49,8.74" target="#fig_18">14</ref>, in which the semantic concepts are divided into six categories highlighted with red boxes.</s><s coords="21,159.21,483.90,327.46,8.74;21,126.67,495.85,153.66,8.74">Each semantic category may cover various classes, for example, the object category contains plant, train, etc.</s><s coords="21,287.12,495.85,199.55,8.74;21,126.67,507.81,265.44,8.74">At the lower right corner of each example in Fig. <ref type="figure" coords="21,147.07,507.81,8.49,8.74" target="#fig_18">14</ref>, the semantic corresponding neuron is also identified.</s><s coords="21,396.69,507.81,89.98,8.74;21,126.67,519.76,360.00,8.74;21,126.67,531.72,85.36,8.74">We can also see that black masks are introduced to cover the image content that is not related to the assigned semantics.</s><s coords="21,220.32,531.72,266.35,8.74;21,126.67,543.67,80.70,8.74">Here, it is the proposed Network Dissection that generates these black masks.</s><s coords="22,138.63,123.98,348.04,8.74;22,126.67,135.93,221.25,8.74">The development of the Network Inversion progressively connects the semantic concepts to different component levels in a CNN.</s><s coords="22,352.23,135.93,134.45,8.74;22,126.67,147.89,360.00,8.74;22,126.67,159.84,102.18,8.74">The fundamental algorithm of Network Inversion illustrated the correlation between one semantic concept and one individual neurons.</s><s coords="22,234.08,159.84,252.59,8.74;22,126.67,171.80,253.80,8.74">Such a correlation was based on an assumption that each semantic concept can be assigned to a single neuron <ref type="bibr" coords="22,362.21,171.80,14.61,8.74" target="#b22">[23]</ref>.</s><s coords="22,386.58,171.80,100.09,8.74;22,126.67,183.75,360.00,8.74;22,126.67,195.71,360.00,8.74;22,126.67,207.66,67.84,8.74">Later, further Network Inversion works revealed that the feature representation can be distributed, which indicated that one semantic concept could be represented by multiple neurons' combination <ref type="bibr" coords="22,165.58,207.66,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="22,179.01,207.66,11.62,8.74" target="#b74">75]</ref>.</s><s coords="22,198.81,207.66,287.87,8.74;22,126.67,219.62,360.00,8.74;22,126.67,231.57,153.73,8.74">Hence, following <ref type="bibr" coords="22,272.54,207.66,10.33,8.74" target="#b2">[3]</ref>'s paradigm, Fong et al. proposed another Network Inversion approach, namely Net2Vec, which visualized the semantic concepts based on neuron combinations <ref type="bibr" coords="22,262.13,231.57,14.61,8.74" target="#b15">[16]</ref>.</s></p><p><s coords="22,138.63,243.53,348.04,8.74;22,126.67,255.48,69.27,8.74">Both methods provide comprehensive visualization results on interpreting CNN hidden neurons.</s><s coords="22,126.67,276.80,15.50,8.74">6.2.</s><s coords="22,147.90,276.77,75.02,8.77">The algorithm.</s><s coords="22,227.90,276.80,258.77,8.74;22,126.67,288.75,360.00,8.74;22,126.67,300.71,360.00,8.74;22,126.67,312.66,39.91,8.74">In this section, we introduce two Network Dissection methods, one method assigns the semantic concept to each individual neuron, while the other builds the correlation between the neuron combinations and the semantic concepts.</s></p><p><s coords="22,126.67,333.98,23.25,8.74">6.2.1.</s><s coords="22,155.01,333.98,193.97,8.74">Network dissection for the individual neuron.</s><s coords="22,353.97,333.98,132.70,8.74;22,126.67,345.94,360.00,8.74;22,126.67,357.89,131.62,8.74">The algorithm of Network Dissection for the individual neuron evaluates the correlation between each single neuron and the semantic concept.</s><s coords="22,262.77,357.89,223.89,8.74;22,126.67,369.85,206.09,8.74">Specifically, every individual neuron is evaluated as a segmentation task to every semantic concept.</s></p><p><s coords="22,138.63,381.80,191.79,8.74">The evaluating process is shown in Fig. <ref type="figure" coords="22,317.68,381.80,8.49,8.74" target="#fig_19">15</ref>.</s><s coords="22,336.45,381.80,150.22,8.74;22,126.67,393.76,360.00,8.74;22,126.67,405.71,209.23,8.74">The input image fetched from the Broden dataset contains pixel-wise annotations for the semantic concepts, which provides the ground truth segmentation masks.</s><s coords="22,342.15,405.71,144.52,8.74;22,126.67,417.67,315.06,8.74">The target neuron's feature map is upsampled to the resolution of the ground truth segmentation masks.</s><s coords="22,446.65,417.67,40.02,8.74;22,126.67,429.62,360.00,8.74;22,126.67,441.58,271.36,8.74">Then the Network Dissection works by measuring the alignment between the upsampled neuron activation map and the ground truth segmentation masks.</s><s coords="22,402.41,441.58,84.26,8.74;22,126.67,453.53,360.00,8.74;22,126.67,465.49,111.91,8.74">If the measurement result is larger than a threshold, the neuron can be viewed as a visual detector for specific semantic concept.</s><s coords="22,138.63,477.44,177.78,8.74;23,138.63,123.98,348.04,9.65;23,126.67,135.93,122.40,8.74">This process can be described as follows: (1) The feature map A f (x) of every neuron f is computed by feeding in every input image x from Broden.</s><s coords="23,254.36,135.93,232.31,9.65;23,126.67,147.89,255.62,8.74">So, the distributions of the activation scores p(a k ) of neuron activation over all images in Broden are computed.</s></p><p><s coords="23,138.63,159.84,348.04,8.74;23,126.67,171.80,360.00,8.74;23,126.67,183.75,88.97,9.65">(2) The top activation maps from all feature maps are selected as valid map regions corresponding to neuron's semantics by setting an activation threshold that P (a f &gt; T f ) = 0.005.</s></p><p><s coords="23,138.63,195.71,348.04,8.74;23,126.67,207.66,291.09,9.65">(3) To match the low-resolution valid map to the ground truth segmentation mask L c for some semantic concept c, the valid map is upsampled:</s></p><formula xml:id="formula_23" coords="23,252.12,226.08,234.55,9.65">M f (x) = S(A f (x) &gt; T f ),<label>(18)</label></formula><p><s coords="23,126.67,244.51,216.87,8.74">where S denotes a bilinear interpolation function.</s></p><p><s coords="23,138.63,256.46,348.04,8.74;23,126.67,268.42,171.22,8.74">(4) The accuracy of neuron f in detecting semantic concept c is determined by the intersection-over-union (IoU) score:</s></p><formula xml:id="formula_24" coords="23,242.78,285.85,243.89,23.22">IoU f,c = |M f (x) L c (x)| |M f (x) L c (x)| ,<label>(19)</label></formula><p><s coords="23,126.67,317.21,360.00,9.65;23,126.67,329.16,26.89,8.74">where L c (x) denotes the ground-truth mask of the semantic concept c on the image x.</s><s coords="23,160.14,329.16,326.53,9.65;23,126.67,341.12,126.33,8.74">If IoU f,c is larger than a threshold (0.04), we consider the neuron f as a visual detector for concept c.</s><s coords="23,257.54,341.12,229.14,8.74;23,126.67,353.07,85.98,8.74">The IoU score indicates the accuracy of neuron f in detecting concept c.</s><s coords="23,216.95,353.07,269.72,8.74;23,126.67,365.03,175.79,8.74">Finally, every neuron's corresponding semantic concept can be determined by calculating its IoU score.</s><s coords="23,138.63,376.98,348.04,8.74;23,126.67,388.94,214.56,8.74">Hence, the Network Dissection for the individual neuron can automatically assign a semantic concept to each convolutional neuron.</s><s coords="23,126.67,411.44,23.25,8.74">6.2.2.</s><s coords="23,155.01,411.44,211.32,8.74">Network dissection for the neuron combinations.</s><s coords="23,371.31,411.44,115.36,8.74;23,126.67,423.39,360.01,8.74;23,126.67,435.35,271.68,8.74">Instead of interpreting the individual neuron, Fong et al. <ref type="bibr" coords="23,264.60,423.39,15.50,8.74" target="#b15">[16]</ref> proposed Net2Vec to evaluate the correlation between the neuron combinations and the semantic concepts.</s><s coords="23,404.68,435.35,81.99,8.74;23,126.67,447.30,360.00,8.74">They implemented this approach as a segmentation task by using convolutional neuron combinations.</s><s coords="23,126.67,459.26,360.00,8.74;23,126.67,471.21,74.25,8.74">Specifically, a learnable concept weight w is used to linearly combine the threshold based activation.</s><s coords="23,207.92,471.21,278.76,8.74;23,126.67,483.17,223.25,8.74">And, it is passed through the sigmoid function σ(x) = 1/(1 + exp(-x)) to predict a segmentation mask M (x; w):</s></p><formula xml:id="formula_25" coords="23,225.11,503.59,257.13,20.14">M (x; w) = σ( k w k • I(A f (x) &gt; T f )), (<label>20</label></formula><formula xml:id="formula_26" coords="23,482.24,503.59,4.43,8.74">)</formula><p><s coords="23,126.67,531.50,336.26,9.30">where k is the number of neurons in a layer, and I(•) is the indicator function.</s><s coords="23,467.25,531.50,19.43,8.74;23,126.67,543.46,360.00,8.74;23,126.67,555.41,43.26,8.74">This function selects a subset of neurons in one layer whose activation is larger than the threshold.</s><s coords="23,177.46,555.41,309.22,8.74;23,126.67,567.37,116.42,8.74">Hence, this subset of neurons can be used to generate the activation mask for specific semantic.</s></p><p><s coords="23,138.63,579.32,348.05,8.74;23,126.67,591.28,360.00,8.74;23,126.67,603.23,133.08,8.74">Fong et al experimentally found that, for the segmentation task, materials and parts reached near optimal performance around k = 8, which was much more quickly than that of objects k = 16.</s><s coords="23,268.81,603.23,217.86,8.74;23,126.67,615.19,360.00,8.74;23,126.67,627.14,79.15,8.74">For each concept c, the weights w were learned using stochastic gradient descent with momentum to minimize per-pixel binary cross entropy loss.</s></p><p><s coords="23,138.63,639.10,348.05,9.65;23,126.67,651.05,77.82,8.74">Similar to the single neurons case, the IoU com score for neuron combinations is computed as well:</s></p><formula xml:id="formula_27" coords="23,236.64,663.74,245.61,23.23">IoU com = |M f,w (x) L c (x)| |M f,w (x) L c (x)| . (<label>21</label></formula><formula xml:id="formula_28" coords="23,482.24,670.48,4.43,8.74">)</formula><p><s coords="23,126.67,692.46,360.00,9.65;23,126.67,704.41,146.72,8.74">If the IoU com score is larger than a threshold, we consider this neuron combinations as a visual detector for concept c.</s><s coords="24,138.63,350.38,348.05,8.74;24,126.67,362.33,150.47,8.74">In fact, using the learned weights to combine neurons outperforms using a single neuron on the segmentation tasks.</s><s coords="24,282.19,362.33,204.48,8.74;24,126.67,374.29,270.73,8.74">As a result, the generated segmentation masks demonstrate more complete and obvious objects in the image.</s><s coords="24,401.89,374.29,84.79,8.74;24,126.67,386.25,287.40,8.74">In the next section, we demonstrate the visualization results by these two approaches.</s><s coords="24,397.18,482.79,89.49,8.74;24,126.67,494.75,360.00,8.74;24,126.67,506.70,35.73,8.74">For each neuron, the top left shows the predicted semantic concepts, and the top right shows the neuron number.</s><s coords="24,167.00,506.70,319.68,9.65;24,126.67,518.66,193.57,8.74">As mentioned, if the IoU com score is larger than a threshold, we consider this neuron as a visual detector for concept c.</s><s coords="24,324.43,518.66,162.25,8.74;24,126.67,530.61,360.00,8.74;24,126.67,542.57,99.43,8.74">Each layer's visual detector number is summarized in the left part of Fig. <ref type="figure" coords="24,279.02,530.61,8.49,8.74" target="#fig_23">17</ref>, which counts the number of unique concepts matched with neurons.</s></p><p><s coords="24,138.63,554.52,348.05,8.74;24,126.67,566.48,223.27,8.74">From the figures, we can find that: 1) Every image highlights the regions that cause the high neural activation from a real image.</s><s coords="24,354.82,566.48,131.85,8.74;24,126.67,578.43,152.90,8.74">2) The predicted labels match the highlighted regions pretty well.</s><s coords="24,284.43,578.43,202.25,8.74;24,126.67,590.39,360.00,8.74;24,126.67,602.34,166.60,8.74">3) From the number of the detector summary, the color concept dominates at lower layers (CL 1 and CL 2), while more object and texture detectors emerge in CL 5.</s></p><p><s coords="24,138.63,614.30,348.04,8.74;24,126.67,626.25,360.00,8.74;24,126.67,638.21,107.70,8.74">Compared with previous visualization methods, we conclude that the CNNs could detect the basic information, such as color and texture by all layer neurons rather than lower layer neurons.</s><s coords="24,238.53,638.21,248.14,8.74;24,126.67,650.16,278.60,8.74">And the color information can be preserved even in higher layers, since many color detectors are also found in these layers.</s><s coords="26,138.63,159.84,348.04,8.74;26,126.67,171.80,360.00,8.74;26,126.67,183.75,199.98,8.74">With such an evaluation, we can find that the Network Dissection based Visualization could effectively applied into evaluating different CNN optimization methods with a perspective of network interpretability.</s><s coords="26,126.67,203.01,23.25,8.74">6.3.3.</s><s coords="26,155.01,203.01,217.14,8.74">Network dissection for the neuron combinations.</s><s coords="26,377.13,203.01,109.54,8.74;26,126.67,214.97,214.84,8.74">The visualization results by using combined neurons are shown in Fig. <ref type="figure" coords="26,328.78,214.97,8.49,8.74" target="#fig_23">18</ref>.</s><s coords="26,346.56,214.97,140.11,8.74;26,126.67,226.92,360.00,8.74;26,126.67,238.88,174.00,8.74">The first and third rows are the segmentation results by the individual neuron, while the second and fourth rows are segmented by neuron combinations.</s><s coords="26,306.37,238.88,180.30,8.74;26,126.67,250.83,360.00,8.74;26,126.67,262.79,268.46,8.74">As we can see, for semantic visualization of "dog" and "airplane" using the weighted combination method, the predicted masks are informative and salient for most of the examples.</s><s coords="26,402.22,262.79,84.46,8.74;26,126.67,274.74,360.00,8.74;26,126.67,286.70,189.46,8.74">This suggests that, although neurons that are specific to a concept can be found, these do not optimally represented or fully cover with the concept.</s><s coords="26,126.67,305.96,15.50,8.74">6.4.</s><s coords="26,147.90,305.93,70.67,8.77">The summary.</s><s coords="26,223.55,305.96,270.29,8.74;26,126.67,317.91,360.00,8.74;26,126.67,329.87,36.61,8.74">The Network Dissection is a distinguished visualization method to interpret the CNNs, which can automatically assign semantic concepts to internal neurons.</s><s coords="26,167.40,329.87,319.27,8.74;26,126.67,341.82,360.00,8.74;26,126.67,353.78,299.85,8.74">By measuring the alignment between the unsampled neuron activation and the ground truth images with semantic labels, Network Dissection can visualize the types of semantic concepts represented by each convolutional neuron.</s><s coords="26,430.84,353.78,55.83,8.74;26,126.67,365.73,281.65,8.74">The Net2Vec also verifies that the CNNs feature representation is distributed.</s><s coords="26,413.16,365.73,73.51,8.74;26,126.67,377.69,360.00,8.74;26,126.67,389.64,360.00,8.74;26,126.67,401.60,202.61,8.74">Additionally, the Network Dissection can be utilized to evaluate various training conditions, which shows the training conditions can have a significant effect on the interpretability of the representation learned by hidden neurons.</s><s coords="26,335.09,401.60,151.59,8.74;26,126.67,413.55,240.02,8.74">Hence, it is another representative example for CNN visualization and CNN optimization.</s><s coords="26,126.67,435.74,170.83,8.77">7. CNN visualization application.</s><s coords="26,302.48,435.77,184.19,8.74;26,126.67,447.73,169.55,8.74">In this section, we review some practical applications of the CNN visualization.</s><s coords="26,302.77,447.73,183.90,8.74;26,126.67,459.68,359.99,8.74;26,126.67,471.64,233.51,8.74">In fact, due to its ability to interpret the CNNs, CNN visualization has became an effective tools to reveal the differences between the way CNNs and humans recognize objects.</s><s coords="26,364.38,471.64,122.29,8.74;26,126.67,483.59,276.88,8.74;27,126.67,123.98,283.43,8.74">We also applied the Network Inversion into an art generation algorithm called style transfer.  of the state-of-the-art CNNs with a successful rate of 99.99% <ref type="bibr" coords="27,391.84,123.98,14.61,8.74" target="#b50">[51]</ref>.</s><s coords="27,414.38,123.98,72.28,8.74;27,126.67,135.93,360.00,8.74">Hence, questions naturally arise as what differences remain between the CNNs and the human vision.</s></p><p><s coords="27,138.63,147.89,348.05,8.74;27,126.67,159.84,28.34,8.74">The Activation Maximization can be well utilized to examine those adversarial noises.</s><s coords="27,161.75,159.84,324.91,8.74;27,126.67,171.80,272.15,8.74">As shown in the Fig. <ref type="figure" coords="27,259.67,159.84,8.49,8.74" target="#fig_24">19</ref>, the adversarial noises are generated by directly maximizing the final layer output for classes via gradient ascent.</s><s coords="27,402.90,171.80,83.77,8.74;27,126.67,183.75,256.48,8.74">And continues until the CNNs confidence for the target class reaches 99.99%.</s><s coords="27,390.67,183.75,96.01,8.74;27,126.67,195.71,360.00,8.74;27,126.67,207.66,210.40,8.74">Adding regularization makes images more recognizable, still far away from human interpretable images, but results have slightly lower confidence scores.</s></p><p><s coords="27,138.63,219.62,348.04,8.74;27,126.67,231.57,360.00,8.74;27,126.67,243.53,77.81,8.74">CNNs recognize these adversarial noises as near-perfect examples of recognizable images, which indicates that the differences between the way CNNs and humans recognize objects.</s><s coords="27,212.10,243.53,274.58,8.74;27,126.67,255.48,62.51,8.74">Although CNNs are now being used for a variety of machine learning tasks.</s><s coords="27,193.45,255.48,293.22,8.74;27,126.67,267.44,200.57,8.74">It is required to understand the generalization capabilities of CNNs, and find potential ways to make them robust.</s><s coords="27,332.05,267.44,154.62,8.74;27,126.67,279.39,261.85,8.74">And the visualization is an optimal way to directly interpret those adversarial threat potentials.</s><s coords="28,126.67,297.51,352.30,8.74">These feature maps in higher layers can be referred as the style of an image <ref type="bibr" coords="28,460.70,297.51,14.61,8.74" target="#b16">[17]</ref>.</s></p><p><s coords="28,138.63,309.47,348.04,8.74;28,126.67,321.42,293.28,8.74">As shown in Fig. <ref type="figure" coords="28,213.69,309.47,8.49,8.74" target="#fig_25">21</ref>, the style transfer generates the output image that combines the style of a painting image a with the content of a photograph p.</s><s coords="28,424.56,321.42,62.11,8.74;28,126.67,333.38,282.62,8.74">We can utilize the Network Inversion visualize the style of the painting image.</s><s coords="28,415.39,333.38,71.27,8.74;28,126.67,345.33,184.73,8.74">The process can be described viewed as jointly minimizing:</s></p><p><s coords="28,191.08,361.93,231.18,9.65">L total = αL content (A(p), A(x)) + βL style (A(a), A(x)),</s></p><p><s coords="28,126.67,378.53,359.99,8.74;28,126.67,390.49,210.75,8.74">where the A(p) is the feature maps of the photograph in one layer and A(a) is the feature maps painting image in multiple layers.</s><s coords="28,344.48,390.49,41.86,8.74;28,391.73,388.60,5.18,6.12;28,391.88,395.96,4.52,6.12;28,402.30,390.49,84.37,8.74;28,126.67,403.19,360.00,8.74;28,126.67,415.15,119.06,8.74">The ratio α β of content loss and style loss adjust the emphasis on matching the content of the photograph or the style of the painting image.</s></p><p><s coords="28,138.63,427.10,348.04,8.74;28,126.67,439.06,360.00,8.74;28,126.67,451.01,249.59,8.74">This process renders the photograph in the style of the painting image, which the appearance of the output image resembles the style of painting image, and the output image shows the same content as the photograph.</s><s coords="28,126.67,469.86,15.50,8.74">7.4.</s><s coords="28,147.90,469.83,51.34,8.77">Summary.</s><s coords="28,204.22,469.86,282.45,8.74;28,126.67,481.82,360.00,8.74">In this section, we briefed several applications of the CNN visualization beyond the scope of CNN interpretability enhancement and optimization.</s><s coords="28,126.67,493.77,289.88,8.74">While more visualization applications still remained undiscussed.</s><s coords="28,424.03,493.77,62.63,8.74;28,126.67,505.73,360.00,8.74;28,126.67,517.68,41.34,8.74">We do believe that the visualization could contribute to the CNN analysis in more and more perspectives.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8." coords="28,126.67,539.29,71.46,8.77">Conclusion.</head><p><s coords="28,203.12,539.32,283.56,8.74;28,126.67,551.27,125.30,8.74">In this paper, we have reviewed the latest developments of the CNN visualization methods.</s><s coords="28,259.89,551.27,226.78,8.74;28,126.67,563.23,359.99,8.74;28,126.67,575.18,281.86,8.74">Four representative visualization methods are delicately presented, in terms of structure, algorithm, operation, and experiment, to cover the state-of-the-art research results of CNN interpretation.</s></p><p><s coords="28,138.63,587.14,348.05,8.74;28,126.67,599.09,360.00,8.74;28,126.67,611.05,232.23,8.74">Trough the study of the representative visualization methods, we can tell that: The CNNs do have hierarchical feature representation mechanism that imitates the hierarchical organization of the human visual cortex.</s><s coords="28,364.38,611.05,122.29,8.74;28,126.67,623.00,360.00,8.74;28,126.67,634.96,118.24,8.74">Also, to reveal the CNN interpretation, the visualization works need to take various perspectives regarding different CNN components.</s><s coords="28,249.19,634.96,237.48,8.74;28,126.67,646.91,336.35,8.74">Moreover, the better interpretability of the CNN introduced by visualization could practically contribute to the CNN optimization.</s></p><p><s coords="28,138.63,658.87,348.04,8.74;28,126.67,670.82,360.00,8.74;28,126.67,682.78,144.57,8.74">Hence, as the CNNs continually dominate the computer vision related tasks, the CNN visualization would play a more and more important role for better understanding and utilizing the CNNs.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,233.73,700.38,145.88,9.30"><head>Figure 1 .</head><label>1</label><figDesc><div><p><s coords="3,233.73,702.07,46.62,7.61">Figure 1.</s><s coords="3,285.33,700.38,94.28,8.74">CaffeNet architecture</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,194.88,269.62,223.60,9.30"><head>Figure 2 .</head><label>2</label><figDesc><div><p><s coords="4,194.88,269.62,223.60,9.30">Figure 2. Convolutional and max-pooling process</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,197.35,262.42,218.64,9.30"><head>Figure 3 .</head><label>3</label><figDesc><div><p><s coords="6,197.35,262.42,218.64,9.30">Figure 3. Human vision and CNNs visualization</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,145.97,298.42,320.65,9.30"><head>3. 3 .Figure 4 .</head><label>34</label><figDesc><div><p><s coords="10,145.97,298.42,320.65,9.30">Figure 4. First layer of CaffeNet visualized by Activation Maximization</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,155.01,680.50,331.66,8.74;10,126.67,692.46,360.00,8.74;10,126.67,704.41,271.24,8.74"><head/><label/><figDesc><div><p><s coords="10,155.01,680.50,108.14,8.74">Final layer visualization.</s><s coords="10,268.14,680.50,218.53,8.74;10,126.67,692.46,308.10,8.74">The final layer of the CaffeNet contains 1000 neurons, which corresponding to the 1000 classes in the ImageNet dataset.</s><s coords="10,439.12,692.46,47.55,8.74;10,126.67,704.41,271.24,8.74">Five different neurons are selected to show the visualized pattern image.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,131.76,407.43,349.82,9.30;11,127.95,419.39,357.44,8.74;11,268.77,431.34,75.80,8.74;11,156.67,330.08,57.60,57.60"><head>Figure 5 .</head><label>5</label><figDesc><div><p><s coords="11,131.76,407.43,349.82,9.30">Figure 5. Hidden layers of CaffeNet visualization by Activation Maximization.</s><s coords="11,127.95,419.39,357.44,8.74;11,268.77,431.34,75.80,8.74">Adapted from "Understanding Neural Networks Through Deep Visualization," by J. Yosinski, 2015.</s></p></div></figDesc><graphic coords="11,156.67,330.08,57.60,57.60" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="11,138.63,468.45,348.05,8.74;11,126.67,480.41,360.00,8.74;11,126.67,492.36,360.00,8.74;11,126.67,504.32,360.00,8.74;11,287.48,540.35,61.20,61.20"><head>Fig. 6 Figure 6 .</head><label>66</label><figDesc><div><p><s coords="11,138.63,468.45,348.05,8.74;11,126.67,480.41,160.81,8.74">Fig. 6 compares the visualized patterns synthesized by the AM and the DGN-AM for five different classes in FL 3.</s><s coords="11,292.29,480.41,194.37,8.74;11,126.67,492.36,360.00,8.74;11,126.67,504.32,360.00,8.74">For the AM shown in the first row of Fig. 6, although we can guess which class the visualized patterns represents, there are multiple duplicate and vague objects in the each visualized pattern, such as three</s></p></div></figDesc><graphic coords="11,287.48,540.35,61.20,61.20" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="12,126.67,432.38,360.00,8.77;12,126.67,444.37,360.00,8.74;12,126.67,456.32,360.00,8.74;12,126.67,468.28,360.00,8.74;12,126.67,480.23,360.00,8.74;12,126.67,492.19,360.00,8.74;12,126.67,504.14,360.00,8.74;12,126.67,516.10,360.00,8.74;12,126.67,528.06,360.00,8.74;12,126.67,540.01,360.00,8.74;12,126.67,551.97,64.76,8.74"><head>4. 1 .</head><label>1</label><figDesc><div><p><s coords="12,147.90,432.38,69.56,8.77">The overview.</s><s coords="12,222.44,432.41,264.23,8.74;12,126.67,444.37,360.00,8.74;12,126.67,456.32,347.34,8.74">While the Activation Maximization interprets the CNNs from the perspective of the neurons, the Deconvolutional Network (DeconvNet) based CNN visualization explains the CNNs from the perspective of the input image.</s><s coords="12,479.20,456.32,7.47,8.74;12,126.67,468.28,360.00,8.74;12,126.67,480.23,118.16,8.74">It finds the selective patterns from the input image that activate a specific neuron in the convolutional layers.</s><s coords="12,250.01,480.23,236.66,8.74;12,126.67,492.19,284.04,8.74">The patterns are reconstructed by projecting the lowdimension neurons' feature maps back to the image dimension.</s><s coords="12,418.70,492.19,67.96,8.74;12,126.67,504.14,360.00,8.74;12,126.67,516.10,360.00,8.74;12,126.67,528.06,110.42,8.74">This projection process is implemented by a DeconvNet structure, which contains deconvolutional layers and unpooling layers, performing the inversed computation of the convolutional and pooling layers.</s><s coords="12,241.52,528.06,245.15,8.74;12,126.67,540.01,360.00,8.74;12,126.67,551.97,64.76,8.74">Rather than purely analyzing the neurons' interests, the DeconvNet based visualization demonstrates a straightforward feature analysis in an image level.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="13,180.49,305.62,251.30,9.30"><head>Figure 7 .</head><label>7</label><figDesc><div><p><s coords="13,180.49,307.30,46.62,7.61">Figure 7.</s><s coords="13,232.09,305.62,199.70,8.74">The structure of the Deconvolutional Network</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="14,206.26,700.38,199.88,9.30"><head>4. 3 . 1 CLFigure 8 .</head><label>318</label><figDesc><div><p><s coords="14,206.26,700.38,199.88,9.30">Figure 8. CaffeNet visualized by DeconvNet</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="15,152.03,676.47,309.29,9.30;15,127.29,688.43,358.76,8.74;15,280.09,700.38,53.16,8.74;15,215.75,518.96,123.68,124.33"><head>Figure 9 .</head><label>9</label><figDesc><div><p><s coords="15,152.03,676.47,309.29,9.30">Figure 9. First and second layer visualization of AlexNet and ZFNet.</s><s coords="15,127.29,688.43,358.76,8.74;15,280.09,700.38,53.16,8.74">Adapted from "Visualizing and Understanding Convolutional Networks," by M.D. Zeiler, 2014.</s></p></div></figDesc><graphic coords="15,215.75,518.96,123.68,124.33" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="16,189.41,219.22,233.57,9.30;16,136.49,231.18,340.36,8.74;16,267.29,243.13,78.76,8.74;16,185.33,126.11,65.96,57.15"><head>Figure 10 .</head><label>10</label><figDesc><div><p><s coords="16,189.41,220.91,52.12,7.61">Figure 10.</s><s coords="16,246.52,219.22,176.47,8.74;16,136.49,231.18,2.77,8.74">Feature evolution during training ZFNet .</s><s coords="16,143.68,231.18,333.17,8.74;16,267.29,243.13,78.76,8.74">Adapted from "Visualizing and Understanding Convolutional Networks," by M.D. Zeiler, 2014.</s></p></div></figDesc><graphic coords="16,185.33,126.11,65.96,57.15" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="17,126.67,374.78,360.00,8.77;17,126.67,386.76,360.00,8.74;17,126.67,398.72,100.24,8.74;17,138.63,410.68,348.04,8.74;17,126.67,422.63,360.00,8.74;17,126.67,434.59,360.00,8.74;17,126.67,446.54,360.00,8.74;17,126.67,458.50,245.66,8.74"><head>5. 2 .</head><label>2</label><figDesc><div><p><s coords="17,147.90,374.78,75.82,8.77">The algorithm.</s><s coords="17,228.70,374.81,257.98,8.74;17,126.67,386.76,360.00,8.74;17,126.67,398.72,100.24,8.74">In this section, we compared the aforementioned two Network Inversion based Visualization methods regarding the network structure and the learning algorithm.</s><s coords="17,138.63,410.68,348.04,8.74;17,126.67,422.63,360.00,8.74;17,126.67,434.59,360.00,8.74;17,126.67,446.54,360.00,8.74;17,126.67,458.50,245.66,8.74">Fig. 11 shows the network implementation for the two Network Inversion based Visualization methods comparing with the original CNN: the Regularizer based Network Inversion is shown in the upper as denoted in green, and the UpconvNet based Network Inversion is shown in the bottom as denoted in orange, while the original CNN is shown in the middle as denoted in blue.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="18,157.45,299.42,298.45,9.30"><head>Figure 11 .</head><label>11</label><figDesc><div><p><s coords="18,157.45,301.11,52.12,7.61">Figure 11.</s><s coords="18,214.55,299.42,241.35,8.74">The data flow of the two Network Inversion algorithms</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="19,126.67,526.50,360.00,8.77;19,126.67,538.49,360.00,8.74;19,126.67,550.44,360.00,8.74;19,126.67,562.40,360.00,8.74;19,126.67,574.35,296.56,8.74;19,126.67,596.82,360.00,8.74;19,126.67,608.77,360.00,8.74;19,126.67,620.73,219.64,8.74"><head>5. 3 .</head><label>3</label><figDesc><div><p><s coords="19,147.90,526.50,304.97,8.77">Experiments with the network inversion based visualization.</s><s coords="19,457.84,526.53,28.83,8.74;19,126.67,538.49,360.00,8.74;19,126.67,550.44,221.81,8.74">In this section, the experiments of Network Inversion based Visualization is demonstrated based on AlexNet trained with ImageNet dataset.</s><s coords="19,355.20,550.44,131.47,8.74;19,126.67,562.40,360.00,8.74;19,126.67,574.35,296.56,8.74">The experiments demonstrate that Network Inversion based Visualization can not only achieve optimal visualization performance, but can be also utilized enhance the CNN design.</s><s coords="19,126.67,596.82,23.25,8.74">5.3.1.</s><s coords="19,155.01,596.82,147.29,8.74">Layer-level visualization analysis.</s><s coords="19,307.29,596.82,179.38,8.74;19,126.67,608.77,177.04,8.74">Visualization from layer level can reveal what features are preserved by each layer.</s><s coords="19,307.79,608.77,178.88,8.74;19,126.67,620.73,219.64,8.74">Fig. 12, shows Regularizer and UpconvNet based visualization from various layers of AlexNet.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="20,162.54,268.15,288.27,9.30;20,162.54,280.10,288.27,8.74;20,162.54,292.06,288.27,8.74;20,162.54,304.01,22.69,8.74"><head>Figure 12 .</head><label>12</label><figDesc><div><p><s coords="20,162.54,269.83,53.88,7.61">Figure 12.</s><s coords="20,221.40,268.15,229.41,8.74;20,162.54,280.10,118.67,8.74">AlexNet reconstruction by Network Inversion with regularizer and UpconvNet.</s><s coords="20,286.46,280.10,164.35,8.74;20,162.54,292.06,288.27,8.74;20,162.54,304.01,22.69,8.74">Adapted from "Inverting Visual Representations with Convolutional Networks," by A. Dosovitskiy, 2016.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="20,162.54,676.47,288.27,9.30;20,162.54,688.43,288.27,8.74;20,162.54,700.38,203.73,8.74"><head>Figure 13 .</head><label>13</label><figDesc><div><p><s coords="20,162.54,678.16,54.98,7.61">Figure 13.</s><s coords="20,222.51,676.47,228.30,8.74;20,162.54,688.43,25.52,8.74">AlexNet reconstruction by perturbing the feature maps.</s><s coords="20,192.46,688.43,258.35,8.74;20,162.54,700.38,203.73,8.74">Adapted from "Inverting Visual Representations with Convolutional Networks," by A. Dosovitskiy, 2016.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="21,146.14,700.38,320.11,9.30"><head>Figure 14 .</head><label>14</label><figDesc><div><p><s coords="21,146.14,702.07,52.12,7.61">Figure 14.</s><s coords="21,203.25,700.38,263.00,8.74">The Broden images that activate certain neurons in AlexNet</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19" coords="22,162.54,664.52,288.27,9.30;22,162.54,676.47,288.27,8.74;22,162.54,688.43,288.27,8.74;22,162.54,700.38,136.85,8.74;22,128.77,512.25,355.82,127.73"><head>Figure 15 .</head><label>15</label><figDesc><div><p><s coords="22,162.54,666.20,53.60,7.61">Figure 15.</s><s coords="22,221.12,664.52,229.68,8.74;22,162.54,676.47,196.02,8.74">Illustration of network dissection for measuring semantic alignment of neuron in a given CNN.</s><s coords="22,362.09,676.47,88.72,8.74;22,162.54,688.43,288.27,8.74;22,162.54,700.38,136.85,8.74">Adapted from "Network Dissection: Quantifying Interpretability of Deep Visual Representations," by D. Bau, 2017.</s></p></div></figDesc><graphic coords="22,128.77,512.25,355.82,127.73" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20" coords="24,180.74,321.01,251.10,9.30"><head>Figure 16 .</head><label>16</label><figDesc><div><p><s coords="24,180.74,322.70,52.12,7.61">Figure 16.</s><s coords="24,237.85,321.01,194.00,8.74">AlexNet visualization by Network Dissection</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21" coords="24,126.67,404.60,363.85,8.77;24,126.67,416.58,360.00,8.74;24,126.67,428.54,360.00,8.74;24,126.67,440.50,347.92,8.74;24,126.67,458.88,360.00,8.74;24,126.67,470.83,360.00,8.74;24,126.67,482.79,267.41,8.74"><head>6. 3 .</head><label>3</label><figDesc><div><p><s coords="24,147.90,404.60,299.41,8.77">Experiments with the network dissection based visualization.</s><s coords="24,452.29,404.63,38.23,8.74;24,126.67,416.58,360.00,8.74;24,126.67,428.54,169.01,8.74">The Network Dissection can be applied to any CNN using a forward pass without the need for training or computing the gradients.</s><s coords="24,299.79,428.54,186.88,8.74;24,126.67,440.50,347.92,8.74">In this section, we demonstrate the Network Dissection based Visualization results based on AlexNet trained with ImageNet.</s><s coords="24,126.67,458.88,23.25,8.74">6.3.1.</s><s coords="24,155.01,458.88,193.77,8.74">Network dissection for the individual neuron.</s><s coords="24,353.76,458.88,132.91,8.74;24,126.67,470.83,216.63,8.74">The visualization results of the individual neuron is demonstrated in the Fig. 16.</s><s coords="24,348.65,470.83,138.03,8.74;24,126.67,482.79,267.41,8.74">In each column, four individual neurons along with two Broden images are shown in each CL.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22" coords="25,162.54,280.98,288.27,9.30;25,162.54,292.93,121.33,8.74"><head>6. 3 . 2 .Figure 17 .Figure 18 .</head><label>321718</label><figDesc><div><p><s coords="25,162.54,282.66,52.47,7.61">Figure 17.</s><s coords="25,220.00,280.98,230.81,8.74;25,162.54,292.93,121.33,8.74">Semantic concept emerging in each layers and under different training conditions</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23" coords="26,126.67,502.82,359.99,8.77;26,126.67,514.81,360.00,8.74;26,126.67,526.76,360.00,8.74;26,126.67,538.72,360.00,8.74;26,126.67,550.67,360.00,8.74;26,126.67,562.63,360.00,8.74;26,175.96,630.94,143.42,5.78;26,334.66,631.32,91.66,5.83;26,176.44,675.99,29.19,5.40;26,226.22,676.81,8.03,5.42;26,256.27,676.44,23.52,5.40;26,296.93,676.78,14.51,5.40;26,326.79,676.33,105.24,5.78"><head>7. 1 .</head><label>1</label><figDesc><div><p><s coords="26,147.90,502.82,264.55,8.77">Visualization analysis for CNN adversarial noises.</s><s coords="26,417.43,502.85,69.24,8.74;26,126.67,514.81,338.55,8.74">The CNNs has achieved impressive performance on a variety of computer vision related tasks.</s><s coords="26,469.52,514.81,17.16,8.74;26,126.67,526.76,360.00,8.74">The CNNs are able to classify objects in images with even beyond human-level accuracy.</s><s coords="26,126.67,538.72,360.00,8.74;26,126.67,550.67,360.00,8.74;26,126.67,562.63,129.09,8.74">However, we can still produce images with adversarial noises to attack the CNN for classification result manipulation, while the noises are completely unperceivable by the human vision recognition.</s><s coords="26,260.14,562.63,226.53,8.74;26,175.96,630.94,143.42,5.78;26,334.66,631.32,91.66,5.83;26,176.44,675.99,29.19,5.40;26,226.22,676.81,8.03,5.42;26,256.27,676.44,23.52,5.40;26,296.93,676.78,14.51,5.40;26,326.79,676.33,105.24,5.78">Such adversarial noises could manipulate the results Tibetan terrier Golden retriever Brittany spaniel Arctic fox Gorilla Photocopier Screen Chimpanzee Eel Backpack Bikini Cliff dwelling Soccer ball Stopwatch</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24" coords="26,151.62,700.38,310.10,9.30"><head>Figure 19 .</head><label>19</label><figDesc><div><p><s coords="26,151.62,702.07,52.12,7.61">Figure 19.</s><s coords="26,208.73,700.38,252.99,8.74">Adversarial noises that manipulate the CNN classification</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25" coords="27,126.67,297.88,360.00,8.77;27,126.67,309.86,360.00,8.74;27,126.67,321.82,360.00,8.74;27,126.67,333.77,360.00,8.74;27,126.67,345.73,322.38,8.74;27,138.63,357.68,348.05,8.74;27,126.67,369.64,360.00,8.74;27,126.67,381.59,360.00,8.74;27,126.67,393.55,360.00,8.74;27,126.67,405.50,360.00,8.74;27,126.67,417.46,244.03,8.74;27,138.63,429.41,348.04,8.74;27,126.67,441.37,360.00,8.74;27,126.67,453.32,360.00,8.74;27,126.67,465.28,360.00,8.74;27,126.67,477.23,360.00,8.74;27,126.67,489.19,360.00,8.74;27,126.67,501.15,360.00,8.74;27,126.67,513.10,360.00,8.74;27,126.67,525.06,166.88,8.74"><head>7. 2 .</head><label>2</label><figDesc><div><p><s coords="27,147.90,297.88,213.24,8.77">Visualization based adversarial examples.</s><s coords="27,366.12,297.91,120.55,8.74;27,126.67,309.86,360.00,8.74;27,126.67,321.82,25.21,8.74">In this section, we present another analysis approaching to demonstrate how the CNNs and the human vision differ.</s><s coords="27,157.43,321.82,329.24,8.74;27,126.67,333.77,360.00,8.74;27,126.67,345.73,106.41,8.74">Recent studies show that the adversarial noises can be well embedded into images forming adversarial examples, which can manipulate the CNN classification results without noticing.</s><s coords="27,237.51,345.73,211.54,8.74">Some adversarial examples are shown in Fig. 20.</s><s coords="27,138.63,357.68,348.05,8.74;27,126.67,369.64,360.00,8.74;27,126.67,381.59,93.72,8.74">To improve the robustness of the CNNs, the traditional techniques such as batch normalization and Dropout, generally do not provide a practical defense against adversarial examples.</s><s coords="27,227.36,381.59,259.31,8.74;27,126.67,393.55,360.00,8.74;27,126.67,405.50,146.49,8.74">Some other strategies such as adversarial training and defensive distillation has been proposed to defense against the adversarial examples, which achieve state-of-art results.</s><s coords="27,278.02,405.50,208.65,8.74;27,126.67,417.46,244.03,8.74">Yet even these specialized algorithms can easily be broken by giving more delicate adversarial examples.</s><s coords="27,138.63,429.41,348.04,8.74;27,126.67,441.37,81.86,8.74">The visualization can provide a solution to further uncover the mystery of adversarial examples.</s><s coords="27,212.79,441.37,273.88,8.74;27,126.67,453.32,50.28,8.74">The activation maps of four convolutional neurons are shown in the Fig. 20.</s><s coords="27,181.47,453.32,305.20,8.74;27,126.67,465.28,288.01,8.74">We can observe that the visualized feature maps have been changed a lot by the adversarial noises, especially for the CL2-97 and CL5-87.</s><s coords="27,418.90,465.28,67.77,8.74;27,126.67,477.23,360.00,8.74;27,126.67,489.19,227.04,8.74">Hence, even the human vision can hard perceive the adversarial examples, the visualization could provide a significantly effective detection approach.</s><s coords="27,360.47,489.19,126.20,8.74;27,126.67,501.15,360.00,8.74;27,126.67,513.10,360.00,8.74;27,126.67,525.06,166.88,8.74">The visualization analysis of the adversarial examples also reveal another major difference between CNN and the human vision: the imperceptible patterns can be captured by the CNNs and greatly affect the classification results.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26" coords="27,126.67,543.54,360.00,8.77;27,126.67,555.53,360.00,8.74;27,126.67,567.48,360.00,8.74;27,333.79,684.49,25.42,7.38;27,375.14,684.49,21.92,7.38;27,214.14,633.93,24.04,7.38;27,206.94,684.49,39.83,7.38;27,307.12,633.93,36.35,7.38;27,294.02,684.49,25.42,7.38;27,254.72,684.49,21.92,7.38"><head>7. 3 .</head><label>3</label><figDesc><div><p><s coords="27,147.90,543.54,199.65,8.77">Visualization analysis for style transfer.</s><s coords="27,352.53,543.57,134.14,8.74;27,126.67,555.53,360.00,8.74;27,126.67,567.48,248.35,8.74">As we discussed in the Section 5, we can visualize the information each layer preserved from the input image, by reconstructing the image from feature maps in one layer.</s><s coords="27,379.61,567.48,107.06,8.74">And we know the higher</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27" coords="27,204.84,700.38,203.67,9.30"><head>Figure 20 .Figure 21 .</head><label>2021</label><figDesc><div><p><s coords="27,204.84,702.07,52.12,7.61">Figure 20.</s><s coords="27,261.94,700.38,146.56,8.74">Adversarial example visualization</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,142.09,563.91,344.59,146.61"><head/><label/><figDesc><div><p><s coords="6,236.97,563.91,249.71,8.74">. With the rapid developments and implementations of</s></p></div></figDesc><table coords="6,142.09,597.91,326.36,112.61"><row><cell cols="5">CNNs, the visualization has been extended to interpret the overall working mecha-</cell></row><row><cell cols="5">nism of CNNs. In Table 1, we give a brief review of four representative visualization</cell></row><row><cell cols="5">methods, namely Activation Maximization, Deconvolutional Networks (DeconNet),</cell></row><row><cell cols="3">Network Inversion, and Network Dissection:</cell><cell/><cell/></row><row><cell>Method</cell><cell>Interpretation Perspective</cell><cell>Focused Layer</cell><cell>Applied Network</cell><cell>Representative Study</cell></row><row><cell>Activation Maximization</cell><cell>Individual Neuron with visualized pattern</cell><cell>CLs FLs</cell><cell>Auto-Encoder, DBN, AlexNet</cell><cell>[26]</cell></row><row><cell>Deconvolutional Neural Networks</cell><cell>Neuron activation in input image</cell><cell>CLs</cell><cell>AlexNet</cell><cell>[55]</cell></row><row><cell>Network Inversion</cell><cell>One layer</cell><cell>CLs FLs</cell><cell>HOG, SIFT, LBD, Bag of words, CaffeNet</cell><cell>[29][64]</cell></row><row><cell>Network Dissection</cell><cell>Individual Neuron with semantic concept</cell><cell>CLs</cell><cell>AlexNet, VGG, GoogLeNet, ResNet</cell><cell>[32][70]</cell></row><row><cell/><cell cols="3">TABLE 1. Visualization methods</cell><cell/></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments.This work was supported in part by <rs type="funder">NSF</rs> <rs type="grantNumber">CNS-1717775</rs>.</p></div>
<div><head coords="29,178.73,102.11,255.89,6.12">HOW CONVOLUTIONAL NEURAL NETWORKS SEE THE WORLD</head></div>
			</div>
			<div type="funding">
<div><p>The authors are supported by <rs type="funder">NSF</rs> Grant <rs type="grantNumber">CNS-1717775</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_f3FKdmn">
					<idno type="grant-number">CNS-1717775</idno>
				</org>
				<org type="funding" xml:id="_gCCPGsS">
					<idno type="grant-number">CNS-1717775</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="29,144.08,147.87,342.60,6.99;29,144.08,157.83,342.60,6.99;29,144.08,167.79,31.99,6.99" xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10584-0_22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="329" to="344"/>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Agrawal, R. Girshick and J. Malik, Analyzing the performance of multilayer neural networks for object recognition, in Proceedings of the European Conference on Computer Vision, 2014, 329-344.</note>
</biblStruct>

<biblStruct coords="29,144.08,177.75,342.59,7.21" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Wasserstein gan</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">M. Arjovsky, S. Chintala and L. Bottou, Wasserstein gan, arXiv preprint, arXiv:1701.07875.</note>
</biblStruct>

<biblStruct coords="29,144.08,187.72,342.60,6.99;29,144.08,197.68,342.59,6.99;29,144.08,207.64,226.27,6.99" xml:id="b2">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.354</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3327"/>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Bau, B. Zhou, A. Khosla, A. Oliva and A. Torralba, Network dissection: Quantifying interpretability of deep visual representations, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, 3319-3327.</note>
</biblStruct>

<biblStruct coords="29,144.08,217.60,342.59,6.99;29,144.08,227.57,342.59,6.99;29,144.08,237.53,304.38,6.99" xml:id="b3">
	<analytic>
		<title level="a" type="main">Flexible, High performance convolutional neural networks for image classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1237</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">D. C. Ciresan, U. Meier, J. Masci, L. Maria Gambardella and J. Schmidhuber, Flexible, High performance convolutional neural networks for image classification, in Proceedings of the International Joint Conference on Artificial Intelligence, vol. 22, 2011, p1237.</note>
</biblStruct>

<biblStruct coords="29,144.08,247.49,342.59,6.99;29,144.08,257.45,181.25,6.99" xml:id="b4">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on BigLearn, NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Collobert, K. Kavukcuoglu and C. Farabet, Torch7: A matlab-like environment for machine learning, in Workshop on BigLearn, NIPS, 2011.</note>
</biblStruct>

<biblStruct coords="29,144.08,267.42,342.59,6.99;29,144.08,277.38,341.14,6.99" xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2"/>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Csurka, C. Dance, L. Fan, J. Willamowski and C. Bray, Visual categorization with bags of keypoints, in Workshop on statistical learning in computer vision, ECCV, vol. 1, 2004, 1-2.</note>
</biblStruct>

<biblStruct coords="29,144.08,287.34,342.60,6.99;29,144.08,297.31,322.32,6.99" xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2005.177</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893"/>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Dalal and B. Triggs, Histograms of oriented gradients for human detection, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2005, 886-893.</note>
</biblStruct>

<biblStruct coords="29,144.08,307.27,342.59,6.99;29,144.08,317.23,342.59,6.99;29,144.08,327.19,31.99,6.99" xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond bits: Reconstructing images from local binary descriptors</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Angelo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Pattern Recognition</title>
		<meeting>the IEEE Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="935" to="938"/>
		</imprint>
	</monogr>
	<note type="raw_reference">E. d'Angelo, A. Alahi and P. Vandergheynst, Beyond bits: Reconstructing images from local binary descriptors, in Proceedings of the IEEE Conference on Pattern Recognition, 2012, 935-938.</note>
</biblStruct>

<biblStruct coords="29,144.08,337.16,342.59,6.99;29,144.08,347.12,342.59,6.99;29,144.08,357.08,139.24,6.99" xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1486" to="1494"/>
		</imprint>
	</monogr>
	<note type="raw_reference">E. L. Denton, S. Chintala, R. Fergus et al., Deep generative image models using a Lapla- cian pyramid of adversarial networks, in Proceedings of the Advances in Neural Information Processing Systems, 2015, 1486-1494.</note>
</biblStruct>

<biblStruct coords="29,144.08,367.04,342.59,6.99;29,144.08,377.01,342.59,6.99;29,144.08,386.97,54.10,6.99" xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="658" to="666"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Dosovitskiy and T. Brox, Generating images with perceptual similarity metrics based on deep networks, in Proceedings of the Advances in Neural Information Processing Systems, 2016, 658-666.</note>
</biblStruct>

<biblStruct coords="29,144.08,396.93,342.59,6.99;29,144.08,406.89,342.59,6.99;29,144.08,416.86,40.46,6.99" xml:id="b10">
	<analytic>
		<title level="a" type="main">Inverting visual representations with convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4829" to="4837"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Dosovitskiy and T. Brox, Inverting visual representations with convolutional networks, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, 4829-4837.</note>
</biblStruct>

<biblStruct coords="29,144.08,426.82,342.59,6.99;29,144.08,436.78,342.59,6.99;29,144.08,446.74,142.42,6.99" xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298761</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1538" to="1546"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Dosovitskiy, J. Tobias Springenberg and T. Brox, Learning to generate chairs with con- volutional neural networks, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, 1538-1546.</note>
</biblStruct>

<biblStruct coords="29,144.08,456.71,342.59,6.99;29,144.08,466.65,322.35,7.01" xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Duchi, E. Hazan and Y. Singer, Adaptive subgradient methods for online learning and stochastic optimization, Journal of Machine Learning Research, 12 (2011), 2121-2159.</note>
</biblStruct>

<biblStruct coords="29,144.08,476.63,342.60,6.99;29,144.08,486.57,251.87,7.01" xml:id="b13">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note type="raw_reference">D. Erhan, Y. Bengio, A. Courville and P. Vincent, Visualizing higher-layer features of a deep network, Technical report, University of Montreal, 1341 (2009), p3.</note>
</biblStruct>

<biblStruct coords="29,144.08,496.56,342.59,6.99;29,144.08,506.52,342.60,6.99;29,144.08,516.46,152.68,7.01" xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2009.167</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645"/>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. F. Felzenszwalb, R. B. Girshick, D. McAllester and D. Ramanan, Object detection with discriminatively trained part-based models, IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 32 (2010), 1627-1645.</note>
</biblStruct>

<biblStruct coords="29,144.08,526.45,342.60,6.99;29,144.08,536.41,245.71,7.21" xml:id="b15">
	<monogr>
		<title level="m" type="main">Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03454</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">R. Fong and A. Vedaldi, Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks, arXiv preprint, arXiv:1801.03454.</note>
</biblStruct>

<biblStruct coords="29,144.08,546.37,342.59,6.99;29,144.08,556.31,163.69,7.24" xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="DOI">10.1167/16.12.326</idno>
		<idno type="arXiv">arXiv:1508.06576</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">326</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. A. Gatys, A. S. Ecker and M. Bethge, A neural algorithm of artistic style, Journal of Vision, 16 (2016), p326, arXiv:1508.06576.</note>
</biblStruct>

<biblStruct coords="29,144.08,566.30,342.59,6.99;29,144.08,576.24,339.26,7.24" xml:id="b17">
	<monogr>
		<title level="m" type="main">Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07376</idno>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">L. A. Gatys, A. S. Ecker and M. Bethge, Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks, arXiv preprint, arXiv:1505.07376, 12.</note>
</biblStruct>

<biblStruct coords="29,144.08,586.22,342.60,6.99;29,144.08,596.18,304.76,8.76" xml:id="b18">
	<monogr>
		<title level="m" type="main">Discriminatively trained deformable part models</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="http://people.cs.uchicago.edu/~rbg/latent-release5/"/>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">R. B. Girshick, P. F. Felzenszwalb and D. McAllester, Discriminatively trained deformable part models, release 5, http://people.cs.uchicago.edu/ ~rbg/latent-release5/.</note>
</biblStruct>

<biblStruct coords="29,144.08,606.15,342.59,6.99;29,144.08,616.11,342.59,6.99;29,144.08,626.07,177.90,6.99" xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.81</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587"/>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Girshick, J. Donahue, T. Darrell and J. Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, 580-587.</note>
</biblStruct>

<biblStruct coords="29,144.08,636.03,342.59,6.99;29,144.08,646.00,342.59,6.99;29,144.08,655.96,54.10,6.99" xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256"/>
		</imprint>
	</monogr>
	<note type="raw_reference">X. Glorot and Y. Bengio, Understanding the difficulty of training deep feedforward neural net- works, in Proceedings of the International Conference on Artificial Intelligence and Statistics, 2010, 249-256.</note>
</biblStruct>

<biblStruct coords="29,144.08,665.92,342.59,6.99;29,144.08,675.89,342.59,6.99;29,144.08,685.85,31.99,6.99" xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10584-0_26</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="392" to="407"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Gong, L. Wang, R. Guo and S. Lazebnik, Multi-scale orderless pooling of deep convolutional activation features, in Proceedings of the European Conference on Computer Vision, 2014, 392-407.</note>
</biblStruct>

<biblStruct coords="29,144.08,695.81,342.59,6.99;29,144.08,705.75,309.45,7.01" xml:id="b22">
	<analytic>
		<title level="a" type="main">Do semantic parts emerge in convolutional neural networks?</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-017-1048-0</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="476" to="494"/>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Gonzalez-Garcia, D. Modolo and V. Ferrari, Do semantic parts emerge in convolutional neural networks?, International Journal of Computer Vision, 126 (2018), 476-494.</note>
</biblStruct>

<biblStruct coords="30,144.08,125.34,342.59,6.99;30,144.08,135.30,342.59,6.99;30,144.08,145.26,139.24,6.99" xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680"/>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville and Y. Bengio, Generative adversarial nets, in Proceedings of the Advances in Neural Information Processing Systems, 2014, 2672-2680.</note>
</biblStruct>

<biblStruct coords="30,144.08,155.22,342.59,6.99;30,144.08,165.19,342.60,6.99;30,144.08,175.15,120.45,6.99" xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="241" to="257"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Gordo, J. Almazán, J. Revaud and D. Larlus, Deep image retrieval: Learning global representations for image search, in Proceedings of the European Conference on Computer Vision, Springer, 2016, 241-257.</note>
</biblStruct>

<biblStruct coords="30,144.08,185.11,342.59,6.99;30,144.08,195.07,319.47,7.21" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">S. Han, H. Mao and W. J. Dally, Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, arXiv preprint, arXiv:1510.00149.</note>
</biblStruct>

<biblStruct coords="30,144.08,205.04,342.60,6.99;30,144.08,215.00,337.63,6.99" xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778"/>
		</imprint>
	</monogr>
	<note type="raw_reference">K. He, X. Zhang, S. Ren and J. Sun, Deep residual learning for image recognition, in Proceed- ings of the IEEE conference on Computer Vision and Pattern Recognition, 2016, 770-778.</note>
</biblStruct>

<biblStruct coords="30,144.08,224.96,342.60,6.99;30,144.08,234.90,160.93,7.01" xml:id="b27">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.7.1527</idno>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554"/>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. E. Hinton, S. Osindero and Y.-W. Teh, A fast learning algorithm for deep belief nets, Neural computation, 18 (2006), 1527-1554.</note>
</biblStruct>

<biblStruct coords="30,144.08,244.89,342.59,6.99;30,144.08,254.83,342.59,7.24;30,144.08,265.39,95.50,6.64" xml:id="b28">
	<analytic>
		<title level="a" type="main">Receptive fields and functional architecture of monkey striate cortex</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
		<idno type="DOI">10.1113/jphysiol.1968.sp008455</idno>
		<ptr target="http://dx.doi.org/10.1113/jphysiol.1968.sp008455"/>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="page" from="215" to="243"/>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. H. Hubel and T. N. Wiesel, Receptive fields and functional architecture of monkey striate cortex, The Journal of Physiology, 195 (1968), 215-243, URL http://dx.doi.org/10.1113/ jphysiol.1968.sp008455.</note>
</biblStruct>

<biblStruct coords="30,144.08,274.78,342.59,6.99;30,144.08,284.71,180.67,7.01" xml:id="b29">
	<analytic>
		<title level="a" type="main">Receptive fields of single neurones in the cat's striate cortex</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
		<idno type="DOI">10.1113/jphysiol.1959.sp006308</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="574" to="591"/>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. H. Hubel and T. N. Wiesel, Receptive fields of single neurones in the cat's striate cortex, The Journal of physiology, 148 (1959), 574-591.</note>
</biblStruct>

<biblStruct coords="30,144.08,294.70,342.60,6.99;30,144.08,304.66,342.59,6.99;30,144.08,314.63,54.10,6.99" xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456"/>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, in Proceedings of the International Conference on Machine Learning, 2015, 448-456.</note>
</biblStruct>

<biblStruct coords="30,144.08,324.59,342.60,6.99;30,144.08,334.55,342.59,6.99;30,144.08,344.51,54.10,6.99" xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456"/>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, in Proceedings of the International Conference on Machine Learning, 2015, 448-456.</note>
</biblStruct>

<biblStruct coords="30,144.08,354.48,342.60,6.99;30,144.08,364.44,342.59,6.99;30,144.08,374.40,224.36,6.99" xml:id="b32">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2647868.2654889</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia</title>
		<meeting>the International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama and T. Darrell, Caffe: Convolutional architecture for fast feature embedding, in Proceedings of the International Conference on Multimedia, 2014, 675-678.</note>
</biblStruct>

<biblStruct coords="30,144.08,384.34,342.59,7.01;30,144.08,394.33,60.69,6.99" xml:id="b33">
	<analytic>
		<title level="a" type="main">The human visual cortex</title>
		<author>
			<persName coords=""><forename type="first">G.-S</forename><surname>Kalanit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rafael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="649" to="677"/>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G.-S. Kalanit and M. Rafael, The human visual cortex, Annual Review of Neuroscience, 27 (2004), 649-677.</note>
</biblStruct>

<biblStruct coords="30,144.08,404.29,342.60,6.99;30,144.08,414.23,180.24,7.01" xml:id="b34">
	<analytic>
		<title level="a" type="main">Identifying natural images from human brain activity</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">N</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature06713</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="page">352</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. N. Kay, T. Naselaris, R. J. Prenger and J. L. Gallant, Identifying natural images from human brain activity, Nature, 452 (2008), p352.</note>
</biblStruct>

<biblStruct coords="30,144.08,424.22,342.59,6.99;30,144.08,434.18,342.59,6.99;30,144.08,444.14,62.57,6.99" xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1150"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Krizhevsky, I. Sutskever and G. E. Hinton, Imagenet classification with deep convolutional neural networks, in Proceedings of the Advances in Neural Information Processing Systems, 2012, 1097-1150.</note>
</biblStruct>

<biblStruct coords="30,144.08,454.10,342.59,6.99;30,144.08,464.07,342.59,6.99;30,144.08,474.00,342.59,7.01;30,144.08,483.99,19.29,6.99" xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep hierarchies in the primate visual cortex: What can we learn for computer vision?</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lappe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piater</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Rodriguez-Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.272</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1847" to="1871"/>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Kruger, P. Janssen, S. Kalkan, M. Lappe, A. Leonardis, J. Piater, A. J. Rodriguez-Sanchez and L. Wiskott, Deep hierarchies in the primate visual cortex: What can we learn for computer vision?, IEEE Transactions on Pattern Analysis and Machine Intelligence, 35 (2013), 1847- 1871.</note>
</biblStruct>

<biblStruct coords="30,144.08,493.95,342.60,6.99;30,144.08,503.92,105.16,7.21" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<title level="m">Adversarial examples in the physical world</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Kurakin, I. Goodfellow and S. Bengio, Adversarial examples in the physical world, arXiv preprint, arXiv:1607.02533.</note>
</biblStruct>

<biblStruct coords="30,144.08,513.88,342.59,6.99;30,144.08,523.82,224.60,7.01" xml:id="b38">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324"/>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. LeCun, L. Bottou, Y. Bengio and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE , 86 (1998), 2278-2324.</note>
</biblStruct>

<biblStruct coords="30,144.08,533.80,325.91,6.99" xml:id="b39">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<title level="m">The mnist database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. LeCun, C. Cortes and C. J. Burges, The mnist database of handwritten digits, 1998.</note>
</biblStruct>

<biblStruct coords="30,144.08,543.77,342.60,6.99;30,144.08,553.73,342.59,6.99;30,144.08,563.69,342.59,6.99;30,144.08,573.66,67.86,6.99" xml:id="b40">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Te- jani, J. Totz, Z. Wang et al., Photo-realistic single image super-resolution using a generative adversarial network, in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2017.</note>
</biblStruct>

<biblStruct coords="30,144.08,583.62,342.60,6.99;30,144.08,593.58,324.07,6.99" xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparse deep belief net model for visual area v2</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="873" to="880"/>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Lee, C. Ekanadham and A. Y. Ng, Sparse deep belief net model for visual area v2, in Proceedings of the Advances in Neural Information Processing Systems, 2008, 873-880.</note>
</biblStruct>

<biblStruct coords="30,144.08,603.54,342.60,6.99;30,144.08,613.51,342.59,6.99;30,144.08,623.47,183.84,6.99" xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553453</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="609" to="616"/>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Lee, R. Grosse, R. Ranganath and A. Y. Ng, Convolutional deep belief networks for scal- able unsupervised learning of hierarchical representations, in Proceedings of the International Conference on Machine Learning, 2009, 609-616.</note>
</biblStruct>

<biblStruct coords="30,144.08,633.43,342.59,6.99;30,144.08,643.39,264.08,6.99" xml:id="b43">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>context. corr abs/1405.0312</idno>
		<title level="m">Microsoft coco: common objects in</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár and C. Zitnick, Microsoft coco: common objects in context. corr abs/1405.0312 (2014).</note>
</biblStruct>

<biblStruct coords="30,144.08,653.36,342.59,6.99;30,144.08,663.29,144.07,7.01" xml:id="b44">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:VISI.0000029664.99615.94</idno>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110"/>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. G. Lowe, Distinctive image features from scale-invariant keypoints, International journal of computer vision, 60 (2004), 91-110.</note>
</biblStruct>

<biblStruct coords="30,144.08,673.28,342.59,6.99;30,144.08,683.24,342.59,6.99;30,144.08,693.21,40.46,6.99" xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299155</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5188" to="5196"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Mahendran and A. Vedaldi, Understanding deep image representations by inverting them, in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2015, 5188-5196.</note>
</biblStruct>

<biblStruct coords="31,144.08,125.34,342.59,6.99;31,144.08,135.27,286.39,7.01" xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing deep convolutional neural networks using natural pre-images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0911-8</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="233" to="255"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Mahendran and A. Vedaldi, Visualizing deep convolutional neural networks using natural pre-images, International Journal of Computer Vision, 120 (2016), 233-255.</note>
</biblStruct>

<biblStruct coords="31,144.08,145.26,342.60,6.99;31,144.08,155.20,135.80,7.01" xml:id="b47">
	<analytic>
		<title level="a" type="main">When crowding of crowding leads to uncrowding</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manassi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sayim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Herzog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="10" to="10"/>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Manassi, B. Sayim and M. H. Herzog, When crowding of crowding leads to uncrowding, Journal of Vision, 13 (2013), 10-10.</note>
</biblStruct>

<biblStruct coords="31,144.08,165.19,342.59,6.99;31,144.08,175.13,207.12,7.01" xml:id="b48">
	<monogr>
		<title level="m" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tyka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06-20">June, 20 (2015</date>
			<publisher>Google Research Blog</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Mordvintsev, C. Olah and M. Tyka, Inceptionism: Going deeper into neural networks, Google Research Blog. Retrieved June, 20 (2015), 14pp.</note>
</biblStruct>

<biblStruct coords="31,144.08,185.11,342.60,6.99;31,144.08,195.07,342.59,6.99;31,144.08,205.04,262.57,6.99" xml:id="b49">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3387" to="3395"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox and J. Clune, Synthesizing the preferred inputs for neurons in neural networks via deep generator networks, in Proceedings of the Advances in Neural Information Processing Systems, 2016, 3387-3395.</note>
</biblStruct>

<biblStruct coords="31,144.08,215.00,342.60,6.99;31,144.08,224.96,342.59,6.99;31,144.08,234.93,177.90,6.99" xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298640</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Nguyen, J. Yosinski and J. Clune, Deep neural networks are easily fooled: High confidence predictions for unrecognizable images, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, 427-436.</note>
</biblStruct>

<biblStruct coords="31,144.08,244.89,342.59,6.99;31,144.08,254.85,342.59,6.99;31,144.08,265.39,70.10,6.64" xml:id="b51">
	<monogr>
		<title level="m" type="main">Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03616</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Nguyen, J. Yosinski and J. Clune, Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks, arXiv preprint, arXiv:1602.03616.</note>
</biblStruct>

<biblStruct coords="31,144.08,274.78,342.59,6.99;31,144.08,284.71,153.10,7.01" xml:id="b52">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2009.191</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359"/>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. J. Pan and Q. Yang, A survey on transfer learning, IEEE Transactions on Knowledge and Data Engineering, 22 (2010), 1345-1359.</note>
</biblStruct>

<biblStruct coords="31,144.08,294.70,342.60,6.99;31,144.08,304.64,117.10,7.01" xml:id="b53">
	<analytic>
		<title level="a" type="main">The attention system of the human brain</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="25" to="42"/>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. I. Posner and S. E. Petersen, The attention system of the human brain, Annual review of neuroscience, 13 (1990), 25-42.</note>
</biblStruct>

<biblStruct coords="31,144.08,314.63,342.59,6.99;31,144.08,324.59,342.59,6.99;31,144.08,334.55,97.48,6.99" xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1137" to="1144"/>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Poultney, S. Chopra, Y. L. Cun et al., Efficient learning of sparse representations with an energy-based model, in Proceedings of the Advances in Neural Information Processing Systems, 2007, 1137-1144.</note>
</biblStruct>

<biblStruct coords="31,144.08,344.51,342.59,6.99;31,144.08,354.45,73.28,7.01" xml:id="b55">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0893-6080(98)00116-6</idno>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="145" to="151"/>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Qian, On the momentum term in gradient descent learning algorithms, Neural networks, 12 (1999), 145-151.</note>
</biblStruct>

<biblStruct coords="31,144.08,364.44,342.59,6.99;31,144.08,374.38,342.59,7.24;31,144.08,384.94,99.74,6.64" xml:id="b56">
	<analytic>
		<title level="a" type="main">Invariant visual representation by single neurons in the human brain</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Q</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Fried</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature03687</idno>
		<ptr target="http://dx.doi.org/10.1038/nature03687"/>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="page" from="1102" to="1107"/>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Q. Quiroga, L. Reddy, G. Kreiman, C. Koch and I. Fried., Invariant visual representation by single neurons in the human brain, Nature, 435 (2005), 1102-1107, URL http://dx.doi. org/10.1038/nature03687.</note>
</biblStruct>

<biblStruct coords="31,144.08,394.33,342.59,6.99;31,144.08,404.29,342.59,6.99;31,144.08,414.23,81.75,7.01" xml:id="b57">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2577031</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Ren, K. He, R. Girshick and J. Sun, Faster R-CNN: towards real-time object detection with region proposal networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39 (2017), 1137-1149.</note>
</biblStruct>

<biblStruct coords="31,144.08,424.22,342.59,6.99;31,144.08,434.15,202.75,7.01" xml:id="b58">
	<analytic>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-2789(92)90242-F</idno>
	</analytic>
	<monogr>
		<title level="m">Nonlinear total variation based noise removal algorithms</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268"/>
		</imprint>
	</monogr>
	<note type="raw_reference">L. I. Rudin, S. Osher and E. Fatemi, Nonlinear total variation based noise removal algorithms, Physica D: nonlinear phenomena, 60 (1992), 259-268.</note>
</biblStruct>

<biblStruct coords="31,144.08,444.14,342.60,6.99;31,144.08,454.10,342.59,6.99;31,144.08,464.07,342.59,6.99;31,144.08,474.00,81.75,7.01" xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2016.2528162</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1285" to="1298"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura and R. M. Summers, Deep convolutional neural networks for computer-aided detection: Cnn architec- tures, dataset characteristics and transfer learning, IEEE Transactions on Medical Imaging, 35 (2016), 1285-1298.</note>
</biblStruct>

<biblStruct coords="31,144.08,483.99,342.59,6.99;31,144.08,493.95,342.59,6.99;31,144.08,503.89,232.10,7.01" xml:id="b60">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature16961</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., Mastering the game of go with deep neural networks and tree search, Nature, 529 (2016), 484-489.</note>
</biblStruct>

<biblStruct coords="31,144.08,513.88,342.59,6.99;31,144.08,523.84,299.70,7.21" xml:id="b61">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">K. Simonyan, A. Vedaldi and A. Zisserman, Deep inside convolutional networks: Visualising image classification models and saliency maps, arXiv preprint, arXiv:1312.6034.</note>
</biblStruct>

<biblStruct coords="31,144.08,533.80,342.59,6.99;31,144.08,543.77,151.27,7.21" xml:id="b62">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recog- nition, arXiv preprint, arXiv:1409.1556.</note>
</biblStruct>

<biblStruct coords="31,144.08,553.73,342.59,6.99;31,144.08,563.69,342.59,6.99;31,144.08,573.66,19.29,6.99" xml:id="b63">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2003.1238663</idno>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Ninth IEEE International Conference on Computer Vision</title>
		<meeting>eeding of Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">1470</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Sivic and A. Zisserman, Video google: A text retrieval approach to object matching in videos, in Proceeding of Ninth IEEE International Conference on Computer Vision, 2003, 1470.</note>
</biblStruct>

<biblStruct coords="31,144.08,583.62,342.60,6.99;31,144.08,593.58,342.59,6.99;31,144.08,603.52,81.75,7.01" xml:id="b64">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever and R. Salakhutdinov, Dropout: a simple way to prevent neural networks from overfitting, Journal of machine learning research, 15 (2014), 1929-1958.</note>
</biblStruct>

<biblStruct coords="31,144.08,613.51,342.60,6.99;31,144.08,623.47,342.59,6.99;31,144.08,633.43,200.86,6.99" xml:id="b65">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9"/>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke and A. Rabinovich, Going deeper with convolutions, in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2015, 1-9.</note>
</biblStruct>

<biblStruct coords="31,144.08,643.39,342.60,6.99;31,144.08,653.36,277.24,7.21" xml:id="b66">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow and R. Fergus, Intriguing properties of neural networks, arXiv preprint, arXiv:1312.6199.</note>
</biblStruct>

<biblStruct coords="31,144.08,663.32,342.60,6.99;31,144.08,673.28,342.59,6.99;31,144.08,683.22,231.45,7.01" xml:id="b67">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408"/>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio and P.-A. Manzagol, Stacked denoising autoen- coders: Learning useful representations in a deep network with a local denoising criterion, Journal of Machine Learning Research, 11 (2010), 3371-3408.</note>
</biblStruct>

<biblStruct coords="31,144.08,693.21,342.60,6.99;31,144.08,703.15,246.33,7.01" xml:id="b68">
	<analytic>
		<title level="a" type="main">On the euclidean distance of images</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1334" to="1339"/>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Wang, Y. Zhang and J. Feng, On the euclidean distance of images, IEEE Transactions on Pattern Analysis and Machine Intelligence, 27 (2005), 1334-1339.</note>
</biblStruct>

<biblStruct coords="32,144.08,125.34,342.60,6.99;32,144.08,135.30,147.03,7.21" xml:id="b69">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torrabla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02379</idno>
		<title level="m">Understanding intra-class knowledge inside cnn</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">D. Wei, B. Zhou, A. Torrabla and W. Freeman, Understanding intra-class knowledge inside cnn, arXiv preprint, arXiv:1507.02379.</note>
</biblStruct>

<biblStruct coords="32,144.08,145.26,342.60,6.99;32,144.08,155.22,232.00,7.21" xml:id="b70">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">J. Yosinski, J. Clune, A. Nguyen, T. Fuchs and H. Lipson, Understanding neural networks through deep visualization, arXiv preprint, arXiv:1506.06579.</note>
</biblStruct>

<biblStruct coords="32,144.08,165.19,342.59,6.99;32,144.08,175.15,275.29,6.99" xml:id="b71">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_53</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. D. Zeiler and R. Fergus, Visualizing and understanding convolutional networks, in Pro- ceedings of the European Conference on Computer Vision, 2014, 818-833.</note>
</biblStruct>

<biblStruct coords="32,144.08,185.11,342.60,6.99;32,144.08,195.07,342.59,6.99" xml:id="b72">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5539957</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2528" to="2535"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. D. Zeiler, D. Krishnan, G. W. Taylor and R. Fergus, Deconvolutional networks, in Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, 2528-2535.</note>
</biblStruct>

<biblStruct coords="32,144.08,205.04,342.60,6.99;32,144.08,215.00,342.59,6.99;32,144.08,224.96,92.68,6.99" xml:id="b73">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126474</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2018-2025</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. D. Zeiler, G. W. Taylor and R. Fergus, Adaptive deconvolutional networks for mid and high level feature learning, in Proceedings of the IEEE International Conference on Computer Vision, 2011, 2018-2025.</note>
</biblStruct>

<biblStruct coords="32,144.08,234.93,342.60,6.99;32,144.08,244.89,173.93,7.21" xml:id="b74">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<title level="m">Object detectors emerge in deep scene CNNs</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">B. Zhou, A. Khosla, A. Lapedriza, A. Oliva and A. Torralba, Object detectors emerge in deep scene CNNs, arXiv preprint, arXiv:1412.6856.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
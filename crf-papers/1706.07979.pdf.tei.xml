<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Methods for Interpreting and Understanding Deep Neural Networks</title>
				<funder>
					<orgName type="full">Institute for Information &amp; Communications Technology Promotion</orgName>
					<orgName type="abbreviated">IITP</orgName>
				</funder>
				<funder ref="#_unGNcD7">
					<orgName type="full">Deutsche Forschungsgemeinschaft</orgName>
					<orgName type="abbreviated">DFG</orgName>
					<idno type="DOI" subtype="crossref">10.13039/501100001659</idno>
				</funder>
				<funder>
					<orgName type="full">Brain Korea 21 Plus Program through the National Research Foundation of Korea</orgName>
				</funder>
				<funder ref="#_CJtSh7s">
					<orgName type="full">German Ministry for Education and Research as Berlin Big Data Center</orgName>
					<orgName type="abbreviated">BBDC</orgName>
				</funder>
				<funder ref="#_WjPdAf3">
					<orgName type="full">Korea government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-06-24">24 Jun 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,147.61,124.23,83.98,8.74"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
							<email>gregoire.montavon@tu-berlin.de</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> Department of Electrical Engineering &amp; Computer Science, Technische Universität Berlin, Marchstr. 23, Berlin 10587, Germany</note>
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<addrLine>Marchstr. 23</addrLine>
									<postCode>10587</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.48,124.23,71.12,8.74"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
							<email>wojciech.samek@hhi.fraunhofer.de</email>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>b</label> Department of Video Coding &amp; Analytics, Fraunhofer Heinrich Hertz Institute, Einsteinufer 37, Berlin 10587, Germany</note>
								<orgName type="department">Department of Video Coding &amp; Analytics</orgName>
								<orgName type="institution">Fraunhofer Heinrich Hertz Institute</orgName>
								<address>
									<addrLine>Einsteinufer 37</addrLine>
									<postCode>10587</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.93,124.23,90.47,8.74"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
							<email>klaus-robert.mueller@tu-berlin.de</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> Department of Electrical Engineering &amp; Computer Science, Technische Universität Berlin, Marchstr. 23, Berlin 10587, Germany</note>
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<addrLine>Marchstr. 23</addrLine>
									<postCode>10587</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>c</label> Department of Brain &amp; Cognitive Engineering, Korea University, Anam-dong 5ga, Seongbuk-gu, Seoul 136-713, South Korea</note>
								<orgName type="department">Department of Brain &amp; Cognitive Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<addrLine>Anam-dong 5ga, Seongbuk-gu</addrLine>
									<postCode>136-713</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>d</label> Max Planck Institute for Informatics, Stuhlsatzenhausweg, Saarbrücken 66123, Germany</note>
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Stuhlsatzenhausweg, Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Methods for Interpreting and Understanding Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-06-24">24 Jun 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">E34C0696BA5ED8A9CFC040764B0949F0</idno>
					<idno type="arXiv">arXiv:1706.07979v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep neural networks</term>
					<term>activation maximization</term>
					<term>sensitivity analysis</term>
					<term>Taylor decomposition</term>
					<term>layer-wise relevance propagation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,37.61,246.67,520.05,8.74;1,37.61,258.62,50.45,8.74">This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions.</s><s coords="1,97.19,258.62,221.09,8.74">It is based on a tutorial given at ICASSP 2017.</s><s coords="1,327.40,258.62,230.26,8.74;1,37.61,270.58,520.05,8.74;1,37.61,282.53,22.14,8.74">It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data.</s><s coords="1,64.18,282.53,224.27,8.74">It also discusses a number of practical applications.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." coords="1,37.61,347.49,77.80,8.77">Introduction</head><p><s coords="1,47.58,370.93,241.09,8.74;1,37.61,382.88,251.06,8.74;1,37.61,394.84,251.06,8.74;1,37.61,406.79,170.05,8.74">Machine learning techniques such as deep neural networks have become an indispensable tool for a wide range of applications such as image classification, speech recognition, or natural language processing.</s><s coords="1,213.82,406.79,74.85,8.74;1,37.61,418.75,251.06,8.74;1,37.61,430.70,171.50,8.74">These techniques have achieved extremely high predictive accuracy, in many cases, on par with human performance.</s></p><p><s coords="1,47.58,443.01,241.10,8.74;1,37.61,454.97,251.05,8.74;1,37.61,466.92,251.06,8.74;1,37.61,478.88,169.47,8.74">In practice, it is also essential to verify for a given task, that the high measured accuracy results from the use of a proper problem representation, and not from the exploitation of artifacts in the data <ref type="bibr" coords="1,157.54,478.88,15.50,8.74" target="#b28">[29,</ref><ref type="bibr" coords="1,175.94,478.88,12.73,8.74" target="#b45">46,</ref><ref type="bibr" coords="1,191.58,478.88,11.62,8.74" target="#b26">27]</ref>.</s><s coords="1,211.37,478.88,77.29,8.74;1,37.61,490.83,251.06,8.74;1,37.61,502.79,251.06,8.74;1,37.61,514.74,121.04,8.74">Techniques for interpreting and understanding what the model has learned have therefore become a key ingredient of a robust validation procedure <ref type="bibr" coords="1,116.56,514.74,15.50,8.74" target="#b50">[51,</ref><ref type="bibr" coords="1,136.22,514.74,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="1,148.14,514.74,7.01,8.74" target="#b4">5]</ref>.</s><s coords="1,165.61,514.74,123.06,8.74;1,37.61,526.70,251.06,8.74;1,37.61,538.66,251.06,8.74;1,37.61,550.61,148.78,8.74">Interpretability is especially important in applications such as medicine or self-driving cars, where the reliance of the model on the correct features must be guaranteed <ref type="bibr" coords="1,152.08,550.61,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="1,170.89,550.61,11.62,8.74" target="#b13">14]</ref>.</s></p><p><s coords="1,47.58,562.92,241.10,8.74;1,37.61,574.88,184.29,8.74">It has been a common belief, that simple models provide higher interpretability than complex ones.</s><s coords="1,227.06,574.88,61.62,8.74;1,37.61,586.83,251.06,8.74;1,37.61,598.79,64.07,8.74">Linear models or basic decision trees still dominate in many applications for this reason.</s><s coords="1,105.93,598.79,182.75,8.74;1,37.61,610.74,251.05,8.74;1,37.61,622.70,251.06,8.74;1,37.61,634.65,188.26,8.74">This belief is however challenged by recent work, in which carefully designed interpretation techniques have shed light on some of the most complex and deepest machine learning models <ref type="bibr" coords="1,148.39,634.65,15.50,8.74" target="#b43">[44,</ref><ref type="bibr" coords="1,167.21,634.65,12.73,8.74" target="#b54">55,</ref><ref type="bibr" coords="1,183.27,634.65,7.75,8.74" target="#b4">5,</ref><ref type="bibr" coords="1,194.33,634.65,12.73,8.74" target="#b36">37,</ref><ref type="bibr" coords="1,210.38,634.65,11.62,8.74" target="#b39">40]</ref>.</s></p><p><s coords="1,47.58,646.96,241.10,8.74;1,37.61,658.92,251.05,8.74;1,37.61,670.87,36.64,8.74">Techniques of interpretation are also becoming increasingly popular as a tool for exploration and analysis in the sciences.</s><s coords="1,84.46,670.87,204.21,8.74;1,37.61,682.83,251.06,8.74;1,306.60,347.52,251.06,8.74;1,306.60,359.48,106.33,8.74">In combination with deep nonlinear machine learning models, they have been able to extract new in-sights from complex physical, chemical, or biological systems <ref type="bibr" coords="1,330.45,359.48,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="1,349.28,359.48,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="1,365.33,359.48,12.73,8.74" target="#b48">49,</ref><ref type="bibr" coords="1,381.38,359.48,12.73,8.74" target="#b42">43,</ref><ref type="bibr" coords="1,397.43,359.48,11.62,8.74" target="#b53">54]</ref>.</s></p><p><s coords="1,316.57,371.43,241.10,8.74;1,306.60,383.39,251.05,8.74;1,306.60,395.34,130.96,8.74">This tutorial gives an overview of techniques for interpreting complex machine learning models, with a focus on deep neural networks (DNN).</s><s coords="1,441.65,395.34,116.01,8.74;1,306.60,407.30,206.67,8.74">It starts by discussing the problem of interpreting modeled concepts (e.g.</s><s coords="1,517.23,407.30,40.43,8.74;1,306.60,419.25,251.06,8.74;1,306.60,431.21,153.16,8.74">predicted classes), and then moves to the problem of explaining individual decisions made by the model.</s><s coords="1,463.84,431.21,93.82,8.74;1,306.60,443.17,251.06,8.74;1,306.60,455.12,251.06,8.74;1,306.60,467.08,251.06,8.74;1,306.60,479.03,54.85,8.74">The tutorial abstracts from the exact neural network structure and domain of application, in order to focus on the more conceptual aspects that underlie the success of these techniques in practical applications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." coords="1,306.60,507.33,81.14,8.77">Preliminaries</head><p><s coords="1,316.57,526.72,241.09,8.74;1,306.60,538.67,251.06,8.74;1,306.60,550.63,251.06,8.74;1,306.60,562.58,112.38,8.74">Techniques of interpretation have been applied to a wide range of practical problems, and various meanings have been attached to terms such as "understanding", "interpreting", or "explaining".</s><s coords="1,424.49,562.58,107.62,8.74">See <ref type="bibr" coords="1,442.55,562.58,15.50,8.74" target="#b31">[32]</ref> for a discussion.</s><s coords="1,537.60,562.58,20.06,8.74;1,306.60,574.54,251.06,8.74;1,306.60,586.49,251.05,8.74;1,306.60,598.45,122.16,8.74">As a first step, it can be useful to clarify the meaning we associate to these words in this tutorial, as well as the type of techniques that are covered.</s></p><p><s coords="1,316.57,610.40,241.10,8.74;1,306.60,622.36,251.06,8.74;1,306.60,634.32,132.55,8.74">We will focus in this tutorial on post-hoc interpretability, i.e. a trained model is given and our goal is to understand what the model predicts (e.g.</s><s coords="1,443.55,634.32,114.11,8.74;1,306.60,646.27,251.06,8.74">categories) in terms what is readily interpretable (e.g. the input variables) <ref type="bibr" coords="1,527.24,646.27,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,542.16,646.27,11.62,8.74" target="#b39">40]</ref>.</s><s coords="1,306.60,658.23,251.06,8.74;1,306.60,670.18,251.06,8.74;1,306.60,682.14,174.15,8.74">Post-hoc interpretability should be contrasted to incorporating interpretability directly into the structure of the model, as done, for example, in <ref type="bibr" coords="1,446.44,682.14,15.50,8.74" target="#b38">[39,</ref><ref type="bibr" coords="1,465.26,682.14,11.62,8.74" target="#b14">15]</ref>.</s><s coords="1,316.57,694.09,241.10,8.74;1,306.60,706.05,251.06,8.74;1,306.60,718.00,251.06,8.74;1,306.60,729.96,21.19,8.74">Also, when using the word "understanding", we refer to a functional understanding of the model, in contrast to a lower-level mechanistic or algorithmic understanding of it.</s><s coords="1,333.54,729.96,224.12,8.74;1,306.60,741.91,251.06,8.74;1,306.60,753.87,232.60,8.74">That is, we seek to characterize the model's blackbox behavior, without however trying to elucidate its inner workings or shed light on its internal representations.</s></p><p><s coords="2,47.58,84.38,241.10,8.74;2,37.61,96.33,251.06,8.74;2,37.61,108.29,72.98,8.74">Throughout this tutorial, we will also make a distinction between interpretation and explanation, by defining these words as follows.</s></p><p><s coords="2,37.61,130.39,62.41,8.77">Definition 1.</s><s coords="2,105.01,130.42,183.67,8.74;2,37.61,142.37,251.06,8.74;2,37.61,154.33,131.37,8.74">An interpretation is the mapping of an abstract concept (e.g. a predicted class) into a domain that the human can make sense of.</s></p><p><s coords="2,37.61,176.42,251.06,8.74;2,37.61,188.37,190.10,8.74">Examples of domains that are interpretable are images (arrays of pixels), or texts (sequences of words).</s><s coords="2,231.84,188.37,56.83,8.74;2,37.61,200.33,176.99,8.74">A human can look at them and read them respectively.</s><s coords="2,218.91,200.33,69.76,8.74;2,37.61,212.28,251.06,8.74;2,37.61,224.24,18.82,8.74">Examples of domains that are not interpretable are abstract vector spaces (e.g.</s><s coords="2,60.97,224.24,227.69,8.74;2,37.61,236.19,139.63,8.74">word embeddings <ref type="bibr" coords="2,140.00,224.24,14.76,8.74" target="#b32">[33]</ref>), or domains composed of undocumented input features (e.g.</s><s coords="2,180.87,236.19,107.81,8.74;2,37.61,248.15,82.14,8.74">sequences with unknown words or symbols).</s><s coords="2,37.61,270.25,251.06,8.77;2,37.61,282.23,251.05,8.74;2,37.61,294.19,179.14,8.74">Definition 2. An explanation is the collection of features of the interpretable domain, that have contributed for a given example to produce a decision (e.g.</s><s coords="2,220.44,294.19,68.23,8.74;2,37.61,306.14,49.65,8.74">classification or regression).</s></p><p><s coords="2,37.61,328.23,251.06,8.74;2,37.61,340.18,251.06,8.74;2,37.61,352.14,157.63,8.74">An explanation can be, for example, a heatmap highlighting which pixels of the input image most strongly support the classification decision <ref type="bibr" coords="2,150.19,352.14,15.50,8.74" target="#b43">[44,</ref><ref type="bibr" coords="2,168.85,352.14,12.73,8.74" target="#b25">26,</ref><ref type="bibr" coords="2,184.72,352.14,7.01,8.74" target="#b4">5]</ref>.</s><s coords="2,199.61,352.14,89.05,8.74;2,37.61,364.09,251.06,8.74;2,37.61,376.05,122.50,8.74">The explanation can be coarse-grained to highlight e.g. which regions of the image support the decision.</s><s coords="2,167.07,376.05,121.60,8.74;2,37.61,388.00,251.06,8.74;2,37.61,399.96,109.13,8.74">It can also be computed at a finer grain, e.g. to include pixels and their color components in the explanation.</s><s coords="2,151.76,399.96,136.91,8.74;2,37.61,411.92,251.06,8.74">In natural language processing, explanations can take the form of highlighted text <ref type="bibr" coords="2,259.40,411.92,15.50,8.74" target="#b30">[31,</ref><ref type="bibr" coords="2,278.16,411.92,7.01,8.74" target="#b2">3]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." coords="2,37.61,442.07,150.99,8.77">Interpreting a DNN Model</head><p><s coords="2,47.58,463.32,241.10,8.74;2,37.61,475.27,215.26,8.74">This section focuses on the problem of interpreting a concept learned by a deep neural network (DNN).</s><s coords="2,255.76,475.27,32.92,8.74;2,37.61,487.23,251.05,8.74;2,37.61,499.18,251.06,8.74;2,37.61,511.14,251.05,8.74;2,37.61,523.09,251.06,8.74;2,37.61,535.05,118.56,8.74">A DNN is a collection of neurons organized in a sequence of multiple layers, where neurons receive as input the neuron activations from the previous layer, and perform a simple computation (e.g. a weighted sum of the input followed by a nonlinear activation).</s><s coords="2,163.73,535.05,124.95,8.74;2,37.61,547.00,251.06,8.74;2,37.61,558.96,91.44,8.74">The neurons of the network jointly implement a complex nonlinear mapping from the input to the output.</s><s coords="2,136.93,558.96,151.74,8.74;2,37.61,570.91,251.06,8.74;2,37.61,582.87,173.07,8.74">This mapping is learned from the data by adapting the weights of each neuron using a technique called error backpropagation <ref type="bibr" coords="2,192.42,582.87,14.61,8.74" target="#b40">[41]</ref>.</s></p><p><s coords="2,47.58,594.87,241.09,8.74;2,37.61,606.82,203.42,8.74">The learned concept that must be interpreted is usually represented by a neuron in the top layer.</s><s coords="2,247.69,606.82,40.98,8.74;2,37.61,618.78,108.51,8.74">Top-layer neurons are abstract (i.e.</s><s coords="2,149.20,618.78,139.47,8.74;2,37.61,630.73,209.29,8.74">we cannot look at them), on the other hand, the input domain of the DNN (e.g.</s><s coords="2,250.62,630.73,38.06,8.74;2,37.61,642.69,128.96,8.74">image or text) is usually interpretable.</s><s coords="2,172.76,642.69,115.91,8.74;2,37.61,654.64,251.06,8.74;2,37.61,666.60,219.75,8.74">We describe below how to build a prototype in the input domain that is interpretable and representative of the abstract learned concept.</s><s coords="2,261.69,666.60,26.97,8.74;2,37.61,678.55,251.06,8.74;2,37.61,690.51,110.76,8.74">Building the prototype can be formulated within the activation maximization framework.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." coords="2,37.61,714.72,156.70,8.74">Activation Maximization (AM)</head><p><s coords="2,47.58,729.96,241.09,8.74;2,37.61,741.91,251.06,8.74;2,37.61,753.87,225.79,8.74">Activation maximization is an analysis framework that searches for an input pattern that produces a maximum model response for a quantity of interest <ref type="bibr" coords="2,218.02,753.87,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="2,231.86,753.87,12.73,8.74" target="#b16">17,</ref><ref type="bibr" coords="2,247.91,753.87,11.62,8.74" target="#b43">44]</ref>.</s></p><p><s coords="2,316.57,84.35,241.10,8.77;2,306.60,96.33,68.31,9.65">Consider a DNN classifier mapping data points x to a set of classes (ω c ) c .</s><s coords="2,379.97,96.33,177.70,8.74;2,306.60,108.29,115.34,9.65">The output neurons encode the modeled class probabilities p(ω c |x).</s><s coords="2,426.34,108.26,131.32,8.77;2,306.60,120.24,187.76,9.65">A prototype x representative of the class ω c can be found by optimizing:</s></p><formula xml:id="formula_0" coords="2,375.22,140.18,113.82,16.21">max x log p(ω c |x) -λ x 2 .</formula><p><s coords="2,306.60,167.20,251.06,8.74;2,306.60,179.15,87.57,8.74">The class probabilities modeled by the DNN are functions with a gradient <ref type="bibr" coords="2,375.91,179.15,14.61,8.74" target="#b10">[11]</ref>.</s><s coords="2,398.52,179.15,159.14,8.74;2,306.60,191.11,102.03,8.74">This allows for optimizing the objective by gradient ascent.</s><s coords="2,413.18,191.11,144.48,8.74;2,306.60,203.06,251.05,9.65;2,306.60,215.02,158.95,8.74">The rightmost term of the objective is an 2 -norm regularizer that implements a preference for inputs that are close to the origin.</s><s coords="2,469.70,215.02,87.96,8.74;2,306.60,226.97,251.06,8.74;2,306.60,238.93,251.06,8.74;2,306.60,250.88,101.94,8.74">When applied to image classification, prototypes thus take the form of mostly gray images, with only a few edge and color patterns at strategic locations <ref type="bibr" coords="2,390.28,250.88,14.61,8.74" target="#b43">[44]</ref>.</s><s coords="2,415.00,250.88,142.67,8.74;2,306.60,262.84,251.06,8.74;2,306.60,274.79,14.42,8.74">These prototypes, although producing strong class response, look in many cases unnatural.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2." coords="2,306.60,299.03,152.49,8.74">Improving AM with an Expert</head><p><s coords="2,316.57,314.30,241.09,8.74;2,306.60,326.25,251.06,9.65;2,306.60,338.21,251.06,8.74;2,306.60,350.17,96.33,8.74">In order to focus on more probable regions of the input space, the 2 -norm regularizer can be replaced by a data density model p(x) called "expert", leading to the new optimization problem:</s></p><formula xml:id="formula_1" coords="2,371.70,372.18,120.87,14.13">max x log p(ω c |x) + log p(x).</formula><p><s coords="2,306.60,397.12,251.06,8.74;2,306.60,409.08,232.87,8.74">Here, the prototype is encouraged to simultaneously produce strong class response and to resemble the data.</s><s coords="2,545.35,409.08,12.31,8.74;2,306.60,421.03,251.06,8.74;2,306.60,432.99,251.06,8.74;2,306.60,444.94,228.09,9.65">By application of the Bayes' rule, the newly defined objective can be identified, up to modeling errors and a constant term, as the class-conditioned data density p(x|ω c ).</s><s coords="2,540.50,444.94,17.16,8.74;2,306.60,456.90,251.06,8.74;2,306.60,468.82,85.58,9.68">The learned prototype thus corresponds to the most likely input x for class ω c .</s><s coords="2,400.96,468.85,156.70,8.74;2,306.60,480.81,107.57,8.74">A possible choice for the expert is the Gaussian RBM <ref type="bibr" coords="2,395.91,480.81,14.61,8.74" target="#b22">[23]</ref>.</s><s coords="2,421.12,480.81,136.54,8.74;2,306.60,492.76,46.57,8.74">Its probability function can be written as:</s></p><formula xml:id="formula_2" coords="2,347.78,512.70,168.70,13.67">log p(x) = j f j (x) -1 2 x Σ -1 x + cst.</formula><p><s coords="2,306.60,538.63,132.74,9.65;2,439.62,543.26,3.30,6.12;2,446.34,538.60,111.33,9.68;2,306.60,550.58,146.43,8.74">where f j (x) = log(1 + exp(w j x + b j )) are factors with parameters learned from the data.</s><s coords="2,457.22,550.58,100.44,8.74;2,306.60,562.54,251.06,8.74;2,306.60,574.49,251.06,8.74;2,306.60,586.45,97.63,8.74">When interpreting concepts such as natural images classes, more complex density models such as convolutional RBM/DBMs <ref type="bibr" coords="2,498.48,574.49,14.61,8.74" target="#b27">[28]</ref>, or pixel-RNNs <ref type="bibr" coords="2,336.13,586.45,15.50,8.74" target="#b51">[52]</ref> are needed.</s></p><p><s coords="2,316.57,598.45,241.09,8.74;2,306.60,610.40,54.84,8.74">In practice, the choice of the expert p(x) plays an important role.</s><s coords="2,365.87,610.40,191.80,8.74;2,306.60,622.36,251.06,8.74;2,306.60,634.32,129.09,8.74">The relation between the expert and the resulting prototype is given qualitatively in Figure <ref type="figure" coords="2,521.16,622.36,3.87,8.74" target="#fig_0">1</ref>, where four cases (a-d) are identified.</s><s coords="2,439.89,634.32,117.78,8.74;2,306.60,646.27,251.06,8.74;2,306.60,658.23,251.06,8.74;2,306.60,670.18,100.37,9.65">On one extreme, the expert is coarse, or simply absent, in which case, the optimization problem reduces to the maximization of the class probability function p(ω c |x).</s><s coords="2,411.73,670.18,145.93,8.74;2,306.60,682.14,251.06,8.74;2,306.60,694.09,251.06,8.74;2,306.60,706.05,94.18,8.74">On the other extreme, the expert is overfitted on some data distribution, and thus, the optimization problem becomes essentially the maximization of the expert p(x) itself.</s><s coords="2,405.19,706.05,152.47,8.74;2,306.60,718.00,251.06,8.74;2,306.60,729.96,251.06,8.74;2,306.60,741.91,122.76,9.65">When using AM for the purpose of model validation, an overfitted expert (case d) must be especially avoided, as the latter could hide interesting failure modes of the model p(ω c |x).</s><s coords="2,433.68,741.91,123.98,8.74;2,306.60,753.87,251.06,8.74;3,37.61,356.41,108.85,8.74">A slightly underfitted expert (case b), e.g. that simply favors images with natural colors, can already be sufficient.</s><s coords="3,151.45,356.41,137.22,8.74;3,37.61,368.37,251.06,8.74;3,37.61,380.32,201.74,9.65">On the other hand, when using AM to gain knowledge on a correctly predicted concept ω c , the focus should be to prevent underfitting.</s><s coords="3,243.55,380.32,45.12,8.74;3,37.61,392.28,251.06,9.65;3,37.61,404.23,251.05,8.74;3,37.61,416.16,187.86,9.68">Indeed, an underfitted expert would expose optima of p(ω c |x) potentially distant from the data, and therefore, the prototype x would not be truly representative of ω c .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3." coords="3,37.61,442.73,155.24,8.74">Performing AM in Code Space</head><p><s coords="3,47.58,460.30,241.09,8.74;3,37.61,472.25,251.06,8.74;3,37.61,484.21,179.92,8.74">In certain applications, data density models p(x) can be hard to learn up to high accuracy, or very complex such that maximizing them becomes difficult.</s><s coords="3,224.86,484.21,63.81,8.74;3,37.61,496.16,224.28,8.74">An alternative class of unsupervised models are generative models.</s><s coords="3,266.26,496.16,22.42,8.74;3,37.61,508.12,251.06,8.74;3,37.61,520.07,234.65,8.74">They do not provide the density function directly, but are able to sample from it, usually via the following two steps:</s></p><p><s coords="3,44.81,539.66,243.86,8.74;3,57.54,551.61,160.14,8.74;3,44.81,565.18,243.86,8.74;3,57.54,577.14,207.64,8.74">1. Sample from a simple distribution q(z) ∼ N (0, I) defined in some abstract code space Z. 2. Apply to the sample a decoding function g : Z → X , that maps it back to the original input domain.</s></p><p><s coords="3,37.61,596.35,251.06,8.74">One such model is the generative adversarial network <ref type="bibr" coords="3,270.41,596.35,14.61,8.74" target="#b18">[19]</ref>.</s><s coords="3,37.61,608.30,251.06,8.74;3,37.61,620.26,251.05,8.74;3,37.61,632.21,142.86,8.74">It learns a decoding function g such that the generated data distribution is as hard as possible to discriminate from the true data distribution.</s><s coords="3,188.39,632.21,100.28,8.74;3,37.61,644.17,251.06,8.74;3,37.61,656.12,183.32,8.74">The decoding function g is learned in competition with a discriminant between the generated and the true distributions.</s><s coords="3,228.63,656.12,60.04,8.74;3,37.61,668.08,251.05,8.74;3,37.61,680.03,118.86,8.74">The decoding function and the discriminant are typically chosen to be multilayer neural networks.</s><s coords="3,47.58,692.36,240.60,9.65;3,37.61,704.32,251.06,8.74;3,37.61,716.27,110.16,8.74">Nguyen et al. <ref type="bibr" coords="3,109.59,692.36,15.50,8.74" target="#b36">[37]</ref> proposed to build a prototype for ω c by incorporating such generative model in the activation maximization framework.</s><s coords="3,152.01,716.27,136.67,8.74;3,37.61,728.23,45.99,8.74">The optimization problem is redefined as:</s></p><formula xml:id="formula_3" coords="3,98.76,748.83,128.77,16.66">max z∈Z log p(ω c | g(z)) -λ z 2 ,</formula><p><s coords="3,306.60,84.38,251.05,8.74;3,306.60,96.33,251.06,8.74;3,306.60,108.29,251.06,9.65">where the first term is a composition of the newly introduced decoder and the original classifier, and where the second term is an 2 -norm regularizer in the code space.</s><s coords="3,306.60,120.21,251.05,8.77;3,306.60,132.20,251.06,9.65;3,306.60,144.12,92.31,8.77">Once a solution z to the optimization problem is found, the prototype for ω c is obtained by decoding the solution, that is, x = g(z ).</s><s coords="3,407.30,144.15,150.37,9.65;3,306.60,156.11,251.06,8.74;3,306.60,168.06,202.90,8.74">In Section 3.1, the 2 -norm regularizer in the input space was understood in the context of image data as favoring gray-looking images.</s><s coords="3,514.14,168.06,43.52,8.74;3,306.60,180.02,251.06,9.65;3,306.60,191.97,251.06,8.74;3,306.60,203.93,29.61,8.74">The effect of the 2 -norm regularizer in the code space can instead be understood as encouraging codes that have high probability.</s><s coords="3,341.28,203.93,216.39,8.74;3,306.60,215.88,251.06,8.74">Note however, that high probability codes do not necessarily map to high density regions of the input space.</s></p><p><s coords="3,316.57,227.84,241.09,8.74;3,306.60,239.79,125.99,8.74">To illustrate the qualitative differences between the methods of Sections 3.1-3.3,</s><s coords="3,436.94,239.79,120.72,8.74;3,306.60,251.75,251.05,8.74;3,306.60,263.70,25.32,8.74">we consider the problem of interpreting MNIST classes as modeled by a three-layer DNN.</s><s coords="3,335.28,263.70,222.38,9.65;3,306.60,275.52,75.40,8.88;3,386.99,274.09,3.97,6.12;3,396.18,275.52,161.48,8.88;3,306.60,287.62,251.05,9.65;3,306.60,299.57,251.06,8.74;3,310.76,311.39,119.69,9.79;3,435.85,309.95,3.97,6.12;3,444.26,311.39,113.40,8.88;3,306.60,323.48,55.04,9.65">We consider for this task (1) a simple 2 -norm regularizer λ xx 2 where x denotes the data mean for ω c , (2) a Gaussian RBM expert p(x), and (3) a generative model with a two-layer decoding function, and the 2 -norm regularizer λ zz 2 where z denotes the code mean for ω c .</s><s coords="3,366.35,323.48,191.31,8.74;3,306.60,335.44,129.86,8.74">Corresponding architectures and found prototypes are shown in Figure <ref type="figure" coords="3,428.71,335.44,3.87,8.74" target="#fig_2">2</ref>.</s><s coords="3,440.68,335.44,116.98,8.74;3,306.60,347.39,135.79,8.74">Each prototype is classified with full certainty by the DNN.</s><s coords="3,445.22,347.39,112.44,8.74;3,306.60,359.35,251.06,8.74;3,306.60,371.30,90.85,8.74">However, only with an expert or a decoding function, the prototypes become sharp and realistic-looking.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4." coords="3,306.60,716.24,152.23,8.74">From Global to Local Analysis</head><p><s coords="3,316.57,729.96,241.09,8.74;3,306.60,741.91,251.06,9.65;3,306.60,753.87,251.06,8.74;4,37.61,84.35,192.47,9.68">When considering complex machine learning problems, probability functions p(ω c |x) and p(x) might be multimodal or strongly elongated, so that no single prototype x fully represents the modeled concept ω c .</s><s coords="4,235.56,84.38,53.11,8.74;4,37.61,96.33,251.06,8.74;4,37.61,108.29,251.06,8.74;4,37.61,120.24,251.06,9.65;4,37.61,132.20,128.80,8.74">The issue of multimodality is raised by Nguyen et al. <ref type="bibr" coords="4,214.53,96.33,14.61,8.74" target="#b37">[38]</ref>, who demonstrate in the context of image classification, the benefit of interpreting a class ω c using multiple local prototypes instead of a single global one.</s></p><p><s coords="4,47.58,144.42,241.09,8.74;4,37.61,156.37,181.05,9.65">Producing an exhaustive description of the modeled concept ω c is however not always necessary.</s><s coords="4,225.75,156.37,62.92,8.74;4,37.61,168.33,231.74,8.74">One might instead focus on a particular region of the input space.</s><s coords="4,274.11,168.33,14.56,8.74;4,37.61,180.28,251.05,8.74;4,37.61,192.24,251.05,8.74;4,37.61,204.20,157.05,8.74">For example, biomedical data is best analyzed conditioned on a certain development stage of a medical condition, or in relation to a given subject or organ.</s></p><p><s coords="4,47.58,216.42,241.10,8.74;4,37.61,228.34,191.96,9.68;4,235.05,226.80,3.97,6.12;4,242.81,228.37,45.86,8.74;4,37.61,240.30,178.73,9.68">An expedient way of introducing locality into the analysis is to add a localization term η • xx 0 2 to the AM objective, where x 0 is a reference point.</s><s coords="4,223.09,240.33,65.59,8.74;4,37.61,252.28,165.64,8.74">The parameter η controls the amount of localization.</s><s coords="4,209.18,252.28,79.49,8.74;4,37.61,264.24,251.06,9.65;4,37.61,276.16,251.06,8.77;4,37.61,288.12,229.16,9.68">As this parameter increases, the question "what is a good prototype of ω c ?" becomes however insubstantial, as the prototype x converges to x 0 and thus looses its information content.</s></p><p><s coords="4,47.58,300.37,241.09,9.65;4,37.61,312.29,251.06,8.77;4,37.61,324.28,161.56,9.65">Instead, when trying to interpret the concept ω c locally, a more relevant question to ask is "what features of x make it representative of the concept ω c ?".</s><s coords="4,204.56,324.28,84.11,8.74;4,37.61,336.23,251.06,8.74;4,37.61,348.19,102.83,8.74">This question gives rise to a second type of analysis, that will be the focus of the rest of this tutorial.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." coords="4,37.61,379.91,149.14,8.77">Explaining DNN Decisions</head><p><s coords="4,47.58,402.70,241.09,8.77;4,37.61,414.68,251.06,9.65;4,37.61,426.64,251.06,8.74">In this section, we ask for a given data point x, what makes it representative of a certain concept ω c encoded in some output neuron of the deep neural network (DNN).</s><s coords="4,37.61,438.59,251.05,8.74;4,37.61,450.55,43.35,8.74">The output neuron can be described as a function f (x) of the input.</s><s coords="4,85.94,450.55,202.73,8.74;4,37.61,462.47,139.72,9.68;4,177.34,460.93,4.15,6.12;4,177.34,467.14,12.91,6.12;4,190.74,462.50,97.94,8.74;4,37.61,474.46,251.05,9.65;4,37.61,486.41,106.93,9.65">A common approach is to view the data point x as a collection of features (x i ) d i=1 , and to assign to each of these, a score R i determining how relevant the feature x i is for explaining f (x).</s><s coords="4,148.84,486.41,139.83,8.74">An example is given in Figure <ref type="figure" coords="4,280.92,486.41,3.87,8.74" target="#fig_3">3</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,156.74,515.59,23.41,14.65;4,225.70,542.32,44.56,12.82">DNN output f(x)</head><p><s coords="4,212.87,556.30,70.25,10.99;4,108.16,507.04,29.22,13.16;4,107.74,580.41,68.91,13.16;4,47.58,639.55,241.09,8.74;4,37.61,651.51,100.75,8.74">(evidence for "boat") input x explanation R(x) In this example, an image is presented to a DNN and is classified as "boat".</s><s coords="4,147.30,651.51,141.37,8.74;4,37.61,663.47,251.06,8.74">The prediction (encoded in the output layer) is then mapped back to the input domain.</s><s coords="4,37.61,675.42,251.06,8.74;4,37.61,687.38,243.59,8.74">The explanation takes the form of a heatmap, where pixels with a high associated relevance score are shown in red.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1." coords="4,37.61,713.15,106.44,8.74">Sensitivity Analysis</head><p><s coords="4,47.58,729.96,241.09,8.74;4,37.61,741.91,132.44,8.74">A first approach to identify the most important input features is sensitivity analysis.</s><s coords="4,175.10,741.91,113.57,8.74;4,37.61,753.87,251.06,8.74;4,306.60,84.38,41.26,8.74">It is based on the model's locally evaluated gradient or some other local measure of variation.</s><s coords="4,355.33,84.38,202.34,8.74;4,306.60,96.33,113.47,8.74">A common formulation of sensitivity analysis defines relevance scores as</s></p><formula xml:id="formula_4" coords="4,394.70,111.60,74.86,23.44">R i (x) = ∂f ∂x i 2 ,</formula><p><s coords="4,306.60,142.33,228.85,8.77">where the gradient is evaluated at the data point x.</s><s coords="4,540.50,142.36,17.16,8.74;4,306.60,154.31,251.06,8.74;4,306.60,166.27,72.58,8.74">The most relevant input features are those to which the output is most sensitive.</s><s coords="4,383.41,166.27,174.25,8.74;4,306.60,178.22,251.06,8.74;4,306.60,190.18,135.69,8.74">The technique is easy to implement for a deep neural network, since the gradient can be computed using backpropagation <ref type="bibr" coords="4,407.98,190.18,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="4,426.79,190.18,11.62,8.74" target="#b40">41]</ref>.</s><s coords="4,316.57,202.13,241.10,8.74;4,306.60,214.09,251.06,8.74;4,306.60,226.04,251.06,8.74;4,306.60,238.00,13.28,8.74">Sensitivity analysis has been regularly used in scientific applications of machine learning such as medical diagnosis <ref type="bibr" coords="4,306.60,226.04,14.61,8.74" target="#b24">[25]</ref>, ecological modeling <ref type="bibr" coords="4,418.27,226.04,14.61,8.74" target="#b17">[18]</ref>, or mutagenicity prediction <ref type="bibr" coords="4,306.60,238.00,9.96,8.74" target="#b5">[6]</ref>.</s><s coords="4,324.18,238.00,233.48,8.74;4,306.60,249.95,213.87,8.74">More recently, it was also used for explaining the classification of images by deep neural networks <ref type="bibr" coords="4,502.20,249.95,14.61,8.74" target="#b43">[44]</ref>.</s></p><p><s coords="4,316.57,261.91,241.09,8.74;4,306.60,273.86,251.06,8.74;4,306.60,285.82,171.34,8.74">It is important to note, however, that sensitivity analysis does not produce an explanation of the function value f (x) itself, but rather a variation of it.</s><s coords="4,482.86,285.82,74.80,8.74;4,306.60,297.77,251.06,8.74;4,306.60,309.73,222.50,8.74">Sensitivity scores are indeed a decomposition of the local variation of the function as measured by the gradient square norm:</s></p><formula xml:id="formula_5" coords="4,389.12,326.28,96.04,14.11">d i=1 R i (x) = ∇f (x) 2</formula><p><s coords="4,306.60,348.76,251.06,8.74;4,306.60,360.72,251.06,8.74;4,306.60,372.67,251.06,8.74;4,306.60,384.63,251.06,8.74;4,306.60,396.58,26.21,8.74">Intuitively, when applying sensitivity analysis e.g. to a neural network detecting cars in images, we answer the question "what makes this image more/less a car?", rather than the more basic question "what makes this image a car?".</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2." coords="4,306.60,419.05,148.44,8.74">Simple Taylor Decomposition</head><p><s coords="4,316.57,432.56,241.09,8.74;4,306.60,444.51,251.06,8.74;4,306.60,456.47,181.83,8.74">The Taylor decomposition <ref type="bibr" coords="4,437.19,432.56,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="4,452.09,432.56,7.75,8.74" target="#b4">5]</ref> is a method that explains the model's decision by decomposing the function value f (x) as a sum of relevance scores.</s><s coords="4,496.40,456.47,61.27,8.74;4,306.60,468.42,251.05,8.74;4,306.60,480.38,251.06,8.74;4,306.60,492.30,96.67,8.77">The relevance scores are obtained by identification of the terms of a firstorder Taylor expansion of the function at some root point x for which f ( x) = 0.</s><s coords="4,408.52,492.33,149.14,8.74;4,306.60,504.29,50.70,8.74">This expansion lets us rewrite the function as:</s></p><formula xml:id="formula_6" coords="4,366.14,520.84,131.98,14.11">f (x) = d i=1 R i (x) + O(xx )</formula><p><s coords="4,306.60,543.32,114.49,8.74">where the relevance scores</s></p><formula xml:id="formula_7" coords="4,371.59,558.80,121.09,23.22">R i (x) = ∂f ∂x i x= x • (x i -x i )</formula><p><s coords="4,306.60,590.89,251.06,8.74;4,306.60,602.84,83.95,8.74">are the first-order terms, and where O(xx ) contains all higher-order terms.</s><s coords="4,395.60,602.84,162.06,8.74;4,306.60,614.80,251.06,8.74;4,306.60,626.75,88.34,8.74">Because these higher-order terms are typically non-zero, this analysis only provides a partial explanation of f (x).</s><s coords="4,316.57,638.71,241.09,8.74;4,306.60,650.63,251.06,8.77;4,306.60,662.62,108.97,8.74">However, a special class of functions, piecewise linear and satisfying the property f (t x) = t f (x) for t ≥ 0, is not subject to this limitation.</s><s coords="4,419.71,662.62,137.96,8.74;4,306.60,674.57,251.06,8.74;4,306.60,686.53,163.13,8.74">Examples of such functions used in machine learning are homogeneous linear models, or deep ReLU networks (without biases).</s><s coords="4,473.84,686.53,83.82,8.74;4,306.60,698.45,251.05,9.68;4,306.60,710.44,251.06,8.74;4,306.60,722.36,251.06,8.77;4,306.60,734.35,20.51,8.74">For these functions, we can always find a root point x = lim ε→0 ε • x, that incidentally lies on the same linear region as the data point x, and for which the second and higher-order terms are zero.</s><s coords="4,331.53,734.35,198.04,8.74">In that case, the function can be rewritten as</s></p><formula xml:id="formula_8" coords="4,389.97,750.90,84.32,14.11">f (x) = d i=1 R i (x)</formula><p><s coords="5,37.61,84.38,164.35,8.74">where the relevance scores simplify to</s></p><formula xml:id="formula_9" coords="5,125.79,103.33,74.70,23.22">R i (x) = ∂f ∂x i • x i .</formula><p><s coords="5,37.61,135.41,251.06,8.74;5,37.61,147.37,251.06,8.74;5,37.61,159.32,172.35,8.74">Relevance can here be understood as the product of sensitivity (given by the locally evaluated partial derivative) and saliency (given by the input value).</s><s coords="5,214.34,159.32,74.33,8.74;5,37.61,171.28,251.06,8.74;5,37.61,183.23,97.22,8.74">That is, an input feature is relevant if it is both present in the data, and if the model reacts to it.</s></p><p><s coords="5,47.58,195.19,241.10,8.74;5,37.61,207.14,251.06,8.74;5,37.61,219.10,198.34,8.74">Later in this tutorial, we will also show how this simple technique serves as a primitive for building the more sophisticated deep Taylor decomposition <ref type="bibr" coords="5,217.69,219.10,14.61,8.74" target="#b33">[34]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3." coords="5,37.61,242.49,119.30,8.74">Relevance Propagation</head><p><s coords="5,47.58,256.92,241.09,8.74;5,37.61,268.88,251.06,8.74;5,37.61,280.83,42.18,8.74">An alternative way of decomposing the prediction of a DNN is to make explicit use of its feed-forward graph structure.</s><s coords="5,85.06,280.83,203.61,8.74;5,37.61,292.79,251.05,8.74;5,37.61,304.74,251.06,8.74;5,37.61,316.70,146.70,8.74">The algorithm starts at the output of the network, and moves in the graph in reverse direction, progressively redistributing the prediction score (or total relevance) until the input is reached.</s><s coords="5,188.60,316.70,100.07,8.74;5,37.61,328.65,251.06,8.74;5,37.61,340.61,61.58,8.74">The redistribution process must furthermore satisfy a local relevance conservation principle.</s></p><p><s coords="5,47.58,352.56,241.10,8.74;5,37.61,364.52,251.06,8.74;5,37.61,376.47,251.06,8.74;5,37.61,388.43,70.90,8.74">A physical analogy would be that of an electrical circuit where one injects a certain amount of current at the first endpoint, and measures the resulting current at the other endpoints.</s><s coords="5,112.93,388.43,175.74,8.74;5,37.61,400.38,251.06,8.74;5,37.61,412.34,251.06,8.74;5,37.61,424.29,51.48,8.74">In this physical example, Kirchoff's conservation laws for current apply locally to each node of the circuit, but also ensure the conservation property at a global level.</s></p><p><s coords="5,47.58,436.25,241.10,8.74;5,37.61,448.20,251.06,8.74;5,37.61,460.16,251.06,8.74;5,37.61,472.11,251.05,8.74;5,37.61,484.07,163.25,8.74">The propagation approach was proposed by Landecker et al. <ref type="bibr" coords="5,65.27,448.20,15.50,8.74" target="#b25">[26]</ref> to explain the predictions of hierarchical networks, and was also introduced by Bach et al. <ref type="bibr" coords="5,247.95,460.16,10.52,8.74" target="#b4">[5]</ref> in the context of convolutional DNNs for explaining the predictions of these state-of-the-art models.</s></p><p><s coords="5,47.58,496.02,241.09,8.74;5,37.61,507.98,127.62,8.74">Let us consider a DNN where j and k are indices for neurons at two successive layers.</s><s coords="5,170.69,507.98,117.98,9.65;5,37.61,519.93,204.73,8.74">Let (R k ) k be the relevance scores associated to neurons in the higher layer.</s><s coords="5,246.59,519.93,42.08,8.74;5,37.61,531.89,250.74,9.65;5,37.61,543.84,53.83,8.74">We define R j←k as the share of relevance that flows from neuron k to neuron j.</s><s coords="5,97.34,543.84,191.33,8.74;5,37.61,555.80,251.06,9.65;5,37.61,567.75,100.90,8.74">This share is determined based on the contribution of neuron j to R k , subject to the local relevance conservation constraint</s></p><formula xml:id="formula_10" coords="5,139.16,588.81,58.48,11.15">j R j←k = R k .</formula><p><s coords="5,37.61,609.87,251.06,8.74;5,37.61,621.82,237.41,8.74">The relevance of a neuron in the lower layer is then defined as the total relevance it receives from the higher layer:</s></p><formula xml:id="formula_11" coords="5,130.03,642.88,65.57,11.15">R j = k R j←k</formula><p><s coords="5,37.61,663.93,251.06,8.74;5,37.61,675.89,251.06,8.74;5,37.61,687.84,251.05,8.74;5,37.61,699.80,245.41,8.74">These two equations, when combined, ensure between all consecutive layers a relevance conservation property, which in turn also leads to a global conservation property from the neural network output to the input relevance scores:</s></p><formula xml:id="formula_12" coords="5,71.62,717.89,193.55,14.11">d i=1 R i = • • • = j R j = k R k = • • • = f (x)</formula><p><s coords="5,47.58,741.91,241.09,8.74;5,37.61,753.87,251.06,8.74;5,306.60,84.38,152.76,8.74">It should be noted that there are other explanation techniques that rely on the DNN graph structure, although not producing a decomposition of f (x).</s><s coords="5,463.60,84.38,94.06,8.74;5,306.60,96.33,251.06,8.74;5,306.60,108.29,137.88,8.74">Two examples are the deconvolution by Zeiler and Fergus <ref type="bibr" coords="5,463.20,96.33,14.61,8.74" target="#b54">[55]</ref>, and guided backprop by Springenberg et al. <ref type="bibr" coords="5,426.22,108.29,14.61,8.74" target="#b46">[47]</ref>.</s><s coords="5,448.71,108.29,108.96,8.74;5,306.60,120.24,251.06,8.74;5,306.60,132.20,251.05,8.74;5,306.60,144.15,251.05,8.74">They also work by applying a backward mapping through the graph, and generate interpretable patterns in the input domain, that are associated to a certain prediction or a feature map activation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4." coords="5,306.60,167.62,127.69,8.74">Practical Considerations</head><p><s coords="5,316.57,182.13,241.10,8.74;5,306.60,194.08,251.06,8.74;5,306.60,206.04,251.05,8.74;5,306.60,217.99,176.63,8.74">Explanation techniques that derive from a decomposition principle provide several practical advantages: First, they give an implicit quantification of the share that can be imputed to individual input features.</s><s coords="5,488.48,217.99,69.18,8.74;5,306.60,229.95,251.06,8.74;5,306.60,241.90,186.69,8.74">When the number of input variables is limited, the analysis can therefore be represented as a pie chart or histogram.</s><s coords="5,497.70,241.90,59.96,8.74;5,306.60,253.86,251.06,8.74;5,306.60,265.81,251.06,8.74;5,306.60,277.77,24.44,8.74">If the number of input variables is too large, the decomposition can be coarsened by pooling relevance scores over groups of features.</s></p><p><s coords="5,316.57,289.72,241.10,8.74;5,306.60,301.68,251.05,8.74;5,306.60,313.63,81.37,8.74">For example, in RGB images, the three relevance scores of a pixel can be summed to obtain the relevance score of the whole pixel.</s><s coords="5,393.52,313.63,164.14,8.74;5,306.60,325.59,93.18,8.74">The resulting pixel scores can be displayed as a heatmap.</s><s coords="5,405.19,325.59,152.48,8.74;5,306.60,337.54,251.06,8.74;5,306.60,349.50,251.05,8.74;5,306.60,361.46,251.06,8.74;5,306.60,373.41,58.49,8.74">On an object recognition task, Lapuschkin et al. <ref type="bibr" coords="5,376.48,337.54,15.50,8.74" target="#b26">[27]</ref> further exploited this mechanism by pooling relevance over two large regions of the image: (1) the bounding box of the object to detect and (2) the rest of the image.</s><s coords="5,372.16,373.41,185.51,8.74;5,306.60,385.37,251.06,8.74;5,306.60,397.32,67.03,8.74">This coarse analysis was used to quantify the reliance of the model on the object itself and on its spatial context.</s></p><p><s coords="5,316.57,409.28,241.10,8.74;5,306.60,421.23,251.06,8.74;5,306.60,433.19,251.06,8.74;5,306.60,445.14,101.16,8.74">In addition, when the explanation technique uses propagation in the model's graph, the quantity being propagated can be filtered to only include what flows through a certain neuron or feature map.</s><s coords="5,412.80,445.14,144.86,8.74;5,306.60,457.10,251.05,8.74;5,306.60,469.05,114.29,8.74">This allows to capture individual components of an explanation, that would otherwise be entangled in the heatmap.</s></p><p><s coords="5,316.57,481.01,241.09,8.74;5,306.60,492.96,223.80,8.74">The pooling and filtering capabilities of each explanation technique are shown systematically in Table <ref type="table" coords="5,522.65,492.96,3.87,8.74">1</ref>.</s></p><p><s coords="5,444.05,515.31,72.90,7.86;5,344.24,526.67,75.47,7.86;5,344.24,538.03,54.60,7.86;5,344.24,549.38,87.85,7.86;5,344.24,560.74,71.51,7.86;5,344.24,572.10,80.33,7.86;5,306.60,592.96,251.06,6.99;5,306.60,602.42,20.32,6.99">pooling filtering sensitivity analysis simple Taylor relevance propagation deconvolution <ref type="bibr" coords="5,401.42,560.74,14.34,7.86" target="#b54">[55]</ref> guided backprop <ref type="bibr" coords="5,410.24,572.10,14.34,7.86" target="#b46">[47]</ref> Table <ref type="table" coords="5,330.24,592.96,3.29,6.99">1</ref>: Properties of various techniques for explaining DNN decisions.</s><s coords="5,331.14,602.42,226.53,6.99;5,306.60,611.89,71.08,6.99">The first three entries correspond to the methods introduced in Sections 4.1-4.3.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." coords="5,306.60,649.67,185.14,8.77">The LRP Explanation Framework</head><p><s coords="5,316.57,670.18,241.10,8.74;5,306.60,682.14,251.06,8.74;5,306.60,694.09,223.88,8.74">In this section, we focus on the layer-wise relevance propagation (LRP) technique introduced by Bach et al. <ref type="bibr" coords="5,306.60,694.09,10.52,8.74" target="#b4">[5]</ref> for explaining deep neural network predictions.</s><s coords="5,537.32,694.09,20.34,8.74;5,306.60,706.05,251.06,8.74;5,306.60,718.00,251.06,8.74;5,306.60,729.96,251.06,8.74;5,306.60,741.91,62.42,8.74">LRP is based on the propagation approach described in Section 4.3, and has been used in a number of practical applications, in particular, for model validation and analysis of scientific data.</s><s coords="5,373.30,741.91,184.36,8.74;5,306.60,753.87,89.72,8.74">Some of these applications are discussed in Sections 8.1 and 8.2.</s></p><p><s coords="6,47.58,84.38,241.09,8.74;6,37.61,96.33,251.06,8.74;6,37.61,108.29,251.06,8.74">LRP is first described algorithmically in Section 5.1, and then shown in Section 5.2 to correspond in some cases to a deep Taylor decomposition of the model's decision <ref type="bibr" coords="6,270.41,108.29,14.61,8.74" target="#b33">[34]</ref>.</s><s coords="6,37.61,120.24,251.05,8.74;6,37.61,132.20,154.58,8.74">Practical recommendations and tricks to make efficient use of LRP are then given in Section 6.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1." coords="6,37.61,153.70,146.35,8.74">Propagation Rules for DNNs</head><p><s coords="6,47.58,166.23,241.09,8.74;6,37.61,178.19,175.95,8.74">In the original paper <ref type="bibr" coords="6,145.87,166.23,9.96,8.74" target="#b4">[5]</ref>, LRP was applied to bag-ofwords and deep neural network models.</s><s coords="6,220.00,178.19,68.67,8.74;6,37.61,190.14,167.61,8.74">In this tutorial, we focus on the second type of models.</s><s coords="6,209.53,190.14,79.14,8.74;6,37.61,202.10,169.04,8.74;6,110.15,219.99,29.14,9.65;6,156.39,225.02,3.30,6.12;6,162.25,219.99,53.89,9.65;6,37.61,237.89,251.06,9.65;6,37.61,249.85,251.06,9.65;6,37.61,261.80,103.32,8.74">Let the neurons of the DNN be described by the equation a k = σ j a j w jk + b k , with a k the neuron activation, (a j ) j the activations from the previous layer, and w jk , b k the weight and bias parameters of the neuron.</s><s coords="6,146.43,261.80,142.24,8.74;6,37.61,273.76,195.74,8.74">The function σ is a positive and monotonically increasing activation function.</s></p><p><s coords="6,47.58,285.71,241.10,8.74;6,37.61,297.67,251.05,8.74;6,37.61,309.63,102.32,8.74">One propagation rule that fulfills local conservation properties, and that was shown to work well in practice is the αβ-rule given by:</s></p><formula xml:id="formula_13" coords="6,72.69,324.86,215.98,30.61">R j = k α a j w + jk j a j w + jk -β a j w - jk j a j w - jk R k ,<label>(1)</label></formula><p><s coords="6,37.61,363.18,36.60,8.74;6,74.21,361.61,6.12,6.12;6,84.19,363.18,27.17,8.74;6,111.36,361.61,6.23,6.12;6,121.44,363.18,167.22,8.74;6,37.61,375.14,251.05,8.74;6,37.61,387.09,251.06,8.74;6,37.61,399.05,251.06,8.74;6,37.61,411.00,68.77,8.74">where () + and () -denote the positive and negative parts respectively, and where the parameters α and β are chosen subject to the constraints α -β = 1 and β ≥ 0. To avoid divisions by zero, small stabilizing terms can be introduced when necessary.</s><s coords="6,110.82,411.00,124.39,8.74">The rule can be rewritten as</s></p><formula xml:id="formula_14" coords="6,69.63,426.24,187.03,30.61">R j = k a j w + jk j a j w + jk R ∧ k + k a j w - jk j a j w - jk R ∨ k ,</formula><p><s coords="6,37.61,464.11,36.77,8.74;6,74.46,462.54,5.37,6.12;6,74.38,468.98,4.24,6.12;6,83.75,464.11,61.10,9.65;6,144.93,462.54,5.37,6.12;6,144.86,468.98,4.24,6.12;6,154.23,464.11,40.32,9.65">where R ∧ k = αR k and R ∨ k = -βR k .</s><s coords="6,200.17,464.11,88.50,8.74;6,37.61,476.07,75.41,8.74">It can now be interpreted as follows:</s></p><p><s coords="6,57.54,492.78,52.60,8.74;6,110.22,491.20,5.37,6.12;6,110.14,497.64,4.24,6.12;6,118.72,492.78,150.02,8.74;6,57.54,504.73,211.21,9.65;6,57.54,516.69,72.25,9.65">Relevance R ∧ k should be redistributed to the lowerlayer neurons (a j ) j in proportion to their excitatory effect on a k .</s><s coords="6,134.09,516.69,98.73,8.74;6,232.89,515.11,5.37,6.12;6,232.82,521.55,4.24,6.12;6,241.50,516.69,27.24,8.74;6,57.54,528.64,210.31,9.65;6,57.54,540.60,195.52,9.65">"Counter-relevance" R ∨ k should be redistributed to the lower-layer neurons (a j ) j in proportion to their inhibitory effect on a k .</s></p><p><s coords="6,37.61,557.31,251.06,8.74;6,37.61,569.27,251.06,8.74;6,37.61,581.22,19.93,8.74">Different combinations of parameters α, β were shown to modulate the qualitative behavior of the resulting explanation.</s><s coords="6,61.90,581.22,226.77,8.74;6,37.61,593.18,251.06,9.65;6,37.61,605.13,105.32,8.74">As a naming convention, we denote, for example, by LRP-α 2 β 1 , the fact of having chosen the parameters α = 2 and β = 1 for this rule.</s><s coords="6,149.04,605.13,139.63,8.74;6,37.61,617.09,251.06,8.74;6,37.61,629.04,230.20,8.74">In the context of image classification, a non-zero value for β was shown empirically to have a sparsifying effect on the explanation <ref type="bibr" coords="6,237.43,629.04,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="6,252.31,629.04,11.62,8.74" target="#b33">34]</ref>.</s><s coords="6,275.39,629.04,13.28,8.74;6,37.61,641.00,251.06,9.65;6,37.61,652.95,251.06,9.65;6,37.61,664.91,106.90,8.74">On the BVLC CaffeNet <ref type="bibr" coords="6,130.82,641.00,14.61,8.74" target="#b23">[24]</ref>, LRP-α 2 β 1 was shown to work well, while for the deeper GoogleNet <ref type="bibr" coords="6,202.50,652.95,14.61,8.74" target="#b49">[50]</ref>, LRP-α 1 β 0 was found to be more stable.</s></p><p><s coords="6,47.58,676.86,241.10,9.65;6,37.61,688.82,83.69,8.74">When choosing LRP-α 1 β 0 , the propagation rule reduces to the simpler rule:</s></p><formula xml:id="formula_15" coords="6,113.00,704.05,175.68,30.61">R j = k a j w + jk j a j w + jk R k .<label>(2)</label></formula><p><s coords="6,37.61,741.91,251.06,8.74;6,37.61,753.87,232.49,8.74">The latter rule was also used by Zhang et al. <ref type="bibr" coords="6,238.91,741.91,15.50,8.74" target="#b55">[56]</ref> as part of an explanation method called excitation backprop.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2." coords="6,306.60,84.38,183.88,8.74">LRP and Deep Taylor Decomposition</head><p><s coords="6,316.57,100.53,241.09,8.74;6,306.60,112.48,233.06,9.65">In this section, we show for deep ReLU networks a connection between LRP-α 1 β 0 and Taylor decomposition.</s><s coords="6,543.83,112.48,13.83,8.74;6,306.60,124.44,226.93,8.74">We show in particular that when neurons are defined as</s></p><formula xml:id="formula_16" coords="6,334.82,146.70,194.63,11.15">a k = max 0, j a j w jk + b k with b k ≤ 0,</formula><p><s coords="6,306.60,168.96,251.06,9.65;6,306.60,180.92,251.05,8.74;6,306.60,192.87,138.73,8.74">the application of LRP-α 1 β 0 at a given layer can be seen as computing a Taylor decomposition of the relevance at that layer onto the lower layer.</s><s coords="6,452.09,192.87,105.57,8.74;6,306.60,204.83,251.05,8.74;6,306.60,216.78,251.06,8.74;6,306.60,228.74,50.12,8.74">The name "deep Taylor decomposition" then arises from the iterative application of Taylor decomposition from the top layer down to the input layer.</s></p><p><s coords="6,316.57,240.87,241.10,8.74;6,306.60,252.82,251.06,9.65;6,306.60,264.78,202.40,9.65">Let us assume that the relevance for the neuron k can be written as R k = a k c k , a product of the neuron activation a k and a term c k that is constant and positive.</s><s coords="6,513.41,264.78,44.25,8.74;6,306.60,276.73,232.49,8.74;6,316.57,502.22,241.10,8.74;6,306.60,514.17,49.86,8.74">These two properties allow us to construct a "relevance neuron"  We now would like to propagate the relevance to the lower layer.</s><s coords="6,361.67,514.17,195.99,8.74;6,306.60,526.13,139.99,9.65">For this, we perform a Taylor decomposition of R k on the lower-layer neurons.</s><s coords="6,450.69,526.13,106.97,8.74;6,306.60,538.08,251.06,11.92">We search for the nearest root point ( a j ) j of R k on the segment [(a j 1 w jk ≤0 ) j , (a j ) j ].</s><s coords="6,306.60,551.67,211.02,8.74">The search strategy is visualized in Figure <ref type="figure" coords="6,496.59,551.67,16.83,8.74" target="#fig_5">4(b)</ref>.</s><s coords="6,522.88,551.67,34.78,8.74;6,306.60,563.63,251.06,8.74;6,306.60,575.58,231.99,8.74">Because the relevance neuron is piecewise linear, the Taylor expansion at the root point contains only first-order terms:</s></p><formula xml:id="formula_17" coords="6,367.04,298.99,190.62,11.15">R k = max 0, j a j w jk + b k ,<label>(3)</label></formula><formula xml:id="formula_18" coords="6,368.61,595.74,127.04,38.85">R k = j ∂R k ∂a j ( aj )j • (a j -a j ) R j←k</formula><p><s coords="6,306.60,647.01,251.05,8.74;6,306.60,658.96,251.06,9.65;6,306.60,670.92,44.69,8.74">The first-order terms correspond to the decomposition of R k on the lower-layer neurons and have the closed-form expression</s></p><formula xml:id="formula_19" coords="6,383.84,680.73,96.59,29.11">R j←k = a j w + jk j a j w + jk R k .</formula><p><s coords="6,306.60,718.00,251.06,9.65">The resulting propagation of R k is shown in Figure <ref type="figure" coords="6,537.74,718.00,3.99,8.74" target="#fig_5">4</ref>(c).</s><s coords="6,306.60,729.96,251.06,9.65;6,306.60,741.91,251.06,9.65;6,306.60,753.87,58.53,8.74">Summing R j←k over all neurons k to which neuron j contributes yields exactly the LRP-α 1 β 0 propagation rule of Equation <ref type="bibr" coords="6,349.64,753.87,11.62,8.74" target="#b1">(2)</ref>.</s></p><p><s coords="7,47.58,84.38,241.09,8.74;7,37.61,96.33,112.05,8.74">We now would like to verify that the procedure can be repeated one layer below.</s><s coords="7,155.41,96.33,133.27,8.74;7,37.61,108.29,251.06,9.65;7,37.61,120.24,213.38,9.65">For this, we inspect the structure of R j and observe that it can be written as a product R j = a j c j , where a j is the neuron activation and</s></p><formula xml:id="formula_20" coords="7,79.72,139.85,166.18,64.27">c j = k w + jk j a j w + jk R k = k w + jk max 0, j a j w jk + b k j a j w + jk c k</formula><p><s coords="7,37.61,214.27,196.65,8.74">is positive and also approximately constant.</s><s coords="7,242.95,214.27,45.72,8.74;7,37.61,226.22,251.06,8.74;7,37.61,238.18,251.06,9.65;7,37.61,250.13,215.21,9.65;7,253.09,247.91,6.12,6.12;7,252.82,255.18,7.94,6.12;7,261.43,250.13,26.58,9.65;7,37.61,262.09,174.26,8.74">The latter property arises from the observation that the dependence of c j on the activation a j is only very indirect (diluted by two nested sums), and that the other terms w jk , w + jk , b k , c k are constant or approximately constant.</s></p><p><s coords="7,47.58,274.04,241.09,9.65;7,37.61,286.00,251.06,8.74;7,37.61,297.95,251.06,8.74;7,37.61,309.91,111.30,8.74">The positivity and near-constancy of c j implies that similar relevance neuron to the one of Equation ( <ref type="formula" coords="7,246.23,286.00,4.24,8.74" target="#formula_17">3</ref>) can be built for neuron j, for the purpose of redistributing relevance on the layer before.</s><s coords="7,153.30,309.91,135.37,8.74;7,37.61,321.86,251.06,8.74;7,37.61,333.82,251.06,8.74;7,37.61,345.77,138.46,8.74">The decomposition process can therefore be repeated in the lower layers, until the first layer of the neural network is reached, thus, performing a deep Taylor decomposition <ref type="bibr" coords="7,157.80,345.77,14.61,8.74" target="#b33">[34]</ref>.</s></p><p><s coords="7,47.58,357.73,241.09,8.74;7,37.61,369.69,251.06,8.74;7,37.61,381.64,251.06,8.74;7,37.61,393.60,147.55,8.74">In the derivation above, the segment on which we search for a root point incidentally guarantees (1) membership of the root point to the domain of ReLU activations and (2) positivity of relevance scores.</s><s coords="7,193.00,393.60,95.67,8.74;7,37.61,405.55,195.90,8.74">These guarantees can also be brought to other types of layers (e.g.</s><s coords="7,237.11,405.55,51.56,8.74;7,37.61,417.51,251.06,8.74;7,37.61,429.46,185.80,9.65">input layers receiving real values or pixels intensities), by searching for a root point ( a j ) j on a different segment.</s><s coords="7,230.39,429.46,58.27,8.74;7,37.61,441.42,251.06,8.74;7,37.61,453.37,251.05,8.74;7,37.61,465.33,101.49,8.74">This leads to different propagation rules, some of which are listed in Table <ref type="table" coords="7,65.11,453.37,3.87,8.74">2</ref>. Details on how to derive these rules are given in the original paper <ref type="bibr" coords="7,120.84,465.33,14.61,8.74" target="#b33">[34]</ref>.</s><s coords="7,145.61,465.33,143.06,8.74;7,37.61,477.28,83.27,8.74">We refer to these rules as "deep Taylor LRP" rules.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="7,46.85,502.00,55.03,7.86;7,129.67,502.00,18.56,7.86">Input domain Rule</head><p><s coords="7,46.85,515.94,69.93,7.86;7,46.85,526.90,32.85,7.86">ReLU activations (aj ≥ 0)</s></p><formula xml:id="formula_21" coords="7,46.85,512.53,231.74,90.07">Rj = k ajw + jk j ajw + jk R k Pixel intensities (xi ∈ [li, hi], li ≤ 0 ≤ hi) Ri = j xiwij -liw + ij -hiw - ij i xiwij -liw + ij -hiw - ij Rj Real values (xi ∈ R) Ri = j w 2 ij i w 2 ij</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="7,196.92,584.93,10.03,7.86">Rj</head><p><s coords="7,37.61,615.05,232.83,6.99">Table <ref type="table" coords="7,60.55,615.05,3.29,6.99">2</ref>: Deep Taylor LRP rules derived for various layer types.</s><s coords="7,274.09,615.05,14.58,6.99;7,37.61,624.51,251.06,6.99;7,37.61,633.98,62.39,6.99">The first rule applies to the hidden layers, and the next two rules apply to the first layer.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3." coords="7,37.61,667.53,126.06,8.74">Handling Special Layers</head><p><s coords="7,47.58,682.14,241.10,8.74;7,37.61,694.09,251.06,9.65;7,37.61,706.05,251.06,8.74;7,37.61,718.00,91.66,8.74">Practical neural networks are often composed of special layers, for example, p -pooling layers (including sumpooling and max-pooling as the two extreme cases), and normalization layers.</s><s coords="7,136.86,718.00,151.82,8.74;7,37.61,729.96,251.06,8.74;7,37.61,741.91,251.05,8.74;7,37.61,753.87,114.17,8.74">The original paper by Bach et al. <ref type="bibr" coords="7,37.61,729.96,10.52,8.74" target="#b4">[5]</ref> uses a winner-take-all redistribution policy for maxpooling layers, where all relevance goes to the most activated neuron in the pool.</s><s coords="7,159.02,753.87,129.65,8.74;7,306.60,84.38,251.06,9.65;7,306.60,96.33,74.77,8.74">Instead, Montavon et al. <ref type="bibr" coords="7,273.17,753.87,15.50,8.74" target="#b33">[34]</ref> recommend to apply for p -pooling layers the following propagation rule:</s></p><formula xml:id="formula_22" coords="7,397.67,112.12,68.94,24.72">R j = x j j x j R k ,</formula><p><s coords="7,306.60,146.52,251.06,8.74;7,306.60,158.48,38.00,8.74">i.e. redistribution is proportional to neuron activations in the pool.</s><s coords="7,348.79,158.48,208.87,8.74;7,306.60,170.43,251.06,8.74">This redistribution rule ensures explanation continuity (see Section 7.1 for an introduction to this concept).</s></p><p><s coords="7,316.57,182.39,241.09,8.74;7,306.60,194.34,251.05,8.74">With respect to normalization layers, Bach et al. <ref type="bibr" coords="7,526.88,182.39,10.52,8.74" target="#b4">[5]</ref> proposed to ignore them in the relevance propagation pass.</s><s coords="7,306.60,206.30,251.06,8.74;7,306.60,218.25,251.06,8.74;7,306.60,230.21,251.06,8.74;7,306.60,242.16,110.75,8.74">Alternately, Binder et al. <ref type="bibr" coords="7,420.76,206.30,15.50,8.74" target="#b9">[10]</ref> proposed for these layers a more sophisticated rule based on a local Taylor expansion of the normalization function, with some benefits in terms of explanation selectivity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." coords="7,306.60,271.18,207.30,8.77">Recommendations and Tricks for LRP</head><p><s coords="7,316.57,291.29,241.10,8.74;7,306.60,303.25,203.65,8.74">Machine learning methods are often described in papers at an abstract level, for maximum generality.</s><s coords="7,518.06,303.25,39.60,8.74;7,306.60,315.20,251.06,8.74;7,306.60,327.16,251.06,8.74;7,306.60,339.12,251.06,8.74;7,306.60,351.07,168.09,8.74">However, a good choice of hyperparameters is usually necessary to make them work well on real-world problems, and tricks are often used to make most efficient use of these methods and extend their capabilities <ref type="bibr" coords="7,430.96,351.07,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="7,443.96,351.07,12.73,8.74" target="#b22">23,</ref><ref type="bibr" coords="7,459.19,351.07,11.62,8.74" target="#b34">35]</ref>.</s><s coords="7,478.85,351.07,78.82,8.74;7,306.60,363.03,251.06,8.74;7,306.60,374.98,251.06,8.74;7,306.60,386.94,27.67,8.74">Likewise, the LRP framework introduced in Section 5, also comes with a list of recommendations and tricks, some of which are given below.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1." coords="7,306.60,410.01,180.74,8.74">How to Choose the Model to Explain</head><p><s coords="7,316.57,424.11,241.09,8.74;7,306.60,436.07,79.38,8.74">The LRP approach is aimed at general feedforward computational graphs.</s><s coords="7,390.28,436.07,167.38,8.74;7,306.60,448.02,251.06,8.74;7,306.60,459.98,251.06,8.74;7,306.60,471.93,89.80,8.74">However, it was most thoroughly studied, both theoretically <ref type="bibr" coords="7,404.51,448.02,15.50,8.74" target="#b33">[34]</ref> and empirically <ref type="bibr" coords="7,492.04,448.02,14.61,8.74" target="#b41">[42]</ref>, on specific types of models such as convolutional neural networks with ReLU nonlinearities.</s><s coords="7,401.02,471.93,156.64,8.74;7,306.60,483.89,19.93,8.74">This leads to our first recommendation:</s></p><p><s coords="7,326.53,504.41,211.21,8.74;7,326.53,516.37,118.50,8.74">Apply LRP to classes of models where it was successfully applied in the past.</s><s coords="7,449.35,516.37,88.39,8.74;7,326.53,528.32,210.17,8.74">In absence of trained model of such class, consider training your own.</s><s coords="7,316.57,548.84,241.09,8.74;7,306.60,560.80,251.06,8.74;7,306.60,572.75,251.06,8.74;7,306.60,584.71,251.06,8.74;7,306.60,596.66,154.12,8.74">We have also observed empirically that in order for LRP to produce good explanations, the number of fully connected layers should be kept low, as LRP tends for these layers to redistribute relevance to too many lower-layer neurons, and thus, loose selectivity.</s></p><p><s coords="7,326.53,617.18,211.21,8.74;7,326.53,629.14,211.21,8.74;7,326.53,641.09,96.40,8.74">As a first try, consider a convolutional ReLU network, as deep as needed, but with not too many fully connected layers.</s><s coords="7,428.33,641.09,109.40,8.74;7,326.53,653.05,28.39,8.74">Use dropout <ref type="bibr" coords="7,484.27,641.09,15.50,8.74" target="#b47">[48]</ref> in these layers.</s></p><p><s coords="7,316.57,673.57,241.10,8.74;7,306.60,685.53,251.06,8.74;7,306.60,697.48,251.06,8.74;7,306.60,709.44,251.06,8.74;7,306.60,721.39,145.43,8.74">For the LRP procedure to best match the deep Taylor decomposition framework outlined in Section 5.2, sumpooling or average-pooling layers should be preferred to max-pooling layers, and bias parameters of the network should either be zero or negative.</s></p><p><s coords="7,326.53,741.91,211.21,8.74;7,326.53,753.87,189.55,8.74">Prefer sum-pooling to max-pooling, and force biases to be zero or negative at training time.</s></p><p><s coords="8,47.58,84.38,241.09,8.74;8,37.61,96.33,251.06,8.74;8,37.61,108.29,148.11,8.74">Negative biases will contribute to further sparsify the network activations, and therefore, also to better disentangle the relevance at each layer.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2." coords="8,37.61,139.00,225.44,8.74">How to Choose the LRP Rules for Explanation</head><p><s coords="8,47.58,160.74,241.09,8.74;8,37.61,172.69,251.06,8.74;8,37.61,184.65,251.06,8.74;8,37.61,196.60,251.06,8.74;8,37.61,208.56,55.93,8.74">In presence of a deep neural network that follows the recommendations above, a first set of propagation rules to be tried are the deep Taylor LRP rules of Table <ref type="table" coords="8,252.11,184.65,3.87,8.74">2</ref>, which exhibit a stable behavior, and that are also well understood theoretically.</s><s coords="8,98.06,208.56,190.61,8.74;8,37.61,220.51,251.05,8.74">These rules produce for positive predictions a positive heatmap, where input variables are deemed rel-</s></p><formula xml:id="formula_23" coords="8,37.61,232.47,165.75,9.65">evant if R i &gt; 0 or irrelevant if R i = 0.</formula><p><s coords="8,57.54,258.27,211.21,8.74;8,57.54,270.23,191.88,8.74">As a default choice for relevance propagation, use the deep Taylor LRP rules given in Table <ref type="table" coords="8,241.27,270.23,4.07,8.74">2</ref>.</s></p><p><s coords="8,47.58,296.03,241.09,8.74;8,37.61,307.98,251.06,8.74;8,37.61,319.94,251.06,8.74;8,37.61,331.89,107.88,8.74">In presence of predictive uncertainty, a certain number of input variables might be in contradiction with the prediction, and the concept of "negative relevance" must therefore be introduced.</s><s coords="8,154.69,331.89,133.97,8.74;8,37.61,343.85,251.05,8.74;8,37.61,355.81,251.06,8.74;8,37.61,367.76,78.93,8.74">Negative relevance can be injected into the explanation in a controlled manner by setting the coefficients of the αβ-rule of Equation ( <ref type="formula" coords="8,253.06,355.81,4.24,8.74" target="#formula_13">1</ref>) to an appropriate value.</s></p><p><s coords="8,57.54,393.56,211.21,8.74;8,57.54,405.52,211.21,9.65;8,57.54,417.47,137.58,9.65">If negative relevance is needed, or the heatmaps are too diffuse, replace the rule LRP-α 1 β 0 by LRP-α 2 β 1 in the hidden layers.</s></p><p><s coords="8,47.58,443.28,241.10,9.65;8,37.61,455.23,251.05,8.74;8,37.61,467.19,227.74,8.74">The LRP-α 1 β 0 and LRP-α 2 β 1 rules were shown to work well on image classification <ref type="bibr" coords="8,158.47,455.23,14.61,8.74" target="#b33">[34]</ref>, but there is a potentially much larger set of rules that we can choose from.</s><s coords="8,274.11,467.19,14.56,8.74;8,37.61,479.14,251.06,8.74;8,37.61,491.10,88.54,8.74">For example, the " -rule" <ref type="bibr" coords="8,135.44,479.14,10.52,8.74" target="#b4">[5]</ref> was applied successfully to text categorization <ref type="bibr" coords="8,101.82,491.10,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="8,115.64,491.10,7.01,8.74" target="#b3">4]</ref>.</s><s coords="8,130.58,491.10,158.10,8.74;8,37.61,503.05,251.06,8.74;8,37.61,515.01,251.06,8.74;8,37.61,526.96,99.54,8.74">To choose the most appropriate rule among the set of possible ones, a good approach is to define a heatmap quality criterion, and select the rule at each layer accordingly.</s><s coords="8,143.84,526.96,144.82,8.74;8,37.61,538.92,251.06,8.74;8,37.61,550.87,111.83,8.74">One such quality criterion called "pixel-flipping" measures heatmap selectivity and is later introduced in Section 7.2.</s></p><p><s coords="8,57.54,576.67,211.21,9.65;8,57.54,588.63,211.21,9.65;8,57.54,600.58,211.21,8.74;8,57.54,612.54,81.22,8.74">If the heatmaps obtained with LRP-α 1 β 0 and LRP-α 2 β 1 are unsatisfactory, consider a larger set of propagation rules, and use pixel-flipping to select the best one.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3." coords="8,37.61,643.25,150.64,8.74">Tricks for Implementing LRP</head><p><s coords="8,47.58,664.99,241.10,9.65;8,37.61,676.95,35.97,8.74">Let us consider the LRP-α 1 β 0 propagation rule of Equation (2):</s></p><formula xml:id="formula_24" coords="8,107.43,689.70,111.42,30.61">R j = a j k w + jk j a j w + jk R k ,</formula><p><s coords="8,37.61,729.96,251.06,8.74;8,37.61,741.91,105.99,9.65">where we have for convenience moved the neuron activation a j outside the sum.</s><s coords="8,148.55,741.91,140.12,8.74;8,37.61,753.87,251.06,8.74;8,306.60,84.38,64.54,8.74;8,359.21,108.17,55.12,8.74;8,466.67,108.17,50.15,8.74">This rule can be written as four elementary computations, all of which can also expressed in vector form: element-wise vector form</s></p><formula xml:id="formula_25" coords="8,347.45,120.66,210.21,61.45">z k ← j a j w + jk z ← W + • a (4) s k ← R k /z k s ← R z (5) c j ← k w + jk s k c ← W + • s (6) R j ← a j c j R ← a c<label>(7)</label></formula><p><s coords="8,316.57,195.37,147.05,8.74;8,479.42,195.37,16.05,8.74;8,511.10,195.37,46.56,8.74;8,306.60,207.33,184.07,8.74">In the vector form computations, and denote the element-wise division and multiplication.</s><s coords="8,501.11,207.33,56.55,8.74;8,306.60,219.28,251.05,8.74;8,306.60,231.24,251.06,9.65;8,306.60,243.19,251.06,8.74;8,306.60,255.15,69.23,8.74">The variable W denotes the weight matrix connecting the neurons of the two consecutive layers, and W + is the matrix retaining only the positive weights of W and setting remaining weights to zero.</s><s coords="8,381.85,255.15,175.81,8.74;8,306.60,267.10,135.58,8.74">This vector form is useful to implement LRP for fully connected layers.</s></p><p><s coords="8,316.57,280.00,241.09,8.74;8,306.60,291.95,251.06,8.74;8,306.60,303.91,251.06,9.02;8,306.60,315.86,251.06,8.74">In convolution layers, the matrix-vector multiplications of Equations ( <ref type="formula" coords="8,371.95,291.95,4.24,8.74">4</ref>) and ( <ref type="formula" coords="8,410.17,291.95,4.24,8.74">6</ref>) can be more efficiently implemented by borrowing the forward and backward methods used for forward activation and gradient propagation.</s><s coords="8,306.60,327.82,251.06,8.74;8,306.60,339.77,218.77,8.74">These methods are readily available in many neural network libraries and are typically highly optimized.</s><s coords="8,531.73,339.77,25.93,8.74;8,306.60,351.73,251.06,8.74;8,306.60,363.68,159.82,8.74;8,316.57,510.60,241.10,9.02;8,306.60,522.55,251.05,8.74;8,306.60,534.51,251.06,8.74;8,306.60,546.46,106.82,8.74">Based on these high-level primitives, LRP can implemented by the following sequence of operations: The function lrp receives as arguments the layer through which the relevance should be propagated, the activations "a" at the layer input, and the relevance scores "R" at the layer output.</s><s coords="8,419.93,546.46,137.73,8.74;8,306.60,558.42,161.08,8.74">The function returns the redistributed relevance at the layer input.</s><s coords="8,472.11,558.42,85.55,8.74;8,306.60,570.37,202.45,9.02">Sample code is provided at http://heatmapping.org/tutorial.</s><s coords="8,513.24,570.37,44.43,8.74;8,306.60,582.33,251.06,8.74;8,306.60,594.28,167.45,8.74">This modular approach was also used by Zhang et al. <ref type="bibr" coords="8,499.93,582.33,15.50,8.74" target="#b55">[56]</ref> to implement the excitation backprop method.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4." coords="8,306.60,624.76,205.75,8.74">Translation Trick for Denoising Heatmaps</head><p><s coords="8,316.57,646.27,241.10,8.74;8,306.60,658.23,251.06,8.74;8,306.60,670.18,79.52,8.74">It is sometimes observed that, for classifiers that are not optimally trained or structured, LRP heatmaps have unaesthetic features.</s><s coords="8,393.72,670.18,163.94,8.74;8,306.60,682.14,251.05,8.74;8,306.60,694.09,173.05,8.74">This can be caused, for example, by the presence of noisy first-layer filters, or a large stride parameter in the first convolution layer.</s><s coords="8,484.03,694.09,73.63,8.74;8,306.60,706.05,251.06,8.74;8,306.60,718.00,251.05,8.74;8,306.60,729.96,117.27,8.74">These effects can be mitigated by considering the explanation not of a single input image but the explanations of multiple slightly translated versions of the image.</s><s coords="8,428.12,729.96,129.54,8.74;8,306.60,741.91,251.06,8.74;8,306.60,753.87,251.06,8.74">The heatmaps for these translated versions are then recombined by applying to them the inverse translation operation and averaging them up.</s></p><p><s coords="9,37.61,84.38,251.06,8.74">In mathematical terms, the improved heatmap is given by:</s></p><formula xml:id="formula_26" coords="9,94.88,100.79,136.52,26.80">R (x) = 1 |T | τ ∈T τ -1 (R(τ (x)))</formula><p><s coords="9,37.61,137.12,42.21,8.74;9,80.96,135.54,10.20,6.12;9,94.67,137.12,191.47,8.74;9,37.61,149.07,187.38,8.74">where τ, τ -1 denote the translation and its inverse, and T is the set of all translations of a few pixels.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5." coords="9,37.61,171.24,226.45,8.74">Sliding Window Explanations for Large Images</head><p><s coords="9,47.58,184.44,241.09,8.74;9,37.61,196.40,251.06,8.74;9,37.61,208.35,185.46,8.74">In applications such as medical imaging or scene parsing, the images to be processed are typically larger than the what the neural network receives as input.</s><s coords="9,227.56,208.32,61.11,8.77;9,37.61,220.31,52.38,8.74">Let X be this large image.</s><s coords="9,94.34,220.31,194.33,8.74;9,37.61,232.26,251.06,8.74;9,37.61,244.22,251.05,8.74;9,37.61,256.17,251.06,8.74;9,37.61,268.13,180.25,8.74">The LRP procedure can be extended for this scenario by applying a sliding window strategy, where the neural network is moved through the whole image, and where heatmaps produced at various locations must then be combined into a single large heatmap.</s><s coords="9,222.85,268.13,65.82,8.74;9,37.61,280.08,144.24,8.74">Technically, we define the quantity to explain as:</s></p><formula xml:id="formula_27" coords="9,119.81,300.53,86.66,23.80">g(X) = s∈S f (X[s] x )</formula><p><s coords="9,37.61,332.67,251.06,8.77;9,37.61,344.65,199.49,8.74">where X[s] extracts a patch from the image X at location s, and S is the set of all locations in that image.</s><s coords="9,241.15,344.65,47.51,8.74;9,37.61,356.61,251.05,8.74;9,37.61,368.56,227.22,8.74">Pixels then receive relevance from all patches to which they belong and in which they contribute to the function value f (x).</s><s coords="9,269.24,368.56,19.43,8.74;9,37.61,380.52,153.34,8.74">This technique is illustrated in Figure <ref type="figure" coords="9,183.21,380.52,3.87,8.74">5</ref>.</s></p><formula xml:id="formula_28" coords="9,81.11,400.80,187.39,154.02">CIFAR-10 network patch x heatmap R(x) f(x) input image X aggregated heatmap R(X)</formula><p><s coords="9,37.61,568.48,251.06,6.99;9,37.61,577.95,239.92,6.99">Figure <ref type="figure" coords="9,64.74,568.48,3.29,6.99">5</ref>: Highlighting in a large image pixels that are relevant for the CIFAR-10 class "horse", using the sliding window technique.</s></p><p><s coords="9,47.58,598.95,241.10,8.74;9,37.61,610.90,195.09,8.74">The convolutional neural network is a special case that can technically receive an input of any size.</s><s coords="9,239.47,610.90,49.20,8.74;9,37.61,622.86,251.06,8.74;9,37.61,634.81,120.80,8.74">A heatmap can be obtained directly from it by redistributing the toplayer activations using LRP.</s><s coords="9,161.13,634.81,127.54,8.74;9,37.61,646.77,251.06,8.74;9,37.61,658.72,42.37,8.74">This direct approach can provide a computational gain compared to the sliding window approach.</s><s coords="9,86.72,658.72,201.96,8.74;9,37.61,670.68,251.06,8.74;9,37.61,682.63,124.30,8.74">However, it is not strictly equivalent and can produce unreliable heatmaps, e.g. when the network uses border-padded convolutions.</s><s coords="9,168.62,682.63,120.06,8.74;9,37.61,694.59,167.57,8.74">If in doubt, it is preferable to use the sliding window formulation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6." coords="9,37.61,716.76,137.65,8.74">Visualize Relevant Pattern</head><p><s coords="9,47.58,729.96,241.10,8.74;9,37.61,741.91,251.05,8.74;9,37.61,753.87,207.01,8.74">Due to their characteristic spatial structure, LRP heatmaps readily provide intuition on which input pattern the model has used to make its prediction.</s><s coords="9,249.07,753.87,39.60,8.74;9,306.60,84.38,251.05,8.74;9,306.60,96.33,251.06,8.74;9,306.60,108.29,194.04,8.74">However, in presence of cluttered scenes, a better visualization can be obtained by using the heatmap as a mask to extract relevant pixels (and colors) from the image.</s><s coords="9,507.07,108.29,50.59,8.74;9,306.60,120.21,251.06,8.77;9,306.60,132.20,52.08,8.74">We call the result of the masking operation the pattern P (x) that we compute as:</s></p><formula xml:id="formula_29" coords="9,391.66,144.12,80.95,8.77">P (x) = x R(x).</formula><p><s coords="9,306.60,161.10,251.06,8.74;9,306.60,173.06,251.06,8.74;9,306.60,185.01,251.05,8.74;9,306.60,196.97,87.11,8.74">Here, we assume that the heatmap scores have been preliminarily normalized between 0 and 1 through rescaling and/or clipping so that the masked image remains in the original color space.</s><s coords="9,399.13,196.97,158.53,8.74;9,306.60,208.92,251.06,8.74;9,306.60,220.88,161.54,8.74">This visualization of LRP heatmaps makes it also more directly comparable to the visualizations techniques proposed in <ref type="bibr" coords="9,433.82,220.88,15.50,8.74" target="#b54">[55,</ref><ref type="bibr" coords="9,452.64,220.88,11.62,8.74" target="#b46">47]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7." coords="9,306.60,249.75,178.77,8.77">Quantifying Explanation Quality</head><p><s coords="9,316.57,269.71,241.10,8.74;9,306.60,281.67,91.90,8.74">In Sections 4 and 5, we have introduced a number of explanation techniques.</s><s coords="9,402.73,281.67,154.93,8.74;9,306.60,293.62,251.06,8.74;9,306.60,305.58,251.06,8.74;9,306.60,317.53,251.06,8.74;9,306.60,329.49,152.66,8.74">While each technique is based on its own intuition or mathematical principle, it is also important to define at a more abstract level what are the characteristics of a good explanation, and to be able to test for these characteristics quantitatively.</s><s coords="9,463.59,329.49,94.07,8.74;9,306.60,341.44,251.06,8.74;9,306.60,353.40,107.45,8.74">A quantitative framework allows to compare explanation techniques specifically for a target problem, e.g.</s><s coords="9,416.86,353.40,122.71,8.74">ILSVRC or MIT Places <ref type="bibr" coords="9,521.31,353.40,14.61,8.74" target="#b41">[42]</ref>.</s><s coords="9,543.82,353.40,13.84,8.74;9,306.60,365.35,251.05,8.74;9,306.60,377.31,250.92,8.74">We present in Sections 7.1 and 7.2 two important properties of an explanation, along with possible evaluation metrics.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1." coords="9,306.60,400.23,123.15,8.74">Explanation Continuity</head><p><s coords="9,316.57,414.19,241.10,8.74;9,306.60,426.15,223.65,8.74">A first desirable property of an explanation technique is that it produces a continuous explanation function.</s><s coords="9,534.66,426.15,23.00,8.74;9,306.60,438.10,251.06,8.74;9,306.60,450.06,69.07,8.74">Here, we implicitly assume that the prediction function f (x) is also continuous.</s><s coords="9,380.03,450.06,177.63,8.74;9,306.60,462.01,82.22,8.74">We would like to ensure in particular the following behavior:</s></p><p><s coords="9,326.53,480.62,211.20,8.74;9,326.53,492.58,211.21,8.74;9,326.53,504.53,75.74,8.74">If two data points are nearly equivalent, then the explanations of their predictions should also be nearly equivalent.</s></p><p><s coords="9,306.60,523.14,251.06,8.74;9,306.60,535.07,251.06,8.77;9,306.60,547.05,90.22,8.74">Explanation continuity (or lack of it) can be quantified by looking for the strongest variation of the explanation R(x) in the input domain:</s></p><formula xml:id="formula_30" coords="9,379.77,565.77,104.73,23.25">max x =x R(x) -R(x ) 1 x -x 2 .</formula><p><s coords="9,306.60,598.65,251.06,8.74;9,306.60,610.60,251.05,8.74;9,306.60,622.56,185.10,8.74">When f (x) is a deep ReLU network, both sensitivity analysis and simple Taylor decomposition have sharp discontinuities in their explanation function.</s><s coords="9,499.16,622.56,58.50,8.74;9,306.60,634.51,251.06,8.74">On the other hand, deep Taylor LRP produces continuous explanations.</s><s coords="9,306.60,646.47,251.06,8.74;9,306.60,658.42,107.36,9.65;9,413.97,656.85,3.97,6.12;9,413.97,662.93,6.12,6.12;9,420.58,658.42,137.08,8.74;9,306.60,670.38,87.51,8.74;9,352.48,690.65,151.98,9.65;9,412.45,705.60,92.00,9.65;9,412.45,721.64,53.96,8.74">This is illustrated in Figure <ref type="figure" coords="9,441.28,646.47,4.98,8.74" target="#fig_7">6</ref> for the simple function f (x) = max(x 1 , x 2 ) in R 2 + , here implemented by the twolayer ReLU network f (x) = max 0 , 0.5 max(0, x 1 -x 2 ) + 0.5 max(0, x 2 -x 1 ) + 0.5 max(0,</s></p><formula xml:id="formula_31" coords="9,468.08,721.64,43.71,9.65">x 1 + x 2 ) .</formula><p><s coords="9,316.57,741.91,241.10,8.74;9,306.60,753.87,251.06,8.74;10,37.61,277.56,251.06,8.74;10,37.61,289.52,72.68,8.74">It can be observed that despite the continuity of the prediction function, the explanations offered by sensitivity analysis and simple Taylor decomposition are discontinuous on the line</s></p><formula xml:id="formula_32" coords="10,114.59,289.52,39.59,9.65">x 1 = x 2 .</formula><p><s coords="10,161.50,289.52,127.17,8.74;10,37.61,301.47,130.28,8.74">Here, only deep Taylor LRP produces a smooth transition.</s></p><p><s coords="10,47.58,319.12,241.10,8.74;10,37.61,331.07,251.06,8.74;10,37.61,343.03,251.06,8.74;10,37.61,354.98,225.86,8.74">More generally, techniques that rely on the function's gradient, such as sensitivity analysis or simple Taylor decomposition, are more exposed to the derivative noise <ref type="bibr" coords="10,273.17,343.03,15.50,8.74" target="#b44">[45]</ref> that characterizes complex machine learning models.</s><s coords="10,267.64,354.98,21.03,8.74;10,37.61,366.94,251.06,8.74;10,37.61,378.89,150.35,8.74">Consequently, these techniques are also unlikely to score well in terms of explanation continuity.</s></p><formula xml:id="formula_33" coords="10,44.80,570.11,16.67,19.76">R 1 R 3 R 2 R 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,138.33,548.06,20.78,13.75">R(x)</head><p><s coords="10,37.61,725.28,119.85,6.99;10,184.36,725.28,104.31,6.99;10,37.61,734.75,251.06,6.99;10,37.61,744.21,34.59,6.99">Figure <ref type="figure" coords="10,64.19,725.28,3.29,6.99" target="#fig_8">7</ref>: Classification "2" by a explained by different methods, as we move a handwritten digit from left to right in its receptive field.</s><s coords="10,76.72,744.21,211.94,6.99;10,37.61,753.68,170.90,6.99">Relevance scores are pooled into four quadrants, and are tracked as we apply the translation operation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2." coords="10,306.60,84.38,120.35,8.74">Explanation Selectivity</head><p><s coords="10,316.57,99.43,241.09,8.74;10,306.60,111.39,251.06,8.74;10,306.60,123.34,128.98,8.74">Another desirable property of an explanation is that it redistributes relevance to variables that have the strongest impact on the function f (x).</s><s coords="10,442.04,123.34,115.62,8.74;10,306.60,135.30,251.06,8.74;10,306.60,147.25,251.06,8.74;10,306.60,159.21,105.66,8.74">Bach et al. <ref type="bibr" coords="10,494.59,123.34,10.52,8.74" target="#b4">[5]</ref> and Samek et al. <ref type="bibr" coords="10,334.03,135.30,15.50,8.74" target="#b41">[42]</ref> proposed to quantify selectivity by measuring how fast f (x) goes down when removing features with highest relevance scores.</s></p><p><s coords="10,316.57,171.18,241.10,8.74;10,306.60,183.13,251.06,8.74;10,306.60,195.09,251.05,8.74;10,306.60,207.04,116.37,8.74">The method was introduced for image data under the name "pixel-flipping" <ref type="bibr" coords="10,402.17,183.13,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="10,415.49,183.13,11.62,8.74" target="#b41">42]</ref>, and was also adapted to text data, where words selected for removal have their word embeddings set to zero <ref type="bibr" coords="10,409.69,207.04,9.96,8.74" target="#b2">[3]</ref>.</s><s coords="10,427.40,207.04,129.44,8.74">The method works as follows:</s></p><p><s coords="10,319.16,228.84,195.59,8.77">repeat until all features have been removed:</s></p><p><s coords="10,329.12,248.79,177.29,8.74">• record the current function value f (x)</s></p><p><s coords="10,329.12,264.73,195.34,9.65">• find feature i with highest relevance R i (x)</s></p><p><s coords="10,329.12,280.64,166.66,9.68;10,319.16,300.60,225.95,8.74;10,319.16,312.55,225.96,8.74">• remove that feature (x ← x -{x i }) make a plot with all recorded function values, and return the area under the curve (AUC) for that plot.</s></p><p><s coords="10,306.60,334.36,251.05,8.74;10,306.60,346.32,251.06,8.74;10,306.60,358.27,92.21,8.74">A sharp drop of function's value, characterized by a low AUC score indicates that the correct features have been identified as relevant.</s><s coords="10,403.14,358.27,154.52,8.74;10,306.60,370.23,178.11,8.74">AUC results can be averaged over a large number of examples in the dataset.</s></p><p><s coords="10,316.57,382.20,241.10,8.74;10,306.60,394.16,51.88,8.74">Figure <ref type="figure" coords="10,348.13,382.20,4.98,8.74">8</ref> illustrates the procedure on the same DNN as in Figure <ref type="figure" coords="10,350.74,394.16,3.87,8.74" target="#fig_8">7</ref>.</s><s coords="10,364.52,394.16,193.14,8.74;10,306.60,406.11,251.06,8.74;10,306.60,418.07,25.46,8.74">At each iteration, a patch of size 4 × 4 corresponding to the region with highest relevance is set to black.</s><s coords="10,338.69,418.07,218.97,8.74;10,306.60,430.02,236.29,8.74">The plot on the right keeps track of the function score as the features are being progressively removed.</s><s coords="10,548.53,430.02,9.13,8.74;10,306.60,441.98,251.06,8.74;10,306.60,453.93,251.05,8.74;10,306.60,465.89,96.39,8.74">In this particular case, the plot indicates that deep Taylor LRP is more selective than sensitivity analysis and simple Taylor decomposition.</s></p><p><s coords="10,316.57,477.86,241.10,8.74;10,306.60,489.81,251.06,8.74;10,306.60,501.77,34.18,8.74">It is important to note however, that the result of the analysis depends to some extent on the feature removal process.</s><s coords="10,347.66,501.77,210.01,8.74;10,306.60,513.72,251.05,8.74;10,306.60,525.68,212.72,8.74">Various feature removal strategies can be used, but a general rule is that it should keep as much as possible the image being modified on the data manifold.</s><s coords="10,525.84,525.68,31.83,8.74;10,328.58,578.94,37.08,13.21;10,416.41,579.68,38.89,13.21">Indeed, examples heatmaps</s></p><formula xml:id="formula_34" coords="10,382.24,603.70,14.75,24.35">(1)<label>(2)</label></formula><p><s coords="10,386.72,633.53,10.28,8.69">(1)</s></p><p><s coords="10,382.24,649.19,10.28,8.69">(2)</s></p><formula xml:id="formula_35" coords="10,382.24,663.36,14.75,24.35">(1)<label>(2)</label></formula><p><s coords="10,474.95,665.68,10.91,23.52;10,474.95,624.11,10.91,39.09;10,474.95,606.06,10.91,15.56;10,490.29,706.30,61.77,10.91;11,37.61,84.38,251.06,8.74;11,37.61,96.33,195.32,8.74">average classification score # features removed this guarantees that the DNN continues to work reliably through the whole feature removal procedure.</s><s coords="11,237.09,96.33,51.59,8.74;11,37.61,108.29,251.06,8.74;11,37.61,120.24,41.26,8.74">This in turn makes the analysis less subject to uncontrolled factors of variation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8." coords="11,37.61,149.52,77.31,8.77">Applications</head><p><s coords="11,47.58,169.89,241.10,8.74;11,37.61,181.85,251.06,8.74;11,37.61,193.80,251.05,8.74;11,37.61,205.76,50.92,8.74">Potential applications of explanation techniques are vast and include as diverse domains as extraction of domain knowledge, computer-assisted decisions, data filtering, or compliance.</s><s coords="11,94.09,205.76,194.58,8.74;11,37.61,217.71,251.06,8.74;11,37.61,229.67,62.88,8.74">We focus in this section on two types of applications: validation of a trained model, and analysis of scientific data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1." coords="11,37.61,253.00,95.26,8.74">Model Validation</head><p><s coords="11,47.58,267.37,241.10,8.74;11,37.61,279.32,251.06,8.74">Model validation is usually achieved by measuring the error on some validation set disjoint from the training data.</s><s coords="11,37.61,291.28,251.06,8.74;11,37.61,303.23,251.05,8.74;11,37.61,315.19,251.05,8.74;11,37.61,327.14,251.06,8.74;11,37.61,339.10,54.32,8.74">While providing a simple way to compare different machine learning models in practice, it should be reminded that the validation error is only a proxy for the true error and that the data distribution and labeling process might differ.</s><s coords="11,98.16,339.10,190.51,8.74;11,37.61,351.05,251.06,8.74;11,37.61,363.01,45.72,8.74">A human inspection of the model rendered interpretable can be a good complement to the validation procedure.</s><s coords="11,88.51,363.01,200.16,8.74;11,37.61,374.96,251.06,8.74;11,37.61,386.92,251.06,8.74;11,37.61,398.87,67.63,8.74">We present two recent examples showing how explainability allows to better validate a machine learning model by pointing out at some unsuspected qualitative properties of it.</s></p><p><s coords="11,47.58,410.83,241.10,8.74;11,37.61,422.78,251.06,8.74;11,37.61,434.74,251.06,8.74;11,37.61,446.69,251.06,8.74;11,37.61,458.65,251.06,8.74;11,37.61,470.60,68.52,8.74">Arras et al. <ref type="bibr" coords="11,98.66,410.83,10.52,8.74" target="#b2">[3]</ref> considered a document classification task on the 20-Newsgroup dataset, and compared the explanations of a convolutional neural network (CNN) trained on word2vec inputs to the explanations of a support vector machine (SVM) trained on bag-of-words (BoW) document representations.</s><s coords="11,110.54,470.60,178.14,8.74;11,37.61,482.56,251.06,8.74;11,37.61,494.51,251.06,8.74;11,306.60,84.38,227.87,8.74">They observed that, although both models produce a similar test error, the CNN model assigns most relevance to a small number of keywords, whereas the SVM classifier relies on word count regularities.</s><s coords="11,540.09,84.38,17.57,8.74;11,306.60,96.33,251.06,8.74;11,306.60,108.29,73.20,9.02">Figure <ref type="figure" coords="11,323.19,96.33,4.43,8.74" target="#fig_10">9</ref>(a) displays explanations for an example of the target class sci.space.</s><s coords="11,316.57,120.24,241.10,8.74;11,306.60,132.20,251.06,8.74;11,306.60,144.15,233.30,8.74">Lapuschkin et al. <ref type="bibr" coords="11,401.68,120.24,15.50,8.74" target="#b26">[27]</ref> compared the decisions taken by convolutional DNN transferred from ImageNet, and a Fisher vector classifier on PASCAL VOC 2012 images.</s><s coords="11,544.10,144.15,13.56,8.74;11,306.60,156.11,251.06,8.74;11,306.60,168.06,251.05,8.74;11,306.60,180.02,251.06,8.74">Although both models reach similar classification accuracy on the category "horse", the authors observed that they use different strategies to classify images of that category.</s><s coords="11,306.60,191.97,251.06,8.74">Explanations for a given image are shown in Figure <ref type="figure" coords="11,536.63,191.97,16.83,8.74" target="#fig_10">9(b)</ref>.</s><s coords="11,306.60,203.93,251.06,8.74;11,306.60,215.88,251.05,8.74;11,306.60,227.84,251.05,8.74;11,306.60,239.79,251.06,8.74;11,306.60,251.75,58.33,8.74">The deep neural network looks at the contour of the actual horse, whereas the Fisher vector model (of more rudimentary structure and trained with less data) relies mostly on a copyright tag, that happens to be present on many horse images.</s><s coords="11,369.58,251.75,188.08,8.74;11,306.60,263.70,251.05,8.74;11,306.60,275.66,251.06,8.74;11,306.60,287.62,189.99,8.74">Removing the copyright tag in the test images would consequently significantly decrease the measured accuracy of the Fisher vector model but leave the deep neural network predictions unaffected.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2." coords="11,306.60,310.03,137.65,8.74">Analysis of Scientific Data</head><p><s coords="11,316.57,323.48,241.09,8.74;11,306.60,335.44,251.06,8.74;11,306.60,347.39,251.06,8.74">Beyond model validation, techniques of explanation can also be applied to shed light on scientific problems where human intuition and domain knowledge is often limited.</s><s coords="11,306.60,359.35,251.06,8.74;11,306.60,371.30,251.06,8.74;11,306.60,383.26,251.06,8.74;11,306.60,395.21,251.06,8.74;11,306.60,407.17,251.06,8.74;11,306.60,419.12,54.36,8.74">Simple statistical tests and linear models have proved useful to identify correlations between different variables of a system, however, the measured correlations typically remain weak due to the inability of these models to capture the underlying complexity and nonlinearity of the studied problem.</s><s coords="11,365.63,419.12,192.03,8.74;11,306.60,431.08,251.05,8.74;11,306.60,443.03,251.06,8.74;11,306.60,454.99,251.06,8.74;11,306.60,466.94,28.53,8.74">For a long time, the computational scientist would face a tradeoff between interpretability and predictive power, where linear models would sometimes be preferred to nonlinear models despite their lower predictive power.</s><s coords="11,340.28,466.94,217.37,8.74;11,306.60,478.90,251.06,8.74;11,306.60,490.85,251.06,8.74;11,306.60,502.81,98.93,8.74">We give below a selection of recent works in various fields of research, that combine deep neural networks and explanation techniques to extract insight on the studied scientific problems.</s></p><p><s coords="11,316.57,514.76,241.09,8.74;11,306.60,526.72,251.05,8.74;11,306.60,538.67,221.40,8.74">In the domain of atomistic simulations, powerful machine learning models have been produced to link molecular structure to electronic properties <ref type="bibr" coords="11,464.07,538.67,15.50,8.74" target="#b35">[36,</ref><ref type="bibr" coords="11,482.06,538.67,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="11,497.28,538.67,12.73,8.74" target="#b42">43,</ref><ref type="bibr" coords="11,512.50,538.67,11.62,8.74" target="#b15">16]</ref>.</s><s coords="11,532.14,538.67,25.51,8.74;11,306.60,550.63,251.05,8.74;11,306.60,562.58,217.12,8.74">These models have been trained in a data-driven manner, without simulated physics involved into the prediction.</s><s coords="11,527.98,562.58,29.68,8.74;11,306.60,574.54,251.06,8.74;11,306.60,586.49,251.05,8.74;11,306.60,598.45,251.06,8.74;11,306.60,610.40,141.67,8.74">In particular, Schütt et al. <ref type="bibr" coords="11,402.60,574.54,15.50,8.74" target="#b42">[43]</ref> proposed a deep tensor neural network model that incorporates sufficient structure and representational power to simultaneously achieve high predictive power and explainability.</s><s coords="11,452.94,610.40,104.73,8.74;11,306.60,622.36,251.06,8.74;11,306.60,634.32,251.06,8.74;11,306.60,646.27,251.06,8.74;11,306.60,658.23,251.06,8.74;11,306.60,670.18,251.05,8.74;11,306.60,682.14,251.06,8.74;11,306.60,694.09,45.44,8.74">Using a test-charge perturbation analysis (a variant of sensitivity analysis where one measures the effect on the neural network output of inserting a charge at a given location), three-dimensional response maps were produced that highlight for each individual molecule spatial structures that were the most relevant for explaining the modeled structure-property relationship.</s><s coords="11,359.56,694.09,198.11,8.74;11,306.60,706.05,136.91,8.74">Example of response maps are given in Figure <ref type="figure" coords="11,323.79,706.05,9.08,8.74" target="#fig_11">10</ref>(a) for various molecules.</s></p><p><s coords="11,316.57,718.00,241.10,8.74;11,306.60,729.96,195.10,8.74">Sturm et al. <ref type="bibr" coords="11,369.77,718.00,15.50,8.74" target="#b48">[49]</ref> showed that explanation techniques can also be applied to EEG brain recording data.</s><s coords="11,506.03,729.96,51.64,8.74;11,306.60,741.91,251.06,8.74;11,306.60,753.87,251.06,8.74;12,37.61,394.24,251.06,8.74;12,37.61,406.20,175.71,8.74">Because the input EEG pattern can take different forms (due to different users, environments, or calibration of the acquisition device), it is important to produce an individual explanation that adapts to these parameters.</s><s coords="12,219.09,406.20,69.58,8.74;12,37.61,418.15,251.06,8.74;12,37.61,430.11,251.05,8.74;12,37.61,442.06,251.05,8.74;12,37.61,454.02,251.06,8.74;12,37.61,465.97,251.06,8.74;12,37.61,477.93,251.06,8.74;12,37.61,489.88,251.05,8.74;12,37.61,501.84,251.05,8.74;12,37.61,513.80,44.92,8.74">After training a neural network to map EEG patterns to a set of movements imagined by the user ("right hand" and "foot"), a LRP decomposition of that prediction could be achieved in the EEG input domain (a spatiotemporal signal capturing the electrode measurements at various positions on the skull and at multiple time steps), and pooled temporally to produce EEG heatmaps revealing from which part of the brain the decision for "right hand" or "foot" originates.</s><s coords="12,86.78,513.80,201.89,8.74;12,37.61,525.75,251.06,8.74;12,37.61,537.71,251.06,8.74;12,37.61,549.66,143.27,8.74">An interesting property of decomposition techniques in this context is that temporally pooling preserves the total function value, and thus, still corresponds to a decomposition of the prediction.</s><s coords="12,187.91,549.66,100.76,8.74;12,37.61,561.62,228.42,8.74">Example of these individual EEG brain maps are given in Figure <ref type="figure" coords="12,240.02,561.62,21.68,8.74" target="#fig_11">10(b)</ref>.</s><s coords="12,274.12,561.62,14.56,8.74;12,37.61,573.57,251.06,8.74;12,37.61,585.53,251.06,8.74;12,37.61,597.48,135.41,8.74">For classical linear explanation of neural activation patterns in cognitive brain science experiments or Brain Computer Interfacing, see <ref type="bibr" coords="12,106.61,597.48,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="12,125.42,597.48,12.73,8.74" target="#b29">30,</ref><ref type="bibr" coords="12,141.48,597.48,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="12,157.53,597.48,11.62,8.74" target="#b21">22]</ref>.</s></p><p><s coords="12,47.58,610.40,241.10,8.74;12,37.61,622.36,125.42,8.74">Deep neural networks have also been proposed to make sense of the human genome.</s><s coords="12,169.70,622.36,118.97,8.74;12,37.61,634.32,251.05,8.74;12,37.61,646.27,102.38,8.74">Alipanahi et al. <ref type="bibr" coords="12,243.07,622.36,10.52,8.74" target="#b0">[1]</ref> trained a convolutional neural network to map the DNA sequence to protein binding sites.</s><s coords="12,144.22,646.27,144.46,8.74;12,37.61,658.23,251.06,8.74;12,37.61,670.18,251.06,8.74">In a second step, they asked what are the nucleotides of that sequence that are the most relevant for explaining the presence of these binding sites.</s><s coords="12,37.61,682.14,251.06,8.74;12,37.61,694.09,251.06,8.74;12,37.61,706.05,251.06,8.74;12,37.61,718.00,251.06,8.74;12,37.61,729.96,19.93,8.74">For this, they used a perturbation-based analysis, similar to the sensitivity analysis described in Section 4.1, where the relevance score of each nucleotide is measured based on the effect of mutating it on the neural network prediction.</s><s coords="12,61.92,729.96,226.75,8.74;12,37.61,741.91,251.05,8.74;12,37.61,753.87,251.05,8.74;12,306.60,84.38,146.45,8.74">Other measures of feature importance for individual gene sequences have been proposed <ref type="bibr" coords="12,199.59,741.91,15.50,8.74" target="#b52">[53]</ref> that apply to a broad class of nonlinear models, from deep networks to weighted degree kernel classifiers.</s><s coords="12,458.66,84.38,99.01,8.74;12,306.60,96.33,251.06,8.74;12,306.60,108.29,191.81,8.74">Examples of heatmaps representing relevant genes for various sequences and prediction outcomes are shown in Figure <ref type="figure" coords="12,473.50,108.29,20.76,8.74" target="#fig_11">10(c)</ref>.</s></p><p><s coords="12,316.57,121.97,241.10,8.74;12,306.60,133.93,133.62,8.74">Explanation techniques also have a potential application in the analysis of face images.</s><s coords="12,446.85,133.93,110.82,8.74;12,306.60,145.88,251.06,8.74;12,306.60,157.84,113.72,8.74">These images may reveal a wide range of information about the person's identity, emotional state, or health.</s><s coords="12,424.57,157.84,133.09,8.74;12,306.60,169.79,251.06,8.74;12,306.60,181.75,50.76,8.74">However, interpreting them directly in terms of actual features of the input image can be difficult.</s><s coords="12,365.46,181.75,192.21,8.74;12,306.60,193.70,251.06,8.74;12,306.60,205.66,251.05,8.74;12,306.60,217.61,45.47,8.74">Arbabzadah et al. <ref type="bibr" coords="12,450.80,181.75,10.52,8.74" target="#b1">[2]</ref> applied a LRP technique to identify which pixels in a given image are responsible for explaining, for example, the age and gender attributes.</s><s coords="12,356.98,217.61,200.68,8.74;12,306.60,229.57,69.08,8.74">Example of pixel-wise explanations are shown in Figure <ref type="figure" coords="12,349.67,229.57,8.67,8.74" target="#fig_11">10</ref>(d).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9." coords="12,306.60,271.54,69.44,8.77">Conclusion</head><p><s coords="12,316.57,304.60,241.10,8.74;12,306.60,316.56,251.06,8.74;12,306.60,328.51,168.08,8.74">Building transparent machine learning systems is a convergent approach to both extracting novel domain knowledge and performing model validation.</s><s coords="12,479.10,328.51,78.56,8.74;12,306.60,340.47,251.06,8.74;12,306.60,352.42,251.06,8.74;12,306.60,364.38,61.24,8.74">As machine learning is increasingly used in real-world decision processes, the necessity for transparent machine learning will continue to grow.</s><s coords="12,372.75,364.38,184.91,8.74;12,306.60,376.34,218.70,8.74">Examples that illustrate the limitations of black-box methods were mentioned in Section 8.1.</s></p><p><s coords="12,316.57,390.02,241.10,8.74;12,306.60,401.98,251.06,8.74;12,306.60,413.93,251.06,8.74;12,306.60,425.89,251.06,8.74;12,306.60,437.84,95.70,8.74">This tutorial has covered two key directions for improving machine learning transparency: interpreting the concepts learned by a model by building prototypes, and explaining of the model's decisions by identifying the relevant input variables.</s><s coords="12,409.42,437.84,148.24,8.74;12,306.60,449.80,251.05,8.74;12,306.60,461.75,134.94,8.74">The discussion mainly abstracted from the exact choice of deep neural network, training procedure, or application domain.</s><s coords="12,447.86,461.75,109.81,8.74;12,306.60,473.71,251.06,8.74;12,306.60,485.66,237.28,8.74">Instead, we have focused on the more conceptual developments, and connected them to recent practical successes reported in the literature.</s></p><p><s coords="12,316.57,499.35,241.10,8.74;12,306.60,511.30,251.06,8.74;12,306.60,523.26,62.86,8.74">In particular we have discussed the effect of linking prototypes to the data, via a data density function or a generative model.</s><s coords="12,377.28,523.26,180.38,8.74;12,306.60,535.21,251.06,8.74;12,306.60,547.17,172.74,8.74">We have described the crucial difference between sensitivity analysis and decomposition in terms of what these analyses seek to explain.</s><s coords="12,485.91,547.17,71.75,8.74;12,306.60,559.12,251.06,8.74;12,306.60,571.08,251.06,8.74;12,306.60,583.03,203.16,8.74">Finally, we have outlined the benefit in terms of robustness, of treating the explanation problem with graph propagation techniques rather than with standard analysis techniques.</s></p><p><s coords="12,316.57,596.72,241.09,8.74;12,306.60,608.67,251.06,8.74;12,306.60,620.63,20.51,8.74">This tutorial has focused on post-hoc interpretability, where we do not have full control over the model's structure.</s><s coords="12,332.19,620.63,225.47,8.74;12,306.60,632.58,251.06,8.74;12,306.60,644.54,251.06,8.74;12,306.60,656.50,251.06,8.74;12,306.60,668.45,186.89,8.74;12,316.57,682.14,241.09,8.74;12,306.60,694.09,251.06,8.74;12,306.60,706.05,251.06,8.74;12,306.60,718.00,251.06,8.74;12,306.60,729.96,48.79,8.74">Instead, the techniques of interpretation should apply to a general class of nonlinear machine learning models, no matter how they were trained and who trained themeven fully trained models that are available for download like BVLC CaffeNet <ref type="bibr" coords="12,397.65,668.45,15.50,8.74" target="#b23">[24]</ref> or GoogleNet <ref type="bibr" coords="12,478.00,668.45,15.50,8.74" target="#b49">[50]</ref> In that sense the presented novel technological development in ML allowing for interpretability is an orthogonal strand of research independent of new developments for improving neural network models and their learning algorithms.</s><s coords="12,363.23,729.96,194.43,8.74;12,306.60,741.91,251.06,8.74;12,306.60,753.87,67.58,8.74">We would like to stress that all new developments can in this sense always profit in addition from interpretability.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,37.61,306.66,251.06,6.99;3,37.61,316.10,251.06,7.01;3,37.61,325.59,199.93,6.99"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="3,37.61,306.66,251.06,6.99;3,37.61,316.10,94.61,7.01">Figure 1: Cartoon illustrating how the expert p(x) affects the prototype x found by AM.</s><s coords="3,135.88,316.12,152.79,6.99;3,37.61,325.59,199.93,6.99">The horizontal axis represents the input space, and the vertical axis represents the probability.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,306.60,665.24,251.06,6.99;3,306.60,674.70,251.06,6.99;3,306.60,684.17,189.67,6.99;3,473.90,573.98,80.41,60.30"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s coords="3,306.60,665.24,251.06,6.99;3,306.60,674.70,29.45,6.99">Figure 2: Architectures supporting AM procedures and found prototypes.</s><s coords="3,343.79,674.70,213.87,6.99;3,306.60,684.17,189.67,6.99">Black arrows indicate the forward path and red arrows indicate the reverse path for gradient computation.</s></p></div></figDesc><graphic coords="3,473.90,573.98,80.41,60.30" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,37.61,607.93,251.06,7.01;4,37.61,617.41,54.62,6.99;4,45.93,535.98,56.88,56.88"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s coords="4,37.61,607.93,251.06,7.01;4,37.61,617.41,54.62,6.99">Figure 3: Explanation of the DNN prediction "boat" for an image x given as input.</s></p></div></figDesc><graphic coords="4,45.93,535.98,56.88,56.88" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,306.60,321.26,83.15,8.74;6,389.75,326.12,7.94,6.12;6,402.57,321.26,65.62,9.65;6,468.19,326.12,4.24,6.12;6,477.31,321.26,80.35,9.65;6,306.60,333.21,164.42,8.74;6,311.91,354.41,51.36,13.04;6,327.27,365.91,27.63,13.04;6,494.41,354.41,50.83,13.04;6,506.69,365.91,48.08,13.04;6,388.33,355.06,75.73,13.04"><head/><label/><figDesc><div><p><s coords="6,306.60,321.26,83.15,8.74;6,389.75,326.12,7.94,6.12;6,402.57,321.26,65.62,9.65;6,468.19,326.12,4.24,6.12;6,477.31,321.26,33.12,9.65">with parameters w jk = w jk c k and b k = b k c k .</s><s coords="6,517.47,321.26,40.20,8.74;6,306.60,333.21,164.42,8.74">The relevance neuron is shown in Figure 4(a).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,306.60,461.34,251.06,6.99;6,306.60,470.80,251.06,6.99;6,306.60,480.27,224.05,8.01"><head>Figure 4 :</head><label>4</label><figDesc><div><p><s coords="6,306.60,461.34,229.85,6.99">Figure 4: Diagram of the relevance neuron and its analysis.</s><s coords="6,543.08,461.34,14.58,6.99;6,306.60,470.80,251.06,6.99;6,306.60,480.27,224.05,8.01">The root search domain is shown with a dashed line, and the relevance propagation resulting from decomposing R k is shown in red.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,317.07,385.99,99.38,8.30;8,337.99,403.61,109.84,8.30;8,337.99,415.57,146.45,8.30;8,337.99,427.52,57.53,8.30;8,337.99,445.15,104.61,8.30;8,337.99,457.10,47.07,8.30;8,337.99,469.06,109.84,8.30;8,337.99,486.68,62.76,8.30"><head/><label/><figDesc><div><p><s coords="8,317.07,385.99,99.38,8.30;8,337.99,403.61,109.84,8.30">def lrp(layer,a,R): clone = layer.clone()</s><s coords="8,337.99,415.57,146.45,8.30;8,337.99,427.52,57.53,8.30;8,337.99,445.15,104.61,8.30">clone.W = maximum(0,layer.W) clone.B = 0 z = clone.forward(a)</s><s coords="8,337.99,457.10,47.07,8.30;8,337.99,469.06,109.84,8.30">s = R / z c = clone.backward(s)</s><s coords="8,337.99,486.68,62.76,8.30">return a * c</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,37.61,199.81,251.06,7.71;10,37.61,209.27,251.05,6.99;10,37.61,218.74,251.05,6.99;10,37.61,228.20,251.06,6.99;10,37.61,237.67,20.75,6.99;10,203.36,104.50,81.59,81.59"><head>Figure 6 :</head><label>6</label><figDesc><div><p><s coords="10,37.61,199.81,127.78,7.71">Figure 6: Explaining max(x 1 , x 2 ).</s><s coords="10,170.46,199.81,118.21,6.99;10,37.61,209.27,251.05,6.99">Function values are represented as a contour plot, with dark regions corresponding to high values.</s><s coords="10,37.61,218.74,251.05,6.99;10,37.61,228.20,251.06,6.99;10,37.61,237.67,20.75,6.99">Relevance scores are represented as a vector field, where horizontal and vertical components are the relevance of respective input variables.</s></p></div></figDesc><graphic coords="10,203.36,104.50,81.59,81.59" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,47.58,396.53,241.09,8.74;10,37.61,408.49,251.06,8.74;10,37.61,420.44,251.06,8.74;10,37.61,432.40,251.06,8.74;10,37.61,444.35,251.06,8.74;10,37.61,456.31,184.00,8.74"><head>Figure 7</head><label>7</label><figDesc><div><p><s coords="10,47.58,396.53,241.09,8.74;10,37.61,408.49,251.06,8.74;10,37.61,420.44,148.91,8.74">Figure 7 shows the function value and the relevance scores for each technique, when applying them to a convolutional DNN trained on MNIST.</s><s coords="10,190.50,420.44,98.17,8.74;10,37.61,432.40,251.06,8.74;10,37.61,444.35,74.64,8.74">Although the function itself is relatively low-varying, strong variations occur in the explanations.</s><s coords="10,117.10,444.35,171.57,8.74;10,37.61,456.31,184.00,8.74">Here again, only deep Taylor LRP produces reasonably continuous explanations.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="10,306.60,734.31,251.06,6.99;10,306.60,743.77,251.06,6.99;10,306.60,753.24,190.75,6.99;10,486.58,581.24,67.12,134.25"><head>( 1 )Figure 8 :</head><label>18</label><figDesc><div><p><s coords="10,306.60,734.31,200.08,6.99">Figure 8: Illustration of the "pixel-flipping" procedure.</s><s coords="10,510.23,734.31,47.43,6.99;10,306.60,743.77,251.06,6.99;10,306.60,753.24,190.75,6.99">At each step, the heatmap is used to determine which region to remove (by setting it to black), and the classification score is recorded.</s></p></div></figDesc><graphic coords="10,486.58,581.24,67.12,134.25" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="11,37.61,725.28,251.06,6.99;11,37.61,734.75,251.06,6.99;11,37.61,744.21,251.06,6.99;11,37.61,753.68,168.12,6.99;11,122.41,645.26,81.44,55.97"><head>Figure 9 :</head><label>9</label><figDesc><div><p><s coords="11,37.61,725.28,251.06,6.99;11,37.61,734.75,58.90,6.99">Figure 9: Examples taken from the literature of model validation via explanation.</s><s coords="11,100.16,734.75,188.51,6.99;11,37.61,744.21,54.77,6.99">(a) Explanation of the concept "sci.space" by two text classifiers.</s><s coords="11,97.01,744.21,191.67,6.99;11,37.61,753.68,168.12,6.99">(b) Unexpected use of copyright tags by the Fisher vector model for predicting the class "horse".</s></p></div></figDesc><graphic coords="11,122.41,645.26,81.44,55.97" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="12,37.61,324.37,251.06,6.99;12,37.61,333.84,251.06,6.99;12,37.61,343.30,251.06,6.99;12,37.61,352.77,251.06,6.99;12,37.61,362.23,66.35,6.99"><head>Figure 10 :</head><label>10</label><figDesc><div><p><s coords="12,37.61,324.37,251.06,6.99;12,37.61,333.84,137.15,6.99">Figure 10: Overview of several applications of machine learning explanation techniques in the sciences.</s><s coords="12,180.36,333.84,108.31,6.99;12,37.61,343.30,251.06,6.99;12,37.61,352.77,251.06,6.99;12,37.61,362.23,66.35,6.99">(a) Molecular response maps for quantum chemistry, (b) EEG heatmaps for neuroimaging, (c) extracting relevant information from gene sequences, (d) analysis of facial appearance.</s></p></div></figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head coords="13,37.61,84.35,89.88,8.77">Acknowledgments</head><p>We gratefully acknowledge discussions and comments on the manuscript by our colleagues <rs type="person">Sebastian Lapuschkin</rs>, and <rs type="person">Alexander Binder</rs>.This work was supported by the <rs type="funder">Brain Korea 21 Plus Program through the National Research Foundation of Korea</rs>; the <rs type="funder">Institute for Information &amp; Communications Technology Promotion (IITP)</rs> grant funded by the <rs type="funder">Korea government</rs> [No.<rs type="grantNumber">2017-0-00451</rs>]; the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG)</rs> [grant <rs type="grantNumber">MU 987/17-1</rs>]; and the <rs type="funder">German Ministry for Education and Research as Berlin Big Data Center (BBDC)</rs> [<rs type="grantNumber">01IS14013A</rs>].This publication only reflects the authors views.Funding agencies are not liable for any use that may be made of the information contained herein.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WjPdAf3">
					<idno type="grant-number">2017-0-00451</idno>
				</org>
				<org type="funding" xml:id="_unGNcD7">
					<idno type="grant-number">MU 987/17-1</idno>
				</org>
				<org type="funding" xml:id="_CJtSh7s">
					<idno type="grant-number">01IS14013A</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,55.77,291.47,232.90,6.99;13,55.77,300.93,232.90,6.99;13,55.77,310.40,232.90,6.99;13,55.77,319.86,31.99,6.99" xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</title>
		<author>
			<persName><forename type="first">Babak</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Weirauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<idno type="DOI">10.1038/nbt.3300</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<title level="j" type="abbrev">Nat Biotechnol</title>
		<idno type="ISSN">1087-0156</idno>
		<idno type="ISSNe">1546-1696</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="831" to="838"/>
			<date type="published" when="2015-07-27">jul 2015</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alipanahi, B., Delong, A., Weirauch, M. T., Frey, B. J., jul 2015. Predicting the sequence specificities of DNA-and RNA- binding proteins by deep learning. Nature Biotechnology 33 (8), 831-838.</note>
</biblStruct>

<biblStruct coords="13,55.77,329.32,232.90,6.99;13,55.77,338.79,232.90,6.99;13,55.77,348.25,232.91,6.99;13,55.77,357.72,232.90,6.99;13,55.77,367.18,131.66,6.99" xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifying Individual Facial Expressions by Deconstructing a Neural Network</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-45886-1_28</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Hannover, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016-09-12">2016. 2016. September 12-15, 2016</date>
			<biblScope unit="page" from="344" to="354"/>
		</imprint>
	</monogr>
	<note>GCPR</note>
	<note type="raw_reference">Arbabzadah, F., Montavon, G., Müller, K.-R., Samek, W., 2016. Identifying individual facial expressions by deconstruct- ing a neural network. In: Pattern Recognition -38th German Conference, GCPR 2016, Hannover, Germany, September 12- 15, 2016, Proceedings. pp. 344-354.</note>
</biblStruct>

<biblStruct coords="13,55.77,376.65,232.90,6.99;13,55.77,386.11,232.90,6.99;13,55.77,395.58,189.41,6.99" xml:id="b2">
	<analytic>
		<title level="a" type="main">"What is relevant in a text document?": An interpretable machine learning approach</title>
		<author>
			<persName><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
			<idno type="ORCID">0000-0002-6283-3265</idno>
		</author>
		<idno type="DOI">10.1371/journal.pone.0181142</idno>
		<idno>CoRR abs/1612.07843</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<title level="j" type="abbrev">PLoS ONE</title>
		<idno type="ISSNe">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e0181142</biblScope>
			<date type="published" when="2016">2016</date>
			<publisher>Public Library of Science (PLoS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Arras, L., Horn, F., Montavon, G., Müller, K.-R., Samek, W., 2016. "What is relevant in a text document?": An interpretable machine learning approach. CoRR abs/1612.07843.</note>
</biblStruct>

<biblStruct coords="13,55.77,405.04,232.90,6.99;13,55.77,414.51,232.90,6.99;13,55.77,423.97,103.63,6.99" xml:id="b3">
	<analytic>
		<title level="a" type="main">Explaining Recurrent Neural Network Predictions in Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-5221</idno>
		<idno>CoRR abs/1706.07206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Arras, L., Montavon, G., Müller, K.-R., Samek, W., 2017. Ex- plaining recurrent neural network predictions in sentiment anal- ysis. CoRR abs/1706.07206.</note>
</biblStruct>

<biblStruct coords="13,55.77,433.43,232.90,6.99;13,55.77,442.90,232.91,6.99;13,55.77,452.36,232.90,6.99;13,55.77,461.83,93.25,6.99" xml:id="b4">
	<analytic>
		<title level="a" type="main">On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0130140</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<title level="j" type="abbrev">PLoS ONE</title>
		<idno type="ISSNe">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e0130140</biblScope>
			<date type="published" when="2015-07-10">07 2015</date>
			<publisher>Public Library of Science (PLoS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.- R., Samek, W., 07 2015. On pixel-wise explanations for non- linear classifier decisions by layer-wise relevance propagation. PLOS ONE 10 (7), 1-46.</note>
</biblStruct>

<biblStruct coords="13,55.77,471.29,110.04,6.99;13,216.31,471.29,72.36,6.99;13,55.77,480.76,232.90,6.99;13,55.77,490.22,232.90,6.99;13,55.77,499.69,40.46,6.99" xml:id="b5">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831"/>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Baehrens, D., Schroeter, T., S., Kawanabe, M., Hansen, K., Müller, K.-R., 2010. How to explain individual clas- sification decisions. Journal of Machine Learning Research 11, 1803-1831.</note>
</biblStruct>

<biblStruct coords="13,55.77,509.15,232.90,6.99;13,55.77,518.62,232.91,6.99;13,55.77,528.08,83.89,6.99" xml:id="b6">
	<analytic>
		<title level="a" type="main">An Oaxaca Decomposition for Nonlinear Models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bazen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Joutard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brice</forename><surname>Magdalou</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.2776914</idno>
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<title level="j" type="abbrev">SSRN Journal</title>
		<idno type="ISSNe">1556-5068</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note>Working papers, HAL</note>
	<note type="raw_reference">Bazen, S., Joutard, X., 2013. The Taylor decomposition: A uni- fied generalization of the Oaxaca method to nonlinear models. Working papers, HAL.</note>
</biblStruct>

<biblStruct coords="13,55.77,537.54,232.90,6.99;13,55.77,547.01,232.90,6.99;13,55.77,556.47,152.31,6.99" xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical Recommendations for Gradient-Based Training of Deep Architectures</title>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35289-8_26</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="437" to="478"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Bengio, Y., 2012. Practical recommendations for gradient-based training of deep architectures. In: Neural Networks: Tricks of the Trade -Second Edition. pp. 437-478.</note>
</biblStruct>

<biblStruct coords="13,55.77,565.94,232.91,6.99;13,55.77,575.40,232.90,6.99;13,55.77,584.87,119.50,6.99" xml:id="b8">
	<analytic>
		<title level="a" type="main">On the Analysis and Interpretation of Inhomogeneous Quadratic Forms as Receptive Fields</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Berkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.8.1868</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<title level="j" type="abbrev">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<idno type="ISSNe">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1868" to="1895"/>
			<date type="published" when="2006-08">2006</date>
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Berkes, P., Wiskott, L., 2006. On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields. Neural Computation 18 (8), 1868-1895.</note>
</biblStruct>

<biblStruct coords="13,55.77,594.33,232.90,6.99;13,55.77,603.80,232.90,6.99;13,55.77,613.26,232.90,6.99;13,55.77,622.72,232.90,6.99;13,55.77,632.19,232.90,6.99;13,55.77,641.65,224.46,6.99" xml:id="b9">
	<analytic>
		<title level="a" type="main">Layer-Wise Relevance Propagation for Neural Networks with Local Renormalization Layers</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44781-0_8</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning – ICANN 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016-09-06">2016. September 6-9, 2016</date>
			<biblScope unit="page" from="63" to="71"/>
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
	<note type="raw_reference">Binder, A., Montavon, G., Lapuschkin, S., Müller, K.-R., Samek, W., 2016. Layer-wise relevance propagation for neural networks with local renormalization layers. In: Artificial Neural Networks and Machine Learning -ICANN 2016 -25th Inter- national Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II. pp. 63-71.</note>
</biblStruct>

<biblStruct coords="13,55.77,651.12,232.90,6.99;13,55.77,660.58,192.63,6.99" xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural Networks for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Oxford University Press, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Bishop, C. M., 1995. Neural Networks for Pattern Recognition. Oxford University Press, Inc., New York, NY, USA.</note>
</biblStruct>

<biblStruct coords="13,55.77,670.05,232.91,6.99;13,55.77,679.51,232.91,6.99;13,55.77,688.98,176.93,6.99" xml:id="b11">
	<analytic>
		<title level="a" type="main">Single-trial analysis and classification of ERP components — A tutorial</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lemm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Treder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Haufe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2010.06.048</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<title level="j" type="abbrev">NeuroImage</title>
		<idno type="ISSN">1053-8119</idno>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="814" to="825"/>
			<date type="published" when="2011-05">2011</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Blankertz, B., Lemm, S., Treder, M. S., Haufe, S., Müller, K.- R., 2011. Single-trial analysis and classification of ERP compo- nents -A tutorial. NeuroImage 56 (2), 814-825.</note>
</biblStruct>

<biblStruct coords="13,55.77,698.44,232.90,6.99;13,55.77,707.91,232.90,6.99;13,55.77,717.37,231.64,6.99" xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimizing Spatial filters for Robust EEG Single-Trial Analysis</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lemm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.1109/msp.2008.4408441</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<title level="j" type="abbrev">IEEE Signal Process. Mag.</title>
		<idno type="ISSN">1053-5888</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="56"/>
			<date type="published" when="2008">2008</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Blankertz, B., Tomioka, R., Lemm, S., Kawanabe, M., Müller, K.-R., 2008. Optimizing spatial filters for robust EEG single- trial analysis. IEEE Signal Processing Magazine 25 (1), 41-56.</note>
</biblStruct>

<biblStruct coords="13,55.77,726.83,232.90,6.99;13,55.77,736.30,232.90,6.99;13,55.77,745.76,232.91,6.99;13,55.77,755.23,101.43,6.99" xml:id="b13">
	<analytic>
		<title level="a" type="main">VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving</title>
		<author>
			<persName><forename type="first">Mariusz</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">J</forename><surname>Ackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Yeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Zieba</surname></persName>
		</author>
		<idno type="DOI">10.1109/icra.2018.8461053</idno>
		<idno>CoRR abs/1704.07911</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bojarski, M., Yeres, P., Choromanska, A., Choromanski, K., Firner, B., Jackel, L. D., Muller, U., 2017. Explaining how a deep neural network trained with end-to-end learning steers a car. CoRR abs/1704.07911.</note>
</biblStruct>

<biblStruct coords="13,324.76,85.74,232.91,6.99;13,324.76,95.20,232.91,6.99;13,324.76,104.67,232.90,6.99;13,324.76,114.13,232.91,6.99;13,324.76,123.60,232.91,6.99;13,324.76,133.06,156.47,6.99" xml:id="b14">
	<analytic>
		<title level="a" type="main">Intelligible Models for HealthCare</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2788613</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-08-10">2015. August 10-13, 2015</date>
			<biblScope unit="page" from="1721" to="1730"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., El- hadad, N., 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In: Proceed- ings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Aus- tralia, August 10-13, 2015. pp. 1721-1730.</note>
</biblStruct>

<biblStruct coords="13,324.76,142.53,232.90,6.99;13,324.76,151.99,232.91,6.99;13,324.76,161.45,232.90,6.99;13,324.76,170.92,85.43,6.99" xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine learning of accurate energy-conserving molecular force fields</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
			<idno type="ORCID">0000-0002-1012-4854</idno>
		</author>
		<author>
			<persName><forename type="first">Huziel</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
			<idno type="ORCID">0000-0001-6091-3408</idno>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Poltavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristof</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
			<idno type="ORCID">0000-0001-8342-0964</idno>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.1603015</idno>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<title level="j" type="abbrev">Sci. Adv.</title>
		<idno type="ISSNe">2375-2548</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1603015</biblScope>
			<date type="published" when="2017-05-05">may 2017</date>
			<publisher>American Association for the Advancement of Science (AAAS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Chmiela, S., Tkatchenko, A., Sauceda, H. E., Poltavsky, I., Schütt, K. T., Müller, K.-R., may 2017. Machine learning of accurate energy-conserving molecular force fields. Science Ad- vances 3 (5), e1603015.</note>
</biblStruct>

<biblStruct coords="13,324.76,180.38,232.91,6.99;13,324.76,189.85,232.90,6.99;13,324.76,199.31,232.90,6.99;13,324.76,208.78,232.90,6.99" xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2009 Workshop on Learning Feature Hierarchies</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 1341</note>
	<note type="raw_reference">Erhan, D., Bengio, Y., Courville, A., Vincent, P., Jun. 2009. Visualizing higher-layer features of a deep network. Tech. Rep. 1341, University of Montreal, also presented at the ICML 2009 Workshop on Learning Feature Hierarchies, Montréal, Canada.</note>
</biblStruct>

<biblStruct coords="13,324.76,218.24,232.90,6.99;13,324.76,227.71,232.91,6.99;13,324.76,237.17,232.91,6.99;13,324.76,246.63,31.99,6.99" xml:id="b17">
	<analytic>
		<title level="a" type="main">Review and comparison of methods to study the contribution of variables in artificial neural network models</title>
		<author>
			<persName><forename type="first">Muriel</forename><surname>Gevrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Dimopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sovan</forename><surname>Lek</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0304-3800(02)00257-0</idno>
	</analytic>
	<monogr>
		<title level="j">Ecological Modelling</title>
		<title level="j" type="abbrev">Ecological Modelling</title>
		<idno type="ISSN">0304-3800</idno>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="264"/>
			<date type="published" when="2003-02">feb 2003</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Gevrey, M., Dimopoulos, I., Lek, S., feb 2003. Review and com- parison of methods to study the contribution of variables in artificial neural network models. Ecological Modelling 160 (3), 249-264.</note>
</biblStruct>

<biblStruct coords="13,324.76,256.10,232.90,6.99;13,324.76,265.56,232.91,6.99;13,324.76,275.03,232.90,6.99;13,324.76,284.49,232.91,6.99;13,324.76,293.96,232.90,6.99;13,324.76,303.42,121.15,6.99" xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1145/3422622</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144"/>
			<date type="published" when="2014-12-08">2014. 2014. December 8-13 2014</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
			<pubPlace>Montreal, Quebec, Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde- Farley, D., Ozair, S., Courville, A. C., Bengio, Y., 2014. Gen- erative adversarial nets. In: Advances in Neural Information Processing Systems 27: Annual Conference on Neural Informa- tion Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada. pp. 2672-2680.</note>
</biblStruct>

<biblStruct coords="13,324.76,312.89,232.90,6.99;13,324.76,322.35,232.91,6.99;13,324.76,331.82,174.16,6.99" xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual Interpretation of Kernel‐Based Prediction Models</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timon</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus‐robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1002/minf.201100059</idno>
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<title level="j" type="abbrev">Molecular Informatics</title>
		<idno type="ISSN">1868-1743</idno>
		<idno type="ISSNe">1868-1751</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="817" to="826"/>
			<date type="published" when="2011-09">sep 2011</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Hansen, K., Baehrens, D., Schroeter, T., Rupp, M., Müller, K.- R., sep 2011. Visual interpretation of kernel-based prediction models. Molecular Informatics 30 (9), 817-826.</note>
</biblStruct>

<biblStruct coords="13,324.76,341.28,232.90,6.99;13,324.76,350.74,232.90,6.99;13,324.76,360.21,232.90,6.99;13,324.76,369.67,232.90,6.99;13,324.76,379.14,211.74,6.99" xml:id="b20">
	<analytic>
		<title level="a" type="main">Machine Learning Predictions of Molecular Properties: Accurate Many-Body Potentials and Nonlocality in Chemical Space</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wiktor</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">Anatole</forename><surname>Von Lilienfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jpclett.5b00831</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry Letters</title>
		<title level="j" type="abbrev">J. Phys. Chem. Lett.</title>
		<idno type="ISSN">1948-7185</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2326" to="2331"/>
			<date type="published" when="2015-06-10">jun 2015</date>
			<publisher>American Chemical Society (ACS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Hansen, K., Biegler, F., Ramakrishnan, R., Pronobis, W., von Lilienfeld, O. A., Müller, K.-R., Tkatchenko, A., jun 2015. Ma- chine learning predictions of molecular properties: Accurate many-body potentials and nonlocality in chemical space. The Journal of Physical Chemistry Letters 6 (12), 2326-2331.</note>
</biblStruct>

<biblStruct coords="13,324.76,388.60,232.90,6.99;13,324.76,398.07,232.90,6.99;13,324.76,407.53,232.91,6.99;13,324.76,417.00,88.91,6.99" xml:id="b21">
	<analytic>
		<title level="a" type="main">On the interpretation of weight vectors of linear models in multivariate neuroimaging</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Haufe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Meinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Görgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John-Dylan</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Bießmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2013.10.067</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<title level="j" type="abbrev">NeuroImage</title>
		<idno type="ISSN">1053-8119</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="96" to="110"/>
			<date type="published" when="2014-02">2014</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Haufe, S., Meinecke, F. C., Görgen, K., Dähne, S., Haynes, J.- D., Blankertz, B., Bießmann, F., 2014. On the interpretation of weight vectors of linear models in multivariate neuroimaging. NeuroImage 87, 96-110.</note>
</biblStruct>

<biblStruct coords="13,324.76,426.46,232.91,6.99;13,324.76,435.93,232.91,6.99;13,324.76,445.39,113.49,6.99" xml:id="b22">
	<analytic>
		<title level="a" type="main">A Practical Guide to Training Restricted Boltzmann Machines</title>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35289-8_32</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="599" to="619"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Hinton, G. E., 2012. A practical guide to training restricted Boltzmann machines. In: Neural Networks: Tricks of the Trade -Second Edition. pp. 599-619.</note>
</biblStruct>

<biblStruct coords="13,324.76,454.85,232.91,6.99;13,324.76,464.32,232.91,6.99;13,324.76,473.78,232.90,6.99;13,324.76,483.25,232.90,6.99;13,324.76,492.71,213.10,6.99" xml:id="b23">
	<analytic>
		<title level="a" type="main">Caffe</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2647868.2654889</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-11-03">2014. November 03 -07, 2014</date>
			<biblScope unit="page" from="675" to="678"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Gir- shick, R. B., Guadarrama, S., Darrell, T., 2014. Caffe: Convolu- tional architecture for fast feature embedding. In: Proceedings of the ACM International Conference on Multimedia, MM'14, Orlando, FL, USA, November 03 -07, 2014. pp. 675-678.</note>
</biblStruct>

<biblStruct coords="13,324.76,502.18,232.90,6.99;13,324.76,511.64,232.91,6.99;13,324.76,521.11,232.90,6.99;13,324.76,530.57,232.90,6.99;13,324.76,540.03,216.71,6.99" xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks</title>
		<author>
			<persName><forename type="first">Javed</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Ringnér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lao</forename><forename type="middle">H</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Ladanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><forename type="middle">R</forename><surname>Antonescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Meltzer</surname></persName>
		</author>
		<idno type="DOI">10.1038/89044</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<title level="j" type="abbrev">Nat Med</title>
		<idno type="ISSN">1078-8956</idno>
		<idno type="ISSNe">1546-170X</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="673" to="679"/>
			<date type="published" when="2001-06">jun 2001</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Khan, J., Wei, J. S., Ringnér, M., Saal, L. H., Ladanyi, M., Westermann, F., Berthold, F., Schwab, M., Antonescu, C. R., Peterson, C., Meltzer, P. S., jun 2001. Classification and diag- nostic prediction of cancers using gene expression profiling and artificial neural networks. Nature Medicine 7 (6), 673-679.</note>
</biblStruct>

<biblStruct coords="13,324.76,549.50,232.90,6.99;13,324.76,558.96,232.90,6.99;13,324.76,568.43,232.90,6.99;13,324.76,577.89,232.90,6.99;13,324.76,587.36,196.99,6.99" xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpreting individual classifications of hierarchical networks</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Landecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Thomure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">M A</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrett</forename><forename type="middle">T</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">P</forename><surname>Brumby</surname></persName>
		</author>
		<idno type="DOI">10.1109/cidm.2013.6597214</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-04-19">2013. 2013. 16-19 April, 2013</date>
			<biblScope unit="page" from="32" to="38"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Landecker, W., Thomure, M. D., Bettencourt, L. M. A., Mitchell, M., Kenyon, G. T., Brumby, S. P., 2013. Interpret- ing individual classifications of hierarchical networks. In: IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013, Singapore, 16-19 April, 2013. pp. 32-38.</note>
</biblStruct>

<biblStruct coords="13,324.76,596.82,232.90,6.99;13,324.76,606.29,232.90,6.99;13,324.76,615.75,232.90,6.99;13,324.76,625.22,232.90,6.99;13,324.76,634.68,122.43,6.99" xml:id="b26">
	<analytic>
		<title level="a" type="main">Analyzing Classifiers: Fisher Vectors and Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.318</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06-27">2016. 2016. June 27-30, 2016</date>
			<biblScope unit="page" from="2912" to="2920"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lapuschkin, S., Binder, A., Montavon, G., Müller, K.-R., Samek, W., 2016. Analyzing classifiers: Fisher vectors and deep neural networks. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. pp. 2912-2920.</note>
</biblStruct>

<biblStruct coords="13,324.76,644.14,232.90,6.99;13,324.76,653.61,232.90,6.99;13,324.76,663.07,232.91,6.99;13,324.76,672.54,232.90,6.99;13,324.76,682.00,232.91,6.99;13,324.76,691.47,15.06,6.99" xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553453</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-06-14">2009. 2009. June 14-18, 2009</date>
			<biblScope unit="page" from="609" to="616"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee, H., Grosse, R. B., Ranganath, R., Ng, A. Y., 2009. Con- volutional deep belief networks for scalable unsupervised learn- ing of hierarchical representations. In: Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009. pp. 609- 616.</note>
</biblStruct>

<biblStruct coords="13,324.76,700.93,232.90,6.99;13,324.76,710.40,232.90,6.99;13,324.76,719.86,232.90,6.99;13,324.76,729.32,232.91,6.99;13,324.76,738.79,63.51,6.99" xml:id="b28">
	<analytic>
		<title level="a" type="main">Tackling the widespread and critical impact of batch effects in high-throughput data</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">B</forename><surname>Scharpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Héctor</forename><forename type="middle">Corrada</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Langmead</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Evan</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Baggerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><forename type="middle">A</forename><surname>Irizarry</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrg2825</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<title level="j" type="abbrev">Nat Rev Genet</title>
		<idno type="ISSN">1471-0056</idno>
		<idno type="ISSNe">1471-0064</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="733" to="739"/>
			<date type="published" when="2010-09-14">sep 2010</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Leek, J. T., Scharpf, R. B., Bravo, H. C., Simcha, D., Lang- mead, B., Johnson, W. E., Geman, D., Baggerly, K., Irizarry, R. A., sep 2010. Tackling the widespread and critical impact of batch effects in high-throughput data. Nature Reviews Genetics 11 (10), 733-739.</note>
</biblStruct>

<biblStruct coords="13,324.76,748.25,232.90,6.99;14,55.77,85.74,232.90,6.99;14,55.77,95.20,59.28,6.99" xml:id="b29">
	<analytic>
		<title level="a" type="main">Introduction to machine learning for brain imaging</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lemm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Dickhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2010.11.004</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<title level="j" type="abbrev">NeuroImage</title>
		<idno type="ISSN">1053-8119</idno>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="387" to="399"/>
			<date type="published" when="2011-05">2011</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Lemm, S., Blankertz, B., Dickhaus, T., Müller, K.-R., 2011. Introduction to machine learning for brain imaging. NeuroImage 56 (2), 387-399.</note>
</biblStruct>

<biblStruct coords="14,55.77,104.67,232.90,6.99;14,55.77,114.13,232.90,6.99;14,55.77,123.60,232.90,6.99;14,55.77,133.06,232.91,6.99;14,55.77,142.53,232.91,6.99;14,55.77,151.99,31.99,6.99" xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Neural Models in NLP</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="681" to="691"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, J., Chen, X., Hovy, E. H., Jurafsky, D., 2016. Visualizing and understanding neural models in NLP. In: NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016. pp. 681-691.</note>
</biblStruct>

<biblStruct coords="14,55.77,161.45,232.90,6.99;14,55.77,170.92,59.33,6.99" xml:id="b31">
	<analytic>
		<title level="a" type="main">The Mythos of Model Interpretability</title>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3236386.3241340</idno>
		<idno>CoRR abs/1606.03490</idno>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<title level="j" type="abbrev">Queue</title>
		<idno type="ISSN">1542-7730</idno>
		<idno type="ISSNe">1542-7749</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="57"/>
			<date type="published" when="2016">2016</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Lipton, Z. C., 2016. The mythos of model interpretability. CoRR abs/1606.03490.</note>
</biblStruct>

<biblStruct coords="14,55.77,180.38,232.90,6.99;14,55.77,189.85,232.91,6.99;14,55.77,199.31,232.90,6.99;14,55.77,208.78,232.90,6.99;14,55.77,218.24,232.90,6.99;14,55.77,227.71,232.90,6.99;14,55.77,237.17,40.46,6.99" xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05">2013. December 5-8, 2013</date>
			<biblScope unit="page" from="3111" to="3119"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., Dean, J., 2013. Distributed representations of words and phrases and their compositionality. In: Advances in Neural Information Pro- cessing Systems 26: 27th Annual Conference on Neural Infor- mation Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States. pp. 3111-3119.</note>
</biblStruct>

<biblStruct coords="14,55.77,246.63,232.90,6.99;14,55.77,256.10,232.90,6.99;14,55.77,265.56,228.46,6.99" xml:id="b33">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep Taylor decomposition</title>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.11.008</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<title level="j" type="abbrev">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222"/>
			<date type="published" when="2017-05">2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Montavon, G., Lapuschkin, S., Binder, A., Samek, W., Müller, K.-R., 2017. Explaining nonlinear classification decisions with deep Taylor decomposition. Pattern Recognition 65, 211-222.</note>
</biblStruct>

<biblStruct coords="14,55.77,275.03,232.90,6.99;14,55.77,284.49,232.90,6.99;14,55.77,293.96,13.87,6.99" xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural Networks: Tricks of the Trade</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35289-8</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Berlin Heidelberg</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Montavon, G., Orr, G., Müller, K.-R., 2012. Neural Networks: Tricks of the Trade, 2nd Edition. Springer Publishing Company, Inc.</note>
</biblStruct>

<biblStruct coords="14,55.77,303.42,232.90,6.99;14,55.77,312.89,232.91,6.99;14,55.77,322.35,232.91,6.99;14,55.77,331.82,232.90,6.99;14,55.77,341.28,55.04,6.99" xml:id="b35">
	<analytic>
		<title level="a" type="main">Machine learning of molecular electronic properties in chemical compound space</title>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivekanand</forename><surname>Gobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Vazquez-Mayagoitia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Anatole Von Lilienfeld</surname></persName>
		</author>
		<idno type="DOI">10.1088/1367-2630/15/9/095003</idno>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<title level="j" type="abbrev">New J. Phys.</title>
		<idno type="ISSNe">1367-2630</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">095003</biblScope>
			<date type="published" when="2013-09-04">sep 2013</date>
			<publisher>IOP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Montavon, G., Rupp, M., Gobre, V., Vazquez-Mayagoitia, A., Hansen, K., Tkatchenko, A., Müller, K.-R., von Lilienfeld, O. A., sep 2013. Machine learning of molecular electronic prop- erties in chemical compound space. New Journal of Physics 15 (9), 095003.</note>
</biblStruct>

<biblStruct coords="14,55.77,350.74,232.90,6.99;14,55.77,360.21,232.90,6.99;14,55.77,369.67,232.90,6.99;14,55.77,379.14,232.90,6.99;14,55.77,388.60,232.90,6.99;14,55.77,398.07,144.79,6.99" xml:id="b36">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. 2016. December 5-10, 2016</date>
			<biblScope unit="page" from="3387" to="3395"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., Clune, J., 2016. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In: Advances in Neu- ral Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain. pp. 3387-3395.</note>
</biblStruct>

<biblStruct coords="14,55.77,407.53,232.90,6.99;14,55.77,417.00,232.90,6.99;14,55.77,426.46,232.91,6.99" xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding Neural Networks via Feature Visualization: A Survey</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-6_4</idno>
		<idno>CoRR abs/1602.03616</idno>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="76"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Nguyen, A., Yosinski, J., Clune, J., 2016. Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks. CoRR abs/1602.03616.</note>
</biblStruct>

<biblStruct coords="14,55.77,435.93,232.91,6.99;14,55.77,445.39,232.90,6.99;14,55.77,454.85,232.91,6.99;14,55.77,464.32,232.90,6.99;14,55.77,473.78,232.90,6.99;14,55.77,483.25,232.91,6.99;14,55.77,492.71,120.09,6.99" xml:id="b38">
	<analytic>
		<title level="a" type="main">Visual explanation of evidence with additive classifiers</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Poulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Szafron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Wishart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pearcy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Anvik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference</title>
		<meeting>The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07-16">2006. July 16-20, 2006</date>
			<biblScope unit="page" from="1822" to="1829"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Poulin, B., Eisner, R., Szafron, D., Lu, P., Greiner, R., Wishart, D. S., Fyshe, A., Pearcy, B., Macdonell, C., Anvik, J., 2006. Vi- sual explanation of evidence with additive classifiers. In: Pro- ceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Ar- tificial Intelligence Conference, July 16-20, 2006, Boston, Mas- sachusetts, USA. pp. 1822-1829.</note>
</biblStruct>

<biblStruct coords="14,55.77,502.18,232.90,6.99;14,55.77,511.64,232.91,6.99;14,55.77,521.11,232.90,6.99;14,55.77,530.57,232.90,6.99;14,55.77,540.03,171.74,6.99" xml:id="b39">
	<analytic>
		<title level="a" type="main">"Why Should I Trust You?"</title>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">Tulio</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13">2016. August 13-17, 2016</date>
			<biblScope unit="page" from="1135" to="1144"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Ribeiro, M. T., Singh, S., Guestrin, C., 2016. "why should I trust you?": Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Confer- ence on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016. pp. 1135-1144.</note>
</biblStruct>

<biblStruct coords="14,55.77,549.50,232.90,6.99;14,55.77,558.96,232.90,6.99;14,55.77,568.43,76.22,6.99" xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1038/323533a0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<title level="j" type="abbrev">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<idno type="ISSNe">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536"/>
			<date type="published" when="1986-10">oct 1986</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Rumelhart, D. E., Hinton, G. E., Williams, R. J., oct 1986. Learning representations by back-propagating errors. Nature 323 (6088), 533-536.</note>
</biblStruct>

<biblStruct coords="14,55.77,577.89,232.90,6.99;14,55.77,587.36,232.90,6.99;14,55.77,596.82,232.90,6.99;14,55.77,606.29,106.30,6.99" xml:id="b41">
	<analytic>
		<title level="a" type="main">Evaluating the Visualization of What a Deep Neural Network Has Learned</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
			<idno type="ORCID">0000-0002-6283-3265</idno>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2016.2599820</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<title level="j" type="abbrev">IEEE Trans. Neural Netw. Learning Syst.</title>
		<idno type="ISSN">2162-237X</idno>
		<idno type="ISSNe">2162-2388</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2660" to="2673"/>
			<date type="published" when="2016">2016</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Samek, W., Binder, A., Montavon, G., Lapuschkin, S., Müller, K.-R., 2016. Evaluating the visualization of what a deep neural network has learned. IEEE Transactions on Neural Networks and Learning Systems, 1-14.</note>
</biblStruct>

<biblStruct coords="14,55.77,615.75,232.91,6.99;14,55.77,625.22,232.91,6.99;14,55.77,634.68,215.40,6.99" xml:id="b42">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">Kristof</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms13890</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<title level="j" type="abbrev">Nat Commun</title>
		<idno type="ISSNe">2041-1723</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017-01-09">jan 2017</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Schütt, K. T., Arbabzadah, F., Chmiela, S., Müller, K. R., Tkatchenko, A., jan 2017. Quantum-chemical insights from deep tensor neural networks. Nature Communications 8, 13890.</note>
</biblStruct>

<biblStruct coords="14,55.77,644.14,232.90,6.99;14,55.77,653.61,232.90,6.99;14,55.77,663.07,153.50,6.99" xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1312.6034</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Simonyan, K., Vedaldi, A., Zisserman, A., 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR abs/1312.6034.</note>
</biblStruct>

<biblStruct coords="14,55.77,672.54,232.91,6.99;14,55.77,682.00,232.91,6.99;14,55.77,691.47,124.88,6.99" xml:id="b44">
	<analytic>
		<title level="a" type="main">Finding Density Functionals with Machine Learning</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieron</forename><surname>Burke</surname></persName>
		</author>
		<idno type="DOI">10.1103/physrevlett.108.253002</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<title level="j" type="abbrev">Phys. Rev. Lett.</title>
		<idno type="ISSN">0031-9007</idno>
		<idno type="ISSNe">1079-7114</idno>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">25</biblScope>
			<date type="published" when="2012-06-19">jun 2012</date>
			<publisher>American Physical Society (APS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Snyder, J. C., Rupp, M., Hansen, K., Müller, K.-R., Burke, K., jun 2012. Finding density functionals with machine learning. Physical Review Letters 108 (25).</note>
</biblStruct>

<biblStruct coords="14,55.77,700.93,232.90,6.99;14,55.77,710.40,232.90,6.99;14,55.77,719.86,189.09,6.99" xml:id="b45">
	<analytic>
		<title level="a" type="main">Batch Effect Confounding Leads to Strong Bias in Performance Estimates Obtained by Cross-Validation</title>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Soneson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Gerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Delorenzi</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0100335</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<title level="j" type="abbrev">PLoS ONE</title>
		<idno type="ISSNe">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">e100335</biblScope>
			<date type="published" when="2014-06-26">06 2014</date>
			<publisher>Public Library of Science (PLoS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Soneson, C., Gerster, S., Delorenzi, M., 06 2014. Batch effect confounding leads to strong bias in performance estimates ob- tained by cross-validation. PLOS ONE 9 (6), 1-13.</note>
</biblStruct>

<biblStruct coords="14,55.77,729.32,232.90,6.99;14,55.77,738.79,232.90,6.99;14,55.77,748.25,80.73,6.99" xml:id="b46">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>CoRR abs/1412.6806</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Springenberg, J. T., Dosovitskiy, A., Brox, T., Riedmiller, M. A., 2014. Striving for simplicity: The all convolutional net. CoRR abs/1412.6806.</note>
</biblStruct>

<biblStruct coords="14,324.76,85.74,232.90,6.99;14,324.76,95.20,232.90,6.99;14,324.76,104.67,232.91,6.99;14,324.76,114.13,103.44,6.99" xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15 (1), 1929-1958.</note>
</biblStruct>

<biblStruct coords="14,324.76,123.60,232.90,6.99;14,324.76,133.06,232.90,6.99;14,324.76,142.53,203.44,6.99" xml:id="b48">
	<analytic>
		<title level="a" type="main">Interpretable deep neural networks for single-trial EEG classification</title>
		<author>
			<persName><forename type="first">Irene</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneumeth.2016.10.008</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<title level="j" type="abbrev">Journal of Neuroscience Methods</title>
		<idno type="ISSN">0165-0270</idno>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="141" to="145"/>
			<date type="published" when="2016-12">dec 2016</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sturm, I., Lapuschkin, S., Samek, W., Müller, K.-R., dec 2016. Interpretable deep neural networks for single-trial EEG classifi- cation. Journal of Neuroscience Methods 274, 141-145.</note>
</biblStruct>

<biblStruct coords="14,324.76,151.99,232.90,6.99;14,324.76,161.45,232.90,6.99;14,324.76,170.92,232.90,6.99;14,324.76,180.38,232.91,6.99;14,324.76,189.85,134.63,6.99" xml:id="b49">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Wei Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06-07">2015. June 7-12, 2015</date>
			<biblScope unit="page" from="1" to="9"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S. E., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions. In: IEEE Conference on Com- puter Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015. pp. 1-9.</note>
</biblStruct>

<biblStruct coords="14,324.76,199.31,232.91,6.99;14,324.76,208.78,232.91,6.99;14,324.76,218.24,134.67,6.99" xml:id="b50">
	<monogr>
		<title level="m" type="main">Methods and Procedures for the Verification and Validation of Artificial Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, Inc., Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Taylor, B. J., 2005. Methods and Procedures for the Verification and Validation of Artificial Neural Networks. Springer-Verlag New York, Inc., Secaucus, NJ, USA.</note>
</biblStruct>

<biblStruct coords="14,324.76,227.71,232.91,6.99;14,324.76,237.17,232.90,6.99;14,324.76,246.63,232.91,6.99;14,324.76,256.10,203.79,6.99" xml:id="b51">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016</date>
			<biblScope unit="page" from="1747" to="1756"/>
		</imprint>
	</monogr>
	<note type="raw_reference">van den Oord, A., Kalchbrenner, N., Kavukcuoglu, K., 2016. Pixel recurrent neural networks. In: Proceedings of the 33nd In- ternational Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016. pp. 1747-1756.</note>
</biblStruct>

<biblStruct coords="14,324.76,265.56,232.91,6.99;14,324.76,275.03,232.91,6.99;14,324.76,284.49,84.96,6.99" xml:id="b52">
	<analytic>
		<title level="a" type="main">ML2Motif—Reliable extraction of discriminative sequence motifs from learning machines</title>
		<author>
			<persName><forename type="first">Null-</forename><forename type="middle">C</forename><surname>Vidovic</surname></persName>
			<idno type="ORCID">0000-0003-3090-6279</idno>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nico</forename><surname>Görnitz</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0174392</idno>
		<idno>CoRR abs/1611.07567</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<title level="j" type="abbrev">PLoS ONE</title>
		<idno type="ISSNe">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e0174392</biblScope>
			<date type="published" when="2016">2016</date>
			<publisher>Public Library of Science (PLoS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Vidovic, M. M.-C., Görnitz, N., Müller, K.-R., Kloft, M., 2016. Feature importance measure for non-linear learning algorithms. CoRR abs/1611.07567.</note>
</biblStruct>

<biblStruct coords="14,324.76,293.96,232.91,6.99;14,324.76,303.42,232.90,6.99;14,324.76,312.89,210.27,6.99" xml:id="b53">
	<analytic>
		<title level="a" type="main">ML2Motif—Reliable extraction of discriminative sequence motifs from learning machines</title>
		<author>
			<persName><forename type="first">Null-</forename><forename type="middle">C</forename><surname>Vidovic</surname></persName>
			<idno type="ORCID">0000-0003-3090-6279</idno>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nico</forename><surname>Görnitz</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0174392</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<title level="j" type="abbrev">PLoS ONE</title>
		<idno type="ISSNe">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e0174392</biblScope>
			<date type="published" when="2017-03-27">03 2017</date>
			<publisher>Public Library of Science (PLoS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Vidovic, M. M.-C., Kloft, M., Müller, K.-R., Görnitz, N., 03 2017. Ml2motif-reliable extraction of discriminative sequence motifs from learning machines. PLOS ONE 12 (3), 1-22.</note>
</biblStruct>

<biblStruct coords="14,324.76,322.35,232.90,6.99;14,324.76,331.82,232.90,6.99;14,324.76,341.28,232.90,6.99;14,324.76,350.74,159.06,6.99" xml:id="b54">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_53</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2014</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014-09-06">2014. September 6-12, 2014</date>
			<biblScope unit="page" from="818" to="833"/>
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
	<note type="raw_reference">Zeiler, M. D., Fergus, R., 2014. Visualizing and understanding convolutional networks. In: Computer Vision -ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6- 12, 2014, Proceedings, Part I. pp. 818-833.</note>
</biblStruct>

<biblStruct coords="14,324.76,360.21,232.90,6.99;14,324.76,369.67,232.91,6.99;14,324.76,379.14,232.91,6.99;14,324.76,388.60,232.90,6.99;14,324.76,398.07,80.32,6.99" xml:id="b55">
	<analytic>
		<title level="a" type="main">Top-Down Neural Attention by Excitation Backprop</title>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_33</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016-10-11">2016. October 11-14, 2016</date>
			<biblScope unit="page" from="543" to="559"/>
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
	<note type="raw_reference">Zhang, J., Lin, Z. L., Brandt, J., Shen, X., Sclaroff, S., 2016. Top-down neural attention by excitation backprop. In: Com- puter Vision -ECCV 2016 -14th European Conference, Am- sterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV. pp. 543-559.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
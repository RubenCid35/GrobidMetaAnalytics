<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explaining Explanations: An Overview of Interpretability of Machine Learning</title>
				<funder ref="#_c3gqk9n">
					<orgName type="full">DARPA XAI</orgName>
				</funder>
				<funder ref="#_9HXBRXM">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Toyota Research Institute</orgName>
					<orgName type="abbreviated">TRI</orgName>
					<idno type="DOI" subtype="crossref">10.13039/100015599</idno>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-02-03">3 Feb 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,91.80,123.15,75.63,9.80"><forename type="first">Leilani</forename><forename type="middle">H</forename><surname>Gilpin</surname></persName>
							<email>lgilpin@mit.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge, MA 02139</note>
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,175.68,123.15,45.85,9.80"><forename type="first">David</forename><surname>Bau</surname></persName>
							<email>davidbau@mit.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge, MA 02139</note>
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.40,123.15,55.02,9.80"><forename type="first">Ben</forename><forename type="middle">Z</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge, MA 02139</note>
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.24,123.15,61.22,9.80"><forename type="first">Ayesha</forename><surname>Bajwa</surname></persName>
							<email>abajwa@mit.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge, MA 02139</note>
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.32,123.15,72.89,9.80"><forename type="first">Michael</forename><surname>Specter</surname></persName>
							<email>specter@mit.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge, MA 02139</note>
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,460.44,123.15,60.05,9.80"><forename type="first">Lalana</forename><surname>Kagal</surname></persName>
							<email>lkagal@mit.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge, MA 02139</note>
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explaining Explanations: An Overview of Interpretability of Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-02-03">3 Feb 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">8FA9216E8E9411A708764722BE3ACAAD</idno>
					<idno type="arXiv">arXiv:1806.00069v3[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,99.20,208.13,200.78,8.03;1,48.96,218.09,146.60,8.03">There has recently been a surge of work in explanatory artificial intelligence (XAI).</s><s coords="1,198.48,218.09,101.53,8.03;1,48.96,228.05,251.05,8.03;1,48.96,238.01,251.15,8.03;1,48.96,247.97,37.88,8.03">This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes.</s><s coords="1,90.48,247.97,209.56,8.03;1,48.96,257.93,251.17,8.03;1,48.96,267.89,95.96,8.03">XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail.</s><s coords="1,150.00,267.89,150.04,8.03;1,48.96,277.85,251.11,8.03;1,48.96,287.81,251.05,8.03;1,48.96,297.77,35.60,8.03">These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected.</s><s coords="1,89.04,297.77,210.97,8.03;1,48.96,307.73,191.24,8.03">However, explanations produced by these systems is neither standardized nor systematically assessed.</s><s coords="1,243.48,307.73,56.56,8.03;1,48.96,317.69,251.18,8.03;1,48.96,327.65,250.98,8.03;1,48.96,337.61,150.56,8.03">In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature.</s><s coords="1,203.64,337.61,96.47,8.03;1,48.96,347.57,251.13,8.03;1,48.96,357.53,99.44,8.03">We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient.</s><s coords="1,151.68,357.53,148.31,8.03;1,48.96,367.49,250.96,8.03;1,48.96,377.45,81.44,8.03">Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,135.60,402.71,77.63,8.91">I. INTRODUCTION</head><p><s coords="1,58.92,422.75,241.26,8.91;1,48.96,434.75,251.32,8.91;1,48.96,446.75,218.37,8.91">As autonomous machines and black-box algorithms begin making decisions previously entrusted to humans, it becomes necessary for these mechanisms to explain themselves.</s><s coords="1,269.64,446.75,30.46,8.91;1,48.96,458.63,251.37,8.91;1,48.96,470.63,251.25,8.91;1,48.96,482.51,171.45,8.91">Despite their success in a broad range of tasks including advertising, movie and book recommendations, and mortgage qualification, there is general mistrust about their results.</s><s coords="1,222.72,482.51,77.29,8.91;1,48.96,494.51,251.22,8.91;1,48.96,506.51,251.17,8.91;1,48.96,518.39,251.14,8.91;1,48.96,530.39,125.61,8.91">In 2016, Angwin et al. <ref type="bibr" coords="1,62.28,494.51,11.60,8.91" target="#b0">[1]</ref> analyzed Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), a widely used criminal risk assessment tool, and found that its predictions were unreliable and racially biased.</s><s coords="1,178.92,530.39,121.33,8.91;1,48.96,542.39,251.10,8.91;1,48.96,554.27,251.34,8.91;1,48.96,566.27,14.13,8.91">Along with this, deep neural networks (DNNs) have been shown to be easily fooled into misclassifying inputs with no resemblance to the true category <ref type="bibr" coords="1,48.96,566.27,10.60,8.91" target="#b1">[2]</ref>.</s><s coords="1,66.60,566.27,233.50,8.91;1,48.96,578.15,251.22,8.91;1,48.96,590.15,251.20,8.91;1,48.96,602.15,117.33,8.91">Extending this observation, a number of techniques have been shown for changing a network's classification of any image to any target class by making imperceptible alterations to the pixels <ref type="bibr" coords="1,111.36,602.15,10.60,8.91" target="#b2">[3]</ref>, <ref type="bibr" coords="1,131.76,602.15,10.60,8.91" target="#b3">[4]</ref>, <ref type="bibr" coords="1,152.16,602.15,10.60,8.91" target="#b4">[5]</ref>.</s><s coords="1,172.56,602.15,127.57,8.91;1,48.96,614.03,251.14,8.91;1,48.96,626.03,43.89,8.91">Adversarial examples are not confined to images; natural language networks can also be fooled <ref type="bibr" coords="1,78.72,626.03,10.60,8.91" target="#b5">[6]</ref>.</s><s coords="1,97.20,626.03,202.86,8.91;1,48.96,638.03,251.32,8.91;1,48.96,649.91,251.22,8.91;1,48.96,661.91,25.89,8.91">Trojaning attacks have been demonstrated <ref type="bibr" coords="1,276.36,626.03,11.60,8.91" target="#b6">[7]</ref> in which inputs remain unchanged, but imperceptible changes are hidden in deep networks to cause them to make targeted errors.</s><s coords="1,79.80,661.91,220.65,8.91;1,48.96,673.79,251.10,8.91;1,48.96,685.79,251.23,8.91;1,48.96,697.79,38.73,8.91">While some defense methods have been developed, more attack methods have also emerged <ref type="bibr" coords="1,209.76,673.79,10.60,8.91" target="#b7">[8]</ref>, <ref type="bibr" coords="1,226.32,673.79,10.60,8.91" target="#b8">[9]</ref>, <ref type="bibr" coords="1,242.76,673.79,15.24,8.91" target="#b9">[10]</ref>, <ref type="bibr" coords="1,264.24,673.79,15.24,8.91" target="#b10">[11]</ref>, and susceptibility to unintuitive errors remains a pervasive problem in DNNs.</s><s coords="1,90.84,697.79,209.36,8.91;1,48.96,709.67,251.37,8.91">The potential for such unexpected behavior and unintentional discrimination highlights the need for explanations.</s></p><p><s coords="1,321.96,207.59,241.29,8.91;1,312.00,219.59,251.10,8.91;1,312.00,231.47,251.12,8.91;1,312.00,243.47,73.89,8.91">As a first step towards creating explanation mechanisms, there is a new line of research in interpretability, loosely defined as the science of comprehending what a model did (or might have done).</s><s coords="1,389.04,243.47,174.28,8.91;1,312.00,255.47,251.10,8.91;1,312.00,267.35,251.34,8.91;1,312.00,279.35,251.00,8.91;1,312.00,291.23,21.33,8.91">Interpretable models and learning methods show great promise; examples include visual cues to find the "focus" of deep neural networks in image recognition and proxy methods to simplify the output of complex systems.</s><s coords="1,337.32,291.23,225.70,8.91;1,312.00,303.23,251.23,8.91;1,312.00,315.23,250.98,8.91;1,312.00,327.11,123.57,8.91">However, there is ample room for improvement, since identifying dominant classifiers and simplifying the problem space does not solve all possible problems associated with understanding opaque models.</s></p><p><s coords="1,321.96,339.11,241.29,8.91">We take the stance that interpretability alone is insufficient.</s><s coords="1,312.00,351.11,251.10,8.91;1,312.00,362.99,251.20,8.91;1,312.00,374.99,251.38,8.91;1,312.00,386.87,173.01,8.91">In order for humans to trust black-box methods, we need explainability -models that are able to summarize the reasons for neural network behavior, gain the trust of users, or produce insights about the causes of their decisions.</s><s coords="1,487.44,386.87,75.80,8.91;1,312.00,398.87,251.10,8.91;1,312.00,410.87,251.38,8.91;1,312.00,422.75,207.93,8.91">While interpetability is a substantial first step, these mechanisms need to also be complete, with the capacity to defend their actions, provide relevant responses to questions, and be audited.</s><s coords="1,525.36,422.75,37.86,8.91;1,312.00,434.75,251.36,8.91;1,312.00,446.75,251.22,8.91;1,312.00,458.63,58.05,8.91">Although interpretability and explainability have been used interchangeably, we argue there are important reasons to distinguish between them.</s><s coords="1,372.36,458.63,190.77,8.91;1,312.00,470.63,138.93,8.91">Explainable models are interpretable by default, but the reverse is not always true.</s><s coords="1,321.96,482.51,241.18,8.91;1,312.00,494.51,226.17,8.91">Some existing deployed systems and regulations make the need for explanatory systems urgent and timely.</s><s coords="1,543.48,494.51,19.50,8.91;1,312.00,506.51,250.98,8.91;1,312.00,518.39,251.00,8.91;1,312.00,530.39,251.10,8.91;1,312.00,542.39,251.02,8.91;1,312.00,554.27,251.02,8.91;1,312.00,566.27,251.32,8.91;1,312.00,578.15,146.25,8.91">With impending regulations like the European Union's "Right to Explanation" <ref type="bibr" coords="1,370.32,518.39,15.24,8.91" target="#b11">[12]</ref>, calls for diversity and inclusion in AI systems <ref type="bibr" coords="1,349.56,530.39,15.34,8.91" target="#b12">[13]</ref>, findings that some automated systems may reinforce inequality and bias <ref type="bibr" coords="1,434.04,542.39,15.34,8.91" target="#b13">[14]</ref>, and requirements for safe and secure AI in safety-critical tasks <ref type="bibr" coords="1,470.40,554.27,15.34,8.91" target="#b14">[15]</ref>, there has been a recent explosion of interest in interpreting the representations and decisions of black-box models.</s><s coords="1,461.88,578.15,101.36,8.91;1,312.00,590.15,251.26,8.91;1,312.00,602.15,218.61,8.91">These models are everywhere, and the development of interpretable and explainable models is scattered throughout various disciplines.</s><s coords="1,536.52,602.15,26.72,8.91;1,312.00,614.03,251.26,8.91;1,312.00,626.03,251.05,8.91;1,312.00,638.03,178.53,8.91">Examples of general "explainable systems" include interpretable AI, explainable ML, causality, safe AI, computational social science, and automatic scientific discovery.</s><s coords="1,494.28,638.03,68.82,8.91;1,312.00,649.91,251.26,8.91;1,312.00,661.91,251.25,8.91;1,312.00,673.79,251.22,8.91;1,312.00,685.79,71.73,8.91">Further, research in explanations and their evaluation are found in machine learning, human computer interaction (HCI), crowd sourcing, machine teaching, AI ethics, technology policy, and many other disciplines.</s><s coords="1,389.64,685.79,173.50,8.91;1,312.00,697.79,251.12,8.91;1,312.00,709.67,251.10,8.91;2,48.96,52.19,251.34,8.91;2,48.96,64.19,153.81,8.91">This paper aims to broadly engage the greater machine learning community on the intersection of these topics, to set best practices, to define key concepts, and to propose evaluation criteria for standardizing explanatory systems often considered in isolation.</s></p><p><s coords="2,58.92,76.19,241.06,8.91;2,48.96,88.19,251.26,8.91;2,48.96,100.19,175.17,8.91">In this survey, we present a set of definitions, construct a taxonomy, and present best practices to start to standardize interpretability and explanatory work in AI.</s><s coords="2,226.80,100.19,73.40,8.91;2,48.96,112.07,251.38,8.91;2,48.96,124.07,251.43,8.91;2,48.96,136.07,91.89,8.91">We review a number of approaches towards explainable AI systems and provide a taxonomy of how one can think about diverse approaches towards explainability.</s><s coords="2,144.24,136.07,155.84,8.91;2,48.96,147.95,251.37,8.91">In Section 2, we define key terms including "explanation", "interpretability", and "explainability".</s><s coords="2,48.96,159.95,251.22,8.91;2,48.96,171.83,70.53,8.91">We compare and contrast our definitions with those accepted in the literature.</s><s coords="2,125.40,171.83,174.61,8.91;2,48.96,183.83,251.37,8.91;2,48.96,195.83,251.10,8.91;2,48.96,207.71,153.45,8.91">In Section 3, we review some classical AI approaches (e.g., causal modeling, constraint reasoning, intelligent user interfaces, planning) but focus mainly on explainable models for deep learning.</s><s coords="2,205.44,207.71,94.74,8.91;2,48.96,219.71,251.32,8.91;2,48.96,231.71,251.12,8.91;2,48.96,243.59,146.61,8.91">We provide a summary of related work papers in Section 4, highlighting differences between definitions of key terms including "explanation", "interpretability", and "explainability".</s><s coords="2,198.96,243.59,101.17,8.91;2,48.96,255.59,251.10,8.91;2,48.96,267.47,76.77,8.91">In Section 5, we present a novel taxonomy that examines what is being explained by these explanations.</s><s coords="2,128.64,267.47,171.54,8.91;2,48.96,279.47,251.10,8.91;2,48.96,291.47,251.12,8.91;2,48.96,303.35,82.05,8.91">We conclude with a discussion addressing open questions and recommend a path to the development and adoption of explainable methods for safety-critical or missioncritical applications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,65.88,323.39,217.19,8.91">II. BACKGROUND AND FOUNDATIONAL CONCEPTS</head><p><s coords="2,58.92,339.35,241.33,8.91;2,48.96,351.35,251.10,8.91;2,48.96,363.23,207.93,8.91">In this section, we provide background information about the key concepts of interpretability and explanability, and describe the meaningful differences between them.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,48.96,383.35,115.74,8.74">A. What is an Explanation?</head><p><s coords="2,58.92,398.87,241.24,8.91;2,48.96,410.87,61.89,8.91">Philosophical texts show much debate over what constitutes an explanation.</s><s coords="2,113.76,410.87,186.44,8.91;2,48.96,422.75,251.37,8.91">Of particular interest is what makes an explanation "good enough" or what really defines an explanation.</s><s coords="2,48.96,434.75,251.13,8.91">Some say a good explanation depends on the question <ref type="bibr" coords="2,280.92,434.75,15.34,8.91" target="#b15">[16]</ref>.</s><s coords="2,48.96,446.63,251.37,8.91;2,48.96,458.63,137.97,8.91">This set of essays discusses the nature of explanation, theory, and the foundations of linguistics.</s><s coords="2,189.96,458.63,110.14,8.91;2,48.96,470.63,240.34,8.91">Although for our work, the most important and interesting work is on "Why questions."</s><s coords="2,291.72,470.63,8.34,8.91;2,48.96,482.51,251.23,8.91;2,48.96,494.51,251.26,8.91;2,48.96,506.51,251.34,8.91;2,48.96,518.39,159.57,8.91">In particular, when you can phrase what you want to know from an algorithm as a why question, there is a natural qualitative representation of when you have answered said question-when you can no longer keep asking why.</s><s coords="2,213.60,518.39,86.60,8.91;2,48.96,530.39,181.29,8.91">There are two whyquestions of interest; why and why-should.</s><s coords="2,234.24,530.39,65.86,8.91;2,48.96,542.27,251.14,8.91;2,48.96,554.27,251.14,8.91;2,48.96,566.27,187.41,8.91">Similarly to the explainable planning literature, philosophers wonder about the why-shouldn't and why-should questions, which can give the kinds of explainability requirements we want.</s></p><p><s coords="2,58.92,578.39,241.18,8.91;2,48.96,590.27,68.61,8.91">There is also discussion in philosophy about what makes the best explanation.</s><s coords="2,120.60,590.27,179.48,8.91;2,48.96,602.27,251.05,8.91;2,48.96,614.15,92.73,8.91">While many say it is inference <ref type="bibr" coords="2,249.84,590.27,15.34,8.91" target="#b16">[17]</ref>, similar views point to the use of abductive reasoning to explain all the possible outcomes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,48.96,634.27,147.28,8.74">B. Interpretability vs. Completeness</head><p><s coords="2,58.92,649.79,241.14,8.91;2,48.96,661.79,219.45,8.91">An explanation can be evaluated in two ways: according to its interpretability, and according to its completeness.</s></p><p><s coords="2,58.92,673.79,241.06,8.91;2,48.96,685.79,200.61,8.91">The goal of interpretability is to describe the internals of a system in a way that is understandable to humans.</s><s coords="2,252.00,685.79,48.04,8.91;2,48.96,697.79,251.07,8.91;2,48.96,709.67,251.38,8.91;2,312.00,52.19,251.34,8.91;2,312.00,64.19,203.25,8.91">The success of this goal is tied to the cognition, knowledge, and biases of the user: for a system to be interpretable, it must produce descriptions that are simple enough for a person to understand using a vocabulary that is meaningful to the user.</s></p><p><s coords="2,321.96,76.19,241.06,8.91;2,312.00,88.19,110.97,8.91">The goal of completeness is to describe the operation of a system in an accurate way.</s><s coords="2,426.36,88.19,136.90,8.91;2,312.00,100.07,250.98,8.91;2,312.00,112.07,65.49,8.91">An explanation is more complete when it allows the behavior of the system to be anticipated in more situations.</s><s coords="2,381.72,112.07,181.52,8.91;2,312.00,124.07,251.26,8.91;2,312.00,135.95,251.12,8.91;2,312.00,147.95,200.01,8.91">When explaining a self-contained computer program such as a deep neural network, a perfectly complete explanation can always be given by revealing all the mathematical operations and parameters in the system.</s></p><p><s coords="2,321.96,159.95,241.28,8.91;2,312.00,171.95,251.17,8.91;2,312.00,183.83,251.37,8.91">The challenge facing explainable AI is in creating explanations that are both complete and interpretable: it is difficult to achieve interpretability and completeness simultaneously.</s><s coords="2,312.00,195.83,251.26,8.91;2,312.00,207.83,251.32,8.91;2,312.00,219.71,157.05,8.91">The most accurate explanations are not easily interpretable to people; and conversely the most interpretable descriptions often do not provide predictive power.</s></p><p><s coords="2,321.96,231.83,241.28,8.91;2,312.00,243.71,251.12,8.91;2,312.00,255.71,251.10,8.91;2,312.00,267.71,196.77,8.91">Herman <ref type="bibr" coords="2,359.52,231.83,16.52,8.91" target="#b17">[18]</ref> notes that we should be wary of evaluating interpretable systems using merely human evaluations of interpretability, because human evaluations imply a strong and specific bias towards simpler descriptions.</s><s coords="2,513.36,267.71,49.84,8.91;2,312.00,279.59,250.98,8.91;2,312.00,291.59,236.13,8.91">He cautions that reliance on human evaluations can lead researchers to create persuasive systems rather than transparent systems.</s><s coords="2,551.40,291.59,11.62,8.91;2,312.00,303.59,251.17,8.91;2,312.00,315.47,184.69,8.91">He presents the following ethical dilemmas that are a central concern when building interpretable systems:</s></p><p><s coords="2,329.28,333.59,216.78,8.91;2,386.88,345.59,101.26,8.91">1) When is it unethical to manipulate an explanation to better persuade users?</s><s coords="2,319.44,357.59,236.22,8.91;2,352.20,369.59,170.98,8.91">2) How do we balance our concerns for transparency and ethics with our desire for interpretability?</s></p><p><s coords="2,321.96,387.59,241.06,8.91;2,312.00,399.59,251.14,8.91;2,312.00,411.59,251.14,8.91;2,312.00,423.47,251.22,8.91;2,312.00,435.47,175.29,8.91">We believe that it is fundamentally unethical to present a simplified description of a complex system in order to increase trust if the limitations of the simplified description cannot be understood by users, and worse if the explanation is optimized to hide undesirable attributes of the system.</s><s coords="2,490.08,435.47,73.36,8.91;2,312.00,447.35,251.10,8.91;2,312.00,459.35,185.49,8.91">Such explanations are inherently misleading, and may result in the user justifiably making dangerous or unfounded conclusions.</s></p><p><s coords="2,321.96,471.35,241.16,8.91;2,312.00,483.35,161.37,8.91">To avoid this trap, explanations should allow a tradeoff between interpretability and completeness.</s><s coords="2,475.68,483.35,87.66,8.91;2,312.00,495.35,251.32,8.91;2,312.00,507.23,251.12,8.91;2,312.00,519.23,61.41,8.91">Rather than providing only simple descriptions, systems should allow for descriptions with higher detail and completeness at the possible cost of interpretability.</s><s coords="2,376.68,519.23,186.54,8.91;2,312.00,531.23,251.15,8.91;2,312.00,543.11,250.98,8.91;2,312.00,555.11,100.29,8.91">Explanation methods should not be evaluated on a single point on this tradeoff, but according to how they behave on the curve from maximum interpretability to maximum completeness.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,312.00,574.75,147.40,8.74">C. Explainability of Deep Networks</head><p><s coords="2,321.96,590.03,241.16,8.91;2,312.00,602.03,251.02,8.91;2,312.00,614.03,251.02,8.91;2,312.00,625.91,35.25,8.91">Explanations of the operation of deep networks have focused on either explaining the processing of the data by a network, or explaining the representation of data inside a network.</s><s coords="2,351.72,625.91,211.48,8.91;2,312.00,637.91,251.07,8.91;2,312.00,649.79,235.89,8.91">An explanation of processing answers "Why does this particular input lead to that particular output?" and is analogous to explaining the execution trace of a program.</s><s coords="2,550.92,649.79,12.18,8.91;2,312.00,661.79,251.34,8.91;2,312.00,673.79,251.34,8.91;2,312.00,685.67,168.09,8.91">An explanation about representation answers "What information does the network contain?" and can be compared to explaining the internal data structures of a program.</s></p><p><s coords="2,321.96,697.79,241.40,8.91;2,312.00,709.67,250.98,8.91;3,48.96,52.19,196.41,8.91">A third approach to interpretability is to create explanationproducing systems with architectures that are designed to simplify interpretation of their own behavior.</s><s coords="3,251.04,52.19,49.16,8.91;3,48.96,64.19,251.25,8.91;3,48.96,76.07,251.12,8.91;3,48.96,88.07,87.81,8.91">Such architectures can be designed to make either their processing, representations, or other aspects of their operation easier for people to understand.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="3,147.36,107.99,54.20,8.91">III. REVIEW</head><p><s coords="3,58.92,123.83,241.18,8.91;3,48.96,135.83,251.14,8.91;3,48.96,147.83,251.49,8.91">Due to the growing number of subfields, as well as the policy and legal ramifications <ref type="bibr" coords="3,179.76,135.83,16.52,8.91" target="#b11">[12]</ref> of opaque systems, the volume of research in interpretability is quickly expanding.</s><s coords="3,48.96,159.71,251.13,8.91;3,48.96,171.71,251.25,8.91;3,48.96,183.71,231.21,8.91">Since it is intractable to review all the papers in the space, we focus on explainable methods in deep neural architectures, and briefly highlight review papers from other subfields.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="3,48.96,203.58,186.54,8.74">A. Explanations of Deep Network Processing</head><p><s coords="3,58.92,219.11,241.14,8.91;3,48.96,231.11,251.05,8.91;3,48.96,242.99,251.36,8.91;3,48.96,253.41,73.81,10.49;3,125.52,253.41,174.13,10.49;3,48.96,266.99,211.65,8.91">Commonly used deep networks derive their decisions using a large number of elementary operations: for example, ResNet <ref type="bibr" coords="3,48.96,242.99,15.34,8.91" target="#b18">[19]</ref>, a popular architecture for image classification, incorporates about 5×10 7 learned parameters and executes about 10 10 floating point operations to classify a single image.</s><s coords="3,264.24,266.99,35.86,8.91;3,48.96,278.87,251.08,8.91;3,48.96,290.87,251.37,8.91">Thus the fundamental problem facing explanations of such processing is to find ways to reduce the complexity of all these operations.</s><s coords="3,48.96,302.87,251.10,8.91;3,48.96,314.75,251.07,8.91;3,48.96,326.75,251.02,8.91;3,48.96,338.63,233.25,8.91">This can be done by either creating a proxy model which behaves similarly to the original model, but in a way that is easier to explain, or by creating a salience map to highlight a small portion of the computation which is most relevant.</s></p><p><s coords="3,58.92,350.87,241.11,8.91;3,48.96,362.75,227.97,8.91">1) Linear Proxy Models: The proxy model approach is exemplified well by the LIME method by Ribeiro <ref type="bibr" coords="3,257.76,362.75,15.34,8.91" target="#b19">[20]</ref>.</s><s coords="3,280.44,362.75,19.50,8.91;3,48.96,374.75,251.10,8.91;3,48.96,386.75,251.29,8.91;3,48.96,398.63,251.14,8.91;3,48.96,410.63,174.45,8.91">With LIME, a black-box system is explained by probing behavior on perturbations of an input, and then that data is used to construct a local linear model that serves as a simplified proxy for the full model in the neighborhood of the input.</s><s coords="3,225.72,410.63,74.41,8.91;3,48.96,422.63,251.17,8.91;3,48.96,434.51,251.12,8.91;3,48.96,446.51,120.45,8.91">Ribeiro shows that the method can be used to identify regions of the input that are most influential for a decision across a variety of types of models and problem domains.</s><s coords="3,171.72,446.51,128.38,8.91;3,48.96,458.39,251.07,8.91;3,48.96,470.39,141.57,8.91">Proxy models such as LIME are predictive: the proxy can be run and evaluated according to its faithfulness to the original system.</s><s coords="3,194.04,470.39,106.06,8.91;3,48.96,482.39,251.25,8.91;3,48.96,494.26,213.33,8.91">Proxy models can also be measured according to their model complexity, for example, number of nonzero dimensions in a LIME model.</s><s coords="3,266.88,494.26,33.22,8.91;3,48.96,506.26,251.22,8.91;3,48.96,518.26,251.46,8.91;3,48.96,530.14,210.09,8.91">Because the proxy model provides a quantifiable relationship between complexity and faithfulness, methods can be benchmarked against each other, making this approach attractive.</s></p><p><s coords="3,58.92,542.38,241.33,8.91;3,48.96,554.26,77.73,8.91">2) Decision Trees: Another appealing type of proxy model is the decision tree.</s><s coords="3,129.48,554.26,170.58,8.91;3,48.96,566.26,251.25,8.91;3,48.96,578.14,251.24,8.91;3,48.96,590.14,123.45,8.91">Efforts to decompose neural networks into decision trees have recently extended work from the 1990s, which focused on shallow networks, to generalizing the process for deep neural networks.</s><s coords="3,175.32,590.14,124.79,8.91;3,48.96,602.14,251.12,8.91;3,48.96,614.02,251.22,8.91;3,48.96,626.02,56.61,8.91">One such method is DeepRED <ref type="bibr" coords="3,48.96,602.14,15.34,8.91" target="#b20">[21]</ref>, which demonstrates a way of extending the CRED <ref type="bibr" coords="3,283.44,602.14,16.64,8.91" target="#b21">[22]</ref> algorithm (designed for shallow networks) to arbitrarily many hidden layers.</s><s coords="3,108.60,626.02,191.46,8.91;3,48.96,638.02,251.22,8.91;3,48.96,649.90,251.22,8.91;3,48.96,661.90,166.77,8.91">DeepRED utilizes several strategies to simplify its decision trees: it uses RxREN <ref type="bibr" coords="3,192.00,638.02,16.64,8.91" target="#b22">[23]</ref> to prune unnecessary input, and it applies algorithm C4.5 <ref type="bibr" coords="3,198.60,649.90,15.34,8.91" target="#b23">[24]</ref>, a statistical method for creating a parsimonious decision tree.</s><s coords="3,218.52,661.90,81.59,8.91;3,48.96,673.78,251.10,8.91;3,48.96,685.78,251.25,8.91;3,48.96,697.78,251.14,8.91;3,48.96,709.66,207.93,8.91">Although DeepRED is able to construct complete trees that are closely faithful to the original network, the generated trees can be quite large, and the implementation of the method takes substantial time and memory and is therefore limited in scalability.</s></p><p><s coords="3,321.96,52.18,241.12,8.91;3,312.00,64.18,251.02,8.91;3,312.00,76.06,251.25,8.91">Another decision tree method is ANN-DT <ref type="bibr" coords="3,498.00,52.18,16.64,8.91" target="#b24">[25]</ref> which uses sampling to create a decision tree: the key idea is to use sampling to expand training using a nearest neighbor method.</s></p><p><s coords="3,321.96,88.06,241.26,8.91;3,312.00,99.94,251.13,8.91">3) Automatic-Rule Extraction: Automatic rule extraction is another well-studied approach for summarizing decisions.</s><s coords="3,312.00,111.94,251.25,8.91;3,312.00,123.94,251.12,8.91;3,312.00,135.82,251.12,8.91;3,312.00,147.82,126.09,8.91">Andrews et al <ref type="bibr" coords="3,369.96,111.94,16.64,8.91" target="#b25">[26]</ref> outlines existing rule extraction techniques, and provides a useful taxonomy of five dimensions of ruleextraction methods including their expressive power, translucency and the quality of rules.</s><s coords="3,441.72,147.82,121.42,8.91;3,312.00,159.82,173.49,8.91">Another useful survey can be found in the master's thesis by Zilke <ref type="bibr" coords="3,466.32,159.82,15.34,8.91" target="#b26">[27]</ref>.</s></p><p><s coords="3,321.96,171.70,241.02,8.91;3,312.00,183.70,231.33,8.91">Decompositional approaches work on the neuron-level to extract rules to mimic the behavior of individual units.</s><s coords="3,547.56,183.70,15.58,8.91;3,312.00,195.58,251.36,8.91;3,312.00,207.58,216.93,8.91">The KT method <ref type="bibr" coords="3,365.88,195.58,16.52,8.91" target="#b27">[28]</ref> goes through each neuron, layer-by-layer and applies an if-then rule by finding a threshold.</s><s coords="3,533.64,207.58,29.36,8.91;3,312.00,219.58,250.98,8.91;3,312.00,231.46,251.34,8.91;3,312.00,243.46,21.93,8.91">Similar to DeepRED, there is a merging step which creates rules in terms of the inputs rather than the outputs of the preceding layer.</s><s coords="3,336.36,243.46,226.76,8.91;3,312.00,255.46,89.97,8.91">This is an exponential approach which is not tangible for deep neural networks.</s><s coords="3,405.48,255.46,157.86,8.91;3,312.00,267.34,251.10,8.91;3,312.00,279.34,95.25,8.91">However, a similar approach proposed by Tsukimoto <ref type="bibr" coords="3,371.76,267.34,16.64,8.91" target="#b28">[29]</ref> achieves polynomial-time complexity, and may be more tangible.</s><s coords="3,411.60,279.34,151.52,8.91;3,312.00,291.22,251.34,8.91;3,312.00,303.22,156.09,8.91">There has also been work on transforming neural network to fuzzy rules <ref type="bibr" coords="3,474.60,291.22,15.34,8.91" target="#b29">[30]</ref>, by transforming each neuron into an approximate rule.</s></p><p><s coords="3,321.96,315.22,241.14,8.91;3,312.00,327.10,251.12,8.91;3,312.00,339.10,126.81,8.91">Pedagogical approaches aim to extract rules by directly mapping inputs to outputs rather than considering the inner workings of a neural network.</s><s coords="3,442.80,339.10,120.22,8.91;3,312.00,351.10,250.98,8.91;3,312.00,362.98,47.85,8.91">These treat the network as a black box, and find trends and functions from the inputs to the outputs.</s><s coords="3,363.24,362.98,199.86,8.91;3,312.00,374.98,196.77,8.91">Validity Interval Analysis is a type of sensitivity analysis to mimic neural network behavior <ref type="bibr" coords="3,489.60,374.98,15.34,8.91" target="#b30">[31]</ref>.</s><s coords="3,512.04,374.98,51.18,8.91;3,312.00,386.86,251.10,8.91;3,312.00,398.86,132.93,8.91">This method finds stable intervals, where there is some correlation between the input and the predicted class.</s><s coords="3,447.96,398.86,115.24,8.91;3,312.00,410.86,146.61,8.91">Another way to extract rules using sampling methods <ref type="bibr" coords="3,416.28,410.86,15.24,8.91" target="#b31">[32]</ref>, <ref type="bibr" coords="3,439.44,410.86,15.34,8.91" target="#b32">[33]</ref>.</s><s coords="3,462.72,410.86,100.50,8.91;3,312.00,422.74,251.14,8.91;3,312.00,434.74,208.41,8.91">Some of these sampling approaches only work on binary input <ref type="bibr" coords="3,480.96,422.74,16.64,8.91" target="#b33">[34]</ref> or use genetic algorithms to produce new training examples <ref type="bibr" coords="3,501.24,434.74,15.34,8.91" target="#b34">[35]</ref>.</s><s coords="3,523.92,434.74,39.20,8.91;3,312.00,446.74,251.37,8.91;3,312.00,458.62,226.41,8.91">Other approaches aim to reverse engineer the neural network, notably, the RxREN algorithm, which is used in DeepRED <ref type="bibr" coords="3,513.37,458.62,20.03,8.91" target="#b20">[21]</ref>.</s></p><p><s coords="3,321.96,470.62,241.19,8.91;3,312.00,482.50,251.14,8.91;3,312.00,494.50,226.77,8.91">Other notable rule-extraction techniques include the MofN algorithm <ref type="bibr" coords="3,355.32,482.50,15.24,8.91" target="#b35">[36]</ref>, which tries to find rules that explain single neurons by clustering and ignoring insignificant neurons.</s><s coords="3,540.84,494.50,22.16,8.91;3,312.00,506.50,251.00,8.91;3,312.00,518.38,251.20,8.91;3,312.00,530.38,96.09,8.91">Similarly, The FERNN <ref type="bibr" coords="3,388.08,506.50,16.64,8.91" target="#b36">[37]</ref> algorithm uses the C4.5 algorithm <ref type="bibr" coords="3,546.48,506.50,16.52,8.91" target="#b23">[24]</ref> and tries to identify the meaningful hidden neurons and inputs to a particular network.</s></p><p><s coords="3,321.96,542.38,241.16,8.91;3,312.00,554.26,250.98,8.91;3,312.00,566.26,42.69,8.91">Although rule-extraction techniques increase the transparency of neural networks, they may not be truly faithful to the model.</s><s coords="3,357.60,566.26,205.62,8.91;3,312.00,578.14,251.14,8.91;3,312.00,590.14,114.82,8.91">With that, there are other methods that are focused on creating trust between the user and the model, even if the model is not "sophisicated."</s></p><p><s coords="3,321.96,602.14,241.16,8.91;3,312.00,614.02,251.02,8.91;3,312.00,626.02,251.12,8.91;3,312.00,638.02,251.14,8.91;3,312.00,649.90,198.81,8.91">4) Salience Mapping: The salience map approach is exemplified by occlusion procedure by Zeiler <ref type="bibr" coords="3,504.24,614.02,15.34,8.91" target="#b37">[38]</ref>, where a network is repeatedly tested with portions of the input occluded to create a map showing which parts of the data actually have influence on the network output.</s><s coords="3,515.52,649.90,47.58,8.91;3,312.00,661.90,251.10,8.91;3,312.00,673.78,251.14,8.91;3,312.00,685.78,134.13,8.91">When deep network parameters can be inspected directly, a salience map can be created more efficiently by directly computing the input gradient (Simonyan <ref type="bibr" coords="3,423.60,685.78,15.02,8.91" target="#b38">[39]</ref>).</s><s coords="3,451.08,685.78,112.02,8.91;3,312.00,697.78,251.02,8.91;3,312.00,709.66,250.98,8.91;4,48.96,52.19,251.25,8.91">Since such derivatives can miss important aspects of the information that flows through a network, a number of other approaches have been designed to propagate quantities other than gradients through the network.</s><s coords="4,48.96,64.19,251.24,8.91;4,48.96,76.07,251.13,8.91">Examples are LRP <ref type="bibr" coords="4,132.24,64.19,15.34,8.91" target="#b39">[40]</ref>, DeepLIFT <ref type="bibr" coords="4,202.44,64.19,15.24,8.91" target="#b40">[41]</ref>, CAM <ref type="bibr" coords="4,253.20,64.19,15.34,8.91" target="#b41">[42]</ref>, Grad-CAM <ref type="bibr" coords="4,75.84,76.07,15.24,8.91" target="#b42">[43]</ref>, Integrated gradients <ref type="bibr" coords="4,184.08,76.07,15.34,8.91" target="#b43">[44]</ref>, and SmoothGrad <ref type="bibr" coords="4,280.92,76.07,15.34,8.91" target="#b44">[45]</ref>.</s><s coords="4,48.96,88.07,251.12,8.91;4,48.96,99.95,251.10,8.91;4,48.96,111.95,251.17,8.91;4,48.96,123.95,68.61,8.91">Each technique strikes a balance between showing areas of high network activation, where neurons fire strongest, and areas of high network sensitivity, where changes would most affect the output.</s><s coords="4,120.24,123.95,179.82,8.91;4,48.96,135.83,104.49,8.91">A comparison of some of these methods can be found in Ancona <ref type="bibr" coords="4,134.28,135.83,15.34,8.91" target="#b45">[46]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,48.96,155.83,206.80,8.74">B. Explanations of Deep Network Representations</head><p><s coords="4,58.92,171.35,241.11,8.91;4,48.96,183.23,251.12,8.91;4,48.96,195.23,251.24,8.91;4,48.96,207.23,251.10,8.91;4,48.96,219.11,251.12,8.91;4,48.96,231.11,22.41,8.91">While the number of individual operations in a network is vast, deep networks are internally organized into a smaller number of subcomponents: for example, the billions of operations of ResNet are organized into about 100 layers, each computing between 64 and 2048 channels of information per pixel.</s><s coords="4,73.80,231.11,226.26,8.91;4,48.96,242.99,251.34,8.91;4,48.96,254.99,71.49,8.91">The explanation of deep network representations aims to understand the role and structure of the data flowing through these bottlenecks.</s><s coords="4,123.12,254.99,177.18,8.91;4,48.96,266.99,251.14,8.91;4,48.96,278.87,251.34,8.91;4,48.96,290.87,251.00,8.91;4,48.96,302.87,251.14,8.91;4,48.96,314.75,251.34,8.91;4,48.96,326.75,50.13,8.91">This work can be divided by the granularity examined: representations can be understood by layer, where all the information flowing through a layer is considered together, and by unit, where single neurons or single filter channels are considered individually, and by vector, where other vector directions in representation space are considered individually.</s></p><p><s coords="4,58.92,338.87,241.14,8.91;4,48.96,350.87,251.31,8.91;4,48.96,362.75,156.57,8.91">1) Role of Layers: Layers can be understood by testing their ability to help solve different problems from the problems the network was originally trained on.</s><s coords="4,208.92,362.75,91.14,8.91;4,48.96,374.75,251.22,8.91;4,48.96,386.75,251.17,8.91;4,48.96,398.63,251.10,8.91;4,48.96,410.63,251.32,8.91;4,48.96,422.63,251.12,8.91;4,48.96,434.51,251.10,8.91;4,48.96,446.51,77.49,8.91">For example Razavian <ref type="bibr" coords="4,48.96,374.75,16.64,8.91" target="#b46">[47]</ref> found that the output of an internal layer of a network trained to classify images of objects in the ImageNet dataset produced a feature vector that could be directly reused to solve a number of other difficult image processing problems including fine-grained classification of different species of birds, classification of scene images, attribute detection, and object localization.</s><s coords="4,130.20,446.51,169.86,8.91;4,48.96,458.39,251.14,8.91;4,48.96,470.39,251.17,8.91;4,48.96,482.39,154.41,8.91">In each case, a simple model such as an SVM was able to directly apply the deep representation to the target problem, beating state-of-the-art performance without training a whole new deep network.</s><s coords="4,207.96,482.39,92.10,8.91;4,48.96,494.27,251.10,8.91;4,48.96,506.27,251.37,8.91;4,48.96,518.27,251.46,8.91;4,48.96,530.15,212.49,8.91">This method of using a layer from one network to solve a new problem is called transfer learning, and it is of immense practical importance, allowing many new problems to be solved without developing new datasets and networks for each new problem.</s><s coords="4,265.68,530.15,34.45,8.91;4,48.96,542.15,251.22,8.91;4,48.96,554.03,119.85,8.91">Yosinksi <ref type="bibr" coords="4,48.96,542.15,16.64,8.91" target="#b47">[48]</ref> described a framework for quantifying transfer learning capabilities in other contexts.</s></p><p><s coords="4,58.92,566.27,241.16,8.91;4,48.96,578.15,251.41,8.91;4,48.96,590.15,84.45,8.91">2) Role of Individual Units: The information within a layer can be further subdivided into individual neurons or individual convolutional filters.</s><s coords="4,138.48,590.15,161.58,8.91;4,48.96,602.15,251.14,8.91;4,48.96,614.03,251.12,8.91;4,48.96,626.03,251.24,8.91;4,48.96,638.03,35.97,8.91">The role of such individual units can be understood qualitatively, by creating visualizations of the input patterns that maximize the response of a single unit, or quantitatively, by testing the ability of a unit to solve a transfer problem.</s><s coords="4,87.60,638.03,212.53,8.91;4,48.96,649.91,251.17,8.91;4,48.96,661.91,251.22,8.91;4,48.96,673.79,114.93,8.91">Visualizations can be created by optimizing an input image using gradient descent <ref type="bibr" coords="4,174.36,649.91,15.34,8.91" target="#b38">[39]</ref>, by sampling images that maximize activation <ref type="bibr" coords="4,133.68,661.91,15.34,8.91" target="#b48">[49]</ref>, or by training a generative network to create such images <ref type="bibr" coords="4,144.72,673.79,15.34,8.91" target="#b49">[50]</ref>.</s><s coords="4,168.24,673.79,132.06,8.91;4,48.96,685.79,229.17,8.91">Units can also be characterized quantitatively by testing their ability to solve a task.</s><s coords="4,283.44,685.79,16.66,8.91;4,48.96,697.79,251.10,8.91;4,48.96,709.67,251.34,8.91;4,312.00,52.19,233.73,8.91">One example of a such a method is network dissection <ref type="bibr" coords="4,253.56,697.79,15.34,8.91" target="#b50">[51]</ref>, which measures the ability of individual units solve a segmentation problem over a broad set of labeled visual concepts.</s><s coords="4,551.40,52.19,11.58,8.91;4,312.00,64.19,251.41,8.91;4,312.00,76.07,251.14,8.91;4,312.00,88.07,251.10,8.91;4,312.00,99.95,251.10,8.91;4,312.00,111.95,173.49,8.91">By quantifying the ability of individual units to locate emergent concepts such as objects, parts, textures, and colors that are not explicit in the original training set, network dissection can be used characterize the kind of information represented by visual networks at each unit of a network.</s></p><p><s coords="4,321.96,124.07,241.50,8.91;4,312.00,135.95,250.98,8.91;4,312.00,147.95,251.03,8.91;4,312.00,159.95,251.12,8.91;4,312.00,171.83,251.14,8.91;4,312.00,183.83,251.46,8.91;4,312.00,195.71,168.69,8.91">A review of explanatory methods focused on understanding unit representations used by visual CNNs can be found in <ref type="bibr" coords="4,312.00,147.95,15.24,8.91" target="#b51">[52]</ref>, which examines methods for visualization of CNN representations in intermediate network layers, diagnosis of these representations, disentanglement representation units, the creation of explainable models, and semantic middle-to-end learning via human-computer interaction.</s></p><p><s coords="4,321.96,207.83,241.06,8.91;4,312.00,219.83,250.98,8.91;4,312.00,231.71,39.09,8.91">Pruning of networks <ref type="bibr" coords="4,414.00,207.83,16.64,8.91" target="#b52">[53]</ref> has also been shown to be a step towards understanding the role of individual neurons in networks.</s><s coords="4,354.48,231.71,208.62,8.91;4,312.00,243.71,250.98,8.91;4,312.00,255.71,53.13,8.91">In particular, large networks that train successfully contain small subnetworks with initializations conducive to optimization.</s><s coords="4,367.44,255.71,195.68,8.91;4,312.00,267.59,250.98,8.91;4,312.00,279.59,227.37,8.91">This demonstrates that there exist training strategies that make it possible to solve the same problems with much smaller networks that may be more interpretable.</s></p><p><s coords="4,321.96,291.71,241.18,8.91;4,312.00,303.59,251.34,8.91;4,312.00,315.59,251.10,8.91;4,312.00,327.59,166.41,8.91">3) Role of Representation Vectors: Closely related to the approach of characterizing individual units is characterizing other directions in the representation vector space formed by linear combinations of individual units.</s><s coords="4,483.36,327.59,79.86,8.91;4,312.00,339.47,251.02,8.91;4,312.00,351.47,251.12,8.91;4,312.00,363.47,207.69,8.91">Concept Activation Vectors (CAVs) <ref type="bibr" coords="4,380.16,339.47,16.64,8.91" target="#b53">[54]</ref> are a framework for interpretation of a neural nets representations by identifying and probing directions that align with human-interpretable concepts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,312.00,383.11,142.84,8.74">C. Explanation-Producing Systems</head><p><s coords="4,321.96,398.63,241.24,8.91;4,312.00,410.51,251.14,8.91;4,312.00,422.51,251.29,8.91;4,312.00,434.51,251.12,8.91;4,312.00,446.39,251.37,8.91">Several different approaches can be taken to create networks that are designed to be easier to explain: networks can be trained to use explicit attention as part of their architecture; they can be trained to learn disentangled representations; or they can be directly trained to create generative explanations.</s></p><p><s coords="4,321.96,458.51,241.14,8.91;4,312.00,470.51,251.17,8.91;4,312.00,482.51,251.02,8.91;4,312.00,494.39,35.25,8.91">1) Attention Networks: Attention-based networks learn functions that provide a weighting over inputs or internal features to steer the information visible to other parts of a network.</s><s coords="4,350.76,494.39,212.62,8.91;4,312.00,506.39,251.26,8.91;4,312.00,518.27,251.24,8.91;4,312.00,530.27,250.98,8.91;4,312.00,542.27,251.10,8.91;4,312.00,554.15,131.85,8.91">Attention-based approaches have shown remarkable success in solving problems such as allowing natural language translation models to process words in an appropriate nonsequential order <ref type="bibr" coords="4,384.24,530.27,15.34,8.91" target="#b54">[55]</ref>, and they have also been applied in domains such as fine-grained image classification <ref type="bibr" coords="4,527.04,542.27,16.52,8.91" target="#b55">[56]</ref> and visual question answering <ref type="bibr" coords="4,424.80,554.15,15.24,8.91" target="#b56">[57]</ref>.</s><s coords="4,448.56,554.15,114.73,8.91;4,312.00,566.15,251.24,8.91;4,312.00,578.15,251.10,8.91;4,312.00,590.03,251.14,8.91;4,312.00,602.03,107.01,8.91">Although units that control attention are not trained for the purpose of creating humanreadable explanations, they do directly reveal a map of which information passes through the network, which can serve as a form of explanation.</s><s coords="4,423.00,602.03,140.14,8.91;4,312.00,613.91,251.22,8.91;4,312.00,625.91,251.20,8.91;4,312.00,637.91,68.13,8.91">Datasets of human attention have been created <ref type="bibr" coords="4,367.56,613.91,15.34,8.91" target="#b57">[58]</ref>, <ref type="bibr" coords="4,390.72,613.91,15.56,8.91" target="#b58">[59]</ref>; these allow systems to be evaluated according to how closely and their internal attention resembles human attention.</s></p><p><s coords="4,321.96,649.91,241.26,8.91;4,312.00,661.91,251.10,8.91;4,312.00,673.79,251.24,8.91;4,312.00,685.79,159.09,8.91">While attention can be observed as a way of extracting explanations, another interesting approach is to train attention explicitly in order to create a network that has behavior that conforms to desired explanations.</s><s coords="4,475.08,685.79,88.18,8.91;4,312.00,697.79,251.22,8.91;4,312.00,709.67,251.14,8.91;5,48.96,52.19,251.00,8.91;5,48.96,64.19,175.65,8.91">This is the technique proposed by Ross <ref type="bibr" coords="4,389.64,697.79,15.24,8.91" target="#b59">[60]</ref>, where input sensitivity of a network is adjusted and measured in order to create networks that are "right for the right reasons;" the method can be used to steer the internal reasoning learned by a network.</s><s coords="5,227.04,64.19,73.18,8.91;5,48.96,76.07,251.17,8.91;5,48.96,88.07,251.10,8.91;5,48.96,99.95,136.41,8.91">They also propose that the method can be used to learn a sequence of models that discover new ways to solve a problem that may not have been discovered by previous instances.</s></p><p><s coords="5,58.92,116.39,241.28,8.91;5,48.96,128.39,251.10,8.91;5,48.96,140.39,129.81,8.91">2) Disentangled Representations: Disentangled representations have individual dimensions that describe meaningful and independent factors of variation.</s><s coords="5,181.08,140.39,119.00,8.91;5,48.96,152.27,251.10,8.91;5,48.96,164.27,251.41,8.91;5,48.96,176.27,251.10,8.91;5,48.96,188.15,158.13,8.91">The problem of separating latent factors is an old problem that has previously been attacked using a variety of techniques such as Principal Component Analysis <ref type="bibr" coords="5,89.28,176.27,15.34,8.91" target="#b60">[61]</ref>, Independent Component Analysis <ref type="bibr" coords="5,261.00,176.27,15.34,8.91" target="#b61">[62]</ref>, and Nonnegative Matrix Factorization <ref type="bibr" coords="5,188.04,188.15,15.24,8.91" target="#b62">[63]</ref>.</s><s coords="5,210.12,188.15,89.98,8.91;5,48.96,200.15,230.01,8.91">Deep networks can be trained to explicitly learn disentangled representations.</s><s coords="5,283.44,200.15,16.66,8.91;5,48.96,212.03,251.13,8.91;5,48.96,224.03,251.12,8.91;5,48.96,236.03,251.62,8.91;5,48.96,247.91,39.69,8.91">One approach that shows promise is Variational Autoencoding <ref type="bibr" coords="5,280.92,212.03,15.34,8.91" target="#b63">[64]</ref>, which trains a network to optimize a model to match the input probability distribution according to information-theoretic measures.</s><s coords="5,92.40,247.91,207.76,8.91;5,48.96,259.91,215.73,8.91">Beta-VAE <ref type="bibr" coords="5,137.04,247.91,16.64,8.91" target="#b64">[65]</ref> is a tuning of the method that has been observed to disentangle factors remarkably well.</s><s coords="5,267.36,259.91,32.84,8.91;5,48.96,271.91,251.29,8.91;5,48.96,283.79,251.22,8.91;5,48.96,295.79,57.57,8.91">Another approach is InfoGAN <ref type="bibr" coords="5,138.60,271.91,15.34,8.91" target="#b65">[66]</ref>, which trains generative adversarial networks with an objective that reduces entanglement between latent factors.</s><s coords="5,112.32,295.79,187.86,8.91;5,48.96,307.67,251.26,8.91;5,48.96,319.67,251.08,8.91;5,48.96,331.67,251.41,8.91;5,48.96,343.55,251.20,8.91;5,48.96,355.55,19.17,8.91">Special loss functions have been suggested for encouraging feed-forward networks to also disentangle their units; this can be used to create interpretable CNNs that have individual units that detect coherent meaningful patches instead of difficult-to-interpret mixtures of patterns <ref type="bibr" coords="5,48.96,355.55,15.34,8.91" target="#b66">[67]</ref>.</s><s coords="5,71.04,355.55,229.24,8.91;5,48.96,367.55,251.02,8.91;5,48.96,379.43,35.25,8.91">Disentangled units can enable the construction of graphs <ref type="bibr" coords="5,48.96,367.55,16.64,8.91" target="#b67">[68]</ref> and decision trees <ref type="bibr" coords="5,149.64,367.55,16.52,8.91" target="#b68">[69]</ref> to elucidate the reasoning of a network.</s><s coords="5,88.20,379.43,212.08,8.91;5,48.96,391.43,251.19,8.91;5,48.96,403.31,213.69,8.91">Architectural alternatives such as capsule networks <ref type="bibr" coords="5,48.96,391.43,16.64,8.91" target="#b69">[70]</ref> can also organize the information in a network into pieces that disentangle and represent higher-level concepts.</s></p><p><s coords="5,58.92,419.75,241.14,8.91;5,48.96,431.75,251.74,8.91;5,48.96,443.75,251.13,8.91">3) Generated Explanations: Finally, deep networks can also be designed to generate their own human-understandable explanations as part of the explicit training of the system.</s><s coords="5,48.96,455.63,251.12,8.91;5,48.96,467.63,251.12,8.91;5,48.96,479.63,136.41,8.91">Explanation generation has been demonstrated as part of systems for visual question answering <ref type="bibr" coords="5,207.12,467.63,16.64,8.91" target="#b70">[71]</ref> as well as in finegrained image classification <ref type="bibr" coords="5,166.20,479.63,15.34,8.91" target="#b71">[72]</ref>.</s><s coords="5,189.00,479.63,111.08,8.91;5,48.96,491.51,251.14,8.91;5,48.96,503.51,196.41,8.91">In addition to solving their primary task, these systems synthesize a "because" sentence that explains the decision in natural language.</s><s coords="5,249.84,503.51,50.36,8.91;5,48.96,515.39,251.12,8.91;5,48.96,527.39,251.10,8.91;5,48.96,539.39,139.89,8.91">The generators for these explanations are trained on large data sets of human-written explanations, and they explain decisions using language that a person would use.</s></p><p><s coords="5,58.92,554.27,241.28,8.91;5,48.96,566.27,251.14,8.91;5,48.96,578.15,93.45,8.91">Multimodal explanations that incorporate both visual pointing and textual explanations can be generated; this is the approach taken in <ref type="bibr" coords="5,123.24,578.15,15.34,8.91" target="#b58">[59]</ref>.</s><s coords="5,145.20,578.15,154.88,8.91;5,48.96,590.15,251.10,8.91;5,48.96,602.15,39.21,8.91">This system builds upon the winner of the 2016 VQA challenge <ref type="bibr" coords="5,153.48,590.15,15.34,8.91" target="#b72">[73]</ref>, with several simplification and additions.</s><s coords="5,91.80,602.15,208.30,8.91;5,48.96,614.03,251.24,8.91;5,48.96,626.03,251.22,8.91;5,48.96,638.03,200.13,8.91">In addition to the question answering task and the internal attention map, the system trains an additional longform explanation generator together with a second attention map optimized as a visual pointing explanation.</s><s coords="5,252.84,638.03,47.29,8.91;5,48.96,649.91,251.24,8.91;5,48.96,661.91,214.29,8.91">Both visual and textual explanations score well individually and together on evaluations of user trust and explanation quality.</s><s coords="5,266.88,661.91,33.32,8.91;5,48.96,673.79,251.07,8.91;5,48.96,685.79,251.43,8.91;5,48.96,697.79,251.14,8.91;5,48.96,709.67,132.09,8.91">Interestingly, the generation of these highly readable explanations is conditioned on the output of the network: the explanations are generated based on the decision, after the decision of the network has already been made.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="5,393.24,52.19,88.31,8.91">IV. RELATED WORK</head><p><s coords="5,321.96,67.19,241.14,8.91;5,312.00,79.19,251.12,8.91;5,312.00,91.07,26.37,8.91">We provide a summary of related review papers, and an overview of interpretability and explainability in other domains.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="5,312.00,109.39,74.26,8.74">A. Interpretability</head><p><s coords="5,321.96,124.31,241.14,8.91;5,312.00,136.31,251.13,8.91">A previous survey has attempted to define taxonomies and best practices for a "strong science" of interpretability <ref type="bibr" coords="5,543.96,136.31,15.34,8.91" target="#b73">[74]</ref>.</s><s coords="5,312.00,148.19,251.14,8.91;5,312.00,160.19,251.10,8.91;5,312.00,172.19,251.14,8.91;5,312.00,184.07,76.53,8.91">The motivation of this paper is similar to ours, noting that "the volume of research on interpretability is rapidly growing" and that there is no clear existing definition or evaluation criteria for interpretability.</s><s coords="5,391.80,184.07,171.34,8.91;5,312.00,196.07,251.02,8.91;5,312.00,207.95,251.37,8.91;5,312.00,219.95,251.12,8.91;5,312.00,231.95,196.17,8.91">The authors define interpretability as "the ability to explain or to present in understandable terms to a human" and suggest a variety of definitions for explainability, converging on the notion that interpretation is the act of discovering the evaluations of an explanation.</s><s coords="5,513.00,231.95,50.32,8.91;5,312.00,243.83,251.26,8.91;5,312.00,255.83,207.45,8.91">The authors attempt to reach consensus on the definition of interpretable machine learning and how it should be measured.</s><s coords="5,523.20,255.83,39.82,8.91;5,312.00,267.83,251.14,8.91;5,312.00,279.71,195.21,8.91">While we are inspired by the taxonomy of this paper, we focus on the explainability aspect rather than interpretability.</s></p><p><s coords="5,321.96,291.59,241.16,8.91;5,312.00,303.59,251.73,8.91;5,312.00,315.47,180.93,8.91">The main contribution of this paper is a taxonomy of modes for interpretability evaluations: application-grounded, human-grounded, and functionally grounded.</s><s coords="5,495.00,315.47,68.02,8.91;5,312.00,327.47,251.08,8.91;5,312.00,339.47,251.12,8.91;5,312.00,351.35,251.08,8.91;5,312.00,363.35,120.21,8.91">The authors state interpretability is required when a problem formulation is incomplete, when the optimization problem -the key definition to solve the majority of machine learning problems -is disconnected from evaluation.</s><s coords="5,434.52,363.35,128.56,8.91;5,312.00,375.35,251.29,8.91;5,312.00,387.23,251.22,8.91;5,312.00,399.23,80.13,8.91">Since their problem statement is the incompleteness criteria of models, resulting in a disconnect between the user and the optimization problem, evaluation approaches are key.</s></p><p><s coords="5,321.96,411.11,241.16,8.91;5,312.00,422.99,143.37,8.91">The first evaluation approach is application-grounded, involving real humans on real tasks.</s><s coords="5,459.36,422.99,103.84,8.91;5,312.00,434.99,251.32,8.91;5,312.00,446.99,251.14,8.91;5,312.00,458.87,157.65,8.91">This evaluation measures how well human-generated explanations can aid other humans in particular tasks, with explanation quality assessed in the true context of the explanation's end tasks.</s><s coords="5,473.40,458.87,89.84,8.91;5,312.00,470.87,192.81,8.91">For instance, a doctor should evaluate diagnosis systems in medicine.</s></p><p><s coords="5,321.96,482.75,241.16,8.91;5,312.00,494.63,211.65,8.91">The second evaluation approach is human-grounded, using human evaluation metrics on simplified tasks.</s><s coords="5,528.48,494.63,34.62,8.91;5,312.00,506.63,251.12,8.91;5,312.00,518.63,76.77,8.91">The key motivation is the difficulty of finding target communities for application testing.</s><s coords="5,392.04,518.63,171.10,8.91;5,312.00,530.51,250.98,8.91;5,312.00,542.51,214.29,8.91">Human-grounded approaches may also be used when specific end-goals, such as identifying errors in safety-critical tasks, are not possible to realize fully.</s></p><p><s coords="5,321.96,554.39,241.16,8.91;5,312.00,566.39,133.89,8.91">The final evaluation metric is functionally grounded evaluation, without human subjects.</s><s coords="5,450.60,566.39,112.53,8.91;5,312.00,578.27,251.29,8.91;5,312.00,590.27,118.41,8.91">In this experimental setup, proxy or simplified tasks are used to prove some formal definition of interpretability.</s><s coords="5,435.24,590.27,127.81,8.91;5,312.00,602.15,251.08,8.91;5,312.00,614.15,39.33,8.91">The authors acknowledge that choosing which proxy to use is a challenge inherent to this approach.</s><s coords="5,354.72,614.15,208.38,8.91;5,312.00,626.15,251.22,8.91;5,312.00,638.03,251.32,8.91;5,312.00,650.03,251.08,8.91;5,312.00,662.03,118.29,8.91">There lies a delicate tradeoff between choosing an interpretable model and a less interpretable proxy method which is more representative of model behavior; the authors acknowledge this point and briefly mention decision trees as a highly interpretable model.</s></p><p><s coords="5,321.96,673.79,241.14,8.91;5,312.00,685.79,251.24,8.91;5,312.00,697.79,251.37,8.91">The authors then discuss open problems, best practices and future work in interpretability research, while heavily encouraging data-driven approaches for discovery in interpretability.</s><s coords="5,312.00,709.67,251.25,8.91;6,48.96,52.19,251.12,8.91;6,48.96,64.19,251.43,8.91;6,48.96,76.07,83.37,8.91">Although the contribution of the interpretability definition, we distinguish our taxonomy by defining different focuses of explanations a model can provide, and how those explanations should be evaluated.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,48.96,102.31,110.36,8.74">B. Explainable AI for HCI</head><p><s coords="6,58.92,120.35,241.06,8.91;6,48.96,132.35,251.08,8.91;6,48.96,144.35,19.17,8.91">One previous review paper of explainable AI performed a sizable data-driven literature analysis of explainable systems <ref type="bibr" coords="6,48.96,144.35,15.34,8.91" target="#b74">[75]</ref>.</s><s coords="6,71.76,144.35,228.32,8.91;6,48.96,156.23,251.14,8.91;6,48.96,168.23,195.81,8.91">In this work, the authors move beyond the classical AI interpretability argument, focusing instead on how to create practical systems with efficacy for real users.</s><s coords="6,249.96,168.23,50.32,8.91;6,48.96,180.23,251.10,8.91;6,48.96,192.11,251.34,8.91;6,48.96,204.11,251.31,8.91;6,48.96,215.99,251.10,8.91;6,48.96,227.99,251.20,8.91;6,48.96,239.99,246.81,8.91">The authors motivate AI systems that are "explainable by design" and present their findings with three contributions: a data-driven network analysis of 289 core papers and 12,412 citing papers for an overview of explainable AI research, a perspective on trends using network analysis, and a proposal for best practices and future work in HCI research pertaining to explainablity.</s></p><p><s coords="6,58.92,252.82,241.17,8.91;6,48.96,264.70,251.10,8.91;6,48.96,276.70,251.12,8.91;6,48.96,288.59,251.12,8.91;6,48.96,300.59,102.09,8.91">Since most of the paper focuses on the literature analysis, the authors highlight only three large areas in their related work section: explainable artificial intelligence (XAI), intelligibility and interpretability in HCI, and analysis methods for trends in research topics.</s></p><p><s coords="6,58.92,313.42,241.18,8.91;6,48.96,325.42,251.12,8.91;6,48.96,337.30,122.97,8.91">The major contribution of this paper is a sizable literature analysis of explainable research, enabled by the citation network the authors constructed.</s><s coords="6,175.68,337.30,124.38,8.91;6,48.96,349.30,251.12,8.91;6,48.96,361.30,251.26,8.91;6,48.96,373.19,251.10,8.91;6,48.96,385.19,251.25,8.91">Papers were aggregated based on a keyword search on variations of the terms "intelligible," "interpretable," "transparency," "glass box," "black box," "scrutable," "counterfacutals," and "explainable," and then pruned down to 289 core papers and 12,412 citing papers.</s><s coords="6,48.96,397.07,251.12,8.91;6,48.96,409.07,251.34,8.91;6,48.96,421.07,251.12,8.91;6,48.96,432.95,251.24,8.91;6,48.96,444.95,251.26,8.91;6,48.96,456.95,251.12,8.91;6,48.96,468.83,251.12,8.91;6,48.96,480.83,17.49,8.91">Using network analysis, the authors identified 28 significant clusters and 9 distinct research communities, including early artificial intelligence, intelligent systems/agents/user interfaces, ambient intelligence, interaction design and learnability, interpretable ML and classifier explainers, algorithmic fairness/accountability/transparency/policy/journalism, causality, psychological theories of explanations, and cognitive tutors.</s><s coords="6,72.12,480.83,227.94,8.91;6,48.96,492.71,246.09,8.91">In contrast, our work is focused on the research in interpretable ML and classifier explainers for deep learning.</s></p><p><s coords="6,58.92,505.55,241.36,8.91;6,48.96,517.55,251.17,8.91;6,48.96,529.55,194.61,8.91">With the same sets of core and citing papers, the authors performed LDA-based topic modeling on the abstract text to determine which communities are related.</s><s coords="6,249.36,529.55,50.92,8.91;6,48.96,541.43,251.14,8.91;6,48.96,553.43,134.13,8.91">The authors found the largest, most central and well-studied network to be intelligence and ambient systems.</s><s coords="6,185.52,553.43,114.56,8.91;6,48.96,565.43,251.37,8.91;6,48.96,577.31,251.26,8.91;6,48.96,589.31,251.43,8.91;6,48.96,601.19,53.01,8.91">In our research, the most important subnetworks are the Explainable AI: Fair, Accountable, and Transparent (FAT) algorithms and Interpretable Machine Learning (iML) subnetwork and the theories of explanations subnetworks.</s><s coords="6,106.20,601.19,193.98,8.91;6,48.96,613.19,251.22,8.91;6,48.96,625.19,229.65,8.91">In particular, the authors provide a distinction between FATML and interpretability; while FATML is focused on societal issues, interpretability is focused on methods.</s><s coords="6,281.16,625.19,18.92,8.91;6,48.96,637.07,251.34,8.91;6,48.96,649.07,251.10,8.91;6,48.96,661.07,81.81,8.91">Theory of explanations joins causality and cognitive psychology with the common threads of counterfactual reasoning and causal explanations.</s><s coords="6,134.28,661.07,165.88,8.91;6,48.96,672.95,106.05,8.91">Both these threads are important factors in our taxonomy analysis.</s></p><p><s coords="6,58.92,685.79,241.02,8.91;6,48.96,697.79,251.10,8.91;6,48.96,709.67,199.29,8.91">In the final section of their paper, the authors name two trends of particular interest to us: ML production rules and a road map to rigorous and usable intelligibility.</s><s coords="6,251.64,709.67,48.64,8.91;6,312.00,52.19,251.12,8.91;6,312.00,64.19,251.32,8.91;6,312.00,76.07,82.65,8.91">The authors note a lack of classical AI methods being applied to interpretability, encouraging broader application of those methods to current research.</s><s coords="6,399.84,76.07,163.26,8.91;6,312.00,88.07,251.22,8.91;6,312.00,99.95,118.53,8.91">Though this paper focused mainly on setting an HCI research agenda in explainability, it raises many points relevant to our work.</s><s coords="6,435.00,99.95,128.07,8.91;6,312.00,111.95,251.10,8.91;6,312.00,123.95,240.09,8.91">Notably, the literature analysis discovered subtopics and subdisciplines in psychology and social science, not yet identified as related in our analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,312.00,142.75,159.52,8.74">C. Explanations for Black-Box Models</head><p><s coords="6,321.96,157.79,241.28,8.91;6,312.00,169.67,251.14,8.91;6,312.00,181.67,162.69,8.91">A recent survey on methods for explaining black-box models <ref type="bibr" coords="6,326.64,169.67,16.52,8.91" target="#b75">[76]</ref> outlined a taxonomy to provide classifications of the main problems with opaque algorithms.</s><s coords="6,477.96,181.67,85.36,8.91;6,312.00,193.67,251.10,8.91;6,312.00,205.55,121.05,8.91">Most of the methods surveyed are applied to neural-network based algorithms, and therefore related to our work.</s></p><p><s coords="6,321.96,217.55,241.38,8.91;6,312.00,229.55,251.12,8.91;6,312.00,241.43,47.97,8.91">The authors provide an overview of methods that explaining decision systems based on opaque and obscure machine learning models.</s><s coords="6,363.72,241.43,199.33,8.91;6,312.00,253.43,213.09,8.91">Their taxonomy is detailed, distinguishing small differing components in explanation approaches (e.g.</s><s coords="6,527.64,253.43,35.46,8.91;6,312.00,265.31,251.00,8.91;6,312.00,277.31,249.01,8.91">Decision tree vs. single tree, neuron activation, SVM, etc.) Their classification examines four features for each explanation method:</s></p><p><s coords="6,321.96,291.23,142.05,8.91">1) The type of the problem faced.</s></p><p><s coords="6,321.96,303.23,240.57,8.91">2) The explanatory capability used to open the black box.</s></p><p><s coords="6,321.96,315.23,227.61,8.91">3) The type of black box model that can be explained.</s></p><p><s coords="6,321.96,327.11,241.29,8.91">4) The type of input data provided to the black box model.</s><s coords="6,312.00,341.15,250.98,8.91;6,312.00,353.03,251.12,8.91;6,312.00,365.03,251.17,8.91;6,312.00,377.03,251.17,8.91;6,312.00,388.91,223.89,8.91">They primarily divide the explanation methods according to the types of problem faced, and identify four groups of explanation methods: methods to explain black box models; methods to explain black box outcomes; methods to inspect black boxes; and methods to design transparent boxes.</s><s coords="6,539.28,388.91,23.82,8.91;6,312.00,400.91,251.10,8.91;6,312.00,412.79,251.12,8.91;6,312.00,424.79,251.37,8.91;6,312.00,436.79,73.29,8.91">Using their classification features and these problem definitions, they discuss and further categorize methods according to the type of explanatory capability adopted, the black box model "opened", and the input data.</s><s coords="6,387.48,436.79,175.62,8.91;6,312.00,448.67,251.08,8.91;6,312.00,460.67,251.10,8.91;6,312.00,472.67,47.61,8.91">Their goal is to review and classify the main black box explanation architectures, so their classifications can serve as a guide to identifying similar problems and approaches.</s><s coords="6,365.28,472.67,198.06,8.91;6,312.00,484.55,251.34,8.91;6,312.00,496.55,36.45,8.91">We find this work a meaningful contribution that is useful for exploring the design space of explanation methods.</s><s coords="6,352.08,496.55,211.02,8.91;6,312.00,508.43,251.20,8.91;6,312.00,520.43,251.10,8.91;6,312.00,532.43,251.29,8.91;6,312.00,544.31,202.29,8.91">Our classification is less finely-divided; rather than subdividing implementation techniques, we examine the focus of the explanatory capability and what each approach can explain, with an emphasis on understanding how different types of explainability methods can be evaluated.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,312.00,563.23,148.72,8.74">D. Explainability in Other Domains</head><p><s coords="6,321.96,578.15,241.09,8.91;6,312.00,590.15,251.14,8.91;6,312.00,602.15,86.85,8.91">Explainable planning <ref type="bibr" coords="6,414.36,578.15,16.64,8.91" target="#b76">[77]</ref> is an emerging discipline that exploits the model-based representations that exist in the planning community.</s><s coords="6,402.96,602.15,160.38,8.91;6,312.00,614.03,138.81,8.91">Some of the key ideas were proposed years ago in plan recognition <ref type="bibr" coords="6,431.64,614.03,15.34,8.91" target="#b77">[78]</ref>.</s><s coords="6,453.60,614.03,109.72,8.91;6,312.00,626.03,251.13,8.91;6,312.00,638.03,251.10,8.91;6,312.00,649.91,99.09,8.91">Explainable planning urges the familiar and common basis for communication with users, while acknowledging the gap between planning algorithms and human problem-solving.</s><s coords="6,413.76,649.91,149.34,8.91;6,312.00,661.91,251.20,8.91;6,312.00,673.79,251.12,8.91;6,312.00,685.79,251.01,8.91">In this paper, the authors outline and provide examples of a number of different types of questions that explanations could answer, like "Why did you do A" or "Why DIDN'T you do B", "Why CAN'T you do C", etc.</s><s coords="6,312.00,697.79,250.98,8.91;6,312.00,709.67,251.34,8.91;7,48.96,52.19,35.49,8.91">In addition, the authors emphasize that articulating a plan in natural language is NOT usually the same thing as explaining the plan.</s><s coords="7,87.84,52.19,212.48,8.91;7,48.96,64.19,251.14,8.91;7,48.96,76.07,251.25,8.91">A request for explanation is "an attempt to uncover a piece of knowledge that the questioner believes must be available to the system and that the questioner does not have".</s><s coords="7,48.96,88.07,251.12,8.91;7,48.96,99.95,73.89,8.91">We discuss the questions an explanation can and should answer in our conclusion.</s></p><p><s coords="7,58.92,112.67,241.14,8.91;7,48.96,124.67,182.73,8.91">Automatic explanation generation is also closely related to computers and machines that can tell stories.</s><s coords="7,234.84,124.67,65.24,8.91;7,48.96,136.55,251.13,8.91;7,48.96,148.55,251.34,8.91;7,48.96,160.55,92.49,8.91">In John Reeves' thesis <ref type="bibr" coords="7,74.40,136.55,15.24,8.91" target="#b78">[79]</ref>, he created the THUNDER program to read stories, construct character summaries, infer beliefs, and understand conflict and resolution.</s><s coords="7,144.12,160.55,156.13,8.91;7,48.96,172.43,231.57,8.91">Other work examines how to represent the necessary structures to do story understanding <ref type="bibr" coords="7,261.36,172.43,15.34,8.91" target="#b79">[80]</ref>.</s><s coords="7,284.52,172.43,15.58,8.91;7,48.96,184.43,251.11,8.91;7,48.96,196.31,251.41,8.91;7,48.96,208.31,164.49,8.91">The Genesis Story-Understanding System <ref type="bibr" coords="7,201.24,184.43,16.52,8.91" target="#b80">[81]</ref> is a working system that understands, uses, and composes stories using higher-level concept patterns and commonsense rules.</s><s coords="7,215.76,208.31,84.34,8.91;7,48.96,220.31,218.73,8.91">Explanation rules are used to supply missing causal or logical connections.</s></p><p><s coords="7,58.92,232.91,241.16,8.91;7,48.96,244.91,251.36,8.91;7,48.96,256.91,88.77,8.91">At the intersection of human robot interaction and storytelling is verbalization; generating explanations for humanrobot interaction <ref type="bibr" coords="7,118.56,256.91,15.34,8.91" target="#b81">[82]</ref>.</s><s coords="7,140.64,256.91,159.56,8.91;7,48.96,268.79,251.34,8.91;7,48.96,280.79,63.21,8.91">Similar approaches are found in abductive reasoning; using a case-based model <ref type="bibr" coords="7,220.92,268.79,16.64,8.91" target="#b82">[83]</ref> or explanatory coherence <ref type="bibr" coords="7,93.12,280.79,15.24,8.91" target="#b83">[84]</ref>.</s><s coords="7,115.92,280.79,184.14,8.91;7,48.96,292.67,251.10,8.91;7,48.96,304.67,251.13,8.91">This is also a well-studied field in brain and cognitive science by filling in the gaps of knowledge by imagining new ideas <ref type="bibr" coords="7,135.84,304.67,16.64,8.91" target="#b84">[85]</ref> or using statistical approaches <ref type="bibr" coords="7,280.92,304.67,15.34,8.91" target="#b85">[86]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="7,142.08,329.51,64.67,8.91">V. TAXONOMY</head><p><s coords="7,58.92,348.95,241.28,8.91;7,48.96,360.83,160.41,8.91">The approaches from the literature that we have examined fall into three different categories.</s><s coords="7,212.76,360.83,87.46,8.91;7,48.96,372.83,251.14,8.91;7,48.96,384.71,251.00,8.91;7,48.96,396.71,251.10,8.91;7,48.96,408.71,251.14,8.91;7,48.96,420.59,155.49,8.91">Some papers propose explanations that, while admittedly non-representative of the underlying decision processes, provide some degree of justification for emitted choices that may be used as response to demands for explanation in order to build human trust in the system's accuracy and reasonableness.</s><s coords="7,207.12,420.59,92.98,8.91;7,48.96,432.59,251.14,8.91;7,48.96,444.59,136.17,8.91">These systems emulate the processing of the data to draw connections between the inputs and outputs of the system.</s></p><p><s coords="7,58.92,457.19,241.16,8.91;7,48.96,469.19,158.97,8.91">The second purpose of an explanation is to explain the representation of data inside the network.</s><s coords="7,211.44,469.19,88.69,8.91;7,48.96,481.07,251.10,8.91;7,48.96,493.07,251.14,8.91;7,48.96,505.07,69.69,8.91">These provide insight about the internal operation of the network and can be used to facilitate explanations or interpretations of activation data within a network.</s><s coords="7,120.72,505.07,179.53,8.91;7,48.96,516.95,251.29,8.91;7,48.96,528.95,251.34,8.91;7,48.96,540.95,118.17,8.91">This is comparative to explaining the internal data structures of the program, to start to gain insights about why certain intermediate representations provide information that enables specific choices.</s></p><p><s coords="7,58.92,553.55,241.74,8.91;7,48.96,565.55,39.09,8.91">The final type of explanation is explanation-producing networks.</s><s coords="7,93.48,565.55,206.70,8.91;7,48.96,577.43,251.34,8.91;7,48.96,589.43,98.37,8.91">These networks are specifically built to explain themselves, and they are designed to simplify the interpretation of an opaque subsystem.</s><s coords="7,149.88,589.43,150.22,8.91;7,48.96,601.43,251.24,8.91;7,48.96,613.31,244.77,8.91">They are steps towards improving the transparency of these subsystems; where processing, representations, or other parts are justified and easier to understand.</s></p><p><s coords="7,58.92,626.03,241.09,8.91;7,48.96,638.03,251.12,8.91;7,48.96,649.91,251.13,8.91">The taxonomy we present is useful given the broad set of existing approaches for achieving varying degrees of interpretability and completeness in machine learning systems.</s><s coords="7,48.96,661.91,251.29,8.91;7,48.96,673.79,251.25,8.91">Two distinct methods claiming to address the same overall problem may, in fact, be answering very different questions.</s><s coords="7,48.96,685.79,251.10,8.91;7,48.96,697.79,251.19,8.91;7,48.96,709.67,112.65,8.91">Our taxonomy attempts to subdivide the problem space, based on existing approaches, to more precisely categorize what has already been accomplished.</s></p><p><s coords="7,321.96,52.19,241.16,8.91;7,312.00,64.19,76.65,8.91">We show the classifications of our reviewed methods per category in Table <ref type="table" coords="7,382.80,64.19,2.93,8.91">I</ref>.</s><s coords="7,390.96,64.19,172.40,8.91;7,312.00,76.07,251.24,8.91;7,312.00,88.07,44.01,8.91">Notice that the processing and explanationproducing roles are much more populated than the representation role.</s><s coords="7,359.28,88.07,203.86,8.91;7,312.00,99.95,251.24,8.91;7,312.00,111.95,13.53,8.91">We believe that this disparity is largely due to the fact that it is difficult to evaluate representation-based models.</s><s coords="7,329.28,111.95,207.69,8.91">User-study evaluations are not always appropriate.</s><s coords="7,540.36,111.95,22.76,8.91;7,312.00,123.95,251.10,8.91;7,312.00,135.83,249.21,8.91">Other numerical methods, like demonstrating better performance by adding or removing representations, are difficult to facilitate.</s></p><p><s coords="7,321.96,148.07,241.14,8.91;7,312.00,159.95,115.53,8.91">The position of our taxonomy is to promote research and evaluation across categories.</s><s coords="7,430.68,159.95,132.42,8.91;7,312.00,171.95,251.24,8.91;7,312.00,183.95,251.10,8.91;7,312.00,195.83,251.08,8.91;7,312.00,207.83,251.22,8.91;7,312.00,219.71,251.00,8.91;7,312.00,231.71,251.10,8.91;7,312.00,243.71,108.33,8.91">Instead of other explanatory and interpretability taxonomies that assess the purpose of explanations <ref type="bibr" coords="7,335.64,183.95,16.64,8.91" target="#b73">[74]</ref> and their connection to the user <ref type="bibr" coords="7,495.48,183.95,15.24,8.91" target="#b74">[75]</ref>, we instead assess the focus on the method, whether the method tries to explain the processing of the data by a network, explain the representation of data inside a network or to be a selfexplaining architecture to gain additional meta predictions and insights about the method.</s></p><p><s coords="7,321.96,255.83,241.40,8.91;7,312.00,267.83,251.17,8.91;7,312.00,279.71,149.85,8.91">We promote this taxonomy, particularly the explanationproducing sub-category, as a way to consider designing neural network architectures and systems.</s><s coords="7,467.88,279.71,95.26,8.91;7,312.00,291.71,251.10,8.91;7,312.00,303.71,249.09,8.91">We also highlight the lack of standardized evaluation metrics, and propose research crossing areas of the taxonomy as future research directions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="7,400.20,439.91,74.63,8.91">VI. EVALUATION</head><p><s coords="7,321.96,456.23,241.48,8.91;7,312.00,468.23,251.22,8.91;7,312.00,480.11,30.81,8.91">Although we outline three different focuses of explanations for deep networks, they do not share the same evaluation criteria.</s><s coords="7,348.60,480.11,214.54,8.91;7,312.00,492.11,209.25,8.91">Most of the work surveyed conducts one of the following types of evaluation of their explanations.</s></p><p><s coords="7,321.96,506.51,204.33,8.91">1) Completeness compared to the original model.</s><s coords="7,529.32,506.51,33.90,8.91;7,336.24,518.39,226.86,8.91;7,336.24,530.39,211.65,8.91">A proxy model can be evaluated directly according to how closely it approximates the original model being explained.</s><s coords="7,321.96,542.39,214.05,8.91">2) Completeness as measured on a substitute task.</s><s coords="7,540.36,542.39,22.78,8.91;7,336.24,554.27,226.89,8.91;7,336.24,566.27,210.57,8.91">Some explanations do not directly explain a model's decisions, but rather some other attribute that can be evaluated.</s><s coords="7,549.36,566.27,13.76,8.91;7,336.24,578.15,226.93,8.91;7,336.24,590.15,227.14,8.91;7,336.24,602.15,155.13,8.91">For example, a salience explanation that is intended to reveal model sensitivity can be evaluated against a brute-force measurement of the model sensitivity.</s><s coords="7,321.96,614.03,160.05,8.91">3) Ability to detect models with biases.</s><s coords="7,484.56,614.03,78.49,8.91;7,336.24,626.03,226.84,8.91;7,336.24,638.03,226.90,8.91;7,336.24,649.91,226.88,8.91;7,336.24,661.91,227.14,8.91;7,336.24,673.79,95.01,8.91">An explanation that reveals sensitivity to a specific phenomenon (such as a presence of a specific pattern in the input) can be tested for its ability to reveal models with the presence or absence of a relevant bias (such as reliance or ignorance of the specific pattern).</s><s coords="7,321.96,685.79,92.13,8.91">4) Human evaluation.</s><s coords="7,418.68,685.79,144.76,8.91;7,336.24,697.79,227.10,8.91;7,336.24,709.67,117.09,8.91">Humans can evaluate explanations for reasonableness, that is how well an explanation matches human expectations.</s><s coords="7,455.52,709.67,107.46,8.91;8,73.20,52.19,227.08,8.91;8,73.20,64.19,227.05,8.91;8,73.20,76.07,227.00,8.91;8,73.20,88.07,176.61,8.91">Human evaluation can also evaluate completeness or substitute-task completeness from the point of view of enabling a person to predict behavior of the original model; or according to helpfulness in revealing model biases to a person.</s><s coords="8,58.92,101.75,241.04,8.91;8,48.96,113.63,251.02,8.91;8,48.96,125.63,251.25,8.91">As we can see in Table <ref type="table" coords="8,170.52,101.75,6.06,8.91">II</ref>, the tradeoff between interpretability and its completeness can be seen not only as a balance between simplicity and accuracy in a proxy model.</s><s coords="8,48.96,137.63,251.10,8.91;8,48.96,149.51,251.12,8.91;8,48.96,161.51,173.73,8.91">The tradeoff can also be made by anchoring explanations to substitute tasks or evaluating explanations in terms of their ability to surface important model biases.</s><s coords="8,226.92,161.51,73.18,8.91;8,48.96,173.51,251.17,8.91;8,48.96,185.39,251.29,8.91;8,48.96,197.39,251.17,8.91;8,48.96,209.27,214.29,8.91">Each of the three types of explanation methods can provide explanations that can be evaluated for completeness (on those critical model characteristics), while still being easier to interpret than a full accounting for every detailed decision of the model.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,48.96,326.95,57.42,8.74">A. Processing</head><p><s coords="8,58.92,341.87,241.38,8.91;8,48.96,353.75,36.45,8.91">Processing models can also be regarded as emulation-based methods.</s><s coords="8,89.40,353.75,210.68,8.91;8,48.96,365.75,121.41,8.91">Proxy methods should be evaluated on their faithfulness to the original model.</s><s coords="8,173.76,365.75,126.34,8.91;8,48.96,377.75,70.29,8.91">A handful of these metrics are described in <ref type="bibr" coords="8,100.08,377.75,15.34,8.91" target="#b19">[20]</ref>.</s><s coords="8,121.80,377.75,178.47,8.91;8,48.96,389.63,115.29,8.91">The key idea is that evaluating completeness to a model should be local.</s><s coords="8,168.24,389.63,131.74,8.91;8,48.96,401.63,251.05,8.91;8,48.96,413.63,251.46,8.91;8,48.96,425.51,58.89,8.91">Even if a model, in our case, a deep neural network, is too complex globally, you can still explain in a way that makes sense locally by approximating local behavior.</s><s coords="8,110.40,425.51,189.73,8.91;8,48.96,437.51,251.25,8.91;8,48.96,449.39,251.12,8.91;8,48.96,461.39,251.12,8.91;8,48.96,473.39,106.53,8.91">Therefore, processing model explanations want to minimize the "complexity" of explanations (essentially, minimize length) as well as "local completeness" (error of interpretable representation relative to actual classifier, near instance being explained).</s></p><p><s coords="8,58.92,485.27,241.28,8.91;8,48.96,497.15,162.57,8.91">Salience methods that highlight sensitive regions for processing are often evaluated qualitatively.</s><s coords="8,214.08,497.15,86.05,8.91;8,48.96,509.15,251.12,8.91;8,48.96,521.15,251.17,8.91;8,48.96,533.03,123.93,8.91">Although they do not directly predict the output of the original method, these methods can also be evaluated for faithfulness, since their intent is to explain model sensitivity.</s><s coords="8,176.04,533.03,124.02,8.91;8,48.96,545.03,251.10,8.91;8,48.96,556.91,251.12,8.91;8,48.96,568.91,94.53,8.91">For example, <ref type="bibr" coords="8,232.20,533.03,16.64,8.91" target="#b45">[46]</ref> conducts an occlusion experiment as ground truth, in the model is tested on many version of an input image where each portion of the image is occluded.</s><s coords="8,147.36,568.91,152.98,8.91;8,48.96,580.91,251.17,8.91;8,48.96,592.79,201.93,8.91">This test determines in a brute-force but computationally inefficient way which parts of an input cause a model to change its outputs the most.</s><s coords="8,256.08,592.79,43.98,8.91;8,48.96,604.79,251.10,8.91;8,48.96,616.79,251.07,8.91;8,48.96,628.67,110.37,8.91">Then each salience method can be evaluated according to how closely the method produces salience maps that correlate with this occlusion-based sensitivity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,48.96,646.99,73.74,8.74">B. Representation</head><p><s coords="8,58.92,661.91,241.18,8.91;8,48.96,673.79,251.24,8.91;8,48.96,685.79,130.29,8.91">Representation-based methods typically characterize the role of portions of the representation by testing the representations on a transfer task.</s><s coords="8,184.08,685.79,116.22,8.91;8,48.96,697.79,251.14,8.91;8,48.96,709.67,251.22,8.91;8,312.00,52.19,251.20,8.91;8,312.00,64.19,251.26,8.91;8,312.00,76.07,186.81,8.91">For example, representation layers are characterized according to their ability to serve as feature input for a transfer problem, and both Network Dissection representation units and Concept Activation Vectors are measured according to their ability to detect or correlate with specific human-understandable concepts.</s></p><p><s coords="8,321.96,88.19,241.28,8.91;8,312.00,100.19,251.22,8.91;8,312.00,112.07,250.98,8.91;8,312.00,124.07,43.17,8.91">Once individual portions of a representation are characterized, they can be tested for explanatory power by evaluating whether their activations can faithfully reveal a specific bias in a network.</s><s coords="8,358.56,124.07,204.58,8.91;8,312.00,136.07,251.10,8.91;8,312.00,147.95,251.12,8.91;8,312.00,159.95,251.14,8.91;8,312.00,171.83,251.08,8.91;8,312.00,183.83,127.53,8.91">For example, Concept Activation Vectors <ref type="bibr" coords="8,530.76,124.07,16.52,8.91" target="#b53">[54]</ref> are evaluated by training several versions of the same network on datasets that are synthesized to contain two different types of signals that can be used to determine the class (the image itself, and an overlaid piece of text which gives the class name with varying reliability).</s><s coords="8,443.76,183.83,119.22,8.91;8,312.00,195.83,251.24,8.91;8,312.00,207.71,251.22,8.91;8,312.00,219.71,251.20,8.91;8,312.00,231.71,251.07,8.91;8,312.00,243.59,235.29,8.91">The faithfulness of CAVs to the network behavior can be verified by evaluating whether classifiers that are known to depend on the text (as evidenced by performance on synthesized tests) exhibit high activations of CAV vectors corresponding to the text, and that classifiers that do not depend on the text exhibits low CAV vectors.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,312.00,263.59,108.30,8.74">C. Explanation-Producing</head><p><s coords="8,321.96,279.11,241.38,8.91;8,312.00,291.11,189.21,8.91">Explanation-producing systems can be evaluated according to how well they match user expectations.</s><s coords="8,507.12,291.11,56.13,8.91;8,312.00,303.11,251.13,8.91;8,312.00,314.99,251.14,8.91;8,312.00,326.99,251.24,8.91;8,312.00,338.87,119.85,8.91">For example, network attention can be compared to human attention <ref type="bibr" coords="8,543.96,303.11,15.34,8.91" target="#b57">[58]</ref>, and disentangled representations can be tested on synthetic datasets that have known latent variables, to determine whether those variables are recovered.</s><s coords="8,434.52,338.87,128.58,8.91;8,312.00,350.87,251.14,8.91;8,312.00,362.87,230.97,8.91">Finally, systems that are trained explicitly to generate human-readable explanations can be tested by similarity to test sets, or by human evaluation.</s></p><p><s coords="8,321.96,374.99,241.16,8.91;8,312.00,386.87,251.00,8.91;8,312.00,398.87,251.12,8.91;8,312.00,410.75,251.37,8.91">One of the difficulties of evaluating explanatory power of explanation-producing systems is that, since the system itself produces the explanation, evaluations necessarily couple evaluation of the system along with evaluation of the explanation.</s><s coords="8,312.00,422.75,251.12,8.91;8,312.00,434.75,251.24,8.91;8,312.00,446.63,251.34,8.91;8,312.00,458.63,186.09,8.91">An explanation that seems unreasonable could indicate either a failure of the system to process information in a reasonable way, or it could indicate the failure of the explanation generator to create a reasonable description.</s><s coords="8,502.32,458.63,60.78,8.91;8,312.00,470.63,251.34,8.91;8,312.00,482.51,251.14,8.91;8,312.00,494.51,251.14,8.91;8,312.00,506.39,35.85,8.91">Conversely, an explanation system that is not faithful to the decisionmaking process could produce a reasonable description even if the underlying system is using unreasonable rules to make the decision.</s><s coords="8,350.64,506.39,212.60,8.91;8,312.00,518.39,174.45,8.91">An evaluation of explanations based on their reasonableness alone can miss these distinctions.</s><s coords="8,489.96,518.39,73.28,8.91;8,312.00,530.39,251.10,8.91;8,312.00,542.27,132.81,8.91">In <ref type="bibr" coords="8,501.84,518.39,15.34,8.91" target="#b73">[74]</ref>, a number of user-study designs are outlined that can help bridge the gap between the model and the user.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,394.80,562.19,85.19,8.91">VII. CONCLUSIONS</head><p><s coords="8,321.96,578.15,241.16,8.91;8,312.00,590.15,251.29,8.91;8,312.00,602.15,251.26,8.91;8,312.00,614.03,251.05,8.91;8,312.00,626.03,251.34,8.91;8,312.00,638.03,251.22,8.91;8,312.00,649.91,145.89,8.91">One common viewpoint in the deep neural network community is that the level of interpretability and theoretical understanding needed to for transparent explanations of large DNNs remains out of reach; for example, as a response to Ali Rahimi's Test of Time NIPS address, Yann LeCunn responded that "The engineering artifacts have almost always preceded the theoretical understanding" <ref type="bibr" coords="8,438.72,649.91,15.34,8.91" target="#b86">[87]</ref>.</s><s coords="8,461.64,649.91,101.49,8.91;8,312.00,661.91,251.26,8.91;8,312.00,673.79,251.08,8.91;8,312.00,685.79,251.12,8.91;8,312.00,697.79,39.69,8.91">However, we assert that, for machine learning systems to achieve wider acceptance among a skeptical populace, it is crucial that such systems be able to provide or permit satisfactory explanations of their decisions.</s><s coords="8,354.72,697.79,208.26,8.91;8,312.00,709.67,251.34,8.91;9,48.96,52.19,251.34,8.91;9,48.96,64.19,164.01,8.91">The progress made so far has been promising, with efforts in explanation of deep network processing, explanation of deep network representation, and system-level explanation production yielding encouraging results.</s></p><p><s coords="9,58.92,76.19,241.14,8.91;9,48.96,88.19,213.21,8.91">We find, though, that the various approaches taken to address different facets of explainability are siloed.</s><s coords="9,266.28,88.19,33.78,8.91;9,48.96,100.07,251.34,8.91;9,48.96,112.07,251.10,8.91;9,48.96,124.07,251.10,8.91;9,48.96,135.95,149.13,8.91">Work in the explainability space tends to advance a particular category of technique, with comparatively little attention given to approaches that merge different categories of techniques to achieve more effective explanation.</s><s coords="9,202.80,135.95,97.26,8.91;9,48.96,147.95,251.12,8.91;9,48.96,159.95,147.45,8.91">Given the purpose and type of explanation, it is not obvious what the best type of explanation metric is and should be.</s><s coords="9,199.44,159.95,100.64,8.91;9,48.96,171.83,251.31,8.91;9,48.96,183.83,109.17,8.91">We encourage the use of diverse metrics that align with the purpose and completeness of the targeted explanation.</s><s coords="9,160.32,183.83,139.98,8.91;9,48.96,195.71,251.20,8.91;9,48.96,207.71,251.34,8.91;9,48.96,219.71,251.38,8.91;9,48.96,231.59,251.13,8.91;9,48.96,243.59,251.12,8.91;9,48.96,255.59,231.33,8.91">Our view is that, as the community learns to advance its work collaboratively by combining ideas from different fields, the overall state of system explanation will improve dramatically, resulting in methods that provide behavioral extrapolation, build trust in deep learning systems, and provide usable insight into deep network operation enabling system behavior understanding and improvement.</s></p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><head coords="9,125.88,275.27,97.31,8.91">ACKNOWLEDGEMENTS</head><p>The work was partially funded by <rs type="funder">DARPA XAI</rs> program <rs type="grantNumber">FA8750-18-C0004</rs>, the <rs type="funder">National Science Foundation</rs> under Grants No. <rs type="grantNumber">1524817</rs>, the <rs type="institution">MIT-IBM Watson AI Lab</rs>, and the <rs type="funder">Toyota Research Institute (TRI)</rs>.The authors also wish to express their appreciation for <rs type="person">Jonathan Frankle</rs> for sharing his insightful feedback on earlier versions of the manuscript.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_c3gqk9n">
					<idno type="grant-number">FA8750-18-C0004</idno>
				</org>
				<org type="funding" xml:id="_9HXBRXM">
					<idno type="grant-number">1524817</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,67.20,387.49,232.74,7.13;9,67.20,396.37,287.18,7.13;9,67.20,405.37,106.87,7.13" xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine Bias *</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Mattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Kirchner</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781003278290-37</idno>
		<ptr target="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing"/>
	</analytic>
	<monogr>
		<title level="m">Ethics of Data and Analytics</title>
		<imprint>
			<publisher>Auerbach Publications</publisher>
			<date type="published" when="2016">May 24, 2019. 2016</date>
			<biblScope unit="page" from="254" to="264"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Angwin, J. Larson, S. Mattu, and L. Kirchner, "Machine bias," https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing [Accessed May 24, 2019], 2016.</note>
</biblStruct>

<biblStruct coords="9,67.20,414.37,232.74,7.13;9,67.20,423.37,232.74,7.13;9,67.20,432.37,232.71,7.13;9,67.20,441.37,106.39,7.13" xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298640</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015</date>
			<biblScope unit="page" from="427" to="436"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Nguyen, J. Yosinski, and J. Clune, "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 427-436.</note>
</biblStruct>

<biblStruct coords="9,67.20,450.37,232.51,7.13;9,67.20,459.37,232.73,7.13;9,67.20,468.25,76.87,7.13" xml:id="b2">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1312.6199</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, "Intriguing properties of neural networks," arXiv preprint arXiv:1312.6199, 2013.</note>
</biblStruct>

<biblStruct coords="9,67.20,477.37,232.57,7.13;9,67.20,486.25,232.50,7.13;9,67.20,495.25,207.31,7.13" xml:id="b3">
	<analytic>
		<title level="a" type="main">Universal Adversarial Perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.17</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
			<biblScope unit="page" from="86" to="94"/>
		</imprint>
	</monogr>
	<note type="raw_reference">S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, "Univer- sal adversarial perturbations," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 86-94.</note>
</biblStruct>

<biblStruct coords="9,67.20,504.25,232.70,7.13;9,67.20,513.25,232.82,7.13;9,67.20,522.25,232.75,7.13;9,67.20,531.13,42.43,7.13" xml:id="b4">
	<analytic>
		<title level="a" type="main">The Limitations of Deep Learning in Adversarial Settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Berkay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="DOI">10.1109/eurosp.2016.36</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE European Symposium on Security and Privacy (EuroS&amp;P)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-03">2016</date>
			<biblScope unit="page" from="372" to="387"/>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, "The limitations of deep learning in adversarial settings," in IEEE European Symposium on Security and Privacy (EuroS&amp;P), 2016, pp. 372-387.</note>
</biblStruct>

<biblStruct coords="9,67.20,540.25,232.58,7.13;9,67.20,549.13,232.63,7.13;9,67.20,558.13,134.54,7.13" xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial Examples for Evaluating Reading Comprehension Systems</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1215</idno>
		<idno>abs/1707.07328</idno>
		<ptr target="http://arxiv.org/abs/1707.07328"/>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Jia and P. Liang, "Adversarial examples for evaluating reading comprehension systems," CoRR, vol. abs/1707.07328, 2017. [Online]. Available: http://arxiv.org/abs/1707.07328</note>
</biblStruct>

<biblStruct coords="9,67.20,567.13,232.75,7.13;9,67.20,576.13,147.55,7.13" xml:id="b6">
	<analytic>
		<title level="a" type="main">Trojaning Attack on Neural Networks</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.14722/ndss.2018.23291</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2018 Network and Distributed System Security Symposium</title>
		<meeting>2018 Network and Distributed System Security Symposium</meeting>
		<imprint>
			<publisher>Internet Society</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang, "Trojaning attack on neural networks," 2017.</note>
</biblStruct>

<biblStruct coords="9,67.20,585.13,232.58,7.13;9,67.20,594.13,201.43,7.13" xml:id="b7">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1412.6572</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">I. J. Goodfellow, J. Shlens, and C. Szegedy, "Explaining and harnessing adversarial examples," arXiv preprint arXiv:1412.6572, 2014.</note>
</biblStruct>

<biblStruct coords="9,67.20,603.13,232.66,7.13;9,67.20,612.13,196.27,7.13" xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, "Towards deep learning models resistant to adversarial attacks," 2018.</note>
</biblStruct>

<biblStruct coords="9,67.20,621.13,232.81,7.13;9,67.20,630.13,136.75,7.13" xml:id="b9">
	<analytic>
		<title level="a" type="main">MAS 2017 die rzteschaft macht mit</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Kraft</surname></persName>
		</author>
		<idno type="DOI">10.4414/saez.2019.17995</idno>
	</analytic>
	<monogr>
		<title level="j">Schweizerische Ärztezeitung</title>
		<title level="j" type="abbrev">Schweiz Ärzteztg</title>
		<idno type="ISSN">0036-7486</idno>
		<idno type="ISSNe">1424-4004</idno>
		<imprint>
			<date type="published" when="2018-02-77">77 Massachusetts Ave., 2 2018</date>
			<publisher>EMH Swiss Medical Publishers, Ltd.</publisher>
		</imprint>
		<respStmt>
			<orgName>MIT</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note>an optional note</note>
	<note type="raw_reference">A. D. Kraft, "Vision by alignment," Ph.D. dissertation, MIT, 77 Mas- sachusetts Ave., 2 2018, an optional note.</note>
</biblStruct>

<biblStruct coords="9,67.20,639.13,232.38,7.13;9,67.20,648.13,232.84,7.13;9,67.20,657.13,213.55,7.13" xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial Examples Are Not Easily Detected</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3128572.3140444</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-11-03">2017</date>
			<biblScope unit="page" from="3" to="14"/>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Carlini and D. Wagner, "Adversarial examples are not easily detected: Bypassing ten detection methods," in Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, 2017, pp. 3-14.</note>
</biblStruct>

<biblStruct coords="9,67.20,666.13,232.69,7.13;9,67.20,675.13,232.73,7.13;9,67.20,684.13,80.83,7.13" xml:id="b11">
	<analytic>
		<title level="a" type="main">European Union Regulations on Algorithmic Decision Making and a “Right to Explanation”</title>
		<author>
			<persName><forename type="first">Bryce</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v38i3.2741</idno>
		<idno type="arXiv">arXiv:1606.08813</idno>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<title level="j" type="abbrev">AI Magazine</title>
		<idno type="ISSN">0738-4602</idno>
		<idno type="ISSNe">2371-9621</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="57"/>
			<date type="published" when="2016">2016</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">B. Goodman and S. Flaxman, "European union regulations on algo- rithmic decision-making and a" right to explanation"," arXiv preprint arXiv:1606.08813, 2016.</note>
</biblStruct>

<biblStruct coords="9,67.20,693.13,232.54,7.13;9,67.20,702.13,232.51,7.13;9,67.20,711.01,200.23,7.13" xml:id="b12">
	<analytic>
		<title level="a" type="main">CRITICAL QUESTIONS FOR BIG DATA</title>
		<author>
			<persName><forename type="first">Danah</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<idno type="DOI">10.1080/1369118x.2012.678878</idno>
	</analytic>
	<monogr>
		<title level="j">Information, Communication &amp; Society</title>
		<title level="j" type="abbrev">Information, Communication &amp; Society</title>
		<idno type="ISSN">1369-118X</idno>
		<idno type="ISSNe">1468-4462</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="662" to="679"/>
			<date type="published" when="2012-06">2012</date>
			<publisher>Informa UK Limited</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Boyd and K. Crawford, "Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon," Information, communication &amp; society, vol. 15, no. 5, pp. 662-679, 2012.</note>
</biblStruct>

<biblStruct coords="9,330.24,53.53,232.69,7.13;9,330.24,62.53,232.51,7.13;9,330.24,71.41,128.23,7.13" xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
			<idno type="ORCID">0000-0001-7154-8629</idno>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
			<idno type="ORCID">0000-0003-0021-0801</idno>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
			<idno type="ORCID">0000-0001-7176-4479</idno>
		</author>
		<idno type="DOI">10.1126/science.aal4230</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<title level="j" type="abbrev">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<idno type="ISSNe">1095-9203</idno>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186"/>
			<date type="published" when="2017-04-14">2017</date>
			<publisher>American Association for the Advancement of Science (AAAS)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Caliskan, J. J. Bryson, and A. Narayanan, "Semantics derived au- tomatically from language corpora contain human-like biases," Science, vol. 356, no. 6334, pp. 183-186, 2017.</note>
</biblStruct>

<biblStruct coords="9,330.24,80.53,232.74,7.13;9,330.24,89.53,232.75,7.13;9,330.24,98.53,50.47,7.13" xml:id="b14">
	<analytic>
		<title level="a" type="main">Research Priorities for Robust and Beneficial Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v36i4.2577</idno>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<title level="j" type="abbrev">AI Magazine</title>
		<idno type="ISSN">0738-4602</idno>
		<idno type="ISSNe">2371-9621</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="105" to="114"/>
			<date type="published" when="2015-12">2015</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Russell, D. Dewey, and M. Tegmark, "Research priorities for robust and beneficial artificial intelligence," Ai Magazine, vol. 36, no. 4, pp. 105-114, 2015.</note>
</biblStruct>

<biblStruct coords="9,330.24,107.53,232.51,7.13;9,330.24,116.53,232.75,7.13;9,330.24,125.53,17.83,7.13" xml:id="b15">
	<monogr>
		<title level="m" type="main">On what we know we don't know: Explanation, theory, linguistics, and how questions shape them</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Bromberger, On what we know we don't know: Explanation, theory, linguistics, and how questions shape them. University of Chicago Press, 1992.</note>
</biblStruct>

<biblStruct coords="9,330.24,134.53,232.74,7.13;9,330.24,143.53,179.35,7.13" xml:id="b16">
	<analytic>
		<title level="a" type="main">The Best Explanation: Criteria for Theory Choice</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Thagard</surname></persName>
		</author>
		<idno type="DOI">10.2307/2025686</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Philosophy</title>
		<title level="j" type="abbrev">The Journal of Philosophy</title>
		<idno type="ISSN">0022-362X</idno>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">76</biblScope>
			<date type="published" when="1978-02">1978</date>
			<publisher>Philosophy Documentation Center</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">P. R. Thagard, "The best explanation: Criteria for theory choice," The journal of philosophy, vol. 75, no. 2, pp. 76-92, 1978.</note>
</biblStruct>

<biblStruct coords="9,330.24,152.65,232.74,7.13;9,330.24,161.65,184.51,7.13" xml:id="b17">
	<monogr>
		<title level="m" type="main">The promise and peril of human evaluation for model interpretability</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Herman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07414</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">B. Herman, "The promise and peril of human evaluation for model interpretability," arXiv preprint arXiv:1711.07414, 2017.</note>
</biblStruct>

<biblStruct coords="9,330.24,170.65,232.74,7.13;9,330.24,179.65,232.58,7.13;9,330.24,188.65,144.91,7.13" xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="770" to="778"/>
		</imprint>
	</monogr>
	<note type="raw_reference">K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.</note>
</biblStruct>

<biblStruct coords="9,330.24,197.65,232.61,7.13;9,330.24,206.65,232.70,7.13;9,330.24,215.72,232.71,6.99;9,330.24,224.65,145.39,7.13" xml:id="b19">
	<analytic>
		<title level="a" type="main">"Why Should I Trust You?"</title>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">Tulio</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13">2016</date>
			<biblScope unit="page" from="1135" to="1144"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. T. Ribeiro, S. Singh, and C. Guestrin, "Why should i trust you?: Explaining the predictions of any classifier," in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016, pp. 1135-1144.</note>
</biblStruct>

<biblStruct coords="9,330.24,233.65,232.35,7.13;9,330.24,242.65,232.50,7.13;9,330.24,251.65,129.43,7.13" xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepRED – Rule Extraction from Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Ruben</forename><surname>Zilke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Janssen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46307-0_29</idno>
	</analytic>
	<monogr>
		<title level="m">Discovery Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="457" to="473"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. R. Zilke, E. L. Mencía, and F. Janssen, "Deepred-rule extraction from deep neural networks," in International Conference on Discovery Science. Springer, 2016, pp. 457-473.</note>
</biblStruct>

<biblStruct coords="9,330.24,260.65,232.54,7.13;9,330.24,269.65,232.51,7.13;9,330.24,278.65,232.75,7.13;9,330.24,287.65,37.63,7.13" xml:id="b21">
	<analytic>
		<title level="a" type="main">Rule extraction from neural networks via decision tree induction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tsukimoto</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.2001.938448</idno>
	</analytic>
	<monogr>
		<title level="m">IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1870" to="1875"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Sato and H. Tsukimoto, "Rule extraction from neural networks via decision tree induction," in Neural Networks, 2001. Proceedings. IJCNN'01. International Joint Conference on, vol. 3. IEEE, 2001, pp. 1870-1875.</note>
</biblStruct>

<biblStruct coords="9,330.24,296.65,232.74,7.13;9,330.24,305.65,232.62,7.13;9,330.24,314.65,173.95,7.13" xml:id="b22">
	<analytic>
		<title level="a" type="main">Reverse Engineering the Neural Networks for Rule Extraction in Classification Problems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Gethsiyal</forename><surname>Augasta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kathirvalavakumar</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11063-011-9207-8</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<title level="j" type="abbrev">Neural Process Lett</title>
		<idno type="ISSN">1370-4621</idno>
		<idno type="ISSNe">1573-773X</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="150"/>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. G. Augasta and T. Kathirvalavakumar, "Reverse engineering the neural networks for rule extraction in classification problems," Neural processing letters, vol. 35, no. 2, pp. 131-150, 2012.</note>
</biblStruct>

<biblStruct coords="9,330.24,323.65,232.51,7.13;9,330.24,332.65,232.75,7.13;9,330.24,341.65,84.79,7.13" xml:id="b23">
	<analytic>
		<title level="a" type="main">C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Salzberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00993309</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<title level="j" type="abbrev">Mach Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<idno type="ISSNe">1573-0565</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="240"/>
			<date type="published" when="1993">1993. 1994</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. L. Salzberg, "C4. 5: Programs for machine learning by j. ross quinlan. morgan kaufmann publishers, inc., 1993," Machine Learning, vol. 16, no. 3, pp. 235-240, 1994.</note>
</biblStruct>

<biblStruct coords="9,330.24,350.77,232.52,7.13;9,330.24,359.65,232.87,7.13;9,330.24,368.65,232.75,7.13" xml:id="b24">
	<analytic>
		<title level="a" type="main">ANN-DT: an algorithm for extraction of decision trees from artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">P J</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Aldrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">S</forename><surname>Gouws</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.809084</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<title level="j" type="abbrev">IEEE Trans. Neural Netw.</title>
		<idno type="ISSN">1045-9227</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1392" to="1401"/>
			<date type="published" when="1999">1999</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">G. P. Schmitz, C. Aldrich, and F. S. Gouws, "Ann-dt: an algorithm for extraction of decision trees from artificial neural networks," IEEE Transactions on Neural Networks, vol. 10, no. 6, pp. 1392-1401, 1999.</note>
</biblStruct>

<biblStruct coords="9,330.24,377.77,232.82,7.13;9,330.24,386.77,232.62,7.13;9,330.24,395.65,197.35,7.13" xml:id="b25">
	<analytic>
		<title level="a" type="main">Survey and critique of techniques for extracting rules from trained artificial neural networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">B</forename><surname>Tickle</surname></persName>
		</author>
		<idno type="DOI">10.1016/0950-7051(96)81920-4</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<title level="j" type="abbrev">Knowledge-Based Systems</title>
		<idno type="ISSN">0950-7051</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="373" to="389"/>
			<date type="published" when="1995-12">1995</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Andrews, J. Diederich, and A. B. Tickle, "Survey and critique of techniques for extracting rules from trained artificial neural networks," Knowledge-based systems, vol. 8, no. 6, pp. 373-389, 1995.</note>
</biblStruct>

<biblStruct coords="9,330.24,404.77,232.54,7.13;9,330.24,413.77,155.11,7.13" xml:id="b26">
	<analytic>
		<title level="a" type="main">Extracting Rules from Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Zilke</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/4931.003.0022</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Network Learning and Expert Systems</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Technische Universitat Darmstadt</orgName>
		</respStmt>
	</monogr>
	<note>Master's thesis</note>
	<note type="raw_reference">J. R. Zilke, "Extracting Rules from Deep Neural Networks," Master's thesis, Technische Universitat Darmstadt, 2016.</note>
</biblStruct>

<biblStruct coords="9,330.24,422.77,232.82,7.13;9,330.24,431.77,227.35,7.13" xml:id="b27">
	<analytic>
		<title level="a" type="main">Rule generation from neural networks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1114" to="1124"/>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Fu, "Rule generation from neural networks," IEEE Transactions on Systems, Man, and Cybernetics, vol. 24, no. 8, pp. 1114-1124, 1994.</note>
</biblStruct>

<biblStruct coords="9,330.24,440.89,232.87,7.13;9,330.24,449.89,226.63,7.13" xml:id="b28">
	<analytic>
		<title level="a" type="main">Extracting rules from trained neural networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tsukimoto</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.839008</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<title level="j" type="abbrev">IEEE Trans. Neural Netw.</title>
		<idno type="ISSN">1045-9227</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="377" to="389"/>
			<date type="published" when="2000-03">2000</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Tsukimoto, "Extracting rules from trained neural networks," IEEE Transactions on Neural Networks, vol. 11, no. 2, pp. 377-389, 2000.</note>
</biblStruct>

<biblStruct coords="9,330.24,458.89,232.62,7.13;9,330.24,467.89,232.75,7.13;9,330.24,476.89,92.71,7.13" xml:id="b29">
	<analytic>
		<title level="a" type="main">Are artificial neural networks black boxes?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benitez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Requena</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.623216</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<title level="j" type="abbrev">IEEE Trans. Neural Netw.</title>
		<idno type="ISSN">1045-9227</idno>
		<idno type="ISSNe">1941-0093</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1156" to="1164"/>
			<date type="published" when="1997-09">1997</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. M. Benítez, J. L. Castro, and I. Requena, "Are artificial neural networks black boxes?" IEEE Transactions on neural networks, vol. 8, no. 5, pp. 1156-1164, 1997.</note>
</biblStruct>

<biblStruct coords="9,330.24,485.89,232.69,7.13;9,330.24,494.89,232.58,7.13;9,330.24,503.89,92.35,7.13" xml:id="b30">
	<analytic>
		<title level="a" type="main">Extracting rules from artificial neural networks with distributed representations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="505" to="512"/>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Thrun, "Extracting rules from artificial neural networks with dis- tributed representations," in Advances in neural information processing systems, 1995, pp. 505-512.</note>
</biblStruct>

<biblStruct coords="9,330.24,513.01,232.47,7.13;9,330.24,521.89,232.59,7.13;9,330.24,530.89,232.75,7.13;9,330.24,539.89,17.83,7.13" xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatically balancing accuracy and comprehensibility in predictive modeling</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Konig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
		<idno type="DOI">10.1109/icif.2005.1592040</idno>
	</analytic>
	<monogr>
		<title level="m">2005 7th International Conference on Information Fusion</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">U. Johansson, R. Konig, and L. Niklasson, "Automatically balancing accuracy and comprehensibility in predictive modeling," in Information Fusion, 2005 8th International Conference on, vol. 2. IEEE, 2005, pp. 7-pp.</note>
</biblStruct>

<biblStruct coords="9,330.24,548.89,232.62,7.13;9,330.24,557.89,232.75,7.13" xml:id="b32">
	<monogr>
		<title level="m" type="main">Extracting comprehensible models from trained neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Craven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note type="raw_reference">M. W. Craven, "Extracting comprehensible models from trained neural networks," Ph.D. dissertation, University of Wisconsin, Madison, 1996.</note>
</biblStruct>

<biblStruct coords="9,330.24,567.01,232.62,7.13;9,330.24,576.01,232.39,7.13;9,330.24,584.89,112.39,7.13" xml:id="b33">
	<analytic>
		<title level="a" type="main">Symbolic interpretation of artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="DOI">10.1109/69.774103</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<title level="j" type="abbrev">IEEE Trans. Knowl. Data Eng.</title>
		<idno type="ISSN">1041-4347</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="463"/>
			<date type="published" when="1999">1999</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">I. A. Taha and J. Ghosh, "Symbolic interpretation of artificial neural networks," IEEE Transactions on knowledge and data engineering, vol. 11, no. 3, pp. 448-463, 1999.</note>
</biblStruct>

<biblStruct coords="9,330.24,594.01,232.79,7.13;9,330.24,603.01,158.47,7.13" xml:id="b34">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hailesilassie</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1610.05267</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">T. Hailesilassie, "Rule extraction algorithm for deep neural networks: A review," arXiv preprint arXiv:1610.05267, 2016.</note>
</biblStruct>

<biblStruct coords="9,330.24,612.01,232.76,7.13;9,330.23,621.01,232.75,7.13;9,330.23,630.01,59.23,7.13" xml:id="b35">
	<analytic>
		<title level="a" type="main">Extracting refined rules from knowledge-based neural networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">G</forename><surname>Towell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00993103</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<title level="j" type="abbrev">Mach Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<idno type="ISSNe">1573-0565</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="101"/>
			<date type="published" when="1993-10">1993</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">G. G. Towell and J. W. Shavlik, "Extracting refined rules from knowledge-based neural networks," Machine learning, vol. 13, no. 1, pp. 71-101, 1993.</note>
</biblStruct>

<biblStruct coords="9,330.23,639.13,232.81,7.13;9,330.24,648.01,232.75,7.13;9,330.24,657.01,42.55,7.13" xml:id="b36">
	<analytic>
		<title level="a" type="main">Fernn: An algorithm for fast extraction of rules from neural networks</title>
		<author>
			<persName><forename type="first">Rudy</forename><surname>Setiono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><forename type="middle">Kheng</forename><surname>Leow</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1008307919726</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<idno type="ISSN">0924-669X</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="15" to="25"/>
			<date type="published" when="2000">2000</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Setiono and W. K. Leow, "Fernn: An algorithm for fast extraction of rules from neural networks," Applied Intelligence, vol. 12, no. 1-2, pp. 15-25, 2000.</note>
</biblStruct>

<biblStruct coords="9,330.24,666.13,232.57,7.13;9,330.24,675.01,232.51,7.13;9,330.24,684.01,63.19,7.13" xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_53</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2014</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. D. Zeiler and R. Fergus, "Visualizing and understanding convolu- tional networks," in European conference on computer vision. Springer, 2014, pp. 818-833.</note>
</biblStruct>

<biblStruct coords="9,330.24,693.13,232.38,7.13;9,330.24,702.13,232.74,7.13;9,330.24,711.01,125.95,7.13" xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">K. Simonyan, A. Vedaldi, and A. Zisserman, "Deep inside convolutional networks: Visualising image classification models and saliency maps," arXiv preprint arXiv:1312.6034, 2013.</note>
</biblStruct>

<biblStruct coords="10,67.20,53.53,232.70,7.13;10,67.20,62.53,232.57,7.13;10,67.20,71.41,232.75,7.13;10,67.20,80.41,54.07,7.13" xml:id="b39">
	<analytic>
		<title level="a" type="main">On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0130140</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<title level="j" type="abbrev">PLoS ONE</title>
		<idno type="ISSNe">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e0130140</biblScope>
			<date type="published" when="2015-07-10">2015</date>
			<publisher>Public Library of Science (PLoS)</publisher>
		</imprint>
	</monogr>
	<note>p. e0130140</note>
	<note type="raw_reference">S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek, "On pixel-wise explanations for non-linear classifier deci- sions by layer-wise relevance propagation," PloS one, vol. 10, no. 7, p. e0130140, 2015.</note>
</biblStruct>

<biblStruct coords="10,67.20,90.01,232.62,7.13;10,67.20,98.89,232.73,7.13;10,67.20,107.89,80.83,7.13" xml:id="b40">
	<monogr>
		<title level="m" type="main">Reverse-complement parameter sharing improves deep learning models for genomics</title>
		<author>
			<persName><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="DOI">10.1101/103663</idno>
		<idno type="arXiv">arXiv:1704.02685</idno>
		<imprint>
			<date type="published" when="2017-01-27">2017</date>
			<publisher>Cold Spring Harbor Laboratory</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Shrikumar, P. Greenside, and A. Kundaje, "Learning important features through propagating activation differences," arXiv preprint arXiv:1704.02685, 2017.</note>
</biblStruct>

<biblStruct coords="10,67.20,117.49,232.59,7.13;10,67.20,126.49,232.82,7.13;10,67.20,135.37,232.75,7.13;10,67.20,144.37,50.35,7.13" xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.319</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">2016</date>
			<biblScope unit="page" from="2921" to="2929"/>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, "Learning deep features for discriminative localization," in Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on. IEEE, 2016, pp. 2921-2929.</note>
</biblStruct>

<biblStruct coords="10,67.20,153.97,232.70,7.13;10,67.20,162.97,232.74,7.13;10,67.20,171.85,232.75,7.13;10,67.20,180.85,62.95,7.13" xml:id="b42">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</title>
		<author>
			<persName><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.74</idno>
		<ptr target="https://arxiv.org/abs/1610"/>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">02391 v3. 2016</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, "Grad-cam: Visual explanations from deep networks via gradient-based localization," See https://arxiv. org/abs/1610.02391 v3, vol. 7, no. 8, 2016.</note>
</biblStruct>

<biblStruct coords="10,67.20,190.45,232.70,7.13;10,67.20,199.33,166.87,7.13" xml:id="b43">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1703.01365</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">M. Sundararajan, A. Taly, and Q. Yan, "Axiomatic attribution for deep networks," arXiv preprint arXiv:1703.01365, 2017.</note>
</biblStruct>

<biblStruct coords="10,67.20,208.93,232.70,7.13;10,67.20,217.93,232.58,7.13;10,67.20,226.93,232.37,7.13;10,67.20,235.81,99.38,7.13" xml:id="b44">
	<analytic>
		<title level="a" type="main">Signal-to-Noise Ratio (SNR)</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-10-6946-8_300721</idno>
		<idno>abs/1706.03825</idno>
		<ptr target="http://arxiv.org/abs/1706.03825"/>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Ocean Engineering</title>
		<imprint>
			<publisher>Springer Nature Singapore</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1706" to="1706"/>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Smilkov, N. Thorat, B. Kim, F. B. Viégas, and M. Wattenberg, "Smoothgrad: removing noise by adding noise," CoRR, vol. abs/1706.03825, 2017. [Online]. Available: http://arxiv.org/abs/1706.03825</note>
</biblStruct>

<biblStruct coords="10,67.20,243.61,232.79,8.93;10,67.20,254.41,232.62,7.13;10,67.20,263.29,232.38,7.13;10,67.20,272.29,99.38,7.13" xml:id="b45">
	<analytic>
		<title level="a" type="main">Gradient-Based Attribution Methods</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enea</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-6_9</idno>
		<idno>abs/1711.06104</idno>
		<ptr target="http://arxiv.org/abs/1711.06104"/>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="169" to="191"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Ancona, E. Ceolini, A. C. Öztireli, and M. H. Gross, "A unified view of gradient-based attribution methods for deep neural networks," CoRR, vol. abs/1711.06104, 2017. [Online]. Available: http://arxiv.org/abs/1711.06104</note>
</biblStruct>

<biblStruct coords="10,67.20,281.89,232.54,7.13;10,67.20,290.89,232.66,7.13;10,67.20,299.84,232.69,6.99;10,67.20,308.77,129.19,7.13" xml:id="b46">
	<analytic>
		<title level="a" type="main">CNN Features Off-the-Shelf: An Astounding Baseline for Recognition</title>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Sharif</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw.2014.131</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-06">2014. 2014</date>
			<biblScope unit="page" from="512" to="519"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, "Cnn features off-the-shelf: an astounding baseline for recognition," in Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Con- ference on. IEEE, 2014, pp. 512-519.</note>
</biblStruct>

<biblStruct coords="10,67.20,318.37,232.74,7.13;10,67.20,327.25,232.46,7.13;10,67.20,336.25,137.23,7.13" xml:id="b47">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, "How transferable are features in deep neural networks?" in Advances in neural information processing systems, 2014, pp. 3320-3328.</note>
</biblStruct>

<biblStruct coords="10,67.20,345.85,232.61,7.13;10,67.20,354.85,232.63,7.13;10,67.20,363.73,17.83,7.13" xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.319</idno>
		<idno type="arXiv">arXiv:1412.6856</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, "Object detectors emerge in deep scene cnns," arXiv preprint arXiv:1412.6856, 2014.</note>
</biblStruct>

<biblStruct coords="10,67.20,373.33,232.69,7.13;10,67.20,382.33,232.70,7.13;10,67.20,391.21,232.70,7.13;10,67.20,400.21,101.11,7.13" xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding Neural Networks via Feature Visualization: A Survey</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-6_4</idno>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="76"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune, "Syn- thesizing the preferred inputs for neurons in neural networks via deep generator networks," in Advances in Neural Information Processing Systems, 2016, pp. 3387-3395.</note>
</biblStruct>

<biblStruct coords="10,67.20,409.81,232.58,7.13;10,67.20,418.81,232.38,7.13;10,67.20,427.69,168.07,7.13" xml:id="b50">
	<analytic>
		<title level="a" type="main">Network Dissection: Quantifying Interpretability of Deep Visual Representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.354</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, "Network dissection: Quantifying interpretability of deep visual representations," in Computer Vision and Pattern Recognition, 2017.</note>
</biblStruct>

<biblStruct coords="10,67.20,437.29,232.86,7.13;10,67.20,446.29,232.51,7.13;10,67.20,455.17,104.35,7.13" xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual interpretability for deep learning: a survey</title>
		<author>
			<persName><forename type="first">Quan-Shi</forename><surname>Zhang</surname></persName>
			<idno type="ORCID">0000-0002-6108-2738</idno>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1631/fitee.1700808</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<title level="j" type="abbrev">Frontiers Inf Technol Electronic Eng</title>
		<idno type="ISSN">2095-9184</idno>
		<idno type="ISSNe">2095-9230</idno>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="39"/>
			<date type="published" when="2018-01">2018</date>
			<publisher>Zhejiang University Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Q.-s. Zhang and S.-C. Zhu, "Visual interpretability for deep learning: a survey," Frontiers of Information Technology &amp; Electronic Engineering, vol. 19, no. 1, pp. 27-39, 2018.</note>
</biblStruct>

<biblStruct coords="10,67.20,464.77,232.70,7.13;10,67.20,473.77,232.63,7.13;10,67.20,482.77,134.54,7.13" xml:id="b52">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Training pruned neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno>abs/1803.03635</idno>
		<ptr target="http://arxiv.org/abs/1803.03635"/>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Frankle and M. Carbin, "The lottery ticket hypothesis: Training pruned neural networks," CoRR, vol. abs/1803.03635, 2018. [Online]. Available: http://arxiv.org/abs/1803.03635</note>
</biblStruct>

<biblStruct coords="10,67.20,492.25,232.51,7.13;10,67.20,501.25,232.34,7.13;10,67.20,510.25,160.63,7.13" xml:id="b53">
	<monogr>
		<title level="m" type="main">Tcav: Relative concept importance testing with linear concept activation vectors</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11279</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">B. Kim, J. Gilmer, F. Viegas, U. Erlingsson, and M. Wattenberg, "Tcav: Relative concept importance testing with linear concept activation vectors," arXiv preprint arXiv:1711.11279, 2017.</note>
</biblStruct>

<biblStruct coords="10,67.20,519.73,232.75,7.13;10,67.20,528.73,232.66,7.13;10,67.20,537.73,213.67,7.13" xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in Advances in Neural Information Processing Systems, 2017, pp. 6000-6010.</note>
</biblStruct>

<biblStruct coords="10,67.20,547.21,232.74,7.13;10,67.20,556.21,232.61,7.13;10,67.20,565.21,232.82,7.13;10,67.20,574.21,232.75,7.13;10,67.20,583.09,42.43,7.13" xml:id="b55">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="842" to="850"/>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, and Z. Zhang, "The application of two-level attention models in deep convolutional neural network for fine-grained image classification," in Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. IEEE, 2015, pp. 842-850.</note>
</biblStruct>

<biblStruct coords="10,67.20,592.69,232.50,7.13;10,67.20,601.69,232.74,7.13;10,67.20,610.69,171.91,7.13" xml:id="b56">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="297"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Lu, J. Yang, D. Batra, and D. Parikh, "Hierarchical question-image co-attention for visual question answering," in Advances In Neural Information Processing Systems, 2016, pp. 289-297.</note>
</biblStruct>

<biblStruct coords="10,67.20,620.17,232.70,7.13;10,67.20,629.17,232.54,7.13;10,67.20,638.17,232.51,7.13;10,67.20,647.05,90.79,7.13" xml:id="b57">
	<analytic>
		<title level="a" type="main">Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2017.10.001</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<title level="j" type="abbrev">Computer Vision and Image Understanding</title>
		<idno type="ISSN">1077-3142</idno>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="90" to="100"/>
			<date type="published" when="2017-10">2017</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, "Human attention in visual question answering: Do humans and deep networks look at the same regions?" Computer Vision and Image Understanding, vol. 163, pp. 90-100, 2017.</note>
</biblStruct>

<biblStruct coords="10,67.20,656.65,232.51,7.13;10,67.20,665.65,232.58,7.13;10,67.20,674.65,232.51,7.13;10,67.20,683.53,187.10,7.13" xml:id="b58">
	<analytic>
		<title level="a" type="main">Multimodal Explanations: Justifying Decisions and Pointing to the Evidence</title>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00915</idno>
		<idno>abs/1802.08129</idno>
		<ptr target="http://arxiv.org/abs/1802.08129"/>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. H. Park, L. A. Hendricks, Z. Akata, A. Rohrbach, B. Schiele, T. Darrell, and M. Rohrbach, "Multimodal explanations: Justifying decisions and pointing to the evidence," CoRR, vol. abs/1802.08129, 2018. [Online]. Available: http://arxiv.org/abs/1802.08129</note>
</biblStruct>

<biblStruct coords="10,67.20,693.13,232.69,7.13;10,67.20,702.13,232.38,7.13;10,67.20,711.01,130.03,7.13" xml:id="b59">
	<analytic>
		<title level="a" type="main">Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Slavin</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/371</idno>
		<idno type="arXiv">arXiv:1703.03717</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2017-08">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. S. Ross, M. C. Hughes, and F. Doshi-Velez, "Right for the right rea- sons: Training differentiable models by constraining their explanations," arXiv preprint arXiv:1703.03717, 2017.</note>
</biblStruct>

<biblStruct coords="10,330.24,53.53,232.71,7.13;10,330.24,62.53,201.79,7.13" xml:id="b60">
	<analytic>
		<title level="a" type="main">Principal Component Analysis and Factor Analysis</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4757-1904-8_7</idno>
	</analytic>
	<monogr>
		<title level="m">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer New York</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="115" to="128"/>
		</imprint>
	</monogr>
	<note type="raw_reference">I. T. Jolliffe, "Principal component analysis and factor analysis," in Principal component analysis. Springer, 1986, pp. 115-128.</note>
</biblStruct>

<biblStruct coords="10,330.24,71.41,232.54,7.13;10,330.24,80.41,232.75,7.13" xml:id="b61">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0893-6080(00)00026-5</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<title level="j" type="abbrev">Neural Networks</title>
		<idno type="ISSN">0893-6080</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430"/>
			<date type="published" when="2000-06">2000</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Hyvärinen and E. Oja, "Independent component analysis: algorithms and applications," Neural networks, vol. 13, no. 4-5, pp. 411-430, 2000.</note>
</biblStruct>

<biblStruct coords="10,330.24,89.41,232.87,7.13;10,330.24,98.41,232.38,7.13;10,330.24,107.29,232.75,7.13;10,330.24,116.29,84.79,7.13" xml:id="b62">
	<analytic>
		<title level="a" type="main">Algorithms and applications for approximate nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">N</forename><surname>Langville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">Paul</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Plemmons</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csda.2006.11.006</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<title level="j" type="abbrev">Computational Statistics &amp; Data Analysis</title>
		<idno type="ISSN">0167-9473</idno>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="173"/>
			<date type="published" when="2007-09">2007</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. W. Berry, M. Browne, A. N. Langville, V. P. Pauca, and R. J. Plemmons, "Algorithms and applications for approximate nonnegative matrix factorization," Computational statistics &amp; data analysis, vol. 52, no. 1, pp. 155-173, 2007.</note>
</biblStruct>

<biblStruct coords="10,330.24,125.29,232.74,7.13;10,330.24,134.17,105.43,7.13" xml:id="b63">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">D. P. Kingma and M. Welling, "Auto-encoding variational bayes," arXiv preprint arXiv:1312.6114, 2013.</note>
</biblStruct>

<biblStruct coords="10,330.24,143.17,232.39,7.13;10,330.24,152.17,232.54,7.13;10,330.24,161.17,159.79,7.13" xml:id="b64">
	<monogr>
		<title level="m" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, "beta-vae: Learning basic visual concepts with a constrained variational framework," 2016.</note>
</biblStruct>

<biblStruct coords="10,330.24,170.05,232.70,7.13;10,330.24,179.05,232.69,7.13;10,330.24,188.05,232.62,7.13;10,330.24,196.93,179.83,7.13" xml:id="b65">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180"/>
		</imprint>
	</monogr>
	<note type="raw_reference">X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, "Infogan: Interpretable representation learning by informa- tion maximizing generative adversarial nets," in Advances in Neural Information Processing Systems, 2016, pp. 2172-2180.</note>
</biblStruct>

<biblStruct coords="10,330.24,205.93,232.61,7.13;10,330.24,214.93,54.67,7.13" xml:id="b66">
	<analytic>
		<title level="a" type="main">Interpretable Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00920</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Q. Zhang, Y. N. Wu, and S.-C. Zhu, "Interpretable convolutional neural networks," 2018.</note>
</biblStruct>

<biblStruct coords="10,330.24,223.93,232.26,7.13;10,330.24,232.81,180.55,7.13" xml:id="b67">
	<analytic>
		<title level="a" type="main">Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning</title>
		<author>
			<persName><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v31i1.10924</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017-02-13">2017</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Q. Zhang, R. Cao, Y. N. Wu, and S.-C. Zhu, "Growing interpretable part graphs on convnets via multi-shot learning." 2017.</note>
</biblStruct>

<biblStruct coords="10,330.24,241.81,232.47,7.13;10,330.24,250.81,232.74,7.13;10,330.24,259.69,80.83,7.13" xml:id="b68">
	<monogr>
		<title level="m" type="main">Unsupervised learning of neural networks to explain neural networks</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07468</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Q. Zhang, Y. Yang, Y. Liu, Y. N. Wu, and S.-C. Zhu, "Unsupervised learning of neural networks to explain neural networks," arXiv preprint arXiv:1805.07468, 2018.</note>
</biblStruct>

<biblStruct coords="10,330.24,268.69,232.59,7.13;10,330.24,277.69,232.75,7.13;10,330.24,286.69,50.35,7.13" xml:id="b69">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3859" to="3869"/>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Sabour, N. Frosst, and G. E. Hinton, "Dynamic routing between capsules," in Advances in Neural Information Processing Systems, 2017, pp. 3859-3869.</note>
</biblStruct>

<biblStruct coords="10,330.24,295.57,232.51,7.13;10,330.24,304.57,232.74,7.13;10,330.24,313.57,232.71,7.13;10,330.24,322.57,17.83,7.13" xml:id="b70">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">2015</date>
			<biblScope unit="page" from="2425" to="2433"/>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh, "Vqa: Visual question answering," in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2425- 2433.</note>
</biblStruct>

<biblStruct coords="10,330.24,331.45,232.70,7.13;10,330.24,340.45,232.50,7.13;10,330.24,349.45,158.35,7.13" xml:id="b71">
	<analytic>
		<title level="a" type="main">Generating Visual Explanations</title>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_1</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="19"/>
		</imprint>
	</monogr>
	<note type="raw_reference">L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, and T. Darrell, "Generating visual explanations," in European Conference on Computer Vision. Springer, 2016, pp. 3-19.</note>
</biblStruct>

<biblStruct coords="10,330.24,358.33,232.70,7.13;10,330.24,367.33,232.58,7.13;10,330.24,376.33,232.51,7.13;10,330.24,385.33,17.83,7.13" xml:id="b72">
	<analytic>
		<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1044</idno>
		<idno type="arXiv">arXiv:1606.01847</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach, "Multimodal compact bilinear pooling for visual question answering and visual grounding," arXiv preprint arXiv:1606.01847, 2016.</note>
</biblStruct>

<biblStruct coords="10,330.24,394.21,232.81,7.13;10,330.24,403.21,232.38,7.13;10,330.24,412.21,102.50,7.13" xml:id="b73">
	<analytic>
		<title level="a" type="main">Considerations for Evaluation and Generalization in Interpretable Machine Learning</title>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-98131-4_1</idno>
		<ptr target="https://arxiv.org/abs/1702.08608"/>
	</analytic>
	<monogr>
		<title level="m">The Springer Series on Challenges in Machine Learning</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="17"/>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Doshi-Velez and B. Kim, "Towards a rigorous science of interpretable machine learning," arXiv, 2017. [Online]. Available: https://arxiv.org/abs/1702.08608</note>
</biblStruct>

<biblStruct coords="10,330.24,421.09,232.39,7.13;10,330.24,430.09,232.26,7.13;10,330.24,439.09,232.81,7.13;10,330.24,448.09,232.75,7.13;10,330.24,456.97,22.63,7.13" xml:id="b74">
	<analytic>
		<title level="a" type="main">Trends and Trajectories for Explainable, Accountable and Intelligible Systems</title>
		<author>
			<persName><forename type="first">Ashraf</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><surname>Vermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174156</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-21">2018</date>
			<biblScope unit="page">582</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Abdul, J. Vermeulen, D. Wang, B. Y. Lim, and M. Kankanhalli, "Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda," in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 2018, p. 582.</note>
</biblStruct>

<biblStruct coords="10,330.24,465.97,232.79,7.13;10,330.24,474.97,232.74,7.13;10,330.24,483.85,80.83,7.13" xml:id="b75">
	<analytic>
		<title level="a" type="main">A Survey of Methods for Explaining Black Box Models</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
			<idno type="ORCID">0000-0002-2827-7613</idno>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3236009</idno>
		<idno type="arXiv">arXiv:1802.01933</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<title level="j" type="abbrev">ACM Comput. Surv.</title>
		<idno type="ISSN">0360-0300</idno>
		<idno type="ISSNe">1557-7341</idno>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="42"/>
			<date type="published" when="2018-08-22">2018</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">R. Guidotti, A. Monreale, F. Turini, D. Pedreschi, and F. Giannotti, "A survey of methods for explaining black box models," arXiv preprint arXiv:1802.01933, 2018.</note>
</biblStruct>

<biblStruct coords="10,330.24,492.85,232.38,7.13;10,330.24,501.85,232.38,7.13;10,330.24,510.85,99.38,7.13" xml:id="b76">
	<analytic>
		<title level="a" type="main">10256, 1822-05-22, [MELLOR]</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Magazzeni</surname></persName>
		</author>
		<idno type="DOI">10.1163/2210-7886_asc-10256</idno>
		<idno>abs/1709.10256</idno>
		<ptr target="http://arxiv.org/abs/1709.10256"/>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Brill</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Fox, D. Long, and D. Magazzeni, "Explainable planning," CoRR, vol. abs/1709.10256, 2017. [Online]. Available: http://arxiv.org/abs/1709.10256</note>
</biblStruct>

<biblStruct coords="10,330.24,519.73,147.06,7.13" xml:id="b77">
	<analytic>
		<title level="a" type="main">A Circumscriptive Theory of Plan Recognition</title>
		<author>
			<persName coords=""><forename type="first">Henry</forename><forename type="middle">A</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/3839.003.0008</idno>
	</analytic>
	<monogr>
		<title level="m">Intentions in Communication</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1990-06-28"/>
			<biblScope unit="page" from="105" to="134"/>
		</imprint>
	</monogr>
	<note type="raw_reference">H. A. Kautz, "Generalized plan recognition."</note>
</biblStruct>

<biblStruct coords="10,330.24,528.73,232.62,7.13;10,330.24,537.73,150.55,7.13" xml:id="b78">
	<analytic>
		<title level="a" type="main">JUSTICE, MORALITY AND BELIEF IN CONFLICT RESOLUTION PROCESSES</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Reeves</surname></persName>
		</author>
		<idno type="DOI">10.2307/jj.9891586.11</idno>
	</analytic>
	<monogr>
		<title level="m">Dynamics of Political Domination in Africa</title>
		<imprint>
			<publisher>Berghahn Books</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="122" to="135"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. F. Reeves, "Computational morality: A process model of belief conflict and resolution for story understanding," 1991.</note>
</biblStruct>

<biblStruct coords="10,330.24,546.73,232.51,7.13" xml:id="b79">
	<monogr>
		<title level="m" type="main">Story Understanding</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<idno type="DOI">10.1002/0470018860.s00082</idno>
		<imprint>
			<date type="published" when="2006-01-15"/>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">E. T. Mueller, "Story understanding," Encyclopedia of Cognitive Science.</note>
</biblStruct>

<biblStruct coords="10,330.24,555.61,232.47,7.13;10,330.24,564.61,101.59,7.13" xml:id="b80">
	<monogr>
		<title level="m" type="main">The Data Ops Manifesto</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Holmes</surname></persName>
		</author>
		<idno type="DOI">10.59350/9atyw-2nj79</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Front Matter</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Winston and D. Holmes, "The genesis manifesto: Story understanding and human intelligence," 2017.</note>
</biblStruct>

<biblStruct coords="10,330.24,573.61,232.47,7.13;10,330.24,582.49,109.74,7.13" xml:id="b81">
	<monogr>
		<title level="m" type="main">Verbalization: Narration of autonomous robot experience</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Selvaraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">S. Rosenthal, S. P. Selvaraj, and M. M. Veloso, "Verbalization: Narration of autonomous robot experience."</note>
</biblStruct>

<biblStruct coords="10,330.24,591.49,232.57,7.13;10,330.24,600.49,35.35,7.13" xml:id="b82">
	<monogr>
		<title level="m" type="main">Focusing construction and selection of abductive hypotheses</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Leake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. B. Leake, "Focusing construction and selection of abductive hypothe- ses," 1993.</note>
</biblStruct>

<biblStruct coords="10,330.24,609.49,232.47,7.13;10,330.24,618.37,232.71,7.13;10,330.24,627.37,212.35,7.13" xml:id="b83">
	<analytic>
		<title level="a" type="main">The role of coherence in constructing and evaluating abductive explanations</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes, AAAI Spring Symposium on Automated Abduction</title>
		<meeting><address><addrLine>Stanford, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. T. Ng and R. J. Mooney, "The role of coherence in constructing and evaluating abductive explanations," in Working Notes, AAAI Spring Symposium on Automated Abduction, Stanford, California, 1990.</note>
</biblStruct>

<biblStruct coords="10,330.24,636.37,232.74,7.13;10,330.24,645.25,232.63,7.13;10,330.24,654.25,17.83,7.13" xml:id="b84">
	<analytic>
		<title level="a" type="main">Imagination and the generation of new ideas</title>
		<author>
			<persName><forename type="first">Rachel</forename><forename type="middle">W</forename><surname>Magid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sheskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogdev.2014.12.008</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Development</title>
		<title level="j" type="abbrev">Cognitive Development</title>
		<idno type="ISSN">0885-2014</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="99" to="110"/>
			<date type="published" when="2015-04">2015</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. W. Magid, M. Sheskin, and L. E. Schulz, "Imagination and the generation of new ideas," Cognitive Development, vol. 34, pp. 99-110, 2015.</note>
</biblStruct>

<biblStruct coords="10,330.24,663.25,232.46,7.13;10,330.24,672.25,175.03,7.13" xml:id="b85">
	<analytic>
		<title level="a" type="main">Theory of Mind: A Neural Prediction Problem</title>
		<author>
			<persName><forename type="first">Jorie</forename><surname>Koster-Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Saxe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2013.08.020</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<title level="j" type="abbrev">Neuron</title>
		<idno type="ISSN">0896-6273</idno>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="836" to="848"/>
			<date type="published" when="2013-09">2013</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Koster-Hale and R. Saxe, "Theory of mind: a neural prediction problem," Neuron, vol. 79, no. 5, pp. 836-848, 2013.</note>
</biblStruct>

<biblStruct coords="10,330.24,681.13,232.74,7.13;10,330.24,690.13,17.83,7.13" xml:id="b86">
	<analytic>
		<title level="a" type="main">14 Please talk to my heart</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.7560/714908-018</idno>
	</analytic>
	<monogr>
		<title level="m">Whiskey River (Take My Mind)</title>
		<imprint>
			<publisher>University of Texas Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="207" to="218"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. LeCun, "My take on ali rahimi's test of time award talk at nips," 2017.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilling a Neural Network Into a Soft Decision Tree</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,226.60,156.69,64.79,8.80"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">Google Brain Team</note>
								<orgName type="institution">Google Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.87,156.69,69.81,8.80"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">Google Brain Team</note>
								<orgName type="institution">Google Brain Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Distilling a Neural Network Into a Soft Decision Tree</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">92B59F47B684B1637F7DB32AB8009006</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,201.58,216.52,240.63,7.92;1,152.74,227.48,144.70,7.92">Deep neural networks have proved to be a very effective way to perform classification tasks.</s><s coords="1,300.66,227.48,141.54,7.92;1,153.07,238.44,289.38,7.92;1,153.07,249.40,289.14,7.92;1,153.07,260.36,4.41,7.92">They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large [</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,124.72,378.63,94.17,10.57">Introduction</head><p><s coords="1,124.37,403.01,346.18,8.80;1,124.72,414.96,346.10,8.80;1,124.72,426.92,199.62,8.80">The excellent generalization abilities of deep neural nets depend on their use of distributed representations <ref type="bibr" coords="1,260.10,414.96,88.97,8.80" target="#b4">[LeCun et al., 2015]</ref> in their hidden layers, but these representations are hard to understand.</s><s coords="1,327.65,426.92,142.90,8.80;1,124.72,438.87,345.82,8.80;1,124.72,450.83,346.10,8.80;1,124.72,462.78,345.83,8.80;1,124.72,474.74,347.76,8.80">For the first hidden layer we can understand what causes an activation of a unit and for the last hidden layer we can understand the effects of activating a unit, but for the other hidden layers it is much harder to understand the causes and effects of a feature activation in terms of variables that are meaningful such as the input and output variables.</s><s coords="1,124.35,486.69,346.40,8.80;1,124.72,498.65,345.83,8.80;1,124.72,510.60,346.09,8.80;1,124.72,522.56,54.93,8.80">Also, the units in a hidden layer factor the representation of the input vector into a set of feature activations in such a way that the combined effects of the active features can cause an appropriate distributed representation in the next hidden layer.</s><s coords="1,182.78,522.56,288.14,8.80;1,124.72,534.51,345.82,8.80;1,124.72,546.47,200.81,8.80">This makes it very difficult to understand the functional role of any particular feature activation in isolation since its marginal effect depends on the effects of all the other units in the same layer.</s></p><p><s coords="1,139.67,558.42,330.88,8.80;1,124.72,570.38,345.83,8.80;1,124.72,582.33,345.83,8.80;1,124.72,594.29,345.83,8.80;1,124.72,606.24,345.83,8.80;1,124.72,618.20,207.82,8.80">These difficulties are further compounded by the fact that deep neural nets can make reliable decisions by modeling a very large number of weak statistical regularities in the relationship between the inputs and outputs of the training data and there is nothing in the neural network to distinguish the weak regularities that are true properties of the data from the spurious regularities that are created by the sampling peculiarities of the training set.</s><s coords="1,335.87,618.20,136.06,8.80;1,124.72,630.15,345.83,8.80;1,124.72,642.11,345.83,8.80;1,124.72,654.06,70.89,8.80">Faced with all these difficulties, it seems wise to abandon the idea of trying to understand how a deep neural network makes a classification decision by understanding what the individual hidden units do.</s></p><p><s coords="1,139.67,666.02,331.08,8.80;1,124.72,677.97,345.83,8.80;1,18.34,443.79,18.00,166.10">By contrast, it is easy to explain how a decision tree makes any particular classification because this depends on a relatively short sequence of decisions and arXiv:1711.09784v1</s><s coords="1,18.34,372.15,18.00,61.64;1,18.34,342.15,18.00,20.00;1,18.34,302.71,18.00,34.44;1,18.34,257.71,18.00,40.00;2,124.72,104.13,209.79,8.80"><ref type="bibr" coords="1,18.34,372.15,10.29,61.64">[cs.</ref>LG] 27 Nov 2017 each decision is based directly on the input data.</s><s coords="2,337.81,104.13,133.01,8.80;2,124.72,116.08,193.75,8.80">Decision trees, however, do not usually generalize as well as deep neural nets.</s><s coords="2,321.56,116.08,149.00,8.80;2,124.72,128.04,346.19,8.80;2,124.72,139.99,345.82,8.80;2,124.72,151.95,345.83,8.80;2,124.72,163.90,93.26,8.80">Unlike the hidden units in a neural net, a typical node at the lower levels of a decision tree is only used by a very small fraction of the training data so the lower parts of the decision tree tend to overfit unless the size of the training set is exponentially large compared with the depth of the tree.</s></p><p><s coords="2,139.67,176.43,330.88,8.80;2,124.72,188.38,151.76,8.80">In this paper, we propose a novel way of resolving the tension between generalization and interpretability.</s><s coords="2,279.81,188.38,190.74,8.80;2,124.72,200.34,345.83,8.80;2,124.72,212.29,345.82,8.80;2,124.72,224.25,217.26,8.80">Instead of trying to understand how a deep neural network makes its decisions, we use the deep neural network to train a decision tree that mimics the input-output function discovered by the neural network but works in a completely different way.</s><s coords="2,345.41,224.25,125.14,8.80;2,124.72,236.20,345.83,8.80;2,124.72,248.16,345.83,8.80;2,124.72,260.11,61.64,8.80">If there is a large amount of unlabelled data, the neural net can be used to create a much larger labelled data set to train a decision tree, thus overcoming the statistical inefficiency of decision trees.</s><s coords="2,189.70,260.11,280.85,8.80;2,124.72,272.07,345.82,8.80;2,124.22,284.02,346.34,8.80;2,124.72,295.98,129.56,8.80">Even if unlabelled data is unavailable, it may be possible to use recent advances in generative modeling <ref type="bibr" coords="2,303.17,272.07,167.38,8.80">[Goodfellow et al., 2014, Kingma and</ref><ref type="bibr" coords="2,124.22,284.02,61.78,8.80" target="#b6">Welling, 2013]</ref> to generate synthetic unlabelled data from a distribution that is close to the data distribution.</s><s coords="2,257.62,295.98,212.93,8.80;2,124.72,307.93,345.83,8.80;2,124.72,319.89,345.83,8.80;2,124.72,331.84,203.10,8.80">Without using unlabelled data, it is still possible to transfer the generalization abilities of the neural net to a decision tree by using a technique called distillation <ref type="bibr" coords="2,259.36,319.89,84.60,8.80" target="#b7">[Hinton et al., 2015</ref><ref type="bibr" coords="2,343.96,319.89,93.33,8.80" target="#b8">, Bucilu«é et al., 2006</ref>] and a type of decision tree that makes soft decisions.</s></p><p><s coords="2,139.67,344.37,217.75,8.80">At test time, we use the decision tree as our model.</s><s coords="2,360.40,344.37,110.52,8.80;2,124.37,356.33,346.19,8.80;2,124.72,368.28,286.00,8.80">This may perform slightly worse than the neural network but it will often be much faster and we now have a model whose decisions we can explain and engage with directly.</s></p><p><s coords="2,139.67,380.81,279.96,8.80">We start by describing the particular type of decision tree we use.</s><s coords="2,422.69,380.81,47.85,8.80;2,124.37,392.76,346.19,8.80;2,124.72,404.72,102.73,8.80">This choice was made to facilitate easy distillation of the knowledge acquired by a deep neural net into a decision tree.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="2,124.72,438.06,228.03,10.57">The Hierarchical Mixture of Bigots</head><p><s coords="2,124.22,466.21,346.34,8.80;2,124.72,478.17,337.64,9.71;2,124.72,490.12,129.62,8.80">We use soft binary decision trees trained with mini-batch gradient descent, where each inner node i has a learned filter w i and a bias b i , and each leaf node has a learned distribution Q .</s><s coords="2,257.66,490.12,212.89,8.80;2,124.72,502.08,87.59,8.80">At each inner node, the probability of taking the rightmost branch is:</s></p><formula xml:id="formula_0" coords="2,254.33,515.74,216.22,9.71">p i (x) = œÉ(xw i + b i ) (1)</formula><p><s coords="2,124.37,535.39,315.57,8.80">where x is the input to the model and œÉ is the sigmoid logistic function.</s><s coords="2,139.67,547.91,331.16,8.80;2,124.72,559.87,345.82,8.80;2,124.72,571.82,206.93,8.80">This model is a hierarchical mixture of experts <ref type="bibr" coords="2,340.59,547.91,108.52,8.80" target="#b9">[Jordan and Jacobs, 1994]</ref>, but each expert is a actually a bigot who does not look at the data after training, and therefore always produces the same distribution.</s><s coords="2,334.85,571.82,135.71,8.80;2,124.72,583.78,346.03,8.80;2,124.72,595.73,345.83,8.80;2,124.72,607.69,112.85,8.80">The model learns a hierarchy of filters that are used to assign each example to a particular bigot with a particular path probability, and each bigot learns a simple, static distribution over the possible output classes, k.</s></p><formula xml:id="formula_1" coords="2,253.89,631.44,216.66,25.08">Q k = exp(œÜ k ) k exp(œÜ k ) ,<label>(2)</label></formula><p><s coords="2,139.67,666.02,36.01,8.80;2,175.68,670.58,2.37,6.12;2,182.68,666.02,180.29,8.80;2,370.29,664.46,7.51,6.16;2,381.46,666.02,67.48,8.80;2,448.95,670.58,2.37,6.12;2,455.94,666.02,14.60,8.80;2,124.72,677.97,134.27,8.80">where Q ‚Ä¢ denotes the probability distribution at the th leaf, and each œÜ ‚Ä¢ is a learned parameter at that leaf.</s><s coords="3,139.67,403.08,330.88,8.80;3,124.72,415.03,304.17,8.80">In order to avoid very soft decisions in the tree, we introduced an inverse temperature Œ≤ to the filter activations prior to calculating the sigmoid.</s><s coords="3,432.21,415.03,38.34,8.80;3,124.72,426.99,242.07,8.80">Thus the probability of taking the right branch at node i becomes</s></p><formula xml:id="formula_2" coords="3,370.09,427.05,102.39,9.65">p i (x) = œÉ(Œ≤(xw i + b i )).</formula><p><s coords="3,139.67,440.28,330.88,8.80;3,124.72,452.23,346.09,8.80;3,124.72,464.19,346.19,8.80;3,124.72,476.14,153.48,8.80">This model can be used to give a predictive distribution over classes in two different ways, namely by using the distribution from the leaf with the greatest path probability or averaging the distributions over all the leaves, weighted by their respective path probabilities.</s><s coords="3,281.79,476.14,188.76,8.80;3,124.72,488.10,345.82,8.80;3,124.72,500.05,346.02,8.80;3,124.37,512.01,154.91,8.80">If we take the predictive distribution from the leaf with the greatest path probability, the explanation for that prediction is simply the list of all the filters along the path from the route to the leaf together with the binary activation decisions.</s><s coords="3,282.45,512.01,188.11,8.80;3,124.72,523.96,346.19,8.80;3,124.72,535.92,346.20,8.80;3,124.72,547.87,345.83,8.80;3,124.72,559.83,201.79,8.80">If we average the leaf distributions weighted by their respective path probabilities, we find that the model achieves marginally better test accuracy, but this leads to an exponential increase in the complexity of the explanation of the model's predictive distribution on a particular case because it involves the filters at all of the nodes.</s><s coords="3,329.21,559.83,141.54,8.80;3,124.72,571.78,345.82,8.80;3,124.72,583.74,278.44,8.80">For this reason, for the remainder of this paper when we refer to the output of the model, we will be referring to the distribution at the leaf with the maximum path probability.</s></p><p><s coords="3,139.67,597.03,330.88,8.80;3,124.72,608.98,345.83,8.80;3,124.72,620.94,85.32,8.80">We train the soft decision tree using a loss function that seeks to minimize the cross entropy between each leaf, weighted by its path probability, and the target distribution.</s><s coords="3,213.98,620.94,256.84,8.80;3,124.72,632.89,113.14,8.80">For a single training case with input vector x and target distribution T , the loss is:</s></p><formula xml:id="formula_3" coords="3,193.26,668.62,277.30,20.20">L(x) = -log ‚ààLeaf N odes P (x) k T k log Q k (3)</formula><p><s coords="4,124.22,104.13,346.33,8.80;4,124.72,116.08,110.08,8.80">Where T is the target distribution and P (x) is the probability of arriving at leaf node given the input x.</s></p><p><s coords="4,139.67,128.64,331.15,8.80;4,124.72,140.59,322.90,8.80">Unlike most decision trees, our soft decision trees use decision boundaries that are not aligned with the axes defined by the components of the input vector.</s><s coords="4,450.46,140.59,21.47,8.80;4,124.72,152.55,345.83,8.80;4,124.72,164.51,345.82,8.80;4,124.72,176.46,345.82,8.80;4,124.72,188.42,100.88,8.80">Also, they are trained by first picking the size of the tree and then using mini-batch gradient descent to update all of their parameters simultaneously, rather than the more standard greedy approach that decides the splits one node at a time <ref type="bibr" coords="4,124.72,188.42,96.64,8.80" target="#b10">[Friedman et al., 2001]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="4,124.72,221.93,92.94,10.57">Regularizers</head><p><s coords="4,124.37,250.24,346.18,8.80;4,124.72,262.19,345.82,8.80;4,124.72,274.15,104.81,8.80">To avoid getting stuck at poor solutions during the training, we introduced a penalty term that encouraged each internal node to make equal use of both left and right sub-trees.</s><s coords="4,232.99,274.15,237.57,8.80;4,124.72,286.10,345.83,8.80;4,124.72,298.06,345.82,8.80;4,124.72,310.02,165.26,8.80">Without this penalty, the tree tended to get stuck on plateaus in which one or more of the internal nodes always assigned almost all the probability to one of its sub-trees and the gradient of the logistic for this decision was always very close to zero.</s><s coords="4,293.32,310.02,177.23,8.80;4,124.72,321.97,345.82,8.80;4,124.72,333.93,269.41,8.80">The penalty is the cross entropy between the desired average distribution 0.5, 0.5 for the two sub-trees and the actual average distribution Œ±, (1 -Œ±) where Œ± for node i is given by:</s></p><formula xml:id="formula_4" coords="4,252.73,355.30,217.82,26.43">Œ± i = x P i (x)p i (x) x P i (x)<label>(4)</label></formula><p><s coords="4,124.37,393.90,35.52,8.80;4,161.27,392.39,2.82,6.12;4,164.59,393.90,249.54,8.80">where P i (x) is the path probability from the root node to node i.</s><s coords="4,417.43,393.90,53.49,8.80;4,124.72,405.86,171.48,8.80">The penalty summed over all internal nodes is then:</s></p><formula xml:id="formula_5" coords="4,193.03,428.98,273.28,20.20">C = -Œª i‚ààInnerN odes 0.5 log(Œ± i ) + 0.5 log(1 -Œ± i ) (<label>5</label></formula><formula xml:id="formula_6" coords="4,466.31,428.98,4.24,8.80">)</formula><p><s coords="4,139.67,462.18,330.89,8.80;4,124.72,474.13,101.52,8.80">where Œª is a hyper-parameter that determines the strength of the penalty and is set prior to training.</s><s coords="4,229.70,474.13,240.86,8.80;4,124.72,486.09,345.83,8.80;4,124.72,498.04,305.74,8.80">This penalty was based on the assumption that a tree making fairly equal use of alternative sub-trees would usually be better suited to any particular classification task and in practice it did increase accuracy.</s><s coords="4,433.15,498.04,38.78,8.80;4,124.72,510.00,345.83,8.80;4,124.72,521.95,345.83,8.80;4,124.72,533.91,346.10,8.80;4,124.72,545.86,111.68,8.80">However, this assumption is less and less valid as one descends the tree; a penultimate node in the tree may only be responsible for two classes of input, in some non-equal proportion, and penalizing the node for a non-equal split in this case could hurt the accuracy of the model.</s><s coords="4,239.17,545.86,231.38,8.80;4,124.37,557.82,346.19,8.80;4,124.72,569.77,210.19,8.80;4,334.91,568.26,10.37,6.12;4,345.79,569.77,2.77,8.80">We found that we achieved better test accuracy results when the strength of the penalty decayed exponentially with the depth d of the node in the tree so that it was proportional to 2 -d .</s></p><p><s coords="4,139.67,582.33,330.88,8.80;4,124.72,594.29,251.21,8.80">As one descends the tree, the expected fraction of the data that each node sees in any given training batch decreases exponentially.</s><s coords="4,379.52,594.29,91.03,8.80;4,124.72,606.24,345.83,8.80;4,124.72,618.20,40.10,8.80">This means that the computation of the actual probabilities of using the two sub-trees becomes less accurate.</s><s coords="4,168.40,618.20,302.14,8.80;4,124.72,630.15,346.20,8.80;4,124.72,642.11,162.10,8.80">To counter this we can maintain an exponentially decaying running average of the actual probabilities with a time window that is exponentially proportional to the depth of the node.</s><s coords="4,289.83,642.11,180.71,8.80;4,124.72,654.06,345.83,8.80;4,124.72,666.02,345.83,8.80;4,124.72,677.97,218.13,8.80">We found experimentally that we achieved much better test accuracy by using both the exponential decay in the strength of the penalty with depth and the exponential increase in the temporal scale of the window used to compute the running average.</s><s coords="5,124.72,286.61,347.76,8.80">Fig. <ref type="figure" coords="5,143.06,286.61,3.80,8.80">2</ref>: This is a visualization of a soft decision tree of depth 4 trained on MNIST.</s><s coords="5,124.37,298.57,346.18,8.80;5,124.72,310.52,302.79,8.80">The images at the inner nodes are the learned filters, and the images at the leaves are visualizations of the learned probability distribution over classes.</s><s coords="5,430.83,310.52,39.71,8.80;5,124.72,322.48,345.83,8.80;5,124.72,334.43,85.10,8.80">The final most likely classification at each leaf, as well as the likely classifications at each edge are annotated.</s><s coords="5,213.14,334.43,257.41,8.80;5,124.72,346.39,345.82,8.80;5,124.72,358.34,325.53,8.80">If we take for example the right most internal node, we can see that at that level in the tree the potential classifications are only 3 or 8, thus the learned filter is simply learning to distinguish between those two digits.</s><s coords="5,453.57,358.34,16.98,8.80;5,124.72,370.30,345.82,8.80;5,124.72,382.25,96.56,8.80">The result is a filter that looks for the presence of two areas that would join the ends of the 3 to make an 8.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="5,124.72,416.71,112.96,10.57">MNIST Results</head><p><s coords="5,124.37,441.63,346.18,8.80;5,124.72,453.58,345.83,8.80;5,124.72,465.54,111.23,8.80">The number of total parameters at which our soft decision trees start to overfit is typically less than the number of total parameters at which a multi-layer neural network starts to overfit.</s><s coords="5,239.65,465.54,230.90,8.80;5,124.72,477.49,235.89,8.80">This is because the lower nodes of the decision tree only receive a very small fraction of the training data.</s><s coords="5,139.67,489.45,200.46,8.80">This is reflected in performance on MNIST.</s><s coords="5,344.49,489.45,126.06,8.80;5,124.72,501.40,345.83,8.80;5,124.72,513.36,87.61,8.80">With a soft decision tree of depth 8 we were able to achieve a test accuracy of at most 94.45% when training on the true targets.</s><s coords="5,216.10,513.36,254.45,8.80;5,124.72,525.31,347.76,8.80">A neural net with two convolutional hidden layers and a penultimate fully connected layer achieved a much better test accuracy of 99.21%.</s><s coords="5,124.22,537.27,346.54,8.80;5,124.72,549.22,345.83,8.80;5,124.72,561.18,213.34,8.80">We were then able to use the accuracy of the neural net to make a much better soft decision tree by training with soft targets that were a composite of the true labels and the predictions of the neural network.</s><s coords="5,341.38,561.18,129.18,8.80;5,124.72,573.13,345.82,8.80;5,124.72,585.09,301.80,8.80">The soft decision tree trained in this way achieved a test accuracy of 96.76% which is about halfway between the neural net and the soft decision tree trained directly on the data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="5,124.72,615.20,286.42,10.57;5,144.89,629.15,73.73,10.57">Explaining how a soft decision tree makes a classification</head><p><s coords="5,124.37,654.06,346.18,8.80;5,124.72,666.02,345.83,8.80;5,124.72,677.97,345.83,8.80;6,124.72,104.13,242.85,8.80">The main motivation behind this work was to create a model whose behavior is easy to explain; in order to fully understand why a particular example was given a particular classification, one can simply examine all the learned filters along the path between the root and the classification's leaf node.</s><s coords="6,370.90,104.13,99.65,8.80;6,124.72,116.08,345.82,8.80;6,124.72,128.04,33.13,8.80">The crux of this model is that it does not rely on hierarchical features, it relies on hierarchical decisions instead.</s><s coords="6,160.92,128.04,309.62,8.80;6,124.72,139.99,346.02,8.80;6,124.72,151.95,230.11,8.80">The hierarchical features of a traditional neural network allow it to learn robust and novel representations of the input space, but past a single level or two, they become extremely difficult to engage with.</s><s coords="6,358.15,151.95,112.67,8.80;6,124.72,163.90,345.82,8.80;6,124.72,175.86,347.21,8.80;6,124.48,187.81,346.35,8.80;6,124.72,199.77,345.82,8.80;6,124.72,211.72,133.44,8.80">Some current attempts at explanations for neural networks rely on the use of gradient descent to find an input that particularly excites a given neuron <ref type="bibr" coords="6,319.94,175.86,91.67,8.80" target="#b11">[Simonyan et al., 2013</ref><ref type="bibr" coords="6,411.61,175.86,60.33,8.80;6,124.48,187.81,21.64,8.80" target="#b12">, Erhan et al., 2009]</ref>, but this results is a single point on a manifold of inputs, meaning that other inputs could yield the same pattern of neural excitement, and so it does not reflect the entire manifold.</s><s coords="6,261.48,211.72,209.07,8.80;6,124.72,223.68,345.83,8.80;6,124.72,235.63,346.10,8.80;6,124.72,247.59,173.17,8.80">Ribeiro et al. propose a strategy which relies on fitting some explainable model which "acts over absence/presence of interpretable components" to the behavior of a deep neural net around some area of interest in the input space <ref type="bibr" coords="6,206.43,247.59,87.24,8.80" target="#b13">[Ribeiro et al., 2016]</ref>.</s><s coords="6,301.21,247.59,169.34,8.80;6,124.72,259.54,345.82,8.80;6,124.72,271.50,239.48,8.80">This is accomplished by sampling from the input space and querying the model around the area of interest and then fitting an explainable model to the output of the model.</s><s coords="6,367.37,271.50,103.18,8.80;6,124.72,283.45,345.83,8.80;6,124.72,295.41,346.02,8.80;6,124.72,307.36,345.82,8.80;6,124.72,319.32,345.83,8.80;6,124.72,331.27,52.48,8.80">This avoids the problem of attempting to explain a particular output by visualizing a single point on a manifold but introduces the problem of necessitating a new explainable model for every area of interest in the input space, and attempting to explain changes in the model's behavior by first order changes in a discretized interpretation of the input space.</s><s coords="6,180.53,331.27,290.02,8.80;6,124.37,343.23,346.18,8.80;6,124.72,355.18,177.70,8.80">By relying on hierarchical decisions instead of hierarchical features we side-step these problems, as each decision is made at a level of abstraction that the reader can engage with directly.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="6,124.72,387.49,189.96,10.57">Other Data Sets and Results</head><p><s coords="6,124.22,414.60,346.60,8.80;6,124.72,426.55,123.88,8.80">We tried this model on several other data sets, but focused on spatial input for the sake of visualization.</s><s coords="6,251.92,426.55,218.63,8.80;6,124.72,438.51,345.83,8.80;6,124.37,450.46,347.57,8.80;6,124.48,462.42,347.46,8.80;6,124.72,474.37,345.82,8.80;6,124.72,486.33,56.26,8.80">By first training a neural net and then using it to provide soft targets for training a soft decision tree, with a tree of depth 8 we were able to achieve a test accuracy of 80.60% on the Connect4 dataset <ref type="bibr" coords="6,429.78,450.46,42.16,8.80;6,124.48,462.42,22.76,8.80" target="#b14">[Lichman, 2013]</ref> comprised of board states of the popular child's game connect 4 as input, and the final outcome of the game (player 1 win, player 2 win, or tie) as the target value.</s><s coords="6,184.98,486.33,285.56,8.80;6,124.72,498.28,95.18,8.80">Without distilling from a neural net, the best test accuracy we achieved was 78.63%.</s><s coords="6,223.44,498.28,247.11,8.80;6,124.72,510.24,345.83,8.80;6,124.72,522.19,345.83,8.80;6,124.72,534.15,51.36,8.80">Other decision trees trained with gradient descent have been applied to this dataset <ref type="bibr" coords="6,246.39,510.24,88.79,8.80" target="#b15">[Norouzi et al., 2015]</ref> but were only able to achieve a maximum test accuracy of 76.50% at the equivalent depth of 8 and 77.45% at a depth of 20.</s><s coords="6,179.39,534.15,291.16,8.80;6,124.72,546.11,345.83,8.80;6,124.72,558.06,201.11,8.80">This provides an interesting example of the utility of an explainable model -by examining the learned filters of the soft decision tree we are able to learn something about the nature of the game.</s><s coords="6,329.16,558.06,141.39,8.80;6,124.72,570.02,347.49,8.80;6,124.72,581.97,345.83,8.80;6,124.37,593.93,279.65,8.80">From examining the first learned filter we can see that the game can be split into two distinct sub types of gamesgames where the players have placed pieces on the edges of the board, and games where the players have placed pieces in the center of the board.</s><s coords="6,407.33,593.93,63.22,8.80;6,124.72,605.88,345.83,8.80;6,124.72,617.84,167.55,8.80">These two sub games progress in sufficiently different manners that it was beneficial for the decision tree to split them at the root.</s></p><p><s coords="6,139.67,630.15,331.15,8.80;6,124.72,642.11,345.82,8.80;6,124.72,654.06,79.43,8.80">We also ran our model on a non spatial dataset, namely the Letter dataset <ref type="bibr" coords="6,124.72,642.11,67.30,8.80" target="#b14">[Lichman, 2013]</ref>, which is comprised of primitive numerical attributes of capital english characters.</s><s coords="6,207.47,654.06,263.08,8.80;6,124.72,666.02,345.83,8.80;6,124.37,677.97,332.86,8.80">We were able to achieve a test accuracy of 78.0% with a tree of depth 9 trained on the raw training data, and a test accuracy of 81.0% when we distilled from an ensemble of neural nets that had a 95.9% test accuracy.</s><s coords="7,124.72,342.91,345.83,8.80;7,124.72,354.87,110.23,8.80">Fig. <ref type="figure" coords="7,143.65,342.91,3.93,8.80">3</ref>: This is a visualization of the first 2 layers of a soft decision tree trained on the Connect4 data set.</s><s coords="7,238.09,354.87,232.46,8.80;7,124.72,366.82,345.82,8.80;7,124.72,378.78,345.83,8.80;7,124.72,390.74,174.69,8.80">From examining the learned filters we can see that the game can be split into two distinct sub types of games -games where the players have placed pieces on the edges of the board, and games where the players have placed pieces in the center of the board.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" coords="7,124.72,425.58,84.49,10.57">Conclusion</head><p><s coords="7,124.22,450.87,346.33,8.80;7,124.72,462.83,345.83,8.80;7,124.72,474.78,345.82,8.80;7,124.72,486.74,32.60,8.80">We have described a method for using a trained neural net to create a more explicable model in the form of a soft decision tree which is trained by stochastic gradient descent using the predictions of the neural net to give more informative targets.</s><s coords="7,160.65,486.74,309.91,8.80;7,124.72,498.69,346.20,8.80;7,124.72,510.65,166.26,8.80">The soft decision tree uses learned filters to make hierarchical decisions based on an input example and ultimately select a particular static probability distribution over classes as its output.</s><s coords="7,294.30,510.65,176.46,8.80;7,124.72,522.60,346.10,8.80;7,124.37,534.56,242.01,8.80">This soft decision tree generalizes better than one trained on the data directly, but performs worse than the neural net which was used to provide the soft targets for training it.</s><s coords="7,369.34,534.56,101.21,8.80;7,124.72,546.51,347.21,8.80;7,124.37,558.47,346.19,8.80;7,124.72,570.42,347.67,8.80">So if it is essential to be able to explain why a model classifies a particular test case in a particular way, we can use a soft decision tree, but we can still gain some of the benefits of deep neural networks by using them to improve the training of this explicable model.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,124.72,352.22,345.83,8.80;3,124.72,364.18,84.16,8.80;3,193.89,101.03,207.50,239.73"><head>Fig. 1 :</head><label>1</label><figDesc><div><p><s coords="3,124.72,352.22,345.83,8.80;3,124.72,364.18,84.16,8.80">Fig. 1: This diagram shows a soft binary decision tree with a single inner node and two leaf nodes.</s></p></div></figDesc><graphic coords="3,193.89,101.03,207.50,239.73" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,124.72,101.03,345.83,174.12"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="5,124.72,101.03,345.83,174.12" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,179.41,101.03,242.08,230.42"><head/><label/><figDesc><div><p/></div></figDesc><graphic coords="7,179.41,101.03,242.08,230.42" type="bitmap"/></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,124.72,155.93,346.03,8.80;8,134.31,167.89,336.25,8.80;8,134.69,179.84,335.69,8.80;8,134.04,191.80,206.28,8.80" xml:id="b0">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Wei Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015</date>
			<biblScope unit="page" from="1" to="9"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015.</note>
</biblStruct>

<biblStruct coords="8,124.72,203.75,347.21,8.80;8,134.18,215.71,337.75,8.80;8,134.69,227.66,335.86,8.80;8,134.69,239.62,316.70,8.80" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google's neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.</note>
</biblStruct>

<biblStruct coords="8,124.72,251.57,347.76,8.80;8,134.69,263.53,337.25,8.80;8,134.44,275.48,22.69,8.80" xml:id="b2">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName coords=""><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.</note>
</biblStruct>

<biblStruct coords="8,124.72,287.44,345.83,8.80;8,134.33,299.39,336.18,8.80;8,133.46,311.35,338.91,8.80" xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2013.6638947</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-05">2013</date>
			<biblScope unit="page" from="6645" to="6649"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pages 6645-6649. IEEE, 2013.</note>
</biblStruct>

<biblStruct coords="8,124.72,323.30,346.82,8.80;8,133.52,335.26,94.07,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<title level="j" type="abbrev">Nature</title>
		<idno type="ISSN">0028-0836</idno>
		<idno type="ISSNe">1476-4687</idno>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444"/>
			<date type="published" when="2015-05-27">2015</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521 (7553):436-444, 2015.</note>
</biblStruct>

<biblStruct coords="8,124.72,347.21,347.22,8.80;8,134.69,359.17,335.87,8.80;8,134.69,371.12,337.25,8.80;8,134.44,383.08,22.69,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1145/3422622</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144"/>
			<date type="published" when="2014">2014</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.</note>
</biblStruct>

<biblStruct coords="8,124.72,395.04,345.72,8.80;8,134.69,406.99,138.12,8.80" xml:id="b6">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</note>
</biblStruct>

<biblStruct coords="8,124.72,418.95,345.82,8.80;8,134.69,430.90,243.36,8.80" xml:id="b7">
	<analytic>
		<title level="a" type="main">Preprint repository arXiv achieves milestone million uploads</title>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.028530</idno>
		<idno type="arXiv">arXiv:1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">Physics Today</title>
		<title level="j" type="abbrev">Phys. Today</title>
		<idno type="ISSNe">1945-0699</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>AIP Publishing</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.</note>
</biblStruct>

<biblStruct coords="8,124.72,442.86,347.49,8.80;8,134.69,454.81,335.87,8.80;8,134.15,466.77,291.07,8.80" xml:id="b8">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName coords=""><forename type="first">Cristian</forename><surname>Bucilu«é</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150464</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006-08-20">2006</date>
			<biblScope unit="page" from="535" to="541"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Cristian Bucilu«é, Rich Caruana, and Alexandru Niculescu-Mizil. Model compres- sion. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535-541. ACM, 2006.</note>
</biblStruct>

<biblStruct coords="8,124.72,478.72,345.82,8.80;8,134.69,490.68,240.92,8.80" xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the em algorithm</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214"/>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181-214, 1994.</note>
</biblStruct>

<biblStruct coords="8,124.72,502.63,345.06,8.80;8,134.16,514.59,323.69,8.80" xml:id="b10">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName coords=""><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer series in statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2001">2001</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics New York, 2001.</note>
</biblStruct>

<biblStruct coords="8,124.72,526.54,347.49,8.80;8,134.69,538.50,337.79,8.80;8,133.92,550.45,166.01,8.80" xml:id="b11">
	<analytic>
		<title level="a" type="main">Fisher Vector Faces in the Wild</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.5244/c.27.8</idno>
		<idno type="arXiv">arXiv:1312.6034</idno>
	</analytic>
	<monogr>
		<title level="m">Procedings of the British Machine Vision Conference 2013</title>
		<meeting>edings of the British Machine Vision Conference 2013</meeting>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convo- lutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.</note>
</biblStruct>

<biblStruct coords="8,124.72,562.41,345.82,8.80;8,134.69,574.36,337.49,8.80" xml:id="b12">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1341</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of a deep network. University of Montreal, 1341:3, 2009.</note>
</biblStruct>

<biblStruct coords="8,124.72,586.32,346.10,8.80;8,134.43,598.27,337.51,8.80;8,134.44,610.23,216.32,9.08" xml:id="b13">
	<monogr>
		<title level="m" type="main">why should I trust you?": Explaining the predictions of any classifier</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>T√∫lio Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>CoRR, abs/1602.04938</idno>
		<ptr target="http://arxiv.org/abs/1602.04938"/>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marco T√∫lio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should I trust you?": Explaining the predictions of any classifier. CoRR, abs/1602.04938, 2016. URL http://arxiv.org/abs/1602.04938.</note>
</biblStruct>

<biblStruct coords="8,124.72,622.18,349.49,9.08;8,134.69,634.92,75.97,8.30" xml:id="b14">
	<monogr>
		<title level="m" type="main">Table 3: Health datasets from UCI machine learning repository.</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.1806/table-3</idno>
		<ptr target="http://archive.ics.uci.edu/ml"/>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Lichman. UCI machine learning repository, 2013. URL http://archive. ics.uci.edu/ml.</note>
</biblStruct>

<biblStruct coords="8,124.72,646.09,347.21,8.80;8,134.69,658.05,335.86,8.80;8,133.94,670.00,333.73,8.80" xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient non-greedy optimization of decision trees</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1729" to="1737"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Mohammad Norouzi, Maxwell Collins, Matthew A Johnson, David J Fleet, and Pushmeet Kohli. Efficient non-greedy optimization of decision trees. In Advances in Neural Information Processing Systems, pages 1729-1737, 2015.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
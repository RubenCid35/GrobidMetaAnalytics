<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inverting Visual Representations with Convolutional Networks</title>
				<funder ref="#_GVMExuC">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
					<idno type="DOI" subtype="crossref">10.13039/501100000781</idno>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-04-26">26 Apr 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,192.20,147.01,94.59,10.37"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
							<email>dosovits@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Freiburg Freiburg im Breisgau, Germany</note>
								<orgName type="department">Freiburg im Breisgau</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.60,147.01,65.42,10.37"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Freiburg Freiburg im Breisgau, Germany</note>
								<orgName type="department">Freiburg im Breisgau</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inverting Visual Representations with Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-04-26">26 Apr 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">D2C5A0CF5A047F15560755E1B21E501E</idno>
					<idno type="arXiv">arXiv:1506.02753v4[cs.NE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,62.07,259.30,224.30,8.59;1,50.11,271.25,236.25,8.59;1,50.11,283.21,172.71,8.59">Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data.</s><s coords="1,230.14,283.21,56.23,8.59;1,50.11,295.16,236.25,8.59;1,50.11,307.12,192.44,8.59">We propose a new approach to study image representations by inverting them with an up-convolutional neural network.</s><s coords="1,248.86,307.12,37.51,8.59;1,50.11,319.07,236.25,8.59;1,50.11,331.03,116.91,8.59">We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks.</s><s coords="1,173.18,331.03,113.19,8.59;1,50.11,342.98,236.25,8.59;1,50.11,354.94,236.25,8.59;1,50.11,366.89,184.05,8.59">For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features.</s><s coords="1,241.38,366.89,44.98,8.59;1,50.11,378.85,236.25,8.59;1,50.11,390.80,236.25,8.59;1,50.11,402.76,62.94,8.59">Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network.</s><s coords="1,118.84,402.76,167.52,8.59;1,50.11,414.71,236.25,8.59;1,50.11,426.67,236.25,8.59;1,50.11,438.62,52.97,8.59">Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." coords="1,50.11,477.51,76.84,10.75">Introduction</head><p><s coords="1,62.07,499.74,224.30,8.64;1,50.11,511.70,236.25,8.64;1,50.11,523.65,236.25,8.64;1,50.11,535.61,153.64,8.64">A feature representation useful for pattern recognition tasks is expected to concentrate on properties of the input image which are important for the task and ignore the irrelevant properties of the input image.</s><s coords="1,208.03,535.61,78.34,8.64;1,50.11,547.56,236.25,8.64;1,50.11,559.52,236.25,8.64;1,50.11,571.47,236.25,8.64;1,50.11,583.43,236.25,8.64;1,50.11,595.38,34.59,8.64">For example, handdesigned descriptors such as HOG <ref type="bibr" coords="1,189.59,547.56,11.62,8.64" target="#b2">[3]</ref> or SIFT <ref type="bibr" coords="1,237.31,547.56,15.27,8.64" target="#b16">[17]</ref>, explicitly discard the absolute brightness by only considering gradients, precise spatial information by binning the gradients and precise values of the gradients by normalizing the histograms.</s><s coords="1,88.33,595.38,198.04,8.64;1,50.11,607.34,236.25,8.64;1,50.11,619.29,232.61,8.64">Convolutional neural networks (CNNs) trained in a supervised manner <ref type="bibr" coords="1,134.97,607.34,15.77,8.64" target="#b13">[14,</ref><ref type="bibr" coords="1,153.39,607.34,13.28,8.64" target="#b12">13]</ref> are expected to discard information irrelevant for the task they are solving <ref type="bibr" coords="1,233.74,619.29,15.77,8.64" target="#b27">[28,</ref><ref type="bibr" coords="1,252.00,619.29,12.45,8.64" target="#b18">19,</ref><ref type="bibr" coords="1,266.95,619.29,11.83,8.64" target="#b21">22]</ref>.</s></p><p><s coords="1,62.07,632.78,224.30,8.64;1,50.11,644.74,236.25,8.64;1,50.11,656.69,166.51,8.64">In this paper we propose a new approach to analyze which information is preserved by a feature representation and which information is discarded.</s><s coords="1,223.00,656.69,63.36,8.64;1,50.11,668.65,236.25,8.64;1,50.11,680.60,24.07,8.64">We train neural networks to invert feature representations in the following sense.</s><s coords="1,82.47,680.60,203.89,8.64;1,50.11,692.38,236.25,8.82;1,50.11,704.51,236.25,8.64;1,308.86,469.72,80.11,8.64">Given a feature vector, the network is trained to predict the expected pre-image, that is, the (weighted) average of all natural images which could have produced the given feature vector.</s><s coords="1,391.90,469.72,153.21,8.64;1,308.86,481.68,236.25,8.64;1,308.86,493.63,93.83,8.64">The content of this expected pre-image shows image properties which can be confidently inferred from the feature vector.</s><s coords="1,406.26,493.63,138.86,8.64;1,308.86,505.59,204.42,8.64">The amount of blur corresponds to the level of invariance of the feature representation.</s><s coords="1,516.36,505.59,28.75,8.64;1,308.86,517.54,236.25,8.64;1,308.86,529.50,236.25,8.64;1,308.86,541.45,236.25,8.64;1,308.86,553.41,31.11,8.64">We obtain further insights into the structure of the feature space, as we apply the networks to perturbed feature vectors, to interpolations between two feature vectors, or to random feature vectors.</s></p><p><s coords="1,320.82,573.01,224.30,8.64;1,308.86,584.96,236.25,8.64;1,308.86,596.92,236.25,8.64;1,308.86,608.87,236.25,8.64;1,308.86,620.83,236.25,8.64;1,308.86,632.78,68.53,8.64">We apply our inversion method to AlexNet <ref type="bibr" coords="1,497.83,573.01,15.27,8.64" target="#b12">[13]</ref>, a convolutional network trained for classification on ImageNet, as well as to three widely used computer vision features: histogram of oriented gradients (HOG) <ref type="bibr" coords="1,468.90,608.87,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="1,482.59,608.87,7.19,8.64" target="#b6">7]</ref>, scale invariant feature transform (SIFT) <ref type="bibr" coords="1,432.68,620.83,15.27,8.64" target="#b16">[17]</ref>, and local binary patterns (LBP) <ref type="bibr" coords="1,358.31,632.78,15.27,8.64" target="#b20">[21]</ref>.</s><s coords="1,380.76,632.78,164.35,8.64;1,308.86,644.74,236.25,8.64;1,308.86,656.69,152.39,8.64">The SIFT representation comes as a nonuniform, sparse set of oriented keypoints with their corresponding descriptors at various scales.</s><s coords="1,464.31,656.69,80.81,8.64;1,308.86,668.65,129.59,8.64">This is an additional challenge for the inversion task.</s><s coords="1,443.48,668.65,101.64,8.64;1,308.86,680.60,174.31,8.64">LBP features are not differentiable with respect to the input image.</s><s coords="1,488.06,680.60,57.06,8.64;1,308.86,692.56,236.25,8.64;1,308.86,704.51,91.60,8.64">Thus, existing methods based on gradients of representations <ref type="bibr" coords="1,502.66,692.56,16.60,8.64" target="#b18">[19]</ref> could not be applied to them.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1." coords="2,50.11,74.40,82.07,9.85">Related work</head><p><s coords="2,62.07,94.21,224.30,8.64;2,50.11,106.16,98.15,8.64">Our approach is related to a large body of work on inverting neural networks.</s><s coords="2,153.28,106.16,133.08,8.64;2,50.11,118.12,236.25,8.64;2,50.11,130.07,236.25,8.64">These include works making use of backpropagation or sampling <ref type="bibr" coords="2,179.99,118.12,15.77,8.64" target="#b14">[15,</ref><ref type="bibr" coords="2,198.37,118.12,12.45,8.64" target="#b15">16,</ref><ref type="bibr" coords="2,213.42,118.12,12.45,8.64" target="#b17">18,</ref><ref type="bibr" coords="2,228.47,118.12,12.45,8.64" target="#b26">27,</ref><ref type="bibr" coords="2,243.54,118.12,7.47,8.64" target="#b8">9,</ref><ref type="bibr" coords="2,253.61,118.12,13.28,8.64" target="#b24">25]</ref> and, most similar to our approach, other neural networks <ref type="bibr" coords="2,272.26,130.07,10.58,8.64" target="#b1">[2]</ref>.</s><s coords="2,50.11,142.03,236.25,8.64;2,50.11,153.98,236.25,8.64;2,50.11,165.94,110.47,8.64">However, only recent advances in neural network architectures allow us to invert a modern large convolutional network with another network.</s></p><p><s coords="2,62.07,178.69,224.30,8.64;2,50.11,190.64,236.25,8.64;2,50.11,202.60,236.25,8.64;2,50.11,214.55,113.82,8.64">Our approach is not to be confused with the Decon-vNet <ref type="bibr" coords="2,73.54,190.64,15.27,8.64" target="#b27">[28]</ref>, which propagates high level activations backward through a network to identify parts of the image responsible for the activation.</s><s coords="2,170.67,214.55,115.70,8.64;2,50.11,226.51,236.25,8.64;2,50.11,238.47,236.25,8.64;2,50.11,250.42,59.28,8.64">In addition to the high-level feature activations, this reconstruction process uses extra information about maxima locations in intermediate maxpooling layers.</s><s coords="2,112.68,250.42,173.69,8.64;2,50.11,262.38,139.82,8.64">This information has been shown to be crucial for the approach to work <ref type="bibr" coords="2,170.85,262.38,15.27,8.64" target="#b21">[22]</ref>.</s><s coords="2,193.24,262.38,93.12,8.64;2,50.11,274.33,236.25,8.64;2,50.11,286.29,192.81,8.64">A visualization method similar to DeconvNet is by Springenberg et al. <ref type="bibr" coords="2,243.01,274.33,15.27,8.64" target="#b21">[22]</ref>, yet it also makes use of intermediate layer activations.</s></p><p><s coords="2,62.07,299.04,224.30,8.64;2,50.11,310.67,180.42,8.96">Mahendran and Vedaldi <ref type="bibr" coords="2,162.17,299.04,16.60,8.64" target="#b18">[19]</ref> invert a differentiable image representation Φ using gradient descent.</s><s coords="2,235.97,310.99,50.39,8.64;2,50.11,322.63,160.68,9.65;2,210.80,321.05,4.08,6.12;2,217.94,322.95,68.42,8.64;2,50.11,334.90,236.25,8.64;2,50.11,346.54,236.25,9.65;2,50.11,358.81,21.31,8.64">Given a feature vector Φ 0 , they seek for an image x * which minimizes a loss function -the squared Euclidean distance between Φ 0 and Φ(x) plus a regularizer enforcing a natural image prior.</s><s coords="2,75.47,358.81,210.90,8.64;2,50.11,370.77,236.25,8.64;2,50.11,382.72,192.70,8.64">This method is fundamentally different from our approach in that it optimizes the difference between the feature vectors, not the image reconstruction error.</s><s coords="2,247.62,382.72,38.74,8.64;2,50.11,394.68,236.25,8.64;2,50.11,406.63,210.67,8.64">Additionally, it includes a hand-designed natural image prior, while in our case the network implicitly learns such a prior.</s><s coords="2,263.83,406.63,22.54,8.64;2,50.11,418.59,236.25,8.64;2,50.11,430.54,236.25,8.64;2,50.11,442.50,236.25,8.64;2,50.11,454.45,33.33,8.64">Technically, it involves optimization at test time, which requires computing the gradient of the feature representation and makes it relatively slow (the authors report 6s per image on a GPU).</s><s coords="2,86.62,454.45,199.75,8.64;2,50.11,466.41,148.63,8.64">In contrast, the presented approach is only costly when training the inversion network.</s><s coords="2,203.60,466.41,82.77,8.64;2,50.11,478.37,236.25,8.64;2,50.11,490.00,236.25,8.96;2,50.11,502.28,29.08,8.64">Reconstruction from a given feature vector just requires a single forward pass through the network, which takes roughly 5ms per image on a GPU.</s><s coords="2,81.43,502.28,204.93,8.64;2,50.11,514.23,236.25,8.64;2,50.11,526.19,236.25,8.64;2,50.11,538.14,107.35,8.64">The method of <ref type="bibr" coords="2,141.83,502.28,16.60,8.64" target="#b18">[19]</ref> requires gradients of the feature representation, therefore it could not be directly applied to non-differentiable representations such as LBP, or recordings from a real brain <ref type="bibr" coords="2,138.37,538.14,15.27,8.64" target="#b19">[20]</ref>.</s></p><p><s coords="2,62.07,550.89,224.30,8.64;2,50.11,562.85,236.25,8.64;2,50.11,574.80,236.25,8.64;2,50.11,586.76,164.90,8.64">There has been research on inverting various traditional computer vision representations: HOG and dense SIFT <ref type="bibr" coords="2,75.00,574.80,15.27,8.64" target="#b23">[24]</ref>, keypoint-based SIFT <ref type="bibr" coords="2,188.86,574.80,15.27,8.64" target="#b25">[26]</ref>, Local Binary Descriptors <ref type="bibr" coords="2,88.28,586.76,10.58,8.64" target="#b3">[4]</ref>, Bag-of-Visual-Words <ref type="bibr" coords="2,195.92,586.76,15.27,8.64" target="#b10">[11]</ref>.</s><s coords="2,222.20,586.76,64.17,8.64;2,50.11,598.71,236.25,8.64;2,50.11,610.67,236.25,8.64;2,50.11,622.62,208.46,8.64">All these methods are either tailored for inverting a specific feature representation or restricted to shallow representations, while our method can be applied to any feature representation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." coords="2,50.11,647.16,51.80,10.75">Method</head><p><s coords="2,62.07,668.30,224.30,8.99;2,50.11,680.60,236.25,8.64;2,50.11,692.21,182.89,8.99">Denote by (x, φ) random variables representing a natural image and its feature vector, and denote their joint probability distribution by p(x, φ) = p(x)p(φ|x).</s><s coords="2,236.38,692.24,49.99,8.96;2,50.11,704.20,236.25,8.96;2,308.86,75.48,153.14,8.64">Here p(x) is the distribution of natural images and p(φ|x) is the distribu-tion of feature vectors given an image.</s><s coords="2,465.17,75.13,79.95,8.99;2,308.86,87.08,147.20,8.99">As a special case, φ may be a deterministic function of x.</s><s coords="2,459.11,87.43,86.01,8.64;2,308.86,99.07,236.25,8.96;2,308.86,111.34,48.69,8.64">Ideally we would like to find p(x|φ), but direct application of Bayes' theorem is not feasible.</s><s coords="2,360.64,111.34,184.48,8.64;2,308.86,122.98,236.25,8.96;2,308.86,135.25,60.74,8.64">Therefore in this paper we resort to a point estimate f (φ) which minimizes the following mean squared error objective:</s></p><formula xml:id="formula_0" coords="2,388.96,155.12,156.16,11.72">E x,φ ||x -f (φ)|| 2<label>(1)</label></formula><p><s coords="2,308.86,179.77,226.77,8.64">The minimizer of this loss is the conditional expectation:</s></p><formula xml:id="formula_1" coords="2,376.88,199.08,168.23,13.22">f (φ 0 ) = E x [x | φ = φ 0 ],<label>(2)</label></formula><p><s coords="2,308.86,224.10,123.73,8.82">that is, the expected pre-image.</s></p><p><s coords="2,320.82,236.41,224.30,8.64;2,308.86,248.02,27.53,9.68;2,336.39,252.52,2.82,6.12;2,339.71,248.02,205.41,8.99;2,308.86,259.97,236.25,8.99;2,308.86,272.27,47.04,8.64">Given a training set of images and their features {x i , φ i }, we learn the weights w of an an up-convolutional network f (φ, w) to minimize a Monte-Carlo estimate of the loss (1):</s></p><formula xml:id="formula_2" coords="2,353.07,293.41,192.04,21.98">ŵ = arg min w i ||x i -f (φ i , w)|| 2 2 .<label>(3)</label></formula><p><s coords="2,308.86,326.65,236.25,8.64;2,308.86,338.60,236.25,8.64;2,308.86,350.56,71.37,8.64">This means that simply training the network to predict images from their feature vectors results in estimating the expected pre-image.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1." coords="2,308.86,369.92,172.07,9.85">Feature representations to invert</head><p><s coords="2,320.82,388.71,73.30,8.96">Shallow features.</s><s coords="2,398.80,389.09,146.31,8.64;2,308.86,401.05,236.25,8.64;2,308.86,413.00,236.25,8.64;2,308.86,424.96,113.33,8.64">We invert three traditional computer vision feature representations: histogram of oriented gradients (HOG), scale invariant feature transform (SIFT), and local binary patterns (LBP).</s><s coords="2,425.39,424.96,119.72,8.64;2,308.86,436.91,28.49,8.64">We chose these features for a reason.</s><s coords="2,341.14,436.91,203.98,8.64;2,308.86,448.87,127.73,8.64">There has been work on inverting HOG, so we can compare to existing approaches.</s><s coords="2,439.63,448.87,105.48,8.64;2,308.86,460.83,236.25,8.64;2,308.86,472.78,64.45,8.64">LBP is interesting because it is not differentiable, and hence gradient-based methods cannot invert it.</s><s coords="2,379.68,472.78,165.44,8.64;2,308.86,484.74,236.25,8.64;2,308.86,496.69,58.67,8.64">SIFT is a keypoint-based representation, so the network has to stitch different keypoints into a single smooth image.</s></p><p><s coords="2,320.82,508.82,224.30,8.64;2,308.86,520.59,186.96,8.82">For all three methods we use implementations from the VLFeat library <ref type="bibr" coords="2,372.77,520.77,16.60,8.64" target="#b22">[23]</ref> with the default settings.</s><s coords="2,503.48,520.77,41.63,8.64;2,308.86,532.73,236.25,8.64;2,308.86,544.36,236.25,8.96;2,308.86,556.64,236.25,8.64;2,308.86,568.27,215.51,8.96">More precisely, we use the HOG version from Felzenszwalb et al. <ref type="bibr" coords="2,533.50,532.73,11.62,8.64" target="#b6">[7]</ref> with cell size 8, the version of SIFT which is very similar to the original implementation of Lowe <ref type="bibr" coords="2,473.47,556.64,16.60,8.64" target="#b16">[17]</ref> and the LBP version similar to Ojala et al. <ref type="bibr" coords="2,433.40,568.59,16.60,8.64" target="#b20">[21]</ref> with cell size 16.</s><s coords="2,530.72,568.59,14.39,8.64;2,308.86,580.55,236.25,8.64">Before extracting the features we convert images to grayscale.</s><s coords="2,308.86,592.50,227.72,8.64">More details can be found in the supplementary material.</s></p><p><s coords="2,320.82,604.24,36.79,8.96">AlexNet.</s><s coords="2,370.23,604.63,174.87,8.64;2,308.86,616.58,236.25,8.64;2,308.86,628.36,91.63,8.82;2,402.16,626.87,3.49,6.05;2,410.29,628.22,134.82,8.96;2,308.86,640.17,236.25,8.96;2,308.86,652.45,236.25,8.64;2,308.86,664.40,146.75,8.64">We also invert the representation of the AlexNet network <ref type="bibr" coords="2,383.84,616.58,16.60,8.64" target="#b12">[13]</ref> trained on ImageNet, available at the Caffe <ref type="bibr" coords="2,348.13,628.54,16.60,8.64" target="#b9">[10]</ref> website. <ref type="foot" coords="2,402.16,626.87,3.49,6.05" target="#foot_0">1</ref> It consists of 5 convolutional layers and 3 fully connected layers, with rectified linear units (ReLUs) after each layer, and local contrast normalization or max-pooling after some of them.</s><s coords="2,461.75,664.40,83.37,8.64;2,308.86,676.36,157.87,8.64">Exact architecture is shown in the supplementary material.</s><s coords="2,476.12,676.36,68.99,8.64;3,50.11,75.48,236.25,8.64;3,50.11,87.43,128.40,8.64">In what follows, when we say 'output of the layer', we mean the output of the last processing step of this layer.</s><s coords="3,181.52,87.43,104.85,8.64;3,50.11,99.39,236.25,8.64;3,50.11,111.34,236.25,8.64;3,50.11,123.30,183.87,8.64">For example, the output of the first convolutional layer CONV1 would be the result after ReLU, pooling and normalization, and the output of the first fully connected layer FC6 is after ReLU.</s><s coords="3,237.35,123.30,49.01,8.64;3,50.11,135.25,132.68,8.64">FC8 denotes the last layer, before the softmax.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2." coords="3,50.11,156.41,184.56,9.85">Network architectures and training</head><p><s coords="3,62.07,176.18,224.30,8.64;3,50.11,188.14,236.25,8.64;3,50.11,200.09,40.17,8.64">An up-convolutional layer, also often referred to as 'deconvolutional', is a combination of upsampling and convolution <ref type="bibr" coords="3,76.17,200.09,10.58,8.64" target="#b5">[6]</ref>.</s><s coords="3,94.32,199.77,192.05,8.96;3,50.11,211.73,236.25,8.96;3,50.11,224.00,219.45,8.64">We upsample a feature map by a factor 2 by replacing each value by a 2 × 2 block with the original value in the top left corner and all other entries equal to zero.</s><s coords="3,272.74,224.00,13.63,8.64;3,50.11,235.96,236.25,8.64;3,50.11,247.91,41.43,8.64">Architecture of one of our up-convolutional networks is shown in Table <ref type="table" coords="3,84.07,247.91,3.74,8.64" target="#tab_0">1</ref>.</s><s coords="3,94.53,247.91,191.84,8.64;3,50.11,259.87,96.29,8.64">Architectures of other networks are shown in the supplementary material.</s></p><p><s coords="3,62.07,272.20,67.76,8.96">HOG and LBP.</s><s coords="3,133.66,272.27,152.71,8.96;3,50.11,284.55,236.25,8.64;3,50.11,296.18,236.25,8.96;3,50.11,308.46,49.58,8.64">For an image of size W × H, HOG and LBP features of an image form 3-dimensional arrays of sizes W/8 × H/8 × 31 and W/16 × H/16 × 58, respectively.</s><s coords="3,104.48,308.46,181.89,8.64;3,50.11,320.41,132.26,8.64">We use similar CNN architectures for inverting both feature representations.</s><s coords="3,189.34,320.41,97.03,8.64;3,50.11,332.37,236.25,8.64;3,50.11,344.00,236.25,8.96;3,50.11,355.96,236.25,8.96;3,50.11,368.23,26.84,8.64">The networks include a contracting part, which processes the input features through a series of convolutional layers with occasional stride of 2, resulting in a feature map 64 times smaller than the input image.</s><s coords="3,82.51,368.23,203.85,8.64;3,50.11,380.19,236.25,8.64;3,50.11,392.14,123.98,8.64">Then the expanding part of the network again upsamples the feature map to the full image resolution by a series of up-convolutional layers.</s><s coords="3,177.25,392.14,109.11,8.64;3,50.11,404.10,236.25,8.64;3,50.11,416.05,64.71,8.64">The contracting part allows the network to aggregate information over large regions of the input image.</s><s coords="3,118.25,416.05,168.12,8.64;3,50.11,428.01,129.50,8.64">We found this is necessary to successfully estimate the absolute brightness.</s></p><p><s coords="3,62.07,440.34,56.27,8.96">Sparse SIFT.</s><s coords="3,122.08,440.73,164.27,8.64;3,50.11,452.37,236.25,8.96;3,50.11,464.32,236.25,9.65;3,50.11,476.25,236.25,9.68;3,50.11,488.23,236.25,8.96;3,50.11,500.51,98.44,8.64">Running the SIFT detector and descriptor on an image gives a set of N keypoints, where the i-th keypoint is described by its coordinates (x i , y i ), scale s i , orientation α i , and a feature descriptor f i of dimensionality D. In order to apply a convolutional network, we arrange the keypoints on a grid.</s><s coords="3,155.31,500.51,131.06,8.64;3,50.11,512.14,236.25,8.96;3,54.54,524.10,90.70,8.96">We split the image into cells of size d × d (we used d = 4 in our experiments), this yields W/d × H/d cells.</s><s coords="3,152.48,524.42,133.89,8.64;3,50.11,536.37,216.00,8.64">In the rare cases when there are several keypoints in a cell, we randomly select one.</s><s coords="3,273.33,536.37,13.03,8.64;3,50.11,548.33,236.25,8.64;3,50.11,559.97,235.75,9.65;3,50.11,571.92,215.50,9.65">We then assign a vector to each of the cells: a zero vector to a cell without a keypoint and a vector (f i , x i mod d, y i mod d, sin α i , cos α i , log s i ) to a cell with a keypoint.</s><s coords="3,268.65,572.24,17.71,8.64;3,50.11,584.20,91.08,8.64">This results in a feature map</s></p><formula xml:id="formula_3" coords="3,143.21,583.85,143.15,8.99">F of size W/d × H/d ×(D+5).</formula><p><s coords="3,50.11,595.80,192.00,8.99">Then we apply a CNN to F, as described above.</s></p><p><s coords="3,62.07,608.48,36.79,8.96">AlexNet.</s><s coords="3,102.37,608.87,184.00,8.64;3,50.11,620.83,105.47,8.64">To reconstruct from each layer of AlexNet we trained a separate network.</s><s coords="3,158.48,620.83,127.89,8.64;3,50.11,632.78,236.25,8.64;3,50.11,644.74,169.73,8.64">We used two basic architectures: one for reconstructing from convolutional layers and one for reconstructing from fully connected layers.</s><s coords="3,222.76,644.74,63.60,8.64;3,50.11,656.69,236.25,8.64;3,50.11,668.33,236.25,8.96;3,50.11,680.60,69.14,8.64">The network for reconstructing from fully connected layers contains three fully connected layers and 5 up-convolutional layers, as shown in Table <ref type="table" coords="3,111.78,680.60,3.74,8.64" target="#tab_0">1</ref>.</s><s coords="3,122.20,680.60,164.16,8.64;3,50.11,692.56,236.25,8.64;3,50.11,704.51,236.25,8.64">The network for reconstructing from convolutional layers consists of three convolutional and several up-convolutional layers (the exact number depends on the</s></p><formula xml:id="formula_4" coords="3,308.86,271.60,236.25,20.91">(x) = x if x 0 and r(x) = 0.2 • x if x &lt; 0.</formula><p><s coords="3,320.82,295.93,69.64,8.96">Training details.</s><s coords="3,393.75,296.31,151.36,8.64;3,308.86,308.27,89.12,8.64">We trained networks using a modified version of Caffe <ref type="bibr" coords="3,378.89,308.27,15.27,8.64" target="#b9">[10]</ref>.</s><s coords="3,404.25,308.27,140.86,8.64;3,308.86,320.22,89.89,8.64">As training data we used the Ima-geNet <ref type="bibr" coords="3,335.96,320.22,11.62,8.64" target="#b4">[5]</ref> training set.</s><s coords="3,404.31,320.22,140.80,8.64;3,308.86,332.18,177.58,8.64">In some cases we predicted downsampled images to speed up computations.</s><s coords="3,493.82,332.18,51.30,8.64;3,308.86,343.82,236.25,9.65;3,308.86,355.77,54.67,8.96">We used the Adam <ref type="bibr" coords="3,335.79,344.14,16.60,8.64" target="#b11">[12]</ref> optimizer with β 1 = 0.9, β 2 = 0.999 and minibatch size 64.</s><s coords="3,366.87,356.09,178.24,8.64;3,308.86,367.73,135.13,8.96">For most networks we found an initial learning rate λ = 0.001 to work well.</s><s coords="3,448.95,368.05,96.16,8.64;3,308.86,380.00,172.57,8.64">We gradually decreased the learning rate towards the end of training.</s><s coords="3,484.32,380.00,60.79,8.64;3,308.86,391.64,236.25,8.96;3,308.86,403.59,236.25,8.96;3,308.86,415.87,49.79,8.64">The duration of training depended on the network: from 15 epochs (passes through the dataset) for shallower networks to 60 epochs for deeper ones.</s></p><p><s coords="3,320.82,427.92,103.14,8.96">Quantitative evaluation.</s><s coords="3,428.97,428.31,116.14,8.64;3,308.86,440.27,236.25,8.64;3,308.86,451.90,236.25,9.65;3,308.86,463.86,236.25,9.65;3,308.86,475.81,236.25,8.96;3,308.86,488.09,236.25,8.64;3,308.86,500.04,90.25,8.64">As a quantitative measure of performance we used the average normalized reconstruction error, that is the mean of ||x i -f (Φ(x i ))|| 2 /N , where x i is an example from the test set, f is the function implemented by the inversion network and N is a normalization coefficient equal to the average Euclidean distance between images in the test set.</s><s coords="3,405.75,500.04,139.37,8.64;3,308.86,512.00,236.25,8.64;3,308.86,523.95,55.65,8.64">The test set we used for quantitative and qualitative evaluations is a subset of the ImageNet validation set.</s><s coords="3,537.64,592.66,7.47,8.64;3,308.86,604.61,236.25,8.64;3,308.86,616.57,37.90,8.64"><ref type="table" coords="3,537.64,592.66,3.74,8.64" target="#tab_1">2</ref>. Clearly, our method significantly outperforms existing approaches.</s><s coords="3,351.16,616.57,193.95,8.64;3,308.86,628.52,182.91,8.64">This is to be expected, since our method explicitly aims to minimize the reconstruction error.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." coords="3,308.86,547.57,204.72,10.75">Experiments: shallow representations</head><p><s coords="3,315.24,653.89,75.01,7.17;4,62.07,268.53,55.62,8.96">Hoggles <ref type="bibr" coords="3,343.79,654.14,13.28,6.91" target="#b23">[24]</ref> HOG  Colorization.</s><s coords="4,120.67,268.92,165.70,8.64;4,50.11,280.87,236.25,8.64;4,50.11,292.83,166.65,8.64">As mentioned above, we compute the features based on grayscale images, but the task of the networks is to reconstruct the color images.</s><s coords="4,222.58,292.83,63.78,8.64;4,50.11,304.79,236.25,8.64;4,50.11,316.74,236.25,8.64;4,50.11,328.70,223.84,8.64">The features do not contain any color information, so to predict colors the network has to analyze the content of the image and make use of a natural image prior it learned during training.</s><s coords="4,280.28,328.70,6.09,8.64;4,50.11,340.65,236.25,8.64;4,50.11,352.61,24.77,8.64">It does successfully learn to do so, as can be seen in Figures <ref type="figure" coords="4,281.38,340.65,4.98,8.64" target="#fig_0">1</ref> and<ref type="figure" coords="4,67.41,352.61,3.74,8.64" target="#fig_1">3</ref>.</s><s coords="4,79.23,352.61,207.13,8.64;4,50.11,364.56,123.42,8.64">Quite often the colors are predicted correctly, especially for sky, sea, grass, trees.</s><s coords="4,177.76,364.56,108.60,8.64;4,50.11,376.52,236.25,8.64;4,50.11,388.47,164.13,8.64">In other cases, the network cannot predict the color (for example, people in the top row of Figure <ref type="figure" coords="4,90.58,388.47,4.15,8.64" target="#fig_1">3</ref>) and leaves some areas gray.</s><s coords="4,219.10,388.47,67.27,8.64;4,50.11,400.43,236.25,8.64;4,50.11,412.38,46.76,8.64">Occasionally the network predicts the wrong color, such as in the bottom row of Figure <ref type="figure" coords="4,89.40,412.38,3.74,8.64" target="#fig_1">3</ref>.</s></p><p><s coords="4,62.07,425.68,224.30,9.03;4,50.11,438.03,236.25,8.64;4,50.11,449.98,143.09,8.64">HOG. Figure <ref type="figure" coords="4,119.58,426.07,4.98,8.64" target="#fig_2">2</ref> shows an example image, its HOG representation, the results of inversion with existing methods <ref type="bibr" coords="4,66.59,449.98,15.77,8.64" target="#b23">[24,</ref><ref type="bibr" coords="4,84.99,449.98,13.28,8.64" target="#b18">19]</ref> and with our approach.</s><s coords="4,196.74,449.98,89.62,8.64;4,50.11,461.94,236.25,8.64;4,50.11,473.89,236.25,8.64;4,50.11,485.85,54.93,8.64">Most interestingly, the network is able to reconstruct the overall brightness of the image very well, for example the dark regions are reconstructed dark.</s><s coords="4,110.25,485.85,176.12,8.64;4,50.11,497.80,236.25,8.64;4,50.11,509.76,103.77,8.64">This is quite surprising, since the HOG descriptors are normalized and should not contain information about absolute brightness.</s></p><p><s coords="4,62.07,523.45,224.30,8.64;4,50.11,535.41,236.25,8.64;4,50.11,547.36,236.25,8.64;4,50.11,559.32,21.86,8.64">Normalization is always performed with a smoothing 'epsilon', so one might imagine that some information about the brightness is present even in the normalized features.</s><s coords="4,77.46,559.32,208.91,8.64;4,50.11,570.95,236.25,8.96;4,50.11,583.23,137.68,8.64">We checked that the network does not make use of this information: multiplying the input image by 10 or 0.1 hardly changes the reconstruction.</s><s coords="4,191.88,583.23,94.49,8.64;4,50.11,595.18,236.25,8.64;4,50.11,607.14,236.25,8.64;4,50.11,619.09,236.25,8.64;4,50.11,631.05,236.25,8.64;4,50.11,643.00,236.25,8.64;4,50.11,654.96,236.25,8.64;4,50.11,666.91,236.25,8.64;4,50.11,678.87,217.77,8.64">Therefore, we hypothesize that the network reconstructs the overall brightness by 1) analyzing the distribution of the HOG features (if in a cell there is similar amount of gradient in all directions, it is probably noise; if there is one dominating gradient, it must actually be in the image), 2) accumulating gradients over space: if there is much black-to-white gradient in one direction, then probably the brightness in that direction goes from dark to bright and 3) using semantic information.</s></p><p><s coords="4,62.07,692.17,224.30,9.03;4,50.11,704.51,158.15,8.64">SIFT. Figure <ref type="figure" coords="4,117.44,692.56,4.98,8.64">4</ref> shows an image, the detected SIFT keypoints and the resulting reconstruction.</s><s coords="4,213.54,704.51,72.83,8.64;4,326.23,273.81,24.90,8.64;4,378.88,273.81,37.35,8.64;4,438.30,273.81,36.25,8.64;4,498.28,273.81,34.04,8.64;5,50.11,502.81,150.70,8.96">There are roughly Image HOG our SIFT our LBP our  3000 keypoints detected in this image.</s><s coords="5,203.74,503.13,82.62,8.64;5,50.11,515.09,236.25,8.64;5,50.11,527.04,92.03,8.64">Although made from a sparse set of keypoints, the reconstruction looks very natural, just a little blurry.</s><s coords="5,145.80,527.04,140.56,8.64;5,50.11,539.00,236.25,8.64;5,50.11,550.95,133.36,8.64">To achieve such a clear reconstruction the network has to properly rotate and scale the descriptors and then stitch them together.</s><s coords="5,186.45,550.95,99.91,8.64;5,50.11,562.91,65.86,8.64">Obviously it successfully learns to do this.</s></p><p><s coords="5,62.07,608.87,224.30,8.64;5,50.11,620.83,236.25,8.64;5,50.11,632.78,36.80,8.64">For reference we also show a result of another existing method <ref type="bibr" coords="5,81.93,620.83,16.60,8.64" target="#b25">[26]</ref> for reconstructing images from sparse SIFT descriptors.</s><s coords="5,89.93,632.78,196.43,8.64;5,50.11,644.74,236.25,8.64;5,50.11,656.69,236.25,8.64;5,50.11,668.65,236.25,8.64;5,50.11,680.60,194.27,8.64">The results are not directly comparable: while we use the SIFT detector providing circular keypoints, Weinzaepfel et al. <ref type="bibr" coords="5,92.61,656.69,16.60,8.64" target="#b25">[26]</ref> use the Harris affine keypoint detector which yields elliptic keypoints, and the number and the locations of the keypoints may be different from our case.</s><s coords="5,248.41,680.60,37.96,8.64;5,50.11,692.56,236.25,8.64;5,50.11,704.51,97.72,8.64">However, the rough number of keypoints is the same, so a qualitative comparison is still valid.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." coords="5,308.86,501.37,125.89,10.75">Experiments: AlexNet</head><p><s coords="5,320.82,522.29,224.30,8.64;5,308.86,534.24,236.25,8.64;5,308.86,546.20,181.04,8.64">We applied our inversion method to different layers of AlexNet and performed several additional experiments to better understand the feature representations.</s><s coords="5,494.52,546.20,50.60,8.64;5,308.86,558.15,164.10,8.64">More results are shown in the supplementary material.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1." coords="5,308.86,577.68,194.28,9.85">Reconstructions from different layers</head><p><s coords="5,320.82,596.92,224.30,8.64;5,308.86,608.87,36.09,8.64">Figure <ref type="figure" coords="5,350.72,596.92,4.98,8.64">5</ref> shows reconstructions from various layers of AlexNet.</s><s coords="5,350.99,608.87,194.12,8.64;5,308.86,620.83,236.25,8.64;5,308.86,632.78,197.22,8.64">When using features from convolutional layers, the reconstructed images look very similar to the input, but lose fine details as we progress to higher layers.</s><s coords="5,511.85,632.78,33.27,8.64;5,308.86,644.74,236.25,8.64;5,309.11,656.69,61.71,8.64">There is an obvious drop in reconstruction quality when going from CONV5 to FC6.</s><s coords="5,376.15,656.69,168.97,8.64;5,308.86,668.65,236.25,8.64;5,308.86,680.60,236.25,8.64">However, the reconstructions from higher convolutional layers and even fully connected layers preserve color and the approximate object location very well.</s><s coords="5,308.86,692.56,236.25,8.64;5,308.86,704.51,96.36,8.64">Reconstructions from FC7 and FC8 still look similar to the input images, but blurry.</s><s coords="5,408.19,704.51,136.93,8.64;6,50.11,296.57,236.25,8.64;6,50.11,308.52,236.25,8.64;6,50.11,320.48,236.25,8.64;6,50.11,332.43,50.38,8.64">This means that high level features  are much less invariant to color and pose than one might expect: in principle fully connected layers need not preserve any information about colors and locations of objects in the input image.</s><s coords="6,105.27,332.43,181.09,8.64;6,50.11,344.39,121.55,8.64">This is somewhat in contrast with the results of <ref type="bibr" coords="6,61.53,344.39,15.27,8.64" target="#b18">[19]</ref>, as shown in Figure <ref type="figure" coords="6,164.19,344.39,3.74,8.64">6</ref>.</s><s coords="6,176.65,344.39,109.72,8.64;6,50.11,356.34,236.25,8.64;6,50.11,368.30,139.45,8.64">While their reconstructions are sharper, the color and position are completely lost in reconstructions from higher layers.</s></p><p><s coords="6,62.07,380.25,224.30,8.64;6,50.11,392.21,236.25,8.64;6,50.11,404.16,78.37,8.64">For quantitative evaluation before computing the error we up-sample reconstructions to input image size with bilinear interpolation.</s><s coords="6,131.49,404.16,154.88,8.64;6,50.11,416.12,119.42,8.64">Error curves shown in Figure <ref type="figure" coords="6,249.22,404.16,4.98,8.64" target="#fig_6">7</ref> support the conclusions made above.</s><s coords="6,177.78,416.12,108.58,8.64;6,50.36,428.07,236.00,8.64">When reconstructing from FC6, the error is roughly twice as large as from CONV5.</s><s coords="6,50.11,440.03,236.25,8.64;6,50.11,451.98,236.25,8.64;6,50.11,463.94,173.32,8.64">Even when reconstructing from FC8, the error is fairly low because the network manages to get the color and the rough placement of large objects in images right.</s><s coords="6,228.47,463.94,57.89,8.64;6,50.11,475.89,236.25,8.64;6,50.11,487.85,236.25,8.64;6,50.11,499.80,53.16,8.64">For lower layers, the reconstruction error of <ref type="bibr" coords="6,172.25,475.89,16.60,8.64" target="#b18">[19]</ref> is still much higher than of our method, even though visually the images look somewhat sharper.</s><s coords="6,106.49,499.80,179.87,8.64;6,50.11,511.76,236.25,8.64;6,50.11,523.71,236.25,8.64;6,50.11,535.67,21.30,8.64">The reason is that in their reconstructions the color and the precise placement of small details do not perfectly match the input image, which results in a large overall error.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2." coords="6,50.11,554.00,119.39,9.85">Autoencoder training</head><p><s coords="6,62.07,573.01,224.30,8.64;6,50.11,584.96,166.02,8.64">Our inversion network can be interpreted as the decoder of the representation encoded by AlexNet.</s><s coords="6,219.05,584.96,67.32,8.64;6,50.11,596.92,236.25,8.64;6,50.11,608.87,102.43,8.64">The difference to an autoencoder is that the encoder part stays fixed and only the decoder is optimized.</s><s coords="6,157.88,608.87,128.49,8.64;6,50.11,620.83,236.25,8.64;6,50.11,632.78,236.25,8.64;6,50.11,644.74,132.12,8.64">For comparison we also trained autoencoders with the same architecture as our reconstruction nets, i.e., we also allowed the training to fine-tune the parameters of the AlexNet part.</s><s coords="6,190.13,644.74,96.24,8.64;6,50.11,656.69,236.25,8.64;6,50.11,668.65,199.66,8.64">This provides an upper bound on the quality of reconstructions we might expect from the inversion networks (with fixed AlexNet).</s></p><p><s coords="6,62.07,680.60,224.30,8.64;6,50.11,692.56,236.25,8.64;6,50.11,704.51,54.53,8.64">As shown in Figure <ref type="figure" coords="6,156.59,680.60,3.74,8.64" target="#fig_6">7</ref>, autoencoder training yields much lower reconstruction errors when reconstructing from higher layers.</s><s coords="6,108.63,704.51,177.73,8.64;6,320.82,460.53,224.30,8.64;6,308.86,472.49,236.25,8.64;6,308.86,484.44,236.25,8.64;6,308.86,496.40,119.39,8.64">Also the qualitative results in Figure <ref type="figure" coords="6,257.81,704.51,4.98,8.64">6</ref> show An interesting observation with autoencoders is that the reconstruction error is quite high even when reconstructing from CONV1 features, and the best reconstructions were actually obtained from CONV4.</s><s coords="6,434.87,496.40,110.25,8.64;6,308.86,508.03,236.25,8.96;6,309.11,520.31,202.25,8.64">Our explanation is that the convolution with stride 4 and consequent max-pooling in CONV1 loses much information about the image.</s><s coords="6,518.34,520.31,26.78,8.64;6,308.86,532.26,236.25,8.64;6,308.86,544.22,236.25,8.64;6,308.86,556.17,19.10,8.64">To decrease the reconstruction error, it is beneficial for the network to slightly blur the image instead of guessing the details.</s><s coords="6,332.37,556.17,212.74,8.64;6,308.86,568.13,236.25,8.64;6,308.86,580.08,192.85,8.64">When reconstructing from deeper layers, deeper networks can learn a better prior resulting in slightly sharper images and slightly lower reconstruction error.</s><s coords="6,509.09,580.08,36.02,8.64;6,308.86,592.04,236.25,8.64;6,308.86,603.99,102.42,8.64">For even deeper layers, the representation gets too compressed and the error increases again.</s><s coords="6,417.27,603.99,127.85,8.64;6,308.86,615.63,236.24,8.96;6,308.86,627.90,169.91,8.64">We observed (not shown in the paper) that without stride 4 in the first layer, the reconstruction error of autoencoders got much lower.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3." coords="6,308.86,648.92,143.48,9.85">Case study: Colored apple</head><p><s coords="6,320.82,668.65,224.30,8.64;6,308.86,680.60,236.25,8.64;6,308.86,692.56,134.53,8.64">We performed a simple experiment illustrating how the color information influences classification and how it is preserved in the high level features.</s><s coords="6,449.84,692.56,95.27,8.64;6,308.86,704.51,79.38,8.64;7,50.11,232.42,118.90,8.64">We took an image of a red apple (Figure <ref type="figure" coords="6,383.26,704.51,4.98,8.64" target="#fig_7">8</ref>   hue to make it green or blue.</s><s coords="7,174.47,232.42,111.90,8.64;7,50.36,244.37,150.54,8.64">Then we extracted AlexNet FC8 features of the resulting images.</s><s coords="7,207.01,244.37,79.35,8.64;7,50.11,256.33,236.25,8.64;7,50.11,268.28,236.25,8.64;7,50.11,280.24,52.30,8.64">Remind that FC8 is the last layer of the network, so the FC8 features, after application of softmax, give the network's prediction of class probabilities.</s><s coords="7,106.79,280.24,179.58,8.64;7,50.11,292.19,175.78,8.64">The largest activation, hence, corresponds to the network's prediction of the image class.</s><s coords="7,230.18,292.19,56.19,8.64;7,50.11,304.15,236.25,8.64;7,50.11,316.10,236.25,8.64;7,50.11,328.06,236.25,8.64;7,50.11,339.70,236.25,8.96;7,50.11,351.97,19.64,8.64">To check how class-dependent the results of inversion are, we passed three versions of each feature vector through the inversion network: 1) just the vector itself, 2) all activations except the 5 largest ones set to zero, 3) the 5 largest activations set to zero.</s><s coords="7,62.07,364.57,132.30,8.64">This leads to several conclusions.</s><s coords="7,197.38,364.57,88.99,8.64;7,50.11,376.52,236.25,8.64;7,50.11,388.48,236.25,8.64;7,50.11,400.43,46.78,8.64">First, color clearly can be very important for classification, so the feature representation of the network has to be sensitive to it, at least in some cases.</s><s coords="7,99.91,400.43,186.46,8.64;7,50.11,412.39,236.25,8.64;7,50.11,424.34,101.78,8.64">Second, the color of the image can be precisely reconstructed even from FC8 or, equivalently, from the predicted class probabilities.</s><s coords="7,156.15,424.34,130.21,8.64;7,50.11,436.30,236.25,8.64;7,50.11,448.25,215.63,8.64">Third, the reconstruction quality does not depend much on the top predictions of the network but rather on the small probabilities of all other classes.</s><s coords="7,268.65,448.25,17.71,8.64;7,50.11,460.21,236.25,8.64;7,50.11,472.16,236.25,8.64;7,50.11,484.12,121.85,8.64">This is consistent with the 'dark knowledge' idea of <ref type="bibr" coords="7,245.57,460.21,10.79,8.64" target="#b7">[8]</ref>: small probabilities of non-predicted classes carry more information than the prediction itself.</s><s coords="7,178.24,484.12,108.12,8.64;7,50.11,496.07,149.45,8.64">More examples of this are shown in the supplementary material.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4." coords="7,50.11,516.85,207.68,9.85">Robustness of the feature representation</head><p><s coords="7,62.07,536.50,224.30,8.64;7,50.11,548.46,116.95,8.64">We have shown that high level feature maps preserve rich information about the image.</s><s coords="7,171.44,548.46,114.93,8.64;7,50.11,560.41,122.04,8.64">How is this information represented in the feature vector?</s><s coords="7,176.85,560.41,109.52,8.64;7,50.11,572.37,236.25,8.64;7,50.11,584.32,236.25,8.64;7,50.11,596.28,224.89,8.64">It is difficult to answer this question precisely, but we can gain some insight by perturbing the feature representations in certain ways and observing images reconstructed from these perturbed features.</s><s coords="7,279.73,596.28,6.64,8.64;7,50.11,608.23,236.25,8.64;7,50.11,620.19,236.25,8.64;7,50.11,632.14,30.71,8.64">If perturbing the features in a certain way does not change the reconstruction much, then the perturbed property is not important.</s><s coords="7,85.97,632.14,200.40,8.64;7,50.11,644.10,236.25,8.64;7,50.11,656.05,202.81,8.64">For example, if setting a non-zero feature to zero does not change the reconstruction, then this feature does not carry information useful for the reconstruction.</s></p><p><s coords="7,62.07,668.65,145.56,8.64">We applied binarization and dropout.</s><s coords="7,210.54,668.65,75.83,8.64;7,50.11,680.60,236.25,8.64;7,50.11,692.56,236.25,8.64;7,50.11,704.51,236.25,8.64;7,308.86,232.42,236.25,8.64">To binarize the feature vector, we kept the signs of all entries and set their absolute values to a fixed number, selected such that the Euclidean norm of the vector remained unchanged (we tried several other strategies, and this one led to the best result).</s><s coords="7,308.86,244.37,236.25,8.64;7,308.86,256.33,236.25,8.64;7,308.86,268.28,85.67,8.64">For all layers except FC8, feature vector entries are nonnegative, hence, binarization just sets all non-zero entries to a fixed positive value.</s><s coords="7,397.51,268.28,147.61,8.64;7,308.86,279.92,236.25,8.96;7,308.86,292.19,236.25,8.64;7,308.86,304.15,173.25,8.64">To perform dropout, we randomly set 50% of the feature vector entries to zero and then normalize the vector to keep its Euclidean norm unchanged (again, we found this normalization to work best).</s><s coords="7,487.06,304.15,58.05,8.64;7,308.86,316.10,236.25,8.64;7,308.86,328.06,236.25,8.64;7,308.86,340.02,236.25,8.64;7,308.86,351.97,236.25,8.64;7,308.86,363.93,207.01,8.64">Qualitative results of these perturbations of features in different layers of AlexNet are shown in Figure <ref type="figure" coords="7,440.20,328.06,3.74,8.64" target="#fig_8">9</ref>. Quantitative results are shown in Figure <ref type="figure" coords="7,375.90,340.02,3.74,8.64" target="#fig_6">7</ref>. Surprisingly, dropout leads to larger decrease in reconstruction accuracy than binarization, even in the layers where it had been applied during training.</s><s coords="7,518.92,363.93,26.19,8.64;7,308.86,375.88,236.25,8.64;7,308.86,387.84,110.64,8.64">In layers FC7 and especially FC6, binarization hardly changes the reconstruction quality at all.</s><s coords="7,422.47,387.84,122.65,8.64;7,308.86,399.79,236.25,8.64;7,308.86,411.75,236.25,8.64;7,308.86,423.70,203.21,8.64">Although it is known that binarized ConvNet features perform well in classification <ref type="bibr" coords="7,522.88,399.79,10.58,8.64" target="#b0">[1]</ref>, it comes as a surprise that for reconstructing the input image the exact values of the features are not important.</s><s coords="7,517.75,423.70,27.37,8.64;7,308.86,435.66,236.25,8.64;7,308.86,447.61,236.25,8.64">In FC6 virtually all information about the image is contained in the binary code given by the pattern of non-zero activations.</s><s coords="7,308.86,459.57,236.25,8.64;7,308.86,471.52,236.25,8.64;7,308.86,483.48,236.25,8.64;7,308.86,495.43,34.02,8.64">Figures <ref type="figure" coords="7,342.44,459.57,4.98,8.64" target="#fig_6">7</ref> and<ref type="figure" coords="7,369.19,459.57,4.98,8.64" target="#fig_8">9</ref> show that this binary code only emerges when training with the classification objective and dropout, while autoencoders are very sensitive to perturbations in the features.</s></p><p><s coords="7,320.82,508.16,224.30,8.64;7,308.86,520.11,138.59,8.64">To test the robustness of this binary code, we applied binarization and dropout together.</s><s coords="7,454.09,520.11,91.03,8.64;7,308.86,531.75,236.25,8.96;7,308.86,544.02,79.14,8.64">We tried dropping out 50% random activations or 50% least non-zero activations and then binarizing.</s><s coords="7,391.04,543.70,154.06,8.96;7,308.86,555.66,236.25,8.96;7,308.86,567.93,236.25,8.64;7,308.86,579.89,62.61,8.64">Dropping out the 50% least activations reduces the error much less than dropping out 50% random activations and is even better than not applying any dropout for most layers.</s><s coords="7,375.10,579.89,170.01,8.64;7,308.86,591.53,236.25,8.96;7,308.86,603.80,236.25,8.64;7,308.86,615.44,214.88,8.96">However, layers FC6 and FC7 are the most interesting ones: here dropping out 50% random activations decreases the performance substantially, while dropping out 50% least activations only results in a small decrease.</s><s coords="7,527.40,615.75,17.71,8.64;7,308.86,627.71,236.25,8.64;7,308.86,639.66,236.25,8.64;7,308.86,651.62,104.87,8.64">Possibly the exact values of the features in FC6 and FC7 do not affect the reconstruction much, but they estimate the importance of different features.</s><s coords="8,50.11,272.48,236.25,8.64">sponding images generated by the reconstruction networks.</s><s coords="8,50.11,284.43,236.25,8.64;8,50.11,296.39,236.25,8.64;8,50.11,308.34,110.72,8.64">We have seen the reconstructions from feature vectors of actual images, but what if a feature vector was not generated from a natural image?</s><s coords="8,166.87,308.34,119.49,8.64;8,50.11,320.30,236.25,8.64;8,50.11,332.25,164.58,8.64">In Figure <ref type="figure" coords="8,208.14,308.34,9.96,8.64" target="#fig_9">10</ref> we show reconstructions obtained with our networks when interpolating between feature vectors of two images.</s><s coords="8,223.06,332.25,63.30,8.64;8,50.11,344.21,236.25,8.64;8,50.11,356.16,236.25,8.64;8,50.11,368.12,236.25,8.64;8,50.11,380.07,92.42,8.64">It is interesting to see that interpolating CONV5 features leads to a simple overlay of images, but the behavior of interpolations when reconstructing from FC6 is very different: images smoothly morph into each other.</s><s coords="8,148.39,380.07,137.97,8.64;8,50.11,392.03,236.25,8.64;8,50.11,403.98,35.14,8.64">More examples, together with the results for autoencoders, are shown in the supplementary material.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5." coords="7,308.86,672.78,213.57,9.85">Interpolation and random feature vectors</head><p><s coords="8,62.07,416.76,224.30,8.64;8,50.11,428.72,40.03,8.64">Another analysis method is by sampling feature vectors randomly.</s><s coords="8,93.18,428.72,193.18,8.64;8,50.11,440.68,236.25,8.64;8,50.11,452.63,127.15,8.64">Our networks were trained to reconstruct images given their feature representations, but the distribution of the feature vectors is unknown.</s><s coords="8,182.43,452.63,103.94,8.64;8,50.11,464.59,176.58,8.64">Hence, there is no simple principled way to sample from our model.</s><s coords="8,234.13,464.59,52.23,8.64;8,50.11,476.54,236.25,8.64;8,50.11,488.50,186.71,8.64;8,50.11,500.45,204.04,8.64">However, by assuming independence of the features (a very strong and wrong assumption!), we can approximate the of each dimension of the feature vector separately.</s><s coords="8,258.71,500.45,27.66,8.64;8,50.11,512.41,236.25,8.64;8,50.11,524.04,185.22,8.96">To this end we simply computed a histogram of each feature over a set of 4096 images and sampled from those.</s><s coords="8,239.49,524.36,46.86,8.64;8,50.11,536.32,236.25,8.64;8,50.11,548.27,112.07,8.64">We ensured that the sparsity of the random samples is the same as that of the actual feature vectors.</s><s coords="8,165.20,548.27,121.16,8.64;8,50.11,560.23,236.25,8.64;8,50.11,572.18,236.25,8.64;8,50.11,584.14,49.28,8.64">This procedure led to low contrast images, perhaps because by independently sampling each dimension we did not introduce interactions between the features.</s><s coords="8,104.24,584.14,182.13,8.64;8,50.11,595.77,236.25,8.96;8,50.11,608.05,139.71,8.64">Multiplying the feature vectors by a constant factor α = 2 increases the contrast without affecting other properties of the generated images.</s></p><p><s coords="8,62.07,620.83,224.30,8.64;8,50.11,632.78,145.44,8.64">Random samples obtained this way from four top layers of AlexNet are shown in Figure <ref type="figure" coords="8,183.10,632.78,8.30,8.64" target="#fig_10">11</ref>.</s><s coords="8,200.88,632.78,85.49,8.64;8,50.11,644.74,43.98,8.64">No pre-selection was performed.</s><s coords="8,97.14,644.74,189.22,8.64;8,50.11,656.69,236.25,8.64;8,50.11,668.65,84.27,8.64">While samples from CONV5 look much like abstract art, the samples from fully convolutional layers are much more realistic.</s><s coords="8,141.45,668.65,144.92,8.64;8,50.11,680.60,236.25,8.64;8,50.11,692.56,236.25,8.64;8,50.11,704.51,17.43,8.64">This shows that the networks learn a natural image prior that allows them to produce somewhat realistically looking images from random feature vectors.</s><s coords="8,71.58,704.51,214.78,8.64;8,308.86,254.02,236.25,8.64;8,308.86,265.98,209.50,8.64">We found that a much simpler sampling procedure of fitting a single shifted truncated Gaussian to all feature dimensions produces qualitatively very similar images.</s><s coords="8,521.32,265.98,23.79,8.64;8,308.86,277.93,236.25,8.64;8,308.86,289.89,236.25,8.64;8,308.86,301.84,78.21,8.64">These are shown in the supplementary material together with images generated from autoencoders, which look much less like natural images.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." coords="8,308.86,334.01,73.74,10.75">Conclusions</head><p><s coords="8,320.82,358.05,224.30,8.64;8,308.86,370.00,236.25,8.64;8,308.86,381.96,236.25,8.64;8,308.86,393.91,236.25,8.64;8,308.86,405.87,25.19,8.64">We have proposed to invert image representations with up-convolutional networks and have shown that this yields more or less accurate reconstructions of the original images, depending on the level of invariance of the feature representation.</s><s coords="8,339.54,405.87,205.58,8.64;8,308.86,417.82,236.25,8.64;8,308.86,429.78,236.25,8.64;8,308.86,441.73,89.22,8.64">The networks implicitly learn natural image priors which allow the retrieval of information that is obviously lost in the feature representation, such as color or brightness in HOG or SIFT.</s><s coords="8,401.15,441.73,143.97,8.64;8,308.86,453.69,236.25,8.64;8,308.86,465.64,75.24,8.64">The method is very fast at test time and does not require the gradient of the feature representation to be inverted.</s><s coords="8,387.71,465.64,157.41,8.64;8,308.86,477.60,102.49,8.64">Therefore, it can be applied to virtually any image representation.</s><s coords="8,320.82,492.89,224.30,8.64;8,308.86,504.85,236.25,8.64;8,308.86,516.81,236.25,8.64;8,308.86,528.76,236.25,8.64;8,308.86,540.72,236.25,8.64;8,308.86,552.67,236.25,8.64;8,308.86,564.63,236.25,8.64;8,308.86,576.58,236.25,8.64;8,308.86,588.54,236.25,8.64;8,308.86,600.49,130.79,8.64">Application of our method to the representations learned by the AlexNet convolutional network leads do several conclusions: 1) Features from all layers of the network, including the final FC8 layer, preserve the precise colors and the rough position of objects in the image; 2) In higher layers, almost all information about the input image is contained in the pattern of non-zero activations, not their precise values; 3) In the layer FC8, most information about the input image is contained in small probabilities of those classes that are not in top-5 network predictions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,50.11,73.71,124.53,10.75">Supplementary material</head><p><s coords="10,50.11,94.91,236.25,9.03;10,50.11,107.26,36.09,8.64">Network architectures Table <ref type="table" coords="10,179.38,95.30,4.98,8.64" target="#tab_7">3</ref> shows the architecture of AlexNet.</s><s coords="10,94.13,107.26,192.24,8.64;10,50.11,119.21,163.76,8.64">Tables <ref type="table" coords="10,123.99,107.26,4.43,8.64">4</ref><ref type="table" coords="10,128.42,107.26,4.43,8.64" target="#tab_3">5</ref><ref type="table" coords="10,128.42,107.26,4.43,8.64" target="#tab_4">6</ref><ref type="table" coords="10,128.42,107.26,4.43,8.64" target="#tab_5">7</ref><ref type="table" coords="10,132.85,107.26,4.43,8.64" target="#tab_6">8</ref>show the architectures of networks we used for inverting different features.</s><s coords="10,220.76,119.21,65.60,8.64;10,50.11,131.17,236.25,8.64;10,50.11,143.12,77.27,8.64">After each fully connected and convolutional layer there is always a leaky ReLU nonlinearity.</s><s coords="10,131.80,143.12,154.57,8.64;10,50.11,155.08,72.01,8.64">Networks for inverting HOG and LBP have two streams.</s><s coords="10,126.40,155.08,159.97,8.64;10,50.11,167.03,236.25,8.64">Stream A compresses the input features spatially and accumulates information over large regions.</s><s coords="10,50.11,178.99,39.64,8.64;10,110.89,178.99,175.47,8.64;10,50.11,190.94,97.08,8.64">We found crucial to get good estimates of the overall brightness of the image.</s><s coords="10,151.60,190.94,134.76,8.64;10,50.11,202.90,219.85,8.64">Stream B does not compress spatially and hence can better preserve fine local details.</s><s coords="10,276.40,202.90,9.96,8.64;10,50.11,214.85,236.25,8.64;10,50.11,226.81,154.72,8.64">At one points the outputs of the two streams are concatenated and processed jointly, denoted by "J".</s><s coords="10,208.09,226.81,78.28,8.64;10,50.11,238.76,67.24,8.64">K stands for kernel size, S for stride.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,50.11,269.83,100.98,8.96">Shallow features details</head><p><s coords="10,161.06,270.21,125.30,8.64;10,50.11,281.99,236.25,8.82;10,50.11,294.12,151.15,8.64">As mentioned, in the paper, for all three methods we use implementations from the VLFeat library <ref type="bibr" coords="10,80.19,294.12,16.60,8.64" target="#b22">[23]</ref> with the default settings.</s><s coords="10,207.44,294.12,78.92,8.64;10,50.11,305.76,192.52,8.96">We use the Felzenszwalb et al. version of HOG with cell size 8.</s><s coords="10,248.71,306.08,37.65,8.64;10,50.11,317.72,236.25,8.96;10,50.11,329.99,236.25,8.64;10,50.11,341.95,236.25,8.64;10,50.11,353.90,36.52,8.64">For SIFT we used 3 levels per octave, the first octave was 0 (corresponding to full resolution), the number of octaves was set automatically, effectively searching keypoints of all possible sizes.</s></p><formula xml:id="formula_5" coords="10,56.06,383.98,224.35,127.44">Layer Input InSize K S OutSize convA1 HOG 32×32×31 5 2 16×16×256 convA2 convA1 16×16×256 5 2 8×8×512 convA3 convA2 8×8×512 3 2 4×4×1024 upconvA1 convA3 4×4×1024 4 2 8×8×512 upconvA2 upconvA1 8×8×512 4 2 16×16×256 upconvA3 upconvA2 16×16×256 4 2 32×32×128 convB1 HOG 32×32×31 5 1 32×32×128 convB2 convB1 32×32×128 3 1 32×32×128 convJ1 {upconvA3, convB2} 32×32×256 3 1 32×32×256 convJ2 convJ1 32×32×256 3 1 32×32×128 upconvJ4 convJ2 32×32×128 4 2 64×64×64 upconvJ5 upconvJ4 64×64×64 4 2 128×128×32 upconvJ6 upconvJ5 128×128×32 4 2 256×256×3</formula><p><s coords="10,55.23,526.14,226.01,8.64">Table <ref type="table" coords="10,79.61,526.14,3.88,8.64">4</ref>: Network for reconstructing from HOG features.</s><s coords="11,81.87,656.69,204.49,8.64;11,50.11,668.65,236.25,8.64">Figure <ref type="figure" coords="11,109.89,656.69,9.96,8.64" target="#fig_13">14</ref> shows results illustrating the 'dark knowledge' hypothesis, similar to Figure <ref type="figure" coords="11,193.85,668.65,4.98,8.64" target="#fig_7">8</ref> from the main paper.</s></p><formula xml:id="formula_6" coords="10,56.80,552.08,222.87,142.34">Layer Input InSize K S OutSize conv1 SIFT 64×64×133 5 2 32×32×256 conv2 conv1 32×32×256 3 2 16×16×512 conv3 conv2 16×16×512 3 2 8×8×1024 conv4 conv3 8×8×1024 3 2 4×4×2048 conv5 conv4 4×4×2048 3 1 4×4×2048 conv6 conv5 4×4×2048 3 1 4×4×1024 upconv1 conv6 4×4×1024 4 2 8×8×512 upconv2 upconv1 8×8×512 4 2 16×16×256 upconv3 upconv2 16×16×256 4 2 32×32×128 upconv4 upconv3 32×32×128 4 2 64×64×64 upconv5 upconv4 64×64×64 4 2 128×128×32 upconv6 upconv5 128×128×32 4 2 256×256×3</formula><formula xml:id="formula_7" coords="10,314.81,83.04,224.35,118.16">8×8×256 convA2 convA1 8×8×256 5 2 4×4×512 convA3 convA2 4×4×512 3 1 4×4×1024 upconvA1 convA3 4×4×1024 4 2 8×8×512 upconvA2 upconvA1 8×8×512 4 2 16×16×256 convB1 LBP 16×16×58 5 1 16×16×128 convB2 convB1 16×16×128 3 1 16×16×128 convJ1 {upconvA2, convB2} 16×16×384 3 1 16×16×256 convJ2 convJ1 16×16×256 3 1 16×16×128 upconvJ3 convJ2 16×16×128 4 2 32×32×128 upconvJ4 upconvJ3 32×32×128 4 2 64×64×64 upconvJ5 upconvJ4 64×64×64 4 2 128×128×32 upconvJ6 upconvJ5 128×128×32 4 2 256×256×3</formula><p><s coords="11,50.11,680.60,236.25,8.64;11,50.11,692.56,186.22,8.64">We reconstruct from all FC8 features, as well as from only 5 largest ones or all except the 5 largest ones.</s><s coords="11,241.35,692.56,45.01,8.64;11,50.11,704.51,191.11,8.64">It turns out that the top 5 activations are not very important.</s></p><p><s coords="11,320.82,169.90,224.30,8.64;11,308.86,181.85,236.25,8.64;11,308.86,193.81,19.64,8.64">Figure <ref type="figure" coords="11,350.28,169.90,9.96,8.64" target="#fig_0">15</ref> shows images generated by activating single neurons in different layers and setting all other neurons to zero.</s><s coords="11,335.66,193.81,209.45,8.64;11,308.86,205.76,60.86,8.64">Particularly interpretable are images generated this way from FC8.</s><s coords="11,375.53,205.76,169.58,8.64">Every FC8 neuron corresponds to a class.</s><s coords="11,308.86,217.72,236.25,8.64;11,308.86,229.67,236.25,8.64">Hence the image generated from the activation of, say, "apple" neuron, could be expected to be a stereotypical apple.</s><s coords="11,308.86,241.63,236.25,8.64;11,308.86,253.58,89.17,8.64">What we observe looks rather like it might be the average of all images of the class.</s><s coords="11,401.02,253.58,144.10,8.64;11,308.86,265.54,212.24,8.64">For some classes the reconstructions are somewhat interpretable, for others -not so much.</s></p><p><s coords="11,320.82,277.49,224.30,8.64;11,308.86,289.45,236.25,8.64;11,308.86,301.40,203.38,8.64">Qualitative comparison of reconstructions with our method to the reconstructions of <ref type="bibr" coords="11,444.72,289.45,16.60,8.64" target="#b18">[19]</ref> and the results with AlexNet-based autoencoders is given in Figure <ref type="figure" coords="11,498.12,301.40,9.96,8.64" target="#fig_0">16</ref> .</s></p><p><s coords="11,320.82,313.36,224.30,8.64;11,308.86,325.31,236.25,8.64;11,308.86,337.27,38.07,8.64">Reconstructions from feature vectors obtained by interpolating between feature vectors of two images are shown in Figure <ref type="figure" coords="11,336.97,337.27,9.96,8.64" target="#fig_6">17</ref></s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,308.86,365.39,236.25,8.64;1,308.86,376.96,236.25,9.03;1,308.86,388.92,236.25,9.03;1,308.86,401.26,236.25,8.64;1,308.86,413.22,236.25,8.64;1,308.86,425.17,219.02,8.64;1,310.28,300.56,56.70,56.70"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s coords="1,308.86,365.39,236.25,8.64;1,308.86,377.35,188.42,8.64">Figure 1: We train convolutional networks to reconstruct images from different feature representations.</s><s coords="1,505.15,376.96,39.97,8.96;1,308.86,389.31,58.91,8.64">Top row: Input features.</s><s coords="1,374.97,388.92,148.56,9.03">Bottom row: Reconstructed image.</s><s coords="1,530.73,389.31,14.39,8.64;1,308.86,401.26,216.18,8.64">Reconstructions from HOG and SIFT are very realistic.</s><s coords="1,530.73,401.26,14.39,8.64;1,308.86,413.22,236.25,8.64;1,308.86,425.17,219.02,8.64">Reconstructions from AlexNet preserve color and rough object positions even when reconstructing from higher layers.</s></p></div></figDesc><graphic coords="1,310.28,300.56,56.70,56.70" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,320.82,568.75,224.30,8.64;3,308.86,580.70,236.25,8.64;3,308.86,592.66,236.25,8.64;3,308.86,604.61,236.25,8.64;3,308.86,616.57,236.25,8.64;3,308.86,628.52,182.91,8.64"><head>Figures 1 and 3</head><label>3</label><figDesc><div><p><s coords="3,320.82,568.75,224.30,8.64;3,308.86,580.70,156.39,8.64">Figures1 and 3show reconstructions of several images from the ImageNet validation set.</s><s coords="3,469.70,580.70,75.42,8.64;3,308.86,592.66,236.25,8.64">Normalized reconstruction error of different approaches is shown in Table2.</s><s coords="3,308.86,604.61,236.25,8.64;3,308.86,616.57,37.90,8.64">Clearly, our method significantly outperforms existing approaches.</s><s coords="3,351.16,616.57,193.95,8.64;3,308.86,628.52,182.91,8.64">This is to be expected, since our method explicitly aims to minimize the reconstruction error.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,128.65,235.45,337.93,8.64;4,59.08,84.74,94.05,141.28"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s coords="4,128.65,235.45,337.93,8.64">Figure 2: Reconstructing an image from its HOG descriptors with different methods.</s></p></div></figDesc><graphic coords="4,59.08,84.74,94.05,141.28" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,308.86,408.02,236.25,8.64;4,308.86,419.97,236.25,8.64;4,308.86,431.93,236.25,8.64;4,308.86,443.88,21.86,8.64;4,310.09,341.42,57.17,57.17"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s coords="4,308.86,408.02,213.43,8.64">Figure 3: Inversion of shallow image representations.</s><s coords="4,525.75,408.02,19.37,8.64;4,308.86,419.97,236.25,8.64;4,308.86,431.93,236.25,8.64;4,308.86,443.88,21.86,8.64">Note how in the first row the color of grass and trees is predicted correctly in all cases, although it is not contained in the features.</s></p></div></figDesc><graphic coords="4,310.09,341.42,57.17,57.17" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,308.86,668.96,236.25,8.64;4,308.86,680.53,236.25,9.03;4,308.86,692.48,195.33,9.03;4,310.37,572.67,115.76,86.86"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc><div><p><s coords="4,308.86,668.96,236.25,8.64;4,308.86,680.91,94.21,8.64">Figure 4: Reconstructing an image from SIFT descriptors with different methods.</s><s coords="4,407.98,680.53,137.14,9.03;4,308.86,692.48,195.33,9.03">(a) an image, (b) SIFT keypoints, (c) reconstruction of [26], (d) our reconstruction.</s></p></div></figDesc><graphic coords="4,310.37,572.67,115.76,86.86" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,50.11,252.17,236.25,8.64;6,50.11,264.13,99.24,8.64"><head>Figure 7 :</head><label>7</label><figDesc><div><p><s coords="6,50.11,252.17,236.25,8.64;6,50.11,264.13,99.24,8.64">Figure 7: Average normalized reconstruction error depending on the network layer.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,308.86,245.56,236.25,8.64;6,308.86,257.52,236.25,8.64;6,308.86,269.15,236.25,8.96;6,308.86,281.11,236.25,8.96;6,308.86,293.38,236.25,8.64;6,308.86,305.34,88.56,8.64"><head>Figure 8 :</head><label>8</label><figDesc><div><p><s coords="6,308.86,245.56,236.25,8.64;6,308.86,257.52,100.21,8.64">Figure 8: The effect of color on classification and reconstruction from layer FC8.</s><s coords="6,412.55,257.52,132.56,8.64;6,308.86,269.15,236.25,8.96;6,308.86,281.11,236.25,8.96;6,308.86,293.38,49.37,8.64">Left to right: input image, reconstruction from FC8, reconstruction from 5 largest activations in FC8, reconstruction from all FC8 activations except the 5 largest ones.</s><s coords="6,361.25,293.38,183.87,8.64;6,308.86,305.34,88.56,8.64">Below each row the network prediction and its confidence are shown.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,131.92,198.94,331.40,8.64"><head>Figure 9 :</head><label>9</label><figDesc><div><p><s coords="7,131.92,198.94,331.40,8.64">Figure 9: Reconstructions from different layers of AlexNet with disturbed features.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="8,61.93,225.31,212.62,8.64;8,61.93,237.27,30.71,8.64"><head>FC8Figure 10 :</head><label>10</label><figDesc><div><p><s coords="8,61.93,225.31,212.62,8.64;8,61.93,237.27,30.71,8.64">Figure 10: Interpolation between the features of two images.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="8,308.86,201.91,236.25,8.64;8,308.86,213.87,99.18,8.64"><head>FC8Figure 11 :</head><label>11</label><figDesc><div><p><s coords="8,308.86,201.91,236.25,8.64;8,308.86,213.87,99.18,8.64">Figure 11: Images generated from random feature vectors of top layers of AlexNet.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="11,59.46,533.98,217.56,8.64;11,52.28,467.85,56.70,56.70"><head>Figure 12 :</head><label>12</label><figDesc><div><p><s coords="11,59.46,533.98,217.56,8.64">Figure 12: Inversion of shallow image representations.</s></p></div></figDesc><graphic coords="11,52.28,467.85,56.70,56.70" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="11,320.82,169.90,224.30,8.64;11,308.86,181.85,236.25,8.64;11,308.86,193.81,236.25,8.64;11,308.86,205.76,236.25,8.64;11,308.86,217.72,236.25,8.64;11,308.86,229.67,236.25,8.64;11,308.86,241.63,236.25,8.64;11,308.86,253.58,236.25,8.64;11,308.86,265.54,212.24,8.64;11,320.82,277.49,224.30,8.64;11,308.86,289.45,236.25,8.64;11,308.86,301.40,203.38,8.64;11,320.82,313.36,224.30,8.64;11,308.86,325.31,236.25,8.64;11,308.86,337.27,236.25,8.64;11,308.86,349.22,236.25,8.64;11,308.86,361.18,95.77,8.64;11,320.82,373.13,224.30,8.64;11,308.86,385.09,236.25,8.64;11,308.86,397.05,236.25,8.64;11,308.86,409.00,236.25,8.64;11,308.86,420.96,236.25,8.64;11,308.86,432.91,236.25,8.64;11,308.86,444.87,236.25,8.64;11,308.86,456.82,236.25,8.64;11,308.86,468.78,236.25,8.64;11,308.86,480.73,217.31,8.64;11,227.49,175.37,56.70,56.70"><head>Figure 13 :</head><label>13</label><figDesc><div><p><s coords="11,320.82,169.90,224.30,8.64;11,308.86,181.85,236.25,8.64;11,308.86,193.81,19.64,8.64">Figure15shows images generated by activating single neurons in different layers and setting all other neurons to zero.</s><s coords="11,335.66,193.81,209.45,8.64;11,308.86,205.76,60.86,8.64">Particularly interpretable are images generated this way from FC8.</s><s coords="11,375.53,205.76,169.58,8.64">Every FC8 neuron corresponds to a class.</s><s coords="11,308.86,217.72,236.25,8.64;11,308.86,229.67,236.25,8.64">Hence the image generated from the activation of, say, "apple" neuron, could be expected to be a stereotypical apple.</s><s coords="11,308.86,241.63,236.25,8.64;11,308.86,253.58,89.17,8.64">What we observe looks rather like it might be the average of all images of the class.</s><s coords="11,401.02,253.58,144.10,8.64;11,308.86,265.54,212.24,8.64;11,320.82,277.49,224.30,8.64;11,308.86,289.45,236.25,8.64;11,308.86,301.40,203.38,8.64;11,320.82,313.36,224.30,8.64;11,308.86,325.31,236.25,8.64;11,308.86,337.27,236.25,8.64">For some classes the reconstructions are somewhat interpretable, for others -not so much.Qualitative comparison of reconstructions with our method to the reconstructions of<ref type="bibr" coords="11,444.72,289.45,16.60,8.64" target="#b18">[19]</ref> and the results with AlexNet-based autoencoders is given in Figure16.Reconstructions from feature vectors obtained by interpolating between feature vectors of two images are shown in Figure17, both for fixed AlexNet and autoencoder training.</s><s coords="11,308.86,349.22,236.25,8.64;11,308.86,361.18,95.77,8.64;11,320.82,373.13,224.30,8.64;11,308.86,385.09,236.25,8.64;11,308.86,397.05,216.60,8.64">More examples of such interpolations with fixed AlexNet are shown in Figure18.As described in section 5.5 of the main paper, we tried two different distributions for sampling random feature activations: a histogram-based and a truncated Gaussian.</s><s coords="11,528.51,397.05,16.61,8.64;11,308.86,409.00,236.25,8.64;11,308.86,420.96,129.34,8.64">Figure19shows the results with fixed AlexNet network and truncated Gaussian distribution.</s><s coords="11,445.10,420.96,100.01,8.64;11,308.86,432.91,213.46,8.64">Figures20 and 21show images generated with autoencoder-trained networks.</s><s coords="11,525.75,432.91,19.37,8.64;11,308.86,444.87,236.25,8.64;11,308.86,456.82,236.25,8.64;11,308.86,468.78,70.72,8.64">Note that images generated from autoencoders look much less realistic than images generated with a network with fixed AlexNet weights.</s><s coords="11,386.12,468.78,158.99,8.64;11,308.86,480.73,217.31,8.64">This indicates that reconstructing from AlexNet features requires a strong natural image prior.</s></p></div></figDesc><graphic coords="11,227.49,175.37,56.70,56.70" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="12,50.11,613.12,164.59,8.64;12,50.11,625.07,164.59,8.64;12,50.11,636.71,164.59,8.96;12,50.11,648.98,164.59,8.64;12,50.11,660.62,57.10,8.96"><head>Figure 14 :</head><label>14</label><figDesc><div><p><s coords="12,50.11,613.12,164.59,8.64;12,50.11,625.07,164.59,8.64;12,50.11,636.71,164.59,8.96;12,50.11,648.98,164.59,8.64;12,50.11,660.62,57.10,8.96">Figure 14: Left to right: input image, reconstruction from fc8, reconstruction from 5 largest activations in FC8, reconstruction from all FC8 activations except 5 largest ones.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="12,238.21,614.45,299.23,8.64;12,238.21,626.41,299.22,8.64;12,238.21,638.36,299.23,8.64;12,238.21,650.32,119.15,8.64"><head>FC6FC7FC8Figure 15 :Figure 16 :FC8Figure 17 :FC8Figure 18 :FC8Figure 19 :FC8Figure 20 :</head><label>151617181920</label><figDesc><div><p><s coords="12,238.21,614.45,299.23,8.64;12,238.21,626.41,104.17,8.64">Figure 15: Reconstructions from single neuron activations in the fully connected layers of AlexNet.</s><s coords="12,347.98,626.41,189.45,8.64;12,238.21,638.36,299.23,8.64;12,238.21,650.32,119.15,8.64">The FC8 neurons correspond to classes, left to right: kite, convertible, desktop computer, school bus, street sign, soup bowl, bell pepper, soccer ball.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="15,50.11,459.14,495.00,8.64;15,50.11,471.10,266.81,8.64"><head>FC8Figure 21 :</head><label>21</label><figDesc><div><p><s coords="15,50.11,459.14,495.00,8.64;15,50.11,471.10,266.81,8.64">Figure 21: Images generated from random feature vectors of top layers of AlexNet-based autoencoders with the simpler truncated Gaussian distribution (see section 5.5 of the main paper).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,308.86,74.03,236.25,206.52"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="3,343.79,201.60,201.32,8.64;3,308.86,213.55,21.86,8.64">Network for reconstructing from AlexNet FC8 features.</s><s coords="3,333.81,213.55,145.71,8.64">K stands for kernel size, S for stride.</s></p></div></figDesc><table coords="3,315.36,74.03,223.25,104.13"><row><cell>Layer</cell><cell>Input</cell><cell>InSize</cell><cell>K S</cell><cell>OutSize</cell></row><row><cell>fc1</cell><cell>AlexNet-FC8</cell><cell>1000</cell><cell>--</cell><cell>4096</cell></row><row><cell>fc2</cell><cell>fc1</cell><cell>4096</cell><cell>--</cell><cell>4096</cell></row><row><cell>fc3</cell><cell>fc2</cell><cell>4096</cell><cell>--</cell><cell>4096</cell></row><row><cell>reshape</cell><cell>fc3</cell><cell>4096</cell><cell cols="2">--4×4×256</cell></row><row><cell>upconv1</cell><cell>reshape</cell><cell>4×4×256</cell><cell>5 2</cell><cell>8×8×256</cell></row><row><cell>upconv2</cell><cell>upconv1</cell><cell>8×8×256</cell><cell cols="2">5 2 16×16×128</cell></row><row><cell>upconv3</cell><cell>upconv2</cell><cell cols="3">16×16×128 5 2 32×32×64</cell></row><row><cell>upconv4</cell><cell>upconv3</cell><cell cols="3">32×32×64 5 2 64×64×32</cell></row><row><cell>upconv5</cell><cell>upconv4</cell><cell cols="3">64×64×32 5 2 128×128×3</cell></row></table><note coords="3,308.86,248.00,236.25,8.64;3,308.86,259.64,236.25,8.96;3,308.86,271.60,195.20,8.96"><p><s coords="3,308.86,248.00,105.85,8.64">layer to reconstruct from).</s><s coords="3,419.08,248.00,126.04,8.64;3,308.86,259.64,123.58,8.96">Filters in all (up-)convolutional layers have 5 × 5 spatial size.</s><s coords="3,438.39,259.96,106.72,8.64;3,308.86,271.60,195.20,8.96">After each layer we apply leaky ReLU nonlinearity with slope 0.2, that is, r</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,308.86,652.43,236.25,57.13"><head>Table 2 :</head><label>2</label><figDesc/><table coords="3,327.30,652.43,213.70,21.18"><row><cell/><cell>-1 [19]</cell><cell>HOG our</cell><cell>SIFT our</cell><cell>LBP our</cell></row><row><cell>0.61</cell><cell>0.63</cell><cell>0.24</cell><cell>0.28</cell><cell>0.38</cell></row></table><note coords="3,343.39,688.97,201.72,8.64;3,308.86,700.93,83.29,8.64;4,93.66,73.62,24.90,8.64;4,191.07,73.62,21.58,8.64;4,271.46,73.62,52.29,8.64;4,366.07,73.31,24.80,8.74;4,390.87,71.73,10.20,6.12;4,404.06,73.62,16.60,8.64;4,481.37,73.62,15.49,8.64"><p><s coords="3,343.39,688.97,201.72,8.64;3,308.86,700.93,83.29,8.64;4,93.66,73.62,24.90,8.64;4,191.07,73.62,21.58,8.64;4,271.46,73.62,52.29,8.64;4,366.07,73.31,24.80,8.74;4,390.87,71.73,10.20,6.12;4,404.06,73.62,16.60,8.64;4,481.37,73.62,15.49,8.64">Normalized error of different methods when reconstructing from HOG. Image HOG Hoggles [24] HOG -1 [19] Our</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,391.84,704.51,153.27,8.64"><head/><label/><figDesc><div><p><s coords="6,391.84,704.51,153.27,8.64">top left) from Flickr and modified its</s></p></div></figDesc><table coords="7,57.33,72.22,468.61,119.83"><row><cell>Image</cell><cell>CONV3</cell><cell>CONV4</cell><cell>CONV5</cell><cell>FC6</cell><cell>FC7</cell><cell>FC8</cell><cell>CONV3</cell><cell>CONV4</cell><cell>CONV5</cell><cell>FC6</cell><cell>FC7</cell><cell>FC8</cell></row><row><cell>No</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>per-</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>turb</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Bin</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Drop</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>50</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell/><cell/><cell/><cell cols="2">Fixed AlexNet</cell><cell/><cell/><cell/><cell/><cell cols="2">Autoencoder</cell><cell/><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,55.78,73.76,473.56,644.31"><head>Table 5 :</head><label>5</label><figDesc><div><p><s coords="10,91.00,709.43,189.70,8.64">Network for reconstructing from SIFT features.</s></p></div></figDesc><table coords="10,314.81,73.76,214.53,16.19"><row><cell>Layer</cell><cell>Input</cell><cell>InSize</cell><cell>K S</cell><cell>OutSize</cell></row><row><cell>convA1</cell><cell>LBP</cell><cell>16×16×58</cell><cell>5 2</cell><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,315.19,215.92,223.59,106.02"><head>Table 6 :</head><label>6</label><figDesc><div><p><s coords="10,350.85,215.92,187.49,8.64">Network for reconstructing from LBP features.</s></p></div></figDesc><table coords="10,315.19,232.26,223.59,89.68"><row><cell>Layer</cell><cell>Input</cell><cell>InSize</cell><cell>K S</cell><cell>OutSize</cell></row><row><cell>conv1</cell><cell cols="2">AlexNet-CONV5 6×6×256</cell><cell cols="2">3 1 6×6×256</cell></row><row><cell>conv2</cell><cell>conv1</cell><cell>6×6×256</cell><cell cols="2">3 1 6×6×256</cell></row><row><cell>conv3</cell><cell>conv2</cell><cell>6×6×256</cell><cell cols="2">3 1 6×6×256</cell></row><row><cell>upconv1</cell><cell>conv3</cell><cell>6×6×256</cell><cell cols="2">5 2 12×12×256</cell></row><row><cell>upconv2</cell><cell>upconv1</cell><cell cols="3">12×12×256 5 2 24×24×128</cell></row><row><cell>upconv3</cell><cell>upconv2</cell><cell cols="3">24×24×128 5 2 48×48×64</cell></row><row><cell>upconv4</cell><cell>upconv3</cell><cell cols="3">48×48×64 5 2 96×96×32</cell></row><row><cell>upconv5</cell><cell>upconv4</cell><cell cols="3">96×96×32 5 2 192×192×3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,308.86,336.81,236.25,345.73"><head>Table 7 :</head><label>7</label><figDesc><div><p><s coords="10,345.10,336.81,200.02,8.64;10,308.86,348.76,34.02,8.64;10,320.82,374.28,224.29,8.96;10,308.86,386.55,39.57,8.64">Network for reconstructing from AlexNet CONV5 features.The LBP version we used works with 3 × 3 pixel neighborhoods.</s><s coords="10,353.16,386.23,191.96,8.96;10,308.86,398.51,220.41,8.64">Each of the 8 non-central bits is equal to one if the corresponding pixel is brighter than the central one.</s><s coords="10,532.38,398.51,12.73,8.64;10,308.86,410.14,208.65,8.96">All possible 256 patterns are quantized into 58 patterns.</s><s coords="10,521.32,410.46,23.79,8.64;10,308.86,422.10,236.25,8.96;10,308.86,434.37,236.25,8.64;10,308.86,446.33,236.25,8.64;10,308.86,458.28,149.28,8.64">These include 56 patterns with exactly one transition from 0 to 1 when going around the central pixel, plus one quantized pattern comprising two uniform patterns, plus one quantized pattern containing all other patterns.</s><s coords="10,465.08,458.28,80.04,8.64;10,308.86,470.24,236.25,8.64;10,308.86,481.87,60.73,8.96">The quantized LBP patterns are then grouped into local histograms over cells of 16 × 16 pixels.</s></p></div></figDesc><table coords="10,308.86,510.70,236.25,171.84"><row><cell cols="5">Experiments: shallow representations Figure 12 shows</cell></row><row><cell cols="5">several images and their reconstructions from HOG, SIFT</cell></row><row><cell cols="5">and LBP. HOG allows for the best reconstruction, SIFT</cell></row><row><cell cols="5">slightly worse, LBP yet slightly worse. Colors are often</cell></row><row><cell>Layer</cell><cell>Input</cell><cell>InSize</cell><cell>K S</cell><cell>OutSize</cell></row><row><cell>fc1</cell><cell>AlexNet-FC8</cell><cell>1000</cell><cell>--</cell><cell>4096</cell></row><row><cell>fc2</cell><cell>fc1</cell><cell>4096</cell><cell>--</cell><cell>4096</cell></row><row><cell>fc3</cell><cell>fc2</cell><cell>4096</cell><cell>--</cell><cell>4096</cell></row><row><cell>reshape</cell><cell>fc3</cell><cell>4096</cell><cell cols="2">--4×4×256</cell></row><row><cell>upconv1</cell><cell>reshape</cell><cell>4×4×256</cell><cell>5 2</cell><cell>8×8×256</cell></row><row><cell>upconv2</cell><cell>upconv1</cell><cell>8×8×256</cell><cell cols="2">5 2 16×16×128</cell></row><row><cell>upconv3</cell><cell>upconv2</cell><cell cols="3">16×16×128 5 2 32×32×64</cell></row><row><cell>upconv4</cell><cell>upconv3</cell><cell cols="3">32×32×64 5 2 64×64×32</cell></row><row><cell>upconv5</cell><cell>upconv4</cell><cell cols="3">64×64×32 5 2 128×128×3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,308.86,697.48,236.25,20.59"><head>Table 8 :</head><label>8</label><figDesc><div><p><s coords="10,343.79,697.48,201.32,8.64;10,308.86,709.43,21.86,8.64">Network for reconstructing from AlexNet FC8 features.</s></p></div></figDesc><table coords="11,96.05,74.23,400.88,52.80"><row><cell>layer</cell><cell cols="2">CONV1</cell><cell cols="2">CONV2</cell><cell cols="2">CONV3 CONV4</cell><cell cols="2">CONV5</cell><cell>FC6</cell><cell/><cell>FC7</cell><cell/><cell>FC8</cell></row><row><cell cols="14">processing conv1 mpool1 conv2 mpool2 conv3 conv4 conv5 mpool5 fc6 drop6 fc7 drop7 fc8</cell></row><row><cell>steps</cell><cell cols="7">relu1 norm1 relu2 norm2 relu3 relu4 relu5</cell><cell/><cell>relu6</cell><cell/><cell>relu7</cell><cell/><cell/></row><row><cell>out size</cell><cell>55</cell><cell>27</cell><cell>27</cell><cell>13</cell><cell>13</cell><cell>13</cell><cell>13</cell><cell>6</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell cols="2">out channels 96</cell><cell>96</cell><cell>256</cell><cell>256</cell><cell>384</cell><cell>384</cell><cell>256</cell><cell cols="6">256 4096 4096 4096 4096 1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,68.19,136.10,378.51,39.26"><head>Table 3 :</head><label>3</label><figDesc><div><p><s coords="11,183.75,136.42,137.82,8.64">Summary of the AlexNet network.</s><s coords="11,324.66,136.10,122.03,8.96">Input image size is 227 × 227.</s></p></div></figDesc><table coords="11,68.19,166.73,204.67,8.64"><row><cell>Image</cell><cell>HOG our</cell><cell>SIFT our</cell><cell>LBP our</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,323.21,696.35,221.90,6.91;2,308.86,705.81,55.66,6.91"><p><s coords="2,323.21,696.35,221.90,6.91;2,308.86,705.81,55.66,6.91">More precisely, we used CaffeNet, which is almost identical to the original AlexNet.</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head coords="8,308.86,632.66,98.83,10.75">Acknowledgements</head><p>We acknowledge funding by the <rs type="funder">ERC</rs> <rs type="grantName">Starting Grant VideoLearn</rs> (<rs type="grantNumber">279401</rs>).We are grateful to <rs type="person">Aravindh Mahendran</rs> for sharing with us the reconstructions achieved with the method of Mahendran and Vedaldi [19].We thank <rs type="person">Jost Tobias Springenberg</rs> for comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GVMExuC">
					<idno type="grant-number">279401</idno>
					<orgName type="grant-name">Starting Grant VideoLearn</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.04,94.06,216.32,7.77;9,70.03,105.02,216.33,7.77;9,70.03,115.81,65.00,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the Performance of Multilayer Neural Networks for Object Recognition</title>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10584-0_22</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2014</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="329" to="344"/>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Agrawal, R. Girshick, and J. Malik. Analyzing the perfor- mance of multilayer neural networks for object recognition. In ECCV, 2014. 7</note>
</biblStruct>

<biblStruct coords="9,70.04,127.24,216.32,7.93;9,70.03,138.36,149.69,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural Networks for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno type="DOI">10.1093/oso/9780198538493.001.0001</idno>
		<imprint>
			<date type="published" when="1995-11-23">1995</date>
			<publisher>Oxford University PressOxford</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">C. M. Bishop. Neural Networks for Pattern Recognition. Ox- ford Uni. Press, New York, USA, 1995. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,149.78,216.32,7.77;9,70.03,160.74,72.46,7.77;9,171.63,160.74,86.41,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2005.177</idno>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893"/>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In pages 886-893, 2005. 1</note>
</biblStruct>

<biblStruct coords="9,70.04,172.16,216.32,7.77;9,70.03,183.12,216.33,7.77;9,70.03,193.92,216.33,7.93;9,70.03,205.04,27.89,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main">From Bits to Images: Inversion of Local Binary Descriptors</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>D'angelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2013.228</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="874" to="887"/>
			<date type="published" when="2014-05">2014</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">E. d'Angelo, L. Jacques, A. Alahi, and P. Vandergheynst. From bits to images: Inversion of local binary descriptors. IEEE Trans. Pattern Anal. Mach. Intell., 36(5):874-887, 2014. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,216.46,216.32,7.77;9,70.03,227.42,216.33,7.77;9,70.03,238.22,54.78,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009. 3</note>
</biblStruct>

<biblStruct coords="9,70.04,249.81,216.32,7.77;9,70.03,260.77,216.33,7.77;9,70.03,271.56,54.78,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298761</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In CVPR, 2015. 3</note>
</biblStruct>

<biblStruct coords="9,70.04,283.15,216.32,7.77;9,70.03,294.11,216.33,7.77;9,70.03,304.90,187.70,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part-Based Models</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2009.167</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645"/>
			<date type="published" when="2010-09">2010</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra- manan. Object detection with discriminatively trained part based models. TPAMI, 32(9):1627-1645, 2010. 1, 2</note>
</biblStruct>

<biblStruct coords="9,70.04,316.49,216.32,7.77;9,70.03,327.29,191.89,7.93" xml:id="b7">
	<monogr>
		<title level="m" type="main">Figure 2: The distinction between the conventional logits-based knowledge distillation (Hinton, Vinyals &amp; Dean, 2015) and TSKD proposed in this article.</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerjcs.1650/fig-2</idno>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">G. E. Hinton, O. Vinyals, and J. Dean. Distilling the knowl- edge in a neural network. arXiv:1503.02531, 2015. 7</note>
</biblStruct>

<biblStruct coords="9,70.04,338.87,216.32,7.77;9,70.03,349.83,216.33,7.77;9,70.03,360.79,216.33,7.77;9,70.03,371.59,140.79,7.93" xml:id="b8">
	<analytic>
		<title level="a" type="main">Inversion of feedforward neural networks: algorithms and applications</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>El-Sharkawi</surname></persName>
		</author>
		<author>
			<persName><surname>Jae-Byung Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Eggen</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.784232</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<title level="j" type="abbrev">Proc. IEEE</title>
		<idno type="ISSN">0018-9219</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1536" to="1549"/>
			<date type="published" when="1999">1999</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Jensen, R. Reed, R. Marks, M. El-Sharkawi, J.-B. Jung, R. Miyamoto, G. Anderson, and C. Eggen. Inversion of feed- forward neural networks: Algorithms and applications. In Proc. IEEE, pages 1536-1549, 1999. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,383.17,216.32,7.77;9,70.03,394.13,216.33,7.77;9,70.03,404.93,216.33,7.93;9,70.03,416.05,36.85,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main">Caffe</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2647868.2654889</idno>
		<idno type="arXiv">arXiv:1408.5093</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-11-03">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir- shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014. 2, 3</note>
</biblStruct>

<biblStruct coords="9,70.04,427.47,216.32,7.77;9,70.03,438.27,134.87,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main">Image Reconstruction from Bag-of-Visual-Words</title>
		<author>
			<persName><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2014.127</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">June 2014. 2</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Kato and T. Harada. Image reconstruction from bag-of- visual-words. In CVPR, June 2014. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,449.85,216.32,7.77;9,70.03,460.65,112.31,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 3</note>
</biblStruct>

<biblStruct coords="9,70.04,472.24,216.32,7.77;9,70.03,483.20,216.33,7.77;9,70.03,493.99,127.75,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90"/>
			<date type="published" when="2012">2012</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, pages 1106-1114, 2012. 1, 2</note>
</biblStruct>

<biblStruct coords="9,70.04,505.58,216.32,7.77;9,70.03,516.54,216.33,7.77;9,70.03,527.34,216.33,7.93;9,70.03,538.29,106.59,7.93" xml:id="b13">
	<analytic>
		<title level="a" type="main">Backpropagation Applied to Handwritten Zip Code Recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.4.541</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<title level="j" type="abbrev">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<idno type="ISSNe">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551"/>
			<date type="published" when="1989-12">1989</date>
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Compu- tation, 1(4):541-551, 1989. 1</note>
</biblStruct>

<biblStruct coords="9,70.04,549.88,216.32,7.77;9,70.03,560.68,216.33,7.93;9,70.03,571.64,157.76,7.93" xml:id="b14">
	<analytic>
		<title level="a" type="main">Inverse mapping of continuous functions using local and global information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Kil</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.286912</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<title level="j" type="abbrev">IEEE Trans. Neural Netw.</title>
		<idno type="ISSN">1045-9227</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="423"/>
			<date type="published" when="1994-05">1994</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Lee and R. M. Kil. Inverse mapping of continuous func- tions using local and global information. IEEE Transactions on Neural Networks, 5(3):409-423, 1994. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,583.22,216.32,7.77;9,70.03,594.02,171.92,7.93" xml:id="b15">
	<analytic>
		<title level="a" type="main">Inversion of multilayer nets</title>
		<author>
			<persName><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><surname>Kindermann</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.1989.118277</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Linden and J. Kindermann. Inversion of multilayer nets. In Proc. Int. Conf. on Neural Networks, 1989. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,605.60,216.32,7.77;9,70.03,616.40,216.33,7.93;9,70.03,627.52,90.15,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.1023/b:visi.0000029664.99615.94</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<title level="j" type="abbrev">International Journal of Computer Vision</title>
		<idno type="ISSN">0920-5691</idno>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110"/>
			<date type="published" when="2004-11">2004</date>
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91-110, 2004. 1, 2</note>
</biblStruct>

<biblStruct coords="9,70.04,638.94,216.32,7.77;9,70.03,649.74,216.33,7.93;9,70.03,660.70,216.33,7.93;9,70.03,671.82,4.48,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main">Inverting feedforward neural networks using linear and nonlinear programming</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Nishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1271" to="1290"/>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Lu, H. Kita, and Y. Nishikawa. Inverting feedforward neu- ral networks using linear and nonlinear programming. IEEE Transactions on Neural Networks, 10(6):1271-1290, 1999. 2</note>
</biblStruct>

<biblStruct coords="9,70.04,683.24,216.32,7.77;9,70.03,694.04,216.33,7.93;9,70.03,705.16,58.28,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7299155</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">2015. 1, 2, 3, 4, 5, 6, 8, 11</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. In CVPR, 2015. 1, 2, 3, 4, 5, 6, 8, 11, 13</note>
</biblStruct>

<biblStruct coords="9,328.79,76.13,216.32,7.77;9,328.78,87.08,216.33,7.77;9,328.78,97.88,216.33,7.94;9,328.78,109.00,99.11,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main">Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Naselaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2011.08.031</idno>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<title level="j" type="abbrev">Current Biology</title>
		<idno type="ISSN">0960-9822</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="1641" to="1646"/>
			<date type="published" when="2011-10">2011</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Nishimoto, A. Vu, T. Naselaris, Y. Benjamini, B. Yu, and J. Gallant. Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology, 21(19):1641-1646, 2011. 2</note>
</biblStruct>

<biblStruct coords="9,328.79,120.96,216.33,7.77;9,328.78,131.92,216.33,7.77;9,328.78,142.71,203.38,7.93" xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2002.1017623</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Machine Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987"/>
			<date type="published" when="2002-07">2002</date>
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Ojala, M. Pietikäinen, and T. Mäenpää. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. TPAMI, 24(7):971-987, 2002. 1, 2</note>
</biblStruct>

<biblStruct coords="9,328.79,154.83,216.32,7.77;9,328.78,165.79,216.33,7.77;9,328.78,176.59,120.89,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried- miller. Striving for simplicity: The all convolutional net. In ICLR Workshop Track, 2015. 1, 2</note>
</biblStruct>

<biblStruct coords="9,328.79,188.70,216.32,7.77;9,328.78,199.50,216.33,7.93;9,328.78,210.46,194.16,7.93" xml:id="b22">
	<analytic>
		<title level="a" type="main">Vlfeat</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fulkerson</surname></persName>
		</author>
		<idno type="DOI">10.1145/1873951.1874249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-10-25">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Vedaldi and B. Fulkerson. Vlfeat: an open and portable library of computer vision algorithms. In International Con- ference on Multimedia, pages 1469-1472, 2010. 2, 10</note>
</biblStruct>

<biblStruct coords="9,328.79,222.58,216.32,7.77;9,328.78,233.37,216.33,7.93;9,328.78,244.49,22.42,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main">HOGgles: Visualizing Object Detection Features</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2013.8</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-12">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. Hoggles: Visualizing object detection features. ICCV, 2013. 2, 3, 4</note>
</biblStruct>

<biblStruct coords="9,328.79,256.45,216.32,7.77;9,328.78,267.41,216.33,7.77;9,328.78,278.37,216.33,7.77;9,328.78,289.16,216.33,7.93;9,328.78,300.12,216.33,7.93;9,328.78,311.08,146.72,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main">Observer-Based Iterative Fuzzy and Neural Network Model Inversion for Measurement and Control Applications</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Vrkonyi-Kczy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-03737-5_49</idno>
	</analytic>
	<monogr>
		<title level="m">Studies in Computational Intelligence</title>
		<title level="s">Studies in Computational Intelligence</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Rudas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Fodor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kacprzyk</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">243</biblScope>
			<biblScope unit="page" from="681" to="702"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. R. Vrkonyi-Kczy. Observer-based iterative fuzzy and neu- ral network model inversion for measurement and control applications. In I. J. Rudas, J. C. Fodor, and J. Kacprzyk, editors, Towards Intelligent Engineering and Information Technology, volume 243 of Studies in Computational Intelli- gence, pages 681-702. Springer, 2009. 2</note>
</biblStruct>

<biblStruct coords="9,328.79,323.20,216.32,7.77;9,328.78,334.00,216.33,7.93;9,328.78,345.12,76.62,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconstructing an image from its local descriptors</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2011.5995616</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2011. 2, 4, 5</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Weinzaepfel, H. Jegou, and P. Prez. Reconstructing an image from its local descriptors. In CVPR. IEEE Computer Society, 2011. 2, 4, 5</note>
</biblStruct>

<biblStruct coords="9,328.79,357.07,216.32,7.77;9,328.78,367.87,216.33,7.93;9,328.78,378.83,176.22,7.93" xml:id="b26">
	<analytic>
		<title level="a" type="main">Inverting a connectionist network mapping by back-propagation of error</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Annual Conference of the Cognitive Society</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="859" to="865"/>
		</imprint>
	</monogr>
	<note type="raw_reference">R. J. Williams. Inverting a connectionist network mapping by back-propagation of error. In Eighth Annual Conference of the Cognitive Society, pages 859-865, 1986. 2</note>
</biblStruct>

<biblStruct coords="9,328.79,390.94,216.32,7.77;9,328.78,401.74,163.23,7.93" xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10590-1_53</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2014</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="833"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014. 1, 2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Neural Networks via Feature Visualization: A survey</title>
				<funder>
					<orgName type="full">Adobe Systems Inc and Nvidia</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon Research Credits, Auburn University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.53,172.61,54.80,8.80"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
							<email>anhnguyen@auburn.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Auburn University</note>
								<orgName type="institution">Auburn University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.89,172.61,63.68,8.80"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
							<email>yosinski@uber.com</email>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Uber AI Labs</note>
								<orgName type="laboratory">Uber AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.51,172.61,44.13,8.80"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
							<email>jeffclune@uwyo.edu</email>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Uber AI Labs</note>
								<orgName type="laboratory">Uber AI Labs</orgName>
							</affiliation>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> University of Wyoming</note>
								<orgName type="institution">University of Wyoming</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Neural Networks via Feature Visualization: A survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">817F109D169411B2DB92211DEE287FF0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural networks</term>
					<term>feature visualization</term>
					<term>activation maximization</term>
					<term>generator network</term>
					<term>generative models</term>
					<term>optimization In this chapter</term>
					<term>"neuron"</term>
					<term>"cell"</term>
					<term>"unit"</term>
					<term>and "feature" are used interchangeably</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,211.62,269.77,240.62,7.92;1,163.11,280.73,289.14,8.01;1,163.11,291.69,92.21,7.92">A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells.</s><s coords="1,259.21,291.69,193.03,7.92;1,163.11,302.65,289.13,7.92;1,163.11,313.61,206.14,7.92">Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly.</s><s coords="1,372.94,313.61,79.30,7.92;1,163.11,324.57,289.14,7.92;1,162.87,335.53,70.69,7.92">Those methods are known as Activation Maximization (AM) [ ] or Feature Visualization via Optimization.</s><s coords="1,236.65,335.53,215.60,7.92;1,163.11,346.49,289.13,7.92;1,162.03,357.44,291.91,7.92">In this chapter, we ( ) review existing AM techniques in the literature; ( ) discuss a probabilistic interpretation for AM; and ( ) review the applications of AM in debugging and explaining networks.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,153.94,434.81,73.98,8.51">Introduction</head><p><s coords="1,134.77,457.95,347.77,8.80">Understanding the human brain has been a long-standing quest in human history.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="1,134.77,469.90,347.21,8.80">One path to understanding the brain is to study what each neuron codes for [ ],</head><p><s coords="1,134.77,481.86,174.17,8.80">or what information its firing represents.</s><s coords="1,312.24,481.86,55.81,8.80;1,391.70,481.86,88.89,8.80;1,134.77,493.81,345.83,8.80;1,134.77,505.77,346.08,8.80;1,133.60,517.72,34.63,8.80">In the classic 's experiment, Hubel and Wiesel studied a cat's brain by showing the subject different images on a screen while recording the neural firings in the cat's primary visual cortex <ref type="bibr" coords="1,133.60,517.72,31.34,8.80">(Fig. )</ref>.</s><s coords="1,171.55,517.72,309.04,8.90;1,134.77,529.68,194.84,8.80">Among a variety of test images, the researchers found oriented edges to cause high responses in one specific cell [ ].</s><s coords="1,332.92,529.68,147.67,8.90;1,134.00,541.63,253.39,8.90">That cell is referred to as an edge detector and such images are called its preferred stimuli.</s><s coords="1,391.16,541.63,89.43,8.80;1,134.77,553.59,345.83,8.80;1,134.77,565.54,345.83,8.80;1,134.77,577.50,345.83,8.80;1,134.77,589.46,140.72,8.80">The same technique later enabled scientists to discover fundamental findings of how neurons along the visual pathway detect increasingly complex patterns: from circles, edges to faces and high-level concepts such as one's grandmother [ ] or specific celebrities like the actress Halle Berry [ ].</s></p><p><s coords="1,149.71,601.52,330.88,8.80;1,134.77,613.48,290.94,8.80">Similarly, in machine learning (ML), visually inspecting the preferred stimuli of a unit can shed more light into what the neuron is doing <ref type="bibr" coords="1,392.47,613.48,30.52,8.80">[ , ]</ref>.</s><s coords="1,429.01,613.48,51.58,8.80;1,134.77,625.44,345.83,8.80;1,134.77,637.39,146.95,8.80">An intuitive approach is to find such preferred inputs from an existing, large image collection e.g. the training or test set [ ].</s><s coords="1,285.90,637.39,194.69,8.80;2,134.77,321.16,45.55,8.80">However, that method may have undesired properties.</s><s coords="2,183.64,321.16,249.48,8.80">First, it requires testing each neuron on a large image set.</s><s coords="2,436.45,321.16,44.15,8.80;2,134.77,333.11,346.10,8.80;2,134.77,345.07,347.77,8.80;2,134.41,357.02,346.19,8.80;2,134.77,368.98,346.09,8.80;2,134.77,380.93,300.69,8.80">Second, in such a dataset, many informative images that would activate the unit may not exist because the image space is vast and neural behaviors can be complex [ ]. Third, it is often ambiguous which visual features in an image are causing the neuron to fire e.g. if a unit is activated by a picture of a bird on a tree branch, it is unclear if the unit "cares about" the bird or the branch (Fig. <ref type="figure" coords="2,423.32,380.93,4.05,8.80">b</ref>).</s><s coords="2,438.77,380.93,42.09,8.80;2,134.77,392.89,345.82,8.80;2,134.77,404.85,239.36,8.80">Fourth, it is not trivial how to extract a holistic description of what a neuron is for from the typically large set of stimuli preferred by a neuron.</s></p><p><s coords="2,149.71,421.53,330.88,8.80;2,134.77,433.48,345.82,8.80;2,134.77,445.44,180.63,8.80">A common practice is to study the top highest activating images for a unit <ref type="bibr" coords="2,155.92,433.48,31.33,8.80">[ , ]</ref>; however, the top-set may reflect only one among many types of features that are preferred by a unit [ ].</s></p><p><s coords="2,149.71,462.12,330.88,8.80;2,134.77,474.07,241.24,8.80">Instead of finding real images from an existing dataset, one can synthesize the visual stimuli from scratch <ref type="bibr" coords="2,269.84,474.07,103.43,8.80">[ , , , , , , ]</ref>.</s><s coords="2,379.32,474.07,101.27,8.80;2,134.77,486.03,345.83,8.80;2,133.60,497.98,16.71,8.80">The synthesis approach offers multiple advantages: ( ) given a strong image prior, one may synthesize (i.e.</s><s coords="2,153.62,497.98,326.97,8.80;2,134.77,509.94,346.03,8.80;2,134.77,521.89,345.83,8.80;2,134.77,533.85,139.96,8.80">reconstruct) stimuli without the need to access the target model's training set, which may not be available in practice (see Sec. ); ( ) more control over the types and contents of images to synthesize, which helps shed light on more controlled research experiments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="2,134.33,552.80,126.82,7.08">Activation Maximization</head><p><s coords="2,264.49,550.53,216.11,8.80;2,134.77,562.49,69.71,9.36;2,204.47,560.97,33.79,6.12;2,242.72,562.49,237.88,8.80;2,134.41,574.44,348.13,8.80">Let  be the parameters of a classifier that maps an image x âˆˆ R Ã— Ã— (that has  color channels, each of which is  pixels wide and  pixels high) onto a probability distribution over the output classes.</s><s coords="2,134.77,586.40,224.35,9.36;2,359.12,584.88,2.52,6.12;2,359.12,591.09,2.82,6.12;2,362.44,586.40,118.16,9.36;2,134.77,598.35,345.83,8.80;2,134.77,610.31,38.23,8.80">Finding an image x that maximizes the activation    (, x) of a neuron indexed  in a given layer  of the classifier network can be formulated as an optimization problem:</s></p><formula xml:id="formula_0" coords="2,258.73,651.11,221.86,18.14">x * = arg max x (ğ‘ ğ‘™ ğ‘– (ğœƒ, x)) ( )</formula><p><s coords="3,134.41,119.93,346.18,8.90;3,134.77,131.89,106.30,8.80;3,241.06,130.37,2.52,6.12;3,241.06,136.58,2.82,6.12;3,244.38,131.89,236.21,8.90;3,134.77,143.84,345.82,8.80;3,134.77,155.80,168.66,8.80">This problem was introduced as activation maximization (AM) by Erhan, Bengio and others [ ]. Here,    (.) returns the activation value of a single unit as in many previous works <ref type="bibr" coords="3,229.53,143.84,31.93,8.80">[ -]</ref>; however, it can be extended to return any neural response (.) that we wish to study e.g.</s><s coords="3,306.54,155.80,175.99,8.80">activating a group of neurons <ref type="bibr" coords="3,434.52,155.80,45.30,8.80">[ , , ]</ref>.</s><s coords="3,134.41,167.75,346.18,8.80;3,134.77,179.71,243.51,8.80">The remarkable DeepDream visualizations [ ] were created by running AM to activate all the units across a given layer simultaneously.</s><s coords="3,381.60,179.71,99.00,8.80;3,134.41,191.66,92.99,8.80;3,227.39,190.15,2.52,6.12;3,227.39,196.36,2.82,6.12;3,230.71,191.66,251.83,8.80">In this chapter, we will write (.) instead of    (.) when the exact indices ,  can be omitted for generality.</s><s coords="3,149.71,204.54,330.88,8.80;3,134.77,216.49,295.91,8.80">AM is a non-convex optimization problem for which one can attempt to find a local minimum via gradient-based [ ] or non-gradient methods [ ].</s><s coords="3,434.00,216.49,46.54,8.90;3,134.77,228.45,345.83,8.80;3,134.77,240.40,113.97,8.80">In post-hoc interpretability [ ], we often assume access to the parameters and architecture of the network being studied.</s><s coords="3,251.98,240.40,228.87,8.80;3,134.77,252.36,215.93,8.80">In this case, a simple approach is to perform gradient ascent <ref type="bibr" coords="3,164.97,252.36,58.28,8.80">[ , , , ]</ref> with an update rule such as:</s></p><formula xml:id="formula_1" coords="3,256.00,276.47,224.59,23.29">x ğ‘¡+1 = x ğ‘¡ + ğœ– 1 ğœ•ğ‘(ğœƒ, x ğ‘¡ ) ğœ•x ğ‘¡ ( )</formula><p><s coords="3,134.41,309.61,218.17,10.10;3,356.83,309.61,123.76,8.80;3,134.77,321.56,345.83,9.36;3,134.77,333.52,202.32,9.36">That is, starting from a random initialization x 0 (here, a random image), we iteratively take steps in the input space following the gradient of (, x) to find an input x that highly activates a given unit.</s><s coords="3,340.55,333.58,4.04,8.74;3,344.60,338.60,3.97,5.02;3,352.53,333.52,128.06,8.80;3,134.77,345.47,49.85,8.80"> 1 is the step size and is chosen empirically.</s></p><p><s coords="3,149.71,358.35,330.88,8.80;3,134.77,370.30,345.82,8.80;3,134.77,382.26,345.82,8.80;3,134.77,394.21,28.54,8.80">Note that this gradient ascent process is similar to the gradient descent process used to train neural networks via backpropagation [ ], except that here we are optimizing the network input instead of the network parameters , which are frozen.</s><s coords="3,171.46,394.21,309.13,8.80;3,134.77,406.17,252.90,8.80">We may stop the optimization when the neural activation has reached a desired threshold or a certain number of steps has passed.</s></p><p><s coords="3,149.71,419.05,330.88,8.80;3,134.77,431.00,345.83,8.80;3,134.77,442.96,52.16,8.80">In practice, synthesizing an image from scratch to maximize the activation alone (i.e. an unconstrained optimization problem) often yields uninterpretable images [ ].</s><s coords="3,190.25,442.96,290.34,8.90;3,133.60,454.91,182.09,8.90">In a high-dimensional image space, we often find rubbish examples (also known as fooling examples [ ]) e.g.</s><s coords="3,319.01,454.91,161.85,8.80;3,134.77,466.87,275.45,8.80">patterns of high-frequency noise that look like nothing but that highly activate a given unit (Fig. ).</s></p><p><s coords="3,149.71,479.74,330.88,8.80;3,134.77,491.70,345.83,8.90;3,134.77,503.65,346.10,8.80;3,134.77,515.61,226.84,8.80">In a related way, if starting AM optimization from a real image (instead of a random one), we may easily encounter adversarial examples [ ] e.g. an image that is slightly different from the starting image (e.g. of a school bus), but that a network would give an entirely different label e.g.</s><s coords="3,364.92,515.61,61.32,8.80">"ostrich" [ ].</s><s coords="3,429.54,515.61,51.42,8.80;3,134.39,527.56,346.20,8.80;3,134.77,539.52,345.83,8.80;3,134.77,551.47,149.05,8.80">Those early AM visualizations <ref type="bibr" coords="3,217.73,527.56,31.58,8.80">[ , ]</ref> revealed huge security and reliability concerns with machine learning applications and informed a plethora of follow-up adversarial attack and defense research <ref type="bibr" coords="3,257.78,551.47,23.26,8.80">[ , ]</ref>.</s></p><p><s coords="3,134.77,576.31,347.21,9.36;3,134.41,588.26,346.19,8.80;3,134.39,600.22,346.20,8.80;3,158.41,612.17,108.49,8.80">Networks that we visualize Unless otherwise noted, throughout the chapter, we demonstrate AM on CaffeNet, a specific pre-trained model of the well-known AlexNet convnets [ ] to perform single-label image classification on the ILSVRC ImageNet dataset <ref type="bibr" coords="3,238.69,612.17,25.44,8.80">[ , ]</ref>.</s></p><p><s coords="3,152.70,635.75,247.91,8.01">Also sometimes referred to as feature visualization <ref type="bibr" coords="3,355.61,635.75,42.47,7.92">[ , , ]</ref>.</s><s coords="3,403.67,635.75,76.91,7.92;3,134.77,646.71,328.67,7.92">In this chapter, the phrase "visualize a unit" means "synthesize preferred images for a single neuron".</s></p><p><s coords="3,152.70,657.74,327.90,7.92">Therefore, hereafter, we will write () instead of (, ), omitting , for simplicity.</s></p><formula xml:id="formula_2" coords="4,248.25,469.53,232.34,18.14">x * = arg max x (ğ‘(x) -ğ‘…(x)) ( )</formula><p><s coords="4,149.71,498.07,273.41,9.36;4,423.12,496.56,33.79,6.12;4,460.67,498.13,19.93,9.30;4,134.77,510.03,270.48,8.80">For example, to encourage the smoothness in AM images,  : R Ã— Ã— â†’ R may compute the total variation (TV) across an image [ ].</s><s coords="4,409.18,510.03,71.42,8.80;4,134.77,521.98,346.99,8.80;4,134.77,533.94,143.76,8.80">That is, in each update, we follow the gradients to ( ) maximize the neural activation; and ( ) minimize the total variation loss:</s></p><formula xml:id="formula_3" coords="4,233.76,556.63,246.83,23.28">x ğ‘¡+1 = x ğ‘¡ + ğœ– 1 ğœ•ğ‘(x ğ‘¡ ) ğœ•x ğ‘¡ -ğœ– 2 ğœ•ğ‘…(x ğ‘¡ ) ğœ•x ğ‘¡ ( )</formula><p><s coords="4,134.77,588.41,351.02,9.71">However, in practice, we do not always compute the analytical gradient (x  )/x  .</s><s coords="4,134.77,600.37,233.52,9.36;4,368.28,598.85,33.79,6.12;4,405.83,600.43,19.93,9.30;4,425.75,598.85,33.79,6.12;4,463.85,600.37,18.69,8.80;4,134.77,612.32,274.88,9.36">Instead, we may define a regularization operator  : R Ã— Ã— â†’ R Ã— Ã— (e.g. a Gaussian blur kernel), and map x to a more regularized (e.g.</s><s coords="4,412.97,612.32,67.82,8.80;4,134.77,624.28,175.13,8.80">slightly blurrier as in [ ]) version of itself in every step.</s><s coords="4,313.21,624.28,166.52,8.80">In this case, the update step becomes:</s></p><formula xml:id="formula_4" coords="4,254.43,646.97,226.16,23.28">x ğ‘¡+1 = ğ‘Ÿ(x ğ‘¡ ) + ğœ– 1 ğœ•ğ‘(x ğ‘¡ ) ğœ•x ğ‘¡ ( )</formula><p><s coords="5,149.71,119.93,330.88,8.80;5,134.77,131.89,225.88,8.80">Note that this update form in Eq. is strictly more expressive [ ], and allows the use of non-differentiable regularizers (.).</s><s coords="7,149.71,311.08,332.54,8.80;7,134.77,323.04,181.07,8.80">While obtaining limited success, these methods also introduce extra hyperparameters and require further investigation.</s><s coords="7,318.92,323.04,161.67,8.80;7,134.77,334.99,345.83,8.80;7,134.77,346.95,152.64,8.80">For example, if we enforce two stimuli to be different, exactly how far should they be and in which similarity metric should the difference be measured?</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="7,155.77,381.65,324.70,8.51">Activation Maximization via Deep Generator Networks</head><p><s coords="7,134.77,405.70,345.83,8.80;7,134.77,417.65,345.82,8.80;7,134.77,429.61,272.99,8.80">Much previous AM research were optimizing the preferred stimuli directly in the high-dimensional image space where pixel-wise changes are often slow and uncorrelated, yielding high-frequency visualizations (Fig. <ref type="figure" coords="7,386.60,429.61,12.69,8.80">b-e</ref>).</s><s coords="7,411.06,429.61,69.53,8.80;7,134.77,441.56,345.83,8.80;7,134.77,453.52,347.48,8.80;7,134.77,465.48,92.40,8.80">Instead, Nguyen et al. [ ] propose to optimize in the low-dimensional latent space of a deep generator network, which they call Deep Generator Network Activation Maximization (DGN-AM).</s><s coords="7,230.39,465.48,250.57,8.90;7,134.77,477.43,345.83,8.80;7,134.77,489.39,198.56,8.80">They train an image generator network to take in a highly compressed code and output a synthetic image that looks as close to real images from the ImageNet dataset [ ] as possible.</s><s coords="7,337.53,489.39,143.06,8.80;7,134.77,501.34,345.82,8.80;7,134.77,513.30,296.54,8.80">To produce an AM image for a given neuron, the authors optimize in the input latent space of the generator so that it outputs an image that activates the unit of interest (Fig. ).</s><s coords="7,434.62,513.30,47.36,8.80;7,134.77,525.25,346.19,8.80;7,134.77,537.21,345.82,8.80;7,134.77,549.16,337.83,8.80">Intuitively, DGN-AM restricts the search to only the set of images that can be drawn by the prior and encourages the image updates to be more coherent and correlated compared to pixel-wise changes (where each pixel is modified independently).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="7,134.77,575.64,101.05,7.08">Generator networks</head><p><s coords="7,239.13,573.37,241.46,8.80;7,134.77,585.32,53.88,8.80;7,215.55,585.27,166.12,9.40;7,381.67,583.81,33.79,6.12;7,421.06,585.38,21.77,9.30;7,442.83,585.29,19.21,8.83">We denote the sub-network of CaffeNet [ ] that maps images onto -D fc6 features as an encoder  : R Ã— Ã— â†’ R 4096 .</s><s coords="7,466.46,585.32,14.13,8.80;7,134.77,597.28,140.83,9.36;7,275.59,597.25,39.27,9.38;7,314.86,595.76,33.79,6.12;7,353.02,597.28,129.52,9.36">We train a generator network  : R 4096 â†’ R Ã— Ã— to invert  i.e. ((x)) â‰ˆ x.</s><s coords="7,134.77,609.23,345.83,8.80;7,134.77,621.19,347.77,8.80">In addition to the reconstruction losses, the generator was trained using the Generative Adversarial Network (GAN) loss [ ] to improve the image realism.</s><s coords="7,134.77,633.14,345.82,8.80;7,134.07,645.10,347.91,8.90;7,134.77,657.05,347.77,8.80">More training details are in <ref type="bibr" coords="7,259.04,633.14,25.90,8.80">[ , ]</ref>. Intuitively,  can be viewed as an artificial general "painter" that is capable of painting a variety of different types of images, given an arbitrary input description (i.e. a latent code or a condition vector).</s><s coords="8,134.77,321.84,347.77,8.80;8,134.77,333.80,285.30,8.80">that produces an image (middle) that strongly activates a target neuron (e.g. the "candle" output unit) in a given pre-trained network (right).</s><s coords="8,423.37,333.80,57.22,8.80;8,134.77,345.75,345.82,8.80;8,134.77,357.71,260.87,8.80">The iterative optimization procedure involves multiple forward and backward passes through both the generator and the target network being visualized.</s></p><p><s coords="8,134.41,394.97,346.19,8.80;8,134.77,406.93,325.42,8.80">The idea is that  would be able to faithfully portray what a target network has learned, which may be recognizable or unrecognizable patterns to humans.</s><s coords="8,134.77,419.40,345.83,9.36;8,134.77,431.35,209.52,9.36;8,344.29,431.33,136.30,8.83;8,134.77,443.31,229.75,8.80">Optimizing in the latent space Intuitively, we search in the input code space of the generator  to find a code h âˆˆ R 4096 such that the image (h) maximizes the neural activation ((h)) (see <ref type="bibr" coords="8,332.82,443.31,28.43,8.80">Fig. )</ref>.</s><s coords="8,367.83,443.31,104.21,8.80;8,134.77,455.26,59.82,8.80">The AM problem in Eq. now becomes:</s></p><formula xml:id="formula_5" coords="8,240.01,479.24,240.58,18.73">h * = arg max h (ğ‘(ğº(h)) -ğ‘…(h)) ( )</formula><p><s coords="8,134.41,506.81,323.18,8.80">That is, we take steps in the latent space following the below update rule:</s></p><formula xml:id="formula_6" coords="8,225.33,529.31,255.26,23.28">h ğ‘¡+1 = h ğ‘¡ + ğœ– 1 ğœ•ğ‘(ğº(h ğ‘¡ )) ğœ•h ğ‘¡ -ğœ– 2 ğœ•ğ‘…(h ğ‘¡ ) ğœ•h ğ‘¡ ( )</formula><p><s coords="8,149.71,560.89,330.88,9.36;8,134.77,572.85,53.44,9.36">Note that, here, the regularization term (.) is on the latent code h instead of the image x.</s><s coords="8,191.32,572.85,223.04,8.80;8,414.36,577.93,3.97,5.02;8,421.94,572.85,58.65,8.80;8,134.77,584.80,110.80,8.80">Nguyen et al. [ ] implemented a small amount of  2 regularization and also clipped the code.</s><s coords="8,248.71,584.80,231.88,8.80;8,134.77,596.76,167.35,8.80">These hand-designed regularizers can be replaced by a strong, learned prior for the code [ ].</s></p><p><s coords="8,149.71,609.23,331.15,8.80;8,134.77,621.19,345.82,8.80;8,134.77,633.14,88.38,8.80;9,149.71,338.85,330.89,8.80;9,134.77,350.81,345.83,9.36;9,134.77,362.76,294.77,8.80">Optimizing in the latent space of a deep generator network showed a great improvement in image quality compared to previous methods that optimize in the pixel space (Fig.  To improve the image diversity, Nguyen et al. [ ] harnessed a learned realism prior for h via a denoising autoencoder (DAE), and added a small amount of Gaussian noise in every update step to improve image diversity [ ].</s><s coords="9,432.86,362.76,47.73,8.80;9,134.77,374.72,345.82,8.80;9,134.77,386.68,250.72,8.80">In addition to an improvement in image diversity, this AM procedure also has a theoretical probabilistic justification, which is discussed in Section .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,155.16,422.24,325.41,8.51">Probabilistic interpretation for Activation Maximization</head><p><s coords="9,134.77,447.14,345.83,8.80;9,134.77,459.10,345.83,8.80;9,134.77,471.06,345.83,8.80;9,134.77,483.01,140.14,8.80">In this section, we first make a note about the AM objective, and discuss a probabilistically interpretable formulation for AM, which is first proposed in Plug and Play Generative Networks (PPGNs) [ ], and then interpret other AM methods under this framework.</s><s coords="9,278.64,483.01,201.96,8.80;9,134.77,494.97,345.83,8.80;9,133.60,506.92,226.66,8.80">Intuitively, the AM process can be viewed as sampling from a generative model, which is composed of ( ) an image prior and ( ) a recognition network that we want to visualize.</s></p><p><s coords="9,140.69,541.42,3.18,7.08">.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,160.25,541.42,146.23,7.08">Synthesizing selective stimuli</head><p><s coords="9,134.26,561.41,203.27,8.80">We start with a discussion on AM objectives.</s><s coords="9,341.10,561.41,139.49,8.80;9,133.60,573.37,18.32,8.80">In the original AM formulation (Eq.</s><s coords="9,158.73,573.37,196.21,8.80;9,354.94,571.85,2.52,6.12;9,354.94,578.06,2.82,6.12;9,361.13,573.37,120.85,8.80;9,134.77,585.32,345.83,8.80;9,134.77,597.34,5.27,8.74;9,140.03,595.76,2.52,6.12;9,140.03,602.20,12.63,6.51;9,156.59,597.28,280.02,8.80;9,436.61,595.76,2.52,6.12;9,436.61,601.97,2.82,6.12;9,443.35,597.28,19.14,8.80">), we only explicitly maximize the activation    of a unit indexed  in layer ; however, in practice, this objective may surprisingly also increase the activations   Ì¸ = of some other units  in the same layer and even higher than    [ ].</s><s coords="9,465.92,597.28,14.88,8.80;9,134.77,609.23,345.82,8.80;9,134.51,621.19,346.08,8.80;9,134.77,633.14,144.63,8.80">For example, maximizing the output activation for the "hartebeest" class is likely to yield an image that also strongly activates the "impala" unit because these two animals are visually similar [ ].</s><s coords="9,282.90,633.14,197.69,8.80;9,134.77,645.10,232.31,8.80">As the result, there is no guarantee that the target unit will be the highest activated across a layer.</s><s coords="9,370.17,645.10,110.69,8.80;9,134.51,657.05,317.89,8.80">In that case, the resultant visualization may not portray what is unique about the target unit (, ).</s><s coords="10,149.71,395.16,326.13,8.90;10,475.83,393.64,2.52,6.12;10,475.83,399.85,2.82,6.12;10,479.15,395.16,2.83,8.80;10,134.77,407.11,45.15,8.80;10,179.92,405.60,2.52,6.12;10,179.92,412.03,12.63,6.51;10,193.05,407.11,2.83,8.80">Instead, we are interested in selective stimuli that highly activate only    , but not   Ì¸ = .</s><s coords="10,200.86,407.11,149.26,8.80;10,350.12,405.60,2.52,6.12;10,350.12,411.81,2.82,6.12;10,358.41,407.11,122.45,8.80;10,134.77,419.07,184.88,8.80">That is, we wish to maximize    such that it is the highest single activation across the same layer .</s><s coords="10,324.01,419.07,156.58,8.80;10,134.77,431.02,345.83,8.80;10,134.77,442.98,345.83,8.80;10,134.77,454.99,4.67,8.74;10,139.44,453.42,2.52,6.12;10,139.44,459.63,2.82,6.12;10,145.52,454.93,35.05,8.80;10,180.57,453.42,2.52,6.12;10,180.57,459.63,2.82,6.12;10,183.89,453.93,21.09,9.80;10,204.98,460.02,3.30,6.12;10,210.84,454.93,24.42,8.80;10,235.26,453.42,2.52,6.12;10,235.26,459.63,3.30,6.12;10,239.45,454.93,6.74,8.80">To enforce that selectivity, we can either maximize the softmax or log of softmax of the raw activations across a layer <ref type="bibr" coords="10,158.01,442.98,30.28,8.80">[ , ]</ref> where the softmax transformation for unit  across layer  is given as    = exp(   )/ âˆ‘ï¸€  exp(   ).</s><s coords="10,249.49,454.93,231.09,8.80;10,134.77,466.89,346.19,8.80;10,134.77,478.84,345.82,8.80;10,134.77,490.80,134.45,8.80">Such selective stimuli ( ) are more interpretable and preferred in neuroscience [ ] because they contain only visual features exclusively for one unit of interest but not others; ( ) naturally fit in our probabilistic interpretation discussed below.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="10,140.69,530.40,3.18,7.08;10,161.60,530.40,120.22,7.08">. Probabilistic framework</head><p><s coords="10,134.77,555.49,347.21,9.36;10,134.77,567.44,294.66,8.80">Let us assume a joint probability distribution (x, ) where x denotes images, and  is a categorical variable for a given neuron indexed  in layer .</s><s coords="10,432.61,567.44,47.99,8.80;10,134.77,579.40,345.31,8.80">This model can be decomposed into an image density model and an image classifier model:</s></p><formula xml:id="formula_7" coords="10,263.90,609.26,216.69,8.80">ğ‘(x, ğ‘¦) = ğ‘(x)ğ‘(ğ‘¦|x) ( )</formula><p><s coords="10,149.71,633.14,221.24,8.80;10,394.18,633.14,86.05,8.80;10,134.77,645.10,165.62,8.80">Note that, when  is the output layer of an ImageNet -way classifier [ ],  also represents the image category (e.g.</s><s coords="10,303.09,645.10,177.51,8.80;10,134.77,657.05,231.29,8.80">"volcano"), and (|x) is the classification probability distribution (often modeled via softmax).</s></p><p><s coords="11,149.71,119.93,331.08,8.80;11,134.77,131.89,95.21,8.80">We can construct a Metropolis-adjusted Langevin [ ] (MALA) sampler for our (x, ) model [ ].</s><s coords="11,232.66,131.89,248.20,8.80;11,134.77,143.84,208.71,8.80">This variant of MALA [ ] does not have the accept/reject step, and uses the following transition operator:</s></p><formula xml:id="formula_8" coords="11,222.41,167.22,258.19,11.60">x ğ‘¡+1 = x ğ‘¡ + ğœ– 12 âˆ‡ log ğ‘(x ğ‘¡ , ğ‘¦) + ğ‘ (0, ğœ– 2 3 ) ( )</formula><p><s coords="11,134.77,185.36,345.82,9.71;11,134.77,197.31,228.80,8.80">Since  is a categorical variable, and chosen to be a fixed neuron   outside the sampler, the above update rule can be re-written as:</s></p><formula xml:id="formula_9" coords="11,181.55,220.70,299.05,11.60">x ğ‘¡+1 = x ğ‘¡ +ğœ– 12 âˆ‡ log ğ‘(ğ‘¦ = ğ‘¦ ğ‘ |x ğ‘¡ )+ğœ– 12 âˆ‡ log ğ‘(x ğ‘¡ )+ğ‘ (0, ğœ– 2 3 ) ( )</formula><p><s coords="11,134.77,238.83,57.69,8.80;11,192.45,243.92,7.94,5.02;11,204.99,238.83,61.93,8.80;11,266.92,243.92,3.97,5.02;11,275.48,238.83,24.52,8.80;11,300.00,243.92,3.97,5.02;11,308.56,238.83,172.03,8.80;11,134.77,250.79,286.91,8.80">Decoupling  12 into explicit  1 and  2 multipliers, and expanding the âˆ‡ into explicit partial derivatives, we arrive at the following update rule:</s></p><formula xml:id="formula_10" coords="11,180.43,271.21,300.16,23.28">x ğ‘¡+1 = x ğ‘¡ + ğœ– 1 ğœ• log ğ‘(ğ‘¦ = ğ‘¦ ğ‘ |x ğ‘¡ ) ğœ•x ğ‘¡ + ğœ– 2 ğœ• log ğ‘(x ğ‘¡ ) ğœ•x ğ‘¡ + ğ‘ (0, ğœ– 2 3 ) ( )</formula><p><s coords="11,149.71,299.94,330.88,8.80;11,134.77,311.90,136.52,8.80;11,140.99,333.23,14.75,9.30;11,155.75,338.25,3.97,5.02;11,163.82,333.17,316.77,9.71;11,151.19,345.12,153.92,8.90;11,134.77,414.11,275.44,9.36;11,410.20,419.19,3.97,5.02;11,418.18,414.11,51.51,8.80;11,134.77,426.06,278.32,8.90">An intuitive interpretation of the roles of these three terms is illustrated in Fig. <ref type="figure" coords=""/>and<ref type="figure" coords="11,182.81,311.90,88.47,8.80" target="#fig_17">described as follows:</ref> - 1 term: take a step toward an image that causes the neuron   to be the highest activated across a layer (Fig Maximizing raw activations vs. softmax Note that the  1 term in Eq. is not the same as the gradient of raw activation term in Eq. .</s><s coords="11,416.39,426.06,64.20,8.80;11,134.77,438.02,8.48,8.80;11,183.00,521.70,2.77,8.80">We summarize in .</s><s coords="11,149.71,533.66,330.88,8.80;11,134.77,545.61,100.61,8.80">We refer readers to [ ] for a more complete derivation and discussion of the above MALA sampler.</s><s coords="11,238.85,545.61,242.01,8.80;11,134.77,557.57,167.33,8.80">Using the update rule in Eq. , we will next interpret other AM algorithms in the literature.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="11,140.69,589.30,3.18,7.08;11,161.78,589.30,186.75,7.08">. Interpretation of previous algorithms</head><p><s coords="11,134.77,606.52,345.83,8.80;11,134.77,618.47,48.49,8.80">Here, we consider four representative approaches in light of the probabilistic framework:</s></p><p><s coords="11,152.70,635.82,282.66,7.92;11,435.37,635.38,3.65,4.31;11,435.37,640.82,3.65,4.31;11,439.52,635.82,41.07,7.92;11,134.77,646.78,89.24,7.92">We abuse notation slightly in the interest of space and denote as  (0,  2 3 ) a sample from that distribution.</s><s coords="11,226.82,646.78,253.77,7.92;11,134.77,657.74,120.11,7.92">The first step size is given as 12 in anticipation of later splitting into separate 1 and 2 terms.</s><s coords="12,134.41,533.02,348.13,8.80">Table : A comparison of derivatives for use in activation maximization methods.</s></p><p><s coords="12,133.60,544.97,347.00,8.80;12,134.77,556.93,292.12,8.80">(a) has most commonly been used, (b) has worked in the past but with some difficulty, but (c) is correct under the sampler framework in Sec. .</s><s coords="12,435.66,556.93,38.75,8.80">and [ ].</s></p><p><s coords="12,143.95,588.40,227.80,8.80;12,143.95,600.22,2.77,8.80">. AM with no priors [ , , ] (discussed in Sec. ) .</s><s coords="12,151.70,600.22,255.87,8.80;12,143.95,612.04,2.77,8.80">AM with a Gaussian prior [ , , ] (discussed in Sec. ) .</s><s coords="12,151.70,612.04,316.43,8.80;12,143.95,623.86,2.77,8.80">AM with hand-designed priors [ , , , , , ] (discussed in Sec. ) .</s><s coords="12,151.70,623.86,324.59,8.80">AM in the latent space of generator networks [ , ] (discussed in Sec. )</s></p><p><s coords="12,134.33,647.37,196.28,7.08">Activation maximization with no priors.</s><s coords="12,333.66,645.10,102.42,8.80;12,436.08,650.18,3.97,5.02;12,440.55,645.16,8.47,8.74;12,449.02,650.18,3.97,5.02;12,453.49,645.16,8.47,8.74;12,461.96,650.18,3.97,5.02;12,466.43,645.10,14.15,8.80;12,134.77,657.05,346.10,8.80;13,134.77,119.93,246.56,8.80">From Eq. , if we set ( 1 ,  2 ,  3 ) = (1, 0, 0) , we obtain a sampler that follows the neuron gradient directly without contributions from a (x) term or the addition of noise.</s><s coords="13,384.65,119.93,95.94,8.80;13,134.77,131.89,347.77,8.80">In a high-dimensional space, this results in adversarial or rubbish images [ , ] (as discussed in Sec. ).</s><s coords="13,134.26,143.84,346.34,8.80;13,134.77,157.39,44.18,8.80;13,178.94,162.47,3.97,5.02;13,186.73,157.39,111.48,8.80;13,302.72,154.66,32.72,6.51;13,314.36,162.93,9.45,6.12;13,339.41,157.39,141.18,8.80;13,134.77,169.34,108.56,8.80">We can also interpret the optimization procedure in [ , ] as a sampler with a non-zero  1 but with a (x) such that  log (x) x = 0 i.e. a uniform (x) where all images are equally likely.</s></p><p><s coords="13,134.33,184.51,252.33,7.08">Activation maximization with a Gaussian prior.</s><s coords="13,391.51,182.24,89.09,8.80;13,134.77,194.19,345.83,8.80;13,134.77,206.21,6.78,8.74;13,141.55,211.23,3.97,5.02;13,149.54,206.15,331.26,8.80;13,134.77,218.10,345.83,8.80;13,134.77,230.06,265.20,8.80;13,399.96,235.14,3.97,5.02;13,404.43,230.12,8.47,8.74;13,412.90,235.14,3.97,5.02;13,417.37,230.12,8.47,8.74;13,425.84,235.14,3.97,5.02;13,430.31,230.06,51.66,8.80;13,134.77,242.01,347.21,8.80">To avoid producing high-frequency images [ ] that are uninterpretable, several works have used  2 decay, which can be thought of as a simple zero-mean Gaussian prior over images <ref type="bibr" coords="13,168.45,218.10,47.12,8.80">[ , , ]</ref>. From Eq. , if we define a Gaussian (x) centered at the origin (assume the mean image has been subtracted) and set ( 1 ,  2 ,  3 ) = (1, , 0), pulling Gaussian constants into , we obtain the following noiseless update rule:</s></p><formula xml:id="formula_11" coords="13,226.51,266.17,254.08,23.29">x ğ‘¡+1 = (1 -ğœ†)x ğ‘¡ + ğœ• log ğ‘(ğ‘¦ = ğ‘¦ ğ‘ |x ğ‘¡ ) ğœ•x ğ‘¡ ( )</formula><p><s coords="13,149.71,298.97,332.55,8.80;13,134.77,310.93,345.83,8.80;13,134.77,322.88,239.40,8.80">The first term decays the current image slightly toward the origin, as appropriate under a Gaussian image prior, and the second term pulls the image toward higher probability regions for the chosen neuron.</s><s coords="13,377.25,322.88,103.34,8.80;13,134.77,334.84,345.83,8.80;13,134.77,346.79,119.89,8.80">Here, the second term is computed as the derivative of the log of a softmax transformation of all activations across a layer (see Table ).</s></p><p><s coords="13,134.33,361.95,255.28,7.08">Activation maximization with hand-designed priors.</s><s coords="13,392.83,359.68,87.76,8.80;13,134.77,371.64,347.48,8.80;13,134.77,383.59,347.21,8.80;13,134.77,395.55,220.24,8.80">In an effort to outdo the simple Gaussian prior, many works have proposed more creative, handdesigned image priors such as Gaussian blur [ ], total variation [ ], jitter, rotate, scale [ ], and data-driven patch priors [ ].</s><s coords="13,358.15,395.55,122.44,8.80;13,134.77,407.50,161.73,8.80">These priors effectively serve as a simple (x) component in Eq. .</s><s coords="13,299.70,407.50,180.90,8.80;13,134.77,419.46,152.39,8.80;13,287.15,424.54,3.97,5.02;13,294.39,419.46,22.14,8.80">Note that all previous methods considered under this category are noiseless ( 3 = 0).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,134.33,434.62,331.65,7.08">Activation maximization in the latent space of generator networks</head><p><s coords="13,469.29,432.35,11.30,8.80;13,134.77,444.30,347.21,8.80;13,134.77,456.26,347.49,8.80;13,134.77,468.21,289.81,8.80">To ameliorate the problem of poor mixing in the high-dimensional pixel space [ ], several works instead performed optimization in a semantically meaningful, lowdimensional feature space of a generator network <ref type="bibr" coords="13,351.66,468.21,70.15,8.80">[ , , , , ]</ref>.</s></p><p><s coords="13,149.71,479.54,274.39,10.36;13,424.10,486.79,5.01,6.07;13,431.26,481.16,50.71,8.74;13,134.77,493.06,347.21,8.80;13,134.77,505.01,165.17,9.36">That approach can be viewed as re-parameterizing (x) as âˆ«ï¸€ h (x|h)(h), and sampling from the joint probability distribution (h, ) instead of (x, ), treating x as a deterministic variable.</s><s coords="13,303.25,505.01,177.71,8.80;13,134.77,516.97,103.50,8.80">That is, the update rule in Eq. is now changed into the below:</s></p><formula xml:id="formula_12" coords="13,179.80,541.13,300.79,23.28">h ğ‘¡+1 = h ğ‘¡ + ğœ– 1 ğœ• log ğ‘(ğ‘¦ = ğ‘¦ ğ‘ |h ğ‘¡ ) ğœ•h ğ‘¡ + ğœ– 2 ğœ• log ğ‘(h ğ‘¡ ) ğœ•h ğ‘¡ + ğ‘ (0, ğœ– 2 3 ) ( )</formula><p><s coords="13,149.71,574.33,287.12,8.80;13,436.83,579.41,3.97,5.02;13,441.30,574.33,6.87,8.80;13,448.17,579.41,3.97,5.02;13,452.64,574.33,6.87,8.80;13,459.51,579.41,3.97,5.02;13,463.98,574.33,16.61,8.80;13,133.60,586.29,29.42,8.80">In this category, DGN-AM [ ] follows the above rule with ( 1 , 2 , 3 ) = ( , , ).</s><s coords="13,171.19,586.29,242.48,8.80;13,413.67,591.37,3.97,5.02;13,421.46,586.29,59.13,8.80;13,133.60,598.24,309.57,9.36">Specifically, we hand-designed a (h) via clipping and  2 regularization (i.e. a Gaussian prior) to keep the code h within a "realistic" range.</s><s coords="13,447.15,598.24,33.44,8.80;13,134.77,610.20,164.94,8.80">PPGNs follows exactly the update rule in Eq.</s><s coords="16,134.77,119.93,346.19,9.36;16,134.77,131.89,345.83,8.80;16,134.77,143.84,143.85,8.80">Synthesize preferred images activating multiple neurons First, one may synthesize images activating a group of units at the same time to study the interaction between them <ref type="bibr" coords="16,245.68,143.84,30.22,8.80">[ , ]</ref>.</s><s coords="16,281.59,143.84,199.01,8.80;16,134.77,155.80,348.64,8.80;16,134.77,167.75,187.36,8.80">For example, it might be useful to study how a network distinguishes two related and visually similar concepts such as "impala" and "hartebeest" animals in ImageNet [ ].</s><s coords="16,325.45,167.75,155.14,8.80;16,134.77,179.71,345.83,8.90;16,133.83,191.66,147.28,8.80">One way to do this is to synthesize images that maximize the "impala" neuron's activation but also minimize the "hartebeest" neuron's activation.</s><s coords="16,284.91,191.66,195.68,8.80;16,134.77,203.62,208.79,8.80">Second, one may reveal different facets of a neuron [ ] by activating different pairs of units.</s><s coords="16,346.85,203.62,134.01,8.80;16,134.77,215.57,80.59,8.80">That is, activating two units at the same time e.g.</s><s coords="16,218.67,215.57,261.92,8.80;16,134.77,227.53,333.49,8.80">(castle + candle); and (piano + candle) would produce two distinct images of candles that activate the same "candle" unit [ ] (Fig. ).</s><s coords="16,471.57,227.53,9.02,8.80;16,134.77,239.48,345.18,8.80">In addition, this method sometimes also produces interesting, creative art <ref type="bibr" coords="16,447.63,239.48,29.54,8.80">[ , ]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="19,155.77,500.93,156.09,8.51">Discussion and Conclusion</head><p><s coords="19,134.26,525.52,346.33,8.80;19,134.77,537.48,291.84,8.80">While activation maximization has proven a useful tool for understanding neural networks, there are still open challenges and opportunities such as:</s></p><p><s coords="19,140.99,561.41,339.60,9.36;19,151.70,573.37,87.16,8.80">-One might wish to harness AM to compare and contrast the features learned by different models.</s><s coords="19,242.17,573.37,238.43,8.80;19,151.70,585.32,328.89,8.80;19,151.70,597.28,306.05,8.80">That would require a robust, principled AM approach that produces faithful and interpretable visualizations of the learned features for networks trained on different datasets or of different architectures.</s><s coords="19,461.09,597.28,19.50,8.80;19,151.70,609.23,328.89,8.80;19,151.70,621.19,328.89,8.80;19,151.70,633.14,328.89,8.80;19,151.70,645.10,328.89,8.80;19,151.70,657.05,95.03,8.80">This is challenging due to two problems: ( ) the image prior may not be general enough and may have a bias toward a target network or one dataset over the others; ( ) AM optimization on different network architectures, especially of different depths, often requires different hyper-parameter settings to obtain the best performance.</s></p><p><s coords="20,140.99,119.93,341.27,9.36;20,151.70,131.89,73.41,8.80">-It is important for the community to propose rigorous approaches for evaluating AM methods.</s><s coords="20,227.75,131.89,252.84,8.80;20,151.70,143.84,328.89,8.80;20,151.70,155.80,305.71,8.80">A powerful image prior may incur a higher risk of producing misleading visualizations-it is unclear whether a synthesized visual feature comes from the image prior or the target network being studied or both.</s><s coords="20,460.25,155.80,20.34,8.80;20,151.70,167.75,329.09,8.80;20,151.70,179.71,328.89,8.80;20,151.70,191.66,41.01,8.80">Note that we have investigated that and surprisingly found the DGN-AM prior to be able to generate a wide diversity of images including the non-realistic ones (e.g.</s><s coords="20,149.71,348.61,332.54,8.80;20,134.77,360.57,91.57,8.80">Activation maximization techniques enable us to shine light into the blackbox neural networks.</s><s coords="20,229.64,360.57,250.95,8.80;20,134.77,372.52,310.02,8.80">As this survey shows, improving activation maximization techniques improves our ability to understand deep neural networks.</s><s coords="20,448.84,372.52,31.76,8.80;20,134.77,384.48,345.83,8.80;20,134.77,396.43,346.19,8.80;20,134.77,408.39,253.11,8.80">We are excited for what the future holds regarding improved techniques that make neural networks more interpretable and less opaque so we can better understand how deep neural networks do the amazing things that they do.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,265.74,345.82,8.80;2,134.77,277.70,346.09,8.80;2,134.77,289.65,267.15,8.80"><head/><label/><figDesc><div><p><s coords="2,134.77,265.74,345.82,8.80;2,134.77,277.70,346.09,8.80;2,134.77,289.65,267.15,8.80">Fig. : In the classic neuroscience experiment, Hubel and Wiesel discovered a cat's visual cortex neuron (right) that fires strongly and selectively for a light bar (left) when it is in certain positions and orientations [ ].</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,213.89,345.82,8.80;4,134.77,225.84,345.83,8.80;4,134.77,237.80,347.77,8.80;4,134.77,249.75,345.83,8.80;4,133.60,261.71,243.49,8.80"><head/><label/><figDesc><div><p><s coords="4,134.77,213.89,283.77,8.80">Fig. : Example of activation maximization without image priors.</s><s coords="4,421.85,213.89,58.73,8.80;4,134.77,225.84,345.83,8.80;4,134.77,237.80,347.77,8.80;4,134.77,249.75,345.83,8.80;4,133.60,261.71,243.49,8.80">Starting from a random image (a), we iteratively take steps following the gradient to maximize the activation of a given unit, here the "bell pepper" output in CaffeNet [ ].Despite highly activating the unit and being classified as "bell pepper", the image (b) has high frequencies and is not human-recognizable.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,501.07,347.21,8.80;5,134.77,513.03,192.45,8.80;5,327.22,518.11,3.97,5.02;5,334.45,513.03,146.14,8.80;5,134.77,524.98,346.99,8.80;5,134.77,536.94,345.83,8.80;5,134.77,548.89,345.83,8.80;5,134.77,560.85,347.21,8.80;5,134.39,572.80,346.21,8.80;5,134.77,584.76,345.82,8.80;5,134.77,596.71,42.32,8.80;5,162.94,171.60,317.64,309.42"><head>---</head><label/><figDesc><div><p><s coords="5,134.77,501.07,347.21,8.80;5,134.77,513.03,182.91,8.80">Fig. : Activation maximization results of seven methods in the literature (b-h), each employing a different image prior (e.g.</s><s coords="5,320.44,513.09,6.78,8.74;5,327.22,518.11,3.97,5.02;5,334.45,513.03,113.77,8.80"> 2 norm, Gaussian blur, etc.).</s><s coords="5,450.98,513.03,29.60,8.80;5,134.77,524.98,346.99,8.80;5,134.77,536.94,254.75,8.80">Images are synthesized to maximize the output neurons (each corresponding to a class) of the CaffeNet image classifier [ ] trained on ImageNet.</s><s coords="5,392.84,536.94,87.76,8.80;5,134.77,548.89,345.83,8.80;5,134.77,560.85,152.44,8.80">The categories were not cherry-picked, but instead were selected based on the images available in previous papers [ , , , , ].</s><s coords="5,290.73,560.85,191.25,8.80;5,134.39,572.80,346.21,8.80;5,134.77,584.76,275.53,8.80">Overall, while it is a subjective judgement, Activation Maximization via Deep Generator Networks method (h) [ ] produces images with more natural colors and realistic global structures.</s><s coords="5,413.62,584.76,66.97,8.80;5,134.77,596.71,42.32,8.80">Image modified from [ ].</s></p></div></figDesc><graphic coords="5,162.94,171.60,317.64,309.42" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.77,309.89,346.99,8.80;8,134.77,321.84,347.77,8.80;8,134.77,333.80,345.82,8.80;8,134.77,345.75,345.82,8.80;8,134.77,357.71,260.87,8.80;8,208.83,177.23,192.14,95.41"><head/><label/><figDesc><div><p><s coords="8,134.77,309.89,346.99,8.80;8,134.77,321.84,347.77,8.80;8,134.77,333.80,285.30,8.80">Fig. : We search for an input code (red bar) of a deep generator network (left)that produces an image (middle) that strongly activates a target neuron (e.g. the "candle" output unit) in a given pre-trained network (right).</s><s coords="8,423.37,333.80,57.22,8.80;8,134.77,345.75,345.82,8.80;8,134.77,357.71,260.87,8.80">The iterative optimization procedure involves multiple forward and backward passes through both the generator and the target network being visualized.</s></p></div></figDesc><graphic coords="8,208.83,177.23,192.14,95.41" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,231.89,633.14,248.70,8.80;8,134.77,645.10,345.83,8.80;8,134.77,657.05,290.70,8.80"><head/><label/><figDesc><div><p><s coords="8,231.89,633.14,102.70,8.80">; and Fig. b-h vs. Fig.</s><s coords="8,343.51,633.14,9.37,8.80">i).</s><s coords="8,356.20,633.14,124.39,8.80;8,134.77,645.10,345.83,8.80;8,134.77,657.05,290.70,8.80">However, images synthesized by DGN-AM have limited diversity-they are qualitatively similar to the real top-validation images that highest activate a given unit (Fig. ).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,134.77,277.78,346.20,8.80;9,134.77,289.74,345.82,8.80;9,134.77,301.69,78.18,8.80;9,240.61,301.69,210.11,8.80;9,134.77,132.42,345.82,133.90"><head>Fig. :</head><label>:</label><figDesc><div><p><s coords="9,134.77,277.78,346.20,8.80;9,134.77,289.74,345.82,8.80;9,134.77,301.69,78.18,8.80;9,240.61,301.69,134.87,8.80">Fig. : Images synthesized from scratch via DGN-AM method [ ] to highly activate output neurons in the CaffeNet deep neural network [ ], which has learned to classify categories of ImageNet images.</s><s coords="9,378.80,301.69,71.92,8.80">Image from [ ].</s></p></div></figDesc><graphic coords="9,134.77,132.42,345.82,133.90" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,134.77,322.31,345.83,8.80;10,134.51,334.26,346.36,8.80;10,134.77,346.22,346.10,8.80;10,134.77,358.17,347.77,8.80;10,134.77,370.13,71.92,8.80;10,134.77,116.83,345.83,194.01"><head>Fig</head><label/><figDesc><div><p><s coords="10,134.77,322.31,345.83,8.80;10,134.51,334.26,81.70,8.80">Fig. : Side-by-side comparison between real and synthetic stimuli synthesized via DGN-AM [ ].</s><s coords="10,219.52,334.26,261.34,8.80;10,134.77,346.22,300.30,8.80">For each unit, we show the top validation set images that highest activate a given neuron (left) and synthetic images (right).</s><s coords="10,438.40,346.22,42.47,8.80;10,134.77,358.17,347.77,8.80;10,134.77,370.13,71.92,8.80">Note that these synthetic images are of size 227 Ã— 227 i.e. the input size of CaffeNet [ ]. Image from [ ].</s></p></div></figDesc><graphic coords="10,134.77,116.83,345.83,194.01" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="11,305.11,345.12,64.35,8.80;11,140.99,357.03,14.75,9.30;11,155.75,362.05,3.97,5.02;11,163.66,356.97,316.93,8.80;11,151.70,368.93,31.33,8.80;11,140.99,380.83,14.75,9.30;11,155.75,385.86,3.97,5.02;11,163.84,380.77,316.75,8.80;11,151.70,392.73,209.02,8.80"><head/><label/><figDesc><div><p><s coords="11,305.11,345.12,64.35,8.80;11,140.99,357.03,14.75,9.30;11,155.75,362.05,3.97,5.02;11,163.66,356.97,316.93,8.80;11,151.70,368.93,31.33,8.80;11,140.99,383.05,5.73,7.08">. ; red arrow) - 2 term: take a step toward a generic, realistic-looking image (Fig. ; blue arrow).</s><s coords="11,151.70,380.83,4.04,8.74;11,155.75,385.86,3.97,5.02;11,163.84,380.77,316.75,8.80;11,151.70,392.73,209.02,8.80">- 3 term: add a small amount of noise to jump around the search space to encourage image diversity (Fig. ; green arrow).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="12,134.77,243.06,345.82,8.80;12,134.77,255.02,227.20,8.80;12,361.97,260.10,3.97,5.02;12,366.44,255.02,114.43,8.80;12,134.77,266.97,345.83,8.80;12,134.77,278.93,115.78,8.80;12,250.55,277.41,2.30,6.12;12,250.55,283.49,3.01,6.12;12,254.06,278.93,226.54,8.80;12,134.77,290.88,286.07,8.80;12,420.84,289.37,2.30,6.12;12,420.84,295.45,3.01,6.12;12,428.60,290.88,51.99,8.80;12,134.77,302.84,137.55,9.71;12,162.43,186.31,359.18,143.15"><head>Fig</head><label/><figDesc><div><p><s coords="12,134.77,243.06,345.82,8.80;12,134.77,255.02,41.51,8.80">Fig. : AM can be considered as a sampler, traversing in the natural image manifold.</s><s coords="12,180.46,255.02,181.51,8.80;12,361.97,260.10,3.97,5.02;12,366.44,255.02,2.83,8.80">We start from a random initialization â„ 0 .</s><s coords="12,373.45,255.02,107.42,8.80;12,134.77,266.97,345.83,8.80;12,134.77,278.93,115.78,8.80;12,250.55,277.41,2.30,6.12;12,250.55,283.49,3.01,6.12;12,254.06,278.93,6.72,8.80">In every step , we first add a small amount of noise (green arrow), which pushes the sample off the natural-image manifold (â„ â€²  ).</s><s coords="12,264.11,278.93,216.48,8.80;12,134.77,290.88,286.07,8.80;12,420.84,289.37,2.30,6.12;12,420.84,295.45,3.01,6.12;12,428.60,290.88,51.99,8.80;12,134.77,302.84,137.55,9.71">The gradients toward maximizing activation (red arrow) and more realistic images (blue arrow) pull the noisy â„ â€²  back to the manifold at a new sample â„ +1 .</s></p></div></figDesc><graphic coords="12,162.43,186.31,359.18,143.15" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="13,316.03,610.20,164.56,8.80;13,134.77,622.15,345.82,8.80;13,134.77,634.11,65.51,8.80;13,152.70,657.74,210.99,7.92;15,134.33,119.93,346.26,9.36;15,134.77,131.89,346.09,8.80;15,134.77,143.84,163.72,8.80;15,315.52,143.84,146.02,8.80"><head/><label/><figDesc><div><p><s coords="13,316.03,610.20,164.56,8.80;13,134.77,622.15,345.82,8.80;13,134.77,634.11,65.51,8.80;13,152.70,657.74,210.99,7.92;15,134.33,119.93,346.26,9.36;15,134.77,131.89,346.09,8.80;15,134.77,143.84,14.94,8.80">with a better (h) prior learned via a denoising autoencoder [ ]. PPGNs produce images with better diversity than DGN-AM [ ]. 3 = 0 because noise was not used in DGN-AM [ ]. Visualize output units for new tasks We can harness a general learned ImageNet prior to synthesize images for networks trained on a different dataset e.g.</s><s coords="15,153.03,143.84,145.45,8.80;15,315.52,143.84,110.61,8.80">MIT Places dataset [ ] or UCF-activity videos [ ] (Figs.</s><s coords="15,438.22,143.84,23.32,8.80">&amp; ).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="15,134.77,309.57,346.09,8.80;15,134.77,321.53,346.09,8.80;15,134.77,333.48,170.79,8.80;15,321.91,333.48,158.68,8.80;15,134.77,345.44,346.02,8.80;15,134.77,357.39,38.30,8.80;15,134.77,176.21,345.83,121.90"><head>Fig. :</head><label>:</label><figDesc><div><p><s coords="15,134.77,309.57,346.09,8.80;15,134.77,321.53,346.09,8.80;15,134.77,333.48,170.79,8.80;15,321.91,333.48,64.56,8.80">Fig. : Preferred stimuli generated via DGN-AM [ ] for output units of a network trained to classify images on the MIT Places dataset [ ] (left) and a network trained to classify videos from the UCF-dataset (right).</s><s coords="15,389.38,333.48,91.21,8.80;15,134.77,345.44,346.02,8.80;15,134.77,357.39,38.30,8.80">The results suggested that the learned ImageNet prior generalizes well to synthesizing images for other datasets.</s></p></div></figDesc><graphic coords="15,134.77,176.21,345.83,121.90" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="15,134.77,581.69,345.83,8.85;15,134.77,593.69,346.09,8.80;15,134.77,605.65,345.83,8.80;15,134.77,617.60,345.82,8.80;15,134.77,629.56,345.83,8.80;15,134.77,641.51,347.77,8.80;15,134.77,653.47,73.76,8.80;15,134.77,495.07,345.82,75.20"><head>Fig</head><label/><figDesc><div><p><s coords="15,134.77,581.69,345.83,8.85;15,134.77,593.69,134.85,8.80">Fig. : AM images for example hidden units at layer 5 of an CaffeNet [ ] trained to classify images of scenes [ ].</s><s coords="15,272.38,593.69,208.48,8.80;15,134.77,605.65,345.83,8.80;15,134.77,617.60,186.31,8.80">For each unit: the left two images are masked-out real images, each highlighting a region that highly activates the unit via methods in [ ], and humans provide text labels (e.g.</s><s coords="15,323.84,617.60,156.75,8.80;15,134.77,629.56,146.09,8.80">"lighthouse") describing the common theme in the highlighted regions.</s><s coords="15,284.18,629.56,196.42,8.80;15,134.77,641.51,347.77,8.80;15,134.77,653.47,73.76,8.80">The right two images are AM images, which enable the same conclusion regarding what feature a hidden unit has learned.Figure from [ ].</s></p></div></figDesc><graphic coords="15,134.77,495.07,345.82,75.20" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="15,134.77,581.69,345.83,8.85;15,134.77,593.69,346.09,8.80;15,134.77,605.65,345.83,8.80;15,134.77,617.60,345.82,8.80;15,134.77,629.56,345.83,8.80;15,134.77,641.51,347.77,8.80;15,134.77,653.47,73.76,8.80"><head/><label/><figDesc><div><p><s coords="15,134.77,581.69,345.83,8.85;15,134.77,593.69,134.85,8.80">Fig. : AM images for example hidden units at layer 5 of an CaffeNet [ ] trained to classify images of scenes [ ].</s><s coords="15,272.38,593.69,208.48,8.80;15,134.77,605.65,345.83,8.80;15,134.77,617.60,186.31,8.80">For each unit: the left two images are masked-out real images, each highlighting a region that highly activates the unit via methods in [ ], and humans provide text labels (e.g.</s><s coords="15,323.84,617.60,156.75,8.80;15,134.77,629.56,146.09,8.80">"lighthouse") describing the common theme in the highlighted regions.</s><s coords="15,284.18,629.56,196.42,8.80;15,134.77,641.51,347.77,8.80;15,134.77,653.47,73.76,8.80">The right two images are AM images, which enable the same conclusion regarding what feature a hidden unit has learned.Figure from [ ].</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="16,134.77,424.87,345.82,8.80;16,133.83,436.82,346.76,8.80;16,133.60,448.78,346.99,8.80;16,134.77,460.73,327.78,8.80;16,186.64,267.26,242.07,146.14"><head>Fig. :</head><label>:</label><figDesc><div><p><s coords="16,134.77,424.87,345.82,8.80;16,133.83,436.82,346.76,8.80;16,133.60,448.78,48.28,8.80">Fig. : Synthesizing images via DGN-AM [ ] to activate both the "castle" and "candles" units of CaffeNet [ ] produces an image that resembles a castle on fire (top right).</s><s coords="16,185.18,448.78,295.41,8.80;16,134.77,460.73,27.44,8.80">Similarly, "piano" + "candles" produces a candle on a piano (bottom right).</s><s coords="16,165.52,460.73,297.03,8.80">Both rightmost images highly activate the "candles" output neuron.</s></p></div></figDesc><graphic coords="16,186.64,267.26,242.07,146.14" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="17,134.77,523.40,347.77,8.80;17,134.26,535.36,346.33,8.80;17,134.77,547.31,345.82,8.80;17,134.77,559.27,235.76,8.80;17,85.14,437.67,446.51,115.55"><head/><label/><figDesc><div><p><s coords="17,134.77,523.40,347.77,8.80">Fig. : The original ImageNet training set images are in RGB color space (a).</s><s coords="17,134.26,535.36,217.72,8.80">We train CaffeNet [ ] on their BRG versions (b).</s><s coords="17,355.30,535.36,125.29,8.80;17,134.77,547.31,345.82,8.80;17,134.77,559.27,235.76,8.80">The activation maximization images synthesized by DGN-AM [ ], faithfully portray the color space of the images, here BRG, where the network was trained on.</s></p></div></figDesc><graphic coords="17,85.14,437.67,446.51,115.55" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="18,134.77,239.97,347.49,8.80;18,134.77,251.93,347.76,8.80;18,134.77,263.88,346.09,8.80;18,134.77,275.84,94.15,8.80;18,134.77,297.89,345.82,8.80;18,134.77,309.84,94.73,8.80;18,316.48,116.83,309.30,186.43"><head>Fig. :</head><label>:</label><figDesc><div><p><s coords="18,134.77,239.97,347.49,8.80;18,134.77,251.93,347.76,8.80">Fig. : We synthesize input images (right) such that a pre-trained image captioning network (LRCN [ ]) outputs the target caption description (left sentences).</s><s coords="18,134.77,263.88,346.09,8.80;18,134.77,275.84,94.15,8.80">Each image on the right was produced by starting optimization from a different random initialization.</s><s coords="18,134.77,297.89,345.82,8.80;18,134.77,309.84,94.73,8.80">of-the-art image captioner [ ] when it declares birds even when there is no bird in an image (Fig. ).</s></p></div></figDesc><graphic coords="18,316.48,116.83,309.30,186.43" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="18,134.77,498.86,345.82,8.80;18,134.77,510.82,346.19,8.90;18,134.77,522.77,347.77,8.80;18,134.77,534.73,345.83,8.80;18,134.77,546.68,345.83,8.80;18,134.77,558.64,345.83,8.80;18,133.60,570.59,346.99,8.80;18,134.77,582.55,330.11,8.80;18,186.64,341.88,242.08,145.52"><head>Fig. :</head><label>:</label><figDesc><div><p><s coords="18,134.77,498.86,345.82,8.80;18,134.77,510.82,346.19,8.90;18,134.77,522.77,347.77,8.80">Fig. : While synthesizing images to cause an image captioning model [ ] tooutput "A bird is sitting on a branch" via DGN-AM method [ ], we only obtained images of branches or trees that surprisingly has no birds at all (a).</s><s coords="18,134.77,534.73,345.83,8.80;18,134.77,546.68,345.83,8.80;18,134.77,558.64,345.83,8.80;18,133.60,570.59,14.64,8.80">Further tests on real MS COCO images revealed that the model [ ] outputs correct captions for a test image that has a bird (b), but still insists on the existence of the bird, even when it is manually removed via Adobe Photoshop (c).</s><s coords="18,151.56,570.59,329.03,8.80;18,134.77,582.55,330.11,8.80">This suggests the image captioner learned a strong correlation between birds and tree branches-a bias that might exist in the language or image model.</s></p></div></figDesc><graphic coords="18,186.64,341.88,242.08,145.52" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="19,134.77,287.43,345.83,8.80;19,134.77,299.39,345.83,8.80;19,134.77,311.34,345.82,8.80;19,134.77,323.30,345.83,8.80;19,134.77,335.25,189.15,8.80;19,134.77,358.10,346.20,9.36;19,134.77,370.05,345.83,8.80;19,134.77,382.01,345.83,8.80;19,134.77,393.96,345.82,8.80;19,134.77,405.92,345.83,8.80;19,134.77,417.87,345.82,8.80;19,134.77,429.83,345.82,8.80;19,134.77,441.78,345.82,8.80;19,134.77,453.74,345.83,8.80;19,134.77,465.69,174.37,8.80;19,193.80,136.67,257.39,122.92"><head>Fig</head><label/><figDesc><div><p><s coords="19,134.77,287.43,345.83,8.80;19,134.77,299.39,236.46,8.80">Fig. : A segmentation network from [ ] is capable of producing a semantic segmentation map (b) given an input real image (a).</s><s coords="19,374.90,299.39,105.69,8.80;19,134.77,311.34,345.82,8.80;19,134.77,323.30,345.83,8.80;19,134.77,335.25,70.94,8.80">The authors extend the DGN-AM method [ ] to synthesize images (c) to match the target segmentation map (b), which specifies a scene with a building on green grass and under a blue sky background.</s><s coords="19,209.03,335.25,114.89,8.80;19,134.77,358.10,346.20,9.36;19,134.77,370.05,345.83,8.80;19,134.77,382.01,159.85,8.80">Figure modified from [ ]. Synthesize preferred stimuli for real, biological brains While this survey aims at visualizing artificial networks, it is also possible to harness our AM techniques to study biological brains.</s><s coords="19,297.92,382.01,182.68,8.80;19,134.77,393.96,345.82,8.80;19,134.77,405.92,345.83,8.80;19,134.77,417.87,106.75,8.80">Two teams of Neuroscientists [ , ] have recently been able to reconstruct stimuli for neurons in alive macaques' brains using either the ImageNet PPGN (as discussed in Sec. ) [ ] or the DGN-AM (as discussed in Sec. ) [ ].</s><s coords="19,244.84,417.87,235.74,8.80;19,134.77,429.83,345.82,8.80;19,134.77,441.78,345.82,8.80;19,134.77,453.74,345.83,8.80;19,134.77,465.69,174.37,8.80">The synthesized images surprisingly resemble monkeys and human nurses that the subject macaque meets frequently [ ] or show eyes in neurons previously shown to be tuned for detecting faces [ ]. Similar AM frameworks have also been interestingly applied to reconstruct stimuli from EEG or MRI signals of human brains [ , ].</s></p></div></figDesc><graphic coords="19,193.80,136.67,257.39,122.92" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.41,300.41,348.13,153.85"><head>Activation Maximization via Hand-designed Priors</head><label/><figDesc/><table coords="4,134.41,325.81,348.13,128.45"><row><cell>Examples like those in Fig. b are not human-recognizable. While the fact that the</cell></row><row><cell>network responds strongly to such images is intriguing and has strong implications</cell></row><row><cell>for security, if we cannot interpret the images, it limits our ability to understand</cell></row><row><cell>what the unit's purpose is. Therefore, we want to constrain the search to be</cell></row><row><cell>within a distribution of images that we can interpret e.g. photo-realistic images</cell></row><row><cell>or images that look like those in the training set. That can be accomplished by</cell></row><row><cell>incorporating natural image priors into the objective function, which was found</cell></row><row><cell>to substantially improve the recognizability of AM images [ , , , , ].</cell></row><row><cell>For example, an image prior may encourage smoothness [ ] or penalize pixels</cell></row><row><cell>of extreme intensity [ ]. Such constraints are often incorporated into the AM</cell></row><row><cell>formulation as a regularization term ğ‘…(x):</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="20,140.99,191.66,341.55,141.81"><head/><label/><figDesc><div><p><s coords="20,196.02,191.66,166.47,8.80;20,140.99,204.12,340.99,9.36;20,151.70,216.08,330.27,8.80;20,151.70,228.03,177.05,8.80">blurry, cut-up, and BRG images [ ]). -Concepts in modern deep networks can be highly distributed [ , , ]; therefore, it might be promising to apply AM to study networks at a different, larger scale than individual neurons, e.g.</s><s coords="20,332.07,228.03,147.29,8.80;20,140.99,240.49,339.60,9.36;20,151.70,252.44,328.89,8.80;20,151.70,264.40,295.09,8.80;20,140.99,279.13,5.73,7.08">looking at groups of neurons [ ]. -It might be a fruitful direction to combine AM with other tools such as attribution heatmapping [ ] or integrate AM into the testbeds for AI applications [ ] as we move towards safe, transparent, and fair AI.</s><s coords="20,151.70,276.86,330.84,8.80">-One may also perform AM in the parameter space of a D renderer (e.g.</s><s coords="20,151.70,288.81,329.16,8.80;20,151.70,300.77,328.89,8.80;20,151.70,312.72,289.40,8.80">modifying the lighting, object geometry or appearances in a D scene) that renders a D image that strongly activates a unit [ ]. AM in a D space allows us to synthesize stimuli by varying a controlled factor (e.g.</s><s coords="20,444.42,312.72,37.34,8.80;20,151.70,324.68,291.79,8.80">lighting) and thus might offer deeper insights into a model's inner-workings.</s></p></div></figDesc><table/></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments <rs type="person">Anh Nguyen</rs> is supported by <rs type="funder">Amazon Research Credits, Auburn University</rs>, and donations from <rs type="funder">Adobe Systems Inc and Nvidia</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,155.56,121.27,239.31,8.51">Applications of Activation Maximization</head><p><s coords="14,134.77,143.84,347.48,8.80;14,134.77,155.80,203.06,8.80">In this section, we review how one may use activation maximization to understand and explain a pre-trained neural network.</s><s coords="14,340.80,155.80,140.16,8.80;14,134.77,167.75,345.82,8.80;14,134.77,179.71,256.78,8.80">The results below are specifically generated by DGN-AM [ ] and <ref type="bibr" coords="14,284.71,167.75,54.35,8.80">PPGNs [ ]</ref> where the authors harnessed a general image generator network to synthesize AM images.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="20,134.77,488.48,62.92,8.51">References</head><p><s coords="20,143.73,513.26,337.04,7.92;20,151.03,524.22,66.65,7.92">. Akhtar, N., Mian, A.: Threat of adversarial attacks on deep learning in computer vision: A survey.</s><s coords="20,220.75,524.22,62.24,7.92;20,308.27,524.22,4.61,7.92;20,339.64,524.22,3.59,7.92;20,362.43,524.22,29.74,7.92;20,143.73,535.68,2.56,7.92">IEEE Access , -( ) pages .</s><s coords="20,151.27,535.68,282.34,7.92;22,166.89,198.07,185.23,7.92;22,145.00,209.77,2.56,7.92">Alcorn, M.A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.S., <ref type="bibr" coords="20,404.94,535.68,28.67,7.92">Nguyen</ref>  <ref type="figure" coords="22,166.89,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,177.28,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,187.96,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,198.24,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,209.06,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,219.88,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,234.48,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,248.88,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,263.42,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,277.53,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,291.93,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,306.48,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,320.47,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,335.02,198.07,2.56,7.92">,</ref><ref type="figure" coords="22,349.56,198.07,2.56,7.92">,</ref><ref type="figure" coords=""/> .</s><s coords="22,152.54,209.77,328.05,7.92;22,151.27,220.73,194.22,7.92">Nguyen, A., Yosinski, J., Clune, J.: Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.</s><s coords="22,348.54,220.73,10.78,7.92">In:</s></p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
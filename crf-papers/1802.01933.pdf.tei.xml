<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey Of Methods For Explaining Black Box Models</title>
				<funder ref="#_KtQ9Zrd">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_MgZbnzP">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-06-21">21 Jun 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.21,172.79,78.83,8.74"><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Pisa,</note>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> ISTI-CNR, Pisa,</note>
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<settlement>Pisa</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.93,172.79,67.01,8.74"><forename type="first">Anna</forename><surname>Monreale</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Pisa,</note>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.49,172.79,80.17,8.74"><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Pisa,</note>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.22,172.79,59.70,8.74"><forename type="first">Franco</forename><surname>Turini</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Pisa,</note>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.74,184.75,65.40,8.74"><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Pisa,</note>
								<orgName type="institution">University of Pisa</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.69,184.75,69.45,8.74"><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> ISTI-CNR, Pisa,</note>
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<settlement>Pisa</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey Of Methods For Explaining Black Box Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-06-21">21 Jun 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">71B4DD7372DF2505C370E149546606AE</idno>
					<idno type="arXiv">arXiv:1802.01933v3[cs.CY]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-02-14T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Open The Black Box</term>
					<term>Explanations</term>
					<term>Interpretability</term>
					<term>Transparent Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,211.63,259.98,240.62,7.86;1,163.11,270.94,289.14,7.86;1,163.11,281.90,105.68,7.86">In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user.</s><s coords="1,272.85,281.90,179.40,7.86;1,163.11,292.86,122.45,7.86">This lack of explanation constitutes both a practical and an ethical issue.</s><s coords="1,289.26,292.86,162.99,7.86;1,163.11,303.82,289.14,7.86;1,163.11,314.77,140.28,7.86">The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability.</s><s coords="1,306.43,314.77,145.82,7.86;1,163.11,325.73,289.14,7.86;1,163.11,336.69,289.13,7.86;1,163.11,347.65,289.14,7.86;1,163.11,358.61,126.10,7.86">The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation.</s><s coords="1,293.48,358.61,158.76,7.86;1,163.11,369.57,289.13,7.86;1,163.11,380.53,289.13,7.86">The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system.</s><s coords="1,163.11,391.49,289.14,7.86;1,163.11,402.45,289.13,7.86;1,163.11,413.40,84.14,7.86">Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work.</s><s coords="1,250.53,413.40,201.72,7.86;1,163.11,424.36,289.13,7.86;1,163.11,435.32,121.02,7.86">The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="44" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="45" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,134.77,511.06,94.19,10.52">Introduction</head><p><s coords="1,134.77,537.56,345.82,8.74">The last decade has witnessed the rise of ubiquitous opaque decision systems.</s><s coords="1,134.77,549.52,345.82,8.74;1,134.77,561.47,240.95,8.74">These black box systems exploit sophisticated machine learning models to predict individual information that may also be sensitive.</s><s coords="1,379.63,561.47,100.96,8.74;1,134.77,573.43,216.06,8.74">We can consider credit score, insurance risk, health status, as examples.</s><s coords="1,354.93,573.43,125.67,8.74;1,134.77,585.38,345.83,8.74;1,134.77,597.34,174.07,8.74">Machine learning algorithms build predictive models which are able to map user features into a class (outcome or decision) thanks to a learning phase.</s><s coords="1,312.49,597.34,168.10,8.74;1,134.77,609.29,345.83,8.74;1,134.77,621.25,322.99,8.74">This learning process is made possible by the digital traces that people leave behind them while performing everyday activities (e.g., movements, purchases, comments in social networks, etc.).</s><s coords="1,461.17,621.25,19.43,8.74;1,134.77,633.20,295.00,8.74">This enormous amount of data may contain human biases and prejudices.</s><s coords="1,432.60,633.20,47.99,8.74;1,134.77,645.16,345.83,8.74;1,134.77,657.11,90.36,8.74">Thus, decision models learned on them may inherit such biases, possibly leading to unfair and wrong decisions.</s></p><p><s coords="2,149.71,119.99,330.88,8.74;2,134.77,131.95,238.12,8.74">The European Parliament recently adopted the General Data Protection Regulation (GDPR), which will become law in May 2018.</s><s coords="2,376.62,131.95,103.96,8.74;2,134.77,143.90,345.82,8.74;2,134.77,155.86,345.82,8.74;2,134.77,167.81,345.82,8.74;2,134.77,179.77,345.83,8.74;2,134.77,191.72,68.17,8.74">An innovative aspect of the GDPR, which has been much debated, are the clauses on automated (algorithmic) individual decision-making, including profiling, which for the first time introduce, to some extent, a right of explanation for all individuals to obtain "meaningful explanations of the logic involved" when automated decision making takes place.</s><s coords="2,206.83,191.72,273.76,8.74;2,134.77,203.68,345.83,8.74;2,134.77,215.63,345.83,8.74;2,134.77,227.59,107.11,8.74">Despite divergent opinions among legal scholars regarding the real scope of these clauses <ref type="bibr" coords="2,254.62,203.68,16.24,8.74" target="#b31">[31,</ref><ref type="bibr" coords="2,270.85,203.68,16.24,8.74" target="#b101">101,</ref><ref type="bibr" coords="2,287.09,203.68,12.18,8.74" target="#b15">15]</ref>, everybody agrees that the need for the implementation of such a principle is urgent and that it represents today a huge open scientific challenge.</s><s coords="2,245.19,227.59,235.40,8.74;2,134.77,239.54,345.83,8.74">Without an enabling technology capable of explaining the logic of black boxes, the right to an explanation will remain a "dead letter".</s></p><p><s coords="2,149.71,255.20,330.88,8.74;2,134.77,267.15,345.83,8.74;2,134.77,279.11,269.71,8.74">By relying on sophisticated machine learning models trained on massive datasets thanks to scalable, high-performance infrastructures, we risk to create and use decision systems that we do not really understand.</s><s coords="2,407.30,279.11,73.28,8.74;2,134.77,291.06,308.73,8.74">This impacts not only information on ethics, but also on safety and on industrial liability.</s><s coords="2,446.28,291.06,34.32,8.74;2,134.77,303.02,345.82,8.74;2,134.77,314.97,345.82,8.74;2,134.77,326.93,162.07,8.74">Companies increasingly market services and products by embedding machine learning components, often in safety-critical industries such as self-driving cars, robotic assistants, and personalized medicine.</s><s coords="2,299.28,326.93,181.30,8.74;2,134.77,338.88,345.83,8.74;2,134.77,350.84,345.83,8.74;2,134.77,362.79,345.83,8.74;2,134.77,374.75,133.97,8.74">Another inherent risk of these components is the possibility of inadvertently making wrong decisions, learned from artifacts or spurious correlations in the training data, such as recognizing an object in a picture by the properties of the background or lighting, due to a systematic bias in training data collection.</s><s coords="2,271.49,374.75,209.10,8.74;2,134.77,386.70,345.82,8.74;2,134.77,398.66,56.23,8.74">How can companies trust their products without understanding and validating the underlying rationale of their machine learning components?</s><s coords="2,195.03,398.66,285.56,8.74;2,134.77,410.61,264.21,8.74">Gartner predicts that "by 2018 half of business ethics violations will occur through the improper use of Big Data analytics".</s><s coords="2,402.63,410.61,77.96,8.74;2,134.77,422.57,345.83,8.74;2,134.77,434.52,302.49,8.74">Explanation technologies are an immense help to companies for creating safer, more trustable products, and better managing any possible liability they may have.</s><s coords="2,441.10,434.52,39.49,8.74;2,134.77,446.48,345.83,8.74;2,134.77,458.43,345.83,8.74;2,134.77,470.39,345.83,8.74;2,134.77,482.34,124.81,8.74">Likewise, the use of machine learning models in scientific research, for example in medicine, biology, socio-economic sciences, requires an explanation not only for trust and acceptance of results, but also for the sake of the openness of scientific discovery and the progress of research.</s></p><p><s coords="2,149.71,498.00,330.88,8.74;2,134.77,509.95,286.12,8.74">As a consequence, explanation is at the heart of a responsible, open data science, across multiple industry sectors and scientific disciplines.</s><s coords="2,424.48,509.95,56.12,8.74;2,134.77,521.91,345.83,8.74;2,134.77,533.86,32.99,8.74">Different scientific communities studied the problem of explaining machine learning decision models.</s><s coords="2,171.47,533.86,309.12,8.74;2,134.77,545.82,250.77,8.74">However, each community addresses the problem from a different perspective and provides a different meaning to explanation.</s><s coords="2,389.13,545.82,91.47,8.74;2,134.77,557.77,345.83,8.74">Most of the works in the literature come from the machine learning and data mining communities.</s><s coords="2,134.77,569.73,345.82,8.74;2,134.77,581.68,345.83,8.74;2,134.77,593.64,319.47,8.74">The first one is mostly focused on describing how black boxes work, while the second one is more interested in explaining the decisions even without understanding the details on how the opaque decision systems work in general.</s></p><p><s coords="2,149.71,609.29,330.88,8.74;2,134.77,621.25,345.83,8.74;2,134.77,633.20,345.83,8.74;2,134.77,645.16,138.53,8.74">Despite the fact that interpretable machine learning has been a topic for quite some time and received recently much attention, today there are many ad-hoc scattered results, and a systematic organization and classification of these methodologies is missing.</s><s coords="2,277.60,645.16,202.99,8.74;2,134.77,657.11,345.83,8.74;3,134.77,119.99,260.53,8.74">Many questions feed the papers in the literature proposing methodologies for interpreting black box systems <ref type="bibr" coords="2,417.93,657.11,19.99,8.74" target="#b106">[106,</ref><ref type="bibr" coords="2,437.92,657.11,11.99,8.74" target="#b34">34]</ref>: What does it mean that a model is interpretable or transparent?</s><s coords="3,399.93,119.99,80.66,8.74;3,134.77,131.95,32.84,8.74">What is an explanation?</s><s coords="3,171.53,131.95,229.43,8.74">When a model or an explanation is comprehensible?</s><s coords="3,404.89,131.95,75.70,8.74;3,134.77,143.90,345.83,8.74">Which is the best way to provide an explanation and which kind of model is more interpretable?</s><s coords="3,134.77,155.86,295.61,8.74">Which are the problems requiring interpretable models/predictions?</s><s coords="3,434.60,155.86,45.99,8.74;3,134.77,167.81,127.75,8.74">What kind of decision data are affected?</s><s coords="3,266.41,167.81,214.19,8.74;3,134.77,179.77,16.80,8.74">Which type of data records is more comprehensible?</s><s coords="3,154.87,179.77,325.73,8.74;3,134.77,191.72,69.00,8.74">How much are we willing to lose in prediction accuracy to gain any form of interpretability?</s></p><p><s coords="3,149.71,203.92,330.88,8.74;3,134.77,215.87,345.82,8.74;3,134.77,227.83,278.80,8.74">We believe that a clear classification considering simultaneously all these aspects is needed to organize the body of knowledge about research investigating methodologies for opening and understanding the black box.</s><s coords="3,416.63,227.83,63.96,8.74;3,134.77,239.78,345.83,8.74;3,134.77,251.74,159.12,8.74">Existing works tend to provide just a general overview of the problem <ref type="bibr" coords="3,380.78,239.78,15.50,8.74" target="#b59">[59]</ref> highlighting unanswered questions and problems <ref type="bibr" coords="3,275.62,251.74,14.61,8.74" target="#b24">[24]</ref>.</s><s coords="3,297.89,251.74,182.70,8.74;3,134.77,263.69,345.83,8.74;3,134.77,275.65,345.83,8.74;3,134.77,287.60,136.71,8.74">On the other hand, other works focus on particular aspects like the impact of representation formats on comprehensibility <ref type="bibr" coords="3,149.28,275.65,14.61,8.74" target="#b36">[36]</ref>, or the interpretability issues in term of advantages and disadvantages of selected predictive models <ref type="bibr" coords="3,253.21,287.60,14.61,8.74" target="#b27">[27]</ref>.</s><s coords="3,275.39,287.60,205.19,8.74;3,134.77,299.56,345.83,8.74;3,134.77,311.51,345.83,8.74;3,134.77,323.47,345.83,8.74;3,134.77,335.42,345.83,8.74;3,134.77,347.38,44.00,8.74">Consequently, after recognizing four categories of problems and a set of ways to provide an explanation, we have chosen to group the methodologies for opening and understanding black box predictors by considering simultaneously the problem they are facing, the class of solutions proposed for the explanation, the kind of data analyzed and the type of predictor explained.</s></p><p><s coords="3,149.71,359.57,194.60,8.74">The rest of the paper is organized as follows.</s><s coords="3,347.50,359.57,133.09,8.74;3,134.77,371.53,103.46,8.74">Firstly, in Section 3 we discuss what interpretability is.</s><s coords="3,241.61,371.53,238.98,8.74;3,134.77,383.49,288.89,8.74">Section 2 show which are the motivations for requiring explanation for black box systems by illustrating some real cases.</s><s coords="3,427.41,383.49,53.18,8.74;3,134.77,395.44,345.83,8.74;3,134.77,407.40,27.75,8.74">In Section 4 we formalize our problem definitions used to categorize the state of the art works.</s><s coords="3,165.96,407.40,314.63,8.74;3,134.77,419.35,215.50,8.74">Details of the classification and crucial points distinguishing the various approaches and papers are discussed in Section 5.</s><s coords="3,353.41,419.35,127.18,8.74;3,134.77,431.31,159.83,8.74">Sections 6, 7, 8 and 9 present the details of the solutions proposed.</s><s coords="3,297.73,431.31,182.86,8.74;3,134.77,443.26,345.83,8.74;3,134.77,455.22,255.87,8.74">Finally, Section 10 summarizes the crucial aspects emerged from the analysis of the state of the art and discusses which are the open research questions and future research directions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="3,134.77,486.90,205.21,10.52">Needs for Interpretable Models</head><p><s coords="3,134.77,513.41,345.83,8.74;3,134.77,525.37,38.95,8.74">Which are the real problems requiring interpretable models and explainable predictions?</s><s coords="3,179.34,525.37,301.25,8.74;3,134.77,537.32,131.12,8.74">In this section, we briefly report some cases showing how and why black boxes can be dangerous.</s><s coords="3,269.01,537.32,211.58,8.74;3,134.77,549.28,345.82,8.74;3,134.77,561.23,71.10,8.74">Indeed, delegating decisions to black boxes without the possibility of an interpretation may be critical, can create discrimination and trust issues.</s></p><p><s coords="3,149.71,573.43,330.88,8.74;3,134.77,585.38,226.97,8.74">Training a classifier on historical datasets, reporting human decisions, could lead to the discovery of endemic preconceptions <ref type="bibr" coords="3,343.47,585.38,14.61,8.74" target="#b76">[76]</ref>.</s><s coords="3,364.68,585.38,115.91,8.74;3,134.77,597.34,345.82,8.74;3,134.77,609.29,273.71,8.74">Moreover, since these rules can be deeply concealed within the trained classifier, we risk to consider, maybe unconsciously, such practices and prejudices as general rules.</s><s coords="3,413.04,609.29,67.55,8.74;3,134.77,621.25,345.83,8.74;3,134.77,633.20,345.83,8.74;3,134.77,645.16,345.82,8.74;3,134.77,657.11,20.48,8.74">We are warned about a growing "black box society" <ref type="bibr" coords="3,300.26,621.25,14.61,8.74" target="#b74">[74]</ref>, governed by "secret algorithms protected by industrial secrecy, legal protections, obfuscation, so that intentional or unintentional discrimination becomes invisible and mitigation becomes impossible."</s><s coords="4,149.71,119.99,330.88,8.74;4,134.77,131.95,57.39,8.74">Automated discrimination is not new and is not necessarily due to "black box" models.</s><s coords="4,195.81,131.95,284.78,8.74;4,134.77,143.90,325.48,8.74">A computer program for screening job applicants were used during the 1970s and 1980s in St. George's Hospital Medical School (London).</s><s coords="4,463.44,143.90,17.16,8.74;4,134.77,155.86,345.82,8.74;4,134.77,167.81,26.29,8.74">The program used information from applicants' forms, without any reference to ethnicity.</s><s coords="4,164.61,167.81,315.98,8.74;4,134.77,179.77,345.82,8.74;4,134.77,191.72,298.58,8.74">However, the program was found to unfairly discriminate against ethnic minorities and women by inferring this information from surnames and place of birth, and lowering their chances of being selected for interview <ref type="bibr" coords="4,415.08,191.72,14.61,8.74" target="#b62">[62]</ref>.</s></p><p><s coords="4,149.71,204.22,197.12,8.74">More recently, the journalists of propublica.org</s><s coords="4,349.95,204.22,130.65,8.74;4,134.77,216.18,345.82,8.74;4,134.77,228.13,170.49,8.74">have shown that the COMPAS score, a predictive model for the "risk of crime recidivism" (proprietary secret of Northpointe), has a strong ethnic bias.</s><s coords="4,308.81,228.13,171.77,8.74;4,134.77,240.09,345.82,8.74;4,134.77,252.04,345.83,8.74;4,134.77,264.00,134.11,8.74;4,268.87,262.43,3.97,6.12;4,273.34,264.00,2.77,8.74">Indeed, according to this score, a black who did not re-offend were classified as high risk twice as much as whites who did not re-offend, and white repeat offenders were classified as low risk twice as much as black repeat offenders <ref type="foot" coords="4,268.87,262.43,3.97,6.12" target="#foot_0">3</ref> .</s></p><p><s coords="4,149.71,276.50,330.88,8.74;4,134.77,288.46,345.83,8.74;4,134.77,300.41,345.82,8.74;4,134.77,312.37,142.82,8.74">Similarly, a study at Princeton <ref type="bibr" coords="4,286.01,276.50,15.50,8.74" target="#b11">[11]</ref> shows how text and web corpora contain human biases: names that are associated with black people are found to be significantly more associated with unpleasant than with pleasant terms, compared to names associated with whites.</s><s coords="4,280.80,312.37,199.80,8.74;4,134.77,324.32,345.83,8.74;4,134.77,336.28,137.65,8.74">As a consequence, the models learned on such text data for opinion or sentiment mining have a possibility of inheriting the prejudices reflected in the data.</s></p><p><s coords="4,149.71,348.78,196.44,8.74">Another example is related to Amazon.com.</s><s coords="4,350.27,348.78,130.33,8.74;4,134.77,360.74,345.83,8.74;4,134.77,372.69,345.82,8.74;4,134.77,384.65,294.45,8.74;4,429.22,383.07,3.97,6.12;4,433.69,384.65,2.77,8.74">In 2016, the software used to determine the areas of the US to which Amazon would offer free same-day delivery, unintentionally restricted minority neighborhoods from participating in the program (often when every surrounding neighborhood was allowed) <ref type="foot" coords="4,429.22,383.07,3.97,6.12" target="#foot_1">4</ref> .</s></p><p><s coords="4,149.71,397.15,330.89,8.74;4,134.77,409.10,345.83,8.74;4,134.77,421.06,345.82,8.74;4,134.77,433.01,322.34,8.74">With respect to credit bureaus, it is shown in <ref type="bibr" coords="4,345.27,397.15,15.50,8.74" target="#b12">[12]</ref> that banks providing credit scoring for millions of individuals, are often discordant: in a study of 500, 000 records, 29% of consumers received credit scores that differed by at least fifty points among three major US banks (Experian, TransUnion, and Equifax).</s><s coords="4,459.83,433.01,20.76,8.74;4,134.77,444.97,345.83,8.74">Such a difference might mean tens of thousands of dollars over the life of a mortgage.</s><s coords="4,134.77,456.92,345.83,8.74;4,134.77,468.88,234.73,8.74">So much variability implies that the three scoring systems either have a very different and undisclosed bias, or are highly arbitrary.</s></p><p><s coords="4,149.71,481.38,210.48,8.74">As example of bias we can consider <ref type="bibr" coords="4,304.48,481.38,15.50,8.74" target="#b27">[27]</ref> and <ref type="bibr" coords="4,341.92,481.38,14.61,8.74" target="#b84">[84]</ref>.</s><s coords="4,363.14,481.38,117.45,8.74;4,134.77,493.34,345.82,8.74;4,134.77,505.29,74.89,8.74">In these works, the authors show how accurate black box classifiers may result from an accidental artifact in the training data.</s><s coords="4,211.95,505.29,268.64,8.74;4,134.77,517.25,85.14,8.74">In <ref type="bibr" coords="4,223.38,505.29,15.50,8.74" target="#b27">[27]</ref> the military trained a classifier to recognize enemy tanks from friendly tanks.</s><s coords="4,222.58,517.25,258.01,8.74;4,134.77,529.20,251.36,8.74">The classifier resulted in a high accuracy on the test set, but when it was used in the field had very poor performance.</s><s coords="4,389.65,529.20,90.94,8.74;4,134.77,541.16,345.83,8.74;4,134.77,553.11,22.19,8.74">Later was discovered that enemy photos were taken on overcast days, while friendly photos on sunny days.</s><s coords="4,160.16,553.11,320.43,8.74;4,134.77,565.07,345.82,8.74;4,134.77,577.02,117.98,8.74">Similarly, in <ref type="bibr" coords="4,215.82,553.11,15.50,8.74" target="#b84">[84]</ref> is shown that a classifier trained to recognize wolves and husky dogs were basing its predictions to classify a wolf solely on the presence of snow in the background.</s></p><p><s coords="4,149.71,589.53,330.88,8.74;4,134.77,601.48,345.83,8.74;4,134.77,613.44,345.83,8.74;4,134.77,625.39,263.90,8.74">Nowadays, Deep Neural Networks (DNNs) have been reaching very good performances on different pattern-recognition tasks such as visual and text classification which are easily performed by humans: e.g., saying that a tomato is displaced in a picture or that a text is about a certain topic.</s><s coords="4,401.96,625.39,78.63,8.74;5,134.77,119.99,184.30,8.74">Thus, what differ-ences remain between DNNs and humans?</s><s coords="5,322.21,119.99,158.39,8.74;5,134.77,131.95,119.76,8.74">Despite the excellent performance of DNNs it seems to be a lot.</s><s coords="5,258.38,131.95,222.21,8.74;5,134.77,143.90,345.82,8.74;5,134.77,155.86,312.83,8.74">In <ref type="bibr" coords="5,271.38,131.95,15.49,8.74" target="#b93">[93]</ref> it is shown the alteration of an image (e.g. of a tomato) such that the change is undetectable for humans can lead a DNN to tag the image as something else (e.g., labeling a tomato as a dog).</s><s coords="5,451.78,155.86,28.81,8.74;5,134.77,167.81,113.21,8.74">In <ref type="bibr" coords="5,465.09,155.86,15.50,8.74" target="#b69">[69]</ref> a related result is shown.</s><s coords="5,252.35,167.81,228.24,8.74;5,134.77,179.77,345.83,8.74;5,134.77,191.72,272.26,8.74">It is easy to produce images that DNNs believe to be recognizable with 99.99% confidence, but which are completely unrecognizable to humans (e.g., labeling white static noise as a tomato).</s><s coords="5,410.61,191.72,69.98,8.74;5,134.77,203.68,345.82,8.74;5,134.77,215.63,13.89,8.74">Similarly in <ref type="bibr" coords="5,465.09,191.72,15.50,8.74" target="#b46">[46]</ref> visually-indistinguishable training-set are created using DNNs and linear models.</s><s coords="5,151.53,215.63,329.06,8.74;5,134.77,227.59,60.90,8.74">With respect to text, in <ref type="bibr" coords="5,256.44,215.63,15.50,8.74" target="#b58">[58]</ref> effective methods to attack DNN text classifiers are presented.</s><s coords="5,199.17,227.59,281.41,8.74;5,134.77,239.54,345.83,8.74;5,134.77,251.50,211.69,8.74">Experiments show that the perturbations introduced in the text are difficult to be perceived by a human but are still able to fool a state-of-the-art DNN to misclassify a text as any desirable class.</s><s coords="5,349.75,251.50,130.85,8.74;5,134.77,263.45,345.83,8.74;5,134.77,275.41,93.63,8.74">These results show interesting differences between humans and DNNs, and raise reasonable doubts about trusting such black boxes.</s><s coords="5,232.26,275.41,248.33,8.74;5,134.77,287.36,345.82,8.74">In <ref type="bibr" coords="5,245.26,275.41,20.48,8.74" target="#b112">[112]</ref> it is shown how conventional regularization and small generalization error fail to explain why DNNs generalize well in practice.</s><s coords="5,134.77,299.32,345.82,8.74;5,134.77,311.27,266.40,8.74">Specifically, they prove that established state-of-the-art CNN trained for image classification easily fits a random labeling of the training data.</s><s coords="5,403.77,311.27,76.82,8.74;5,134.77,323.23,323.65,8.74">This phenomenon occurs even if the true images are replaced by unstructured random noise.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" coords="5,134.77,356.36,345.83,10.52">Interpretable, Explainable and Comprehensible Models</head><p><s coords="5,134.77,384.31,345.83,8.74;5,134.77,396.26,345.83,8.74;5,134.77,408.22,286.53,8.74">Before presenting the classification of the problems addressed in the literature with respect to black box predictors, and the corresponding solutions and models categorization, it is crucial to understand what interpretability is.</s><s coords="5,424.70,408.22,55.90,8.74;5,134.77,420.17,345.83,8.74;5,134.77,432.13,345.83,8.74">Thus, in this section, we discuss what an interpretable model is, and we analyze the various dimensions of interpretability as well as the desiderata for an interpretable model.</s><s coords="5,134.77,444.08,345.83,8.74;5,134.77,456.04,270.34,8.74">Moreover, we also discuss the meaning of words like interpretability, explainability and comprehensibility which are largely used in the literature.</s></p><p><s coords="5,149.71,468.52,330.88,8.74;5,134.77,480.48,166.32,8.74;5,301.09,478.90,3.97,6.12;5,305.55,480.48,2.77,8.74">To interpret means to give or provide the meaning or to explain and present in understandable terms some concept <ref type="foot" coords="5,301.09,478.90,3.97,6.12" target="#foot_2">5</ref> .</s><s coords="5,311.38,480.48,169.21,8.74;5,134.77,492.43,345.83,8.74;5,134.77,504.39,230.27,8.74">Therefore, in data mining and machine learning, interpretability is defined as the ability to explain or to provide the meaning in understandable terms to a human <ref type="bibr" coords="5,346.77,504.39,14.61,8.74" target="#b24">[24]</ref>.</s><s coords="5,369.72,504.39,110.87,8.74;5,134.77,516.34,345.83,8.74;5,134.77,528.30,294.31,8.74">These definitions assume implicitly that the concepts expressed in the understandable terms composing an explanation are self-contained and do not need further explanations.</s><s coords="5,431.91,528.30,48.68,8.74;5,134.77,540.25,345.83,8.74;5,134.77,552.21,345.82,8.74;5,134.77,564.16,48.48,8.74">Essentially, an explanation is an "interface" between humans and a decision maker that is at the same time both an accurate proxy of the decision maker and comprehensible to humans.</s></p><p><s coords="5,149.71,576.65,330.89,8.74;5,134.77,588.60,345.83,8.74;5,134.77,600.56,58.45,8.74">As shown in the previous section another significant aspect to mention about interpretability is the reason why a system, a service or a method should be interpretable.</s><s coords="5,196.58,600.56,284.01,8.74;5,134.77,612.51,327.01,8.74">On the other hand, an explanation could be not required if there are no decisions that have to be made on the outcome of the prediction.</s><s coords="5,466.04,612.51,14.55,8.74;5,134.77,624.47,345.82,8.74;5,134.77,636.42,345.82,8.74;6,134.77,119.99,345.82,8.74;6,134.77,131.95,112.64,8.74">For example, if we want to know if an image contains a cat or not and this information is not required to take any sort of crucial decision, or there are no consequences for unacceptable results, then we do not need an interpretable model, and we can accept any black box.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="6,134.77,163.37,177.68,8.77">Dimensions of Interpretability</head><p><s coords="6,134.77,184.90,345.83,8.74;6,134.77,196.85,345.82,8.74;6,134.77,208.81,125.11,8.74">In the analysis of the interpretability of predictive models, we can identify a set of dimensions to be taken into consideration, and that characterize the interpretability of the model <ref type="bibr" coords="6,241.61,208.81,14.61,8.74" target="#b24">[24]</ref>.</s></p><p><s coords="6,149.71,221.08,330.88,8.74;6,134.77,233.03,345.83,8.74;6,134.77,244.99,270.34,8.74">Global and Local Interpretability: A model may be completely interpretable, i.e., we are able to understand the whole logic of a model and follow the entire reasoning leading to all the different possible outcomes.</s><s coords="6,409.70,244.99,70.89,8.74;6,134.77,256.94,182.33,8.74">In this case, we are speaking about global interpretability.</s><s coords="6,320.49,256.94,160.10,8.74;6,134.77,268.90,345.82,8.74;6,134.77,280.85,320.30,8.74">Instead, we indicate with local interpretability the situation in which it is possible to understand only the reasons for a specific decision: only the single prediction/decision is interpretable.</s></p><p><s coords="6,149.71,293.12,330.88,8.74;6,134.77,305.08,231.36,8.74">Time Limitation: An important aspect is the time that the user is available or is allowed to spend on understanding an explanation.</s><s coords="6,369.26,305.08,111.33,8.74;6,134.77,317.03,345.82,8.74">The user time availability is strictly related to the scenario where the predictive model has to be used.</s><s coords="6,134.77,328.99,345.82,8.74;6,134.77,340.94,345.83,8.74;6,134.77,352.90,51.56,8.74">Therefore, in some contexts where the user needs to quickly take the decision (e.g., a disaster is imminent), it is preferable to have an explanation simple to understand.</s><s coords="6,189.65,352.90,290.94,8.74;6,134.77,364.85,345.83,8.74;6,134.77,376.81,102.44,8.74">While in contexts where the decision time is not a constraint (e.g., during a procedure to release a loan) one might prefer a more complex and exhaustive explanation.</s></p><p><s coords="6,149.71,389.08,330.88,8.74;6,134.77,401.03,345.83,8.74;6,134.77,412.99,345.83,8.74;6,134.77,424.94,345.83,8.74">Nature of User Expertise: Users of a predictive model may have different background knowledge and experience in the task: decision-makers, scientists, compliance and safety engineers, data scientists, etc. Knowing the user experience in the task is a key aspect of the perception of interpretability of a model.</s><s coords="6,134.77,436.90,345.83,8.74;6,134.77,448.85,144.31,8.74">Domain experts may prefer a larger and more sophisticated model over a smaller and sometimes more opaque one.</s></p><p><s coords="6,149.71,461.12,330.88,8.74;6,134.77,473.08,73.81,8.74">The works reviewed in the literature only implicitly specify if their proposal is global or local.</s><s coords="6,211.66,473.08,268.93,8.74;6,134.77,485.03,345.82,8.74;6,134.77,496.99,120.04,8.74">Just a few of them take into account the nature of user expertise <ref type="bibr" coords="6,152.93,485.03,15.90,8.74" target="#b29">[29,</ref><ref type="bibr" coords="6,168.83,485.03,11.93,8.74" target="#b84">84,</ref><ref type="bibr" coords="6,180.76,485.03,11.93,8.74" target="#b87">87]</ref>, and no one provides real experiments about the time required to understand an explanation.</s><s coords="6,258.46,496.99,222.12,8.74;6,134.77,508.94,217.20,8.74">Instead, some of the works consider the "complexity" of an explanation through an approximation.</s><s coords="6,355.41,508.94,125.19,8.74;6,134.77,520.90,183.98,8.74">For example, they define the model complexity as the model's size (e.g.</s><s coords="6,322.13,520.90,158.47,8.74;6,134.77,532.85,155.19,8.74">tree depth, number of rules, number of conjunctive terms) <ref type="bibr" coords="6,233.50,532.85,16.13,8.74" target="#b22">[22,</ref><ref type="bibr" coords="6,249.63,532.85,12.10,8.74" target="#b32">32,</ref><ref type="bibr" coords="6,261.73,532.85,12.10,8.74" target="#b40">40,</ref><ref type="bibr" coords="6,273.82,532.85,12.10,8.74" target="#b84">84]</ref>.</s><s coords="6,294.25,532.85,186.34,8.74;6,134.77,544.81,191.81,8.74">In the following, we further discuss issues related to the complexity of an explanation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="6,134.77,576.24,213.89,8.77">Desiderata of an Interpretable Model</head><p><s coords="6,134.77,597.76,273.06,8.74">An interpretable model is required to provide an explanation.</s><s coords="6,411.67,597.76,68.92,8.74;6,134.77,609.72,345.83,8.74;6,134.77,621.67,345.83,8.74">Thus, to realize an interpretable model it is necessary to take into account the following list of desiderata which are mentioned by a set of papers in the state of art <ref type="bibr" coords="6,429.12,621.67,11.88,8.74" target="#b4">[5,</ref><ref type="bibr" coords="6,440.99,621.67,11.88,8.74" target="#b24">24,</ref><ref type="bibr" coords="6,452.87,621.67,11.88,8.74" target="#b27">27,</ref><ref type="bibr" coords="6,464.75,621.67,11.88,8.74" target="#b38">38]</ref>:</s></p><p><s coords="6,140.99,645.13,339.60,8.77;6,151.70,657.11,92.11,8.74">-Interpretability: to which extent the model and or the prediction are human understandable.</s><s coords="6,247.84,657.11,232.75,8.74;7,151.70,119.99,144.26,8.74">The most addressed discussion is related to how the interpretability can be measured.</s><s coords="7,299.17,119.99,181.43,8.74;7,151.70,131.95,328.89,8.74;7,151.70,143.90,18.32,8.74">In <ref type="bibr" coords="7,311.51,119.99,15.50,8.74" target="#b27">[27]</ref> a component for measuring the interpretability is the complexity of the predictive model in terms of the model size.</s><s coords="7,174.13,143.90,306.46,8.74;7,151.70,155.86,103.80,8.74">According to the literature, we refer to interpretability also with the name comprehensibility.</s></p><p><s coords="7,140.99,173.07,339.60,8.77">-Accuracy: to which extent the model accurately predict unseen instances.</s></p><p><s coords="7,151.70,185.05,328.89,8.74;7,151.70,197.01,328.89,8.74;7,151.70,208.96,328.89,8.74;7,151.70,220.92,153.17,8.74">The accuracy of a model can be measured using various evaluation measures like the accuracy score, the F1-score <ref type="bibr" coords="7,318.82,197.01,14.60,8.74" target="#b95">[95]</ref>, etc. Producing an interpretable model maintaining competitive levels of accuracy is the most common target among the papers in the literature.</s></p><p><s coords="7,140.99,238.13,339.60,8.77;7,151.70,250.11,42.12,8.74">-Fidelity: to which extent the model is able to accurately imitate a black-box predictor.</s><s coords="7,198.05,250.11,282.55,8.74;7,151.70,262.07,195.01,8.74">The fidelity captures how much is good an interpretable model in the mimic of the behavior of a black-box.</s><s coords="7,350.40,262.07,130.20,8.74;7,151.70,274.02,328.89,8.74;7,151.70,285.98,284.43,8.74">Similarly to the accuracy, the fidelity is measured in terms of accuracy score, F1-score, etc. but with respect to the outcome of the black box which is considered as an oracle.</s></p><p><s coords="7,149.71,329.04,330.88,8.74;7,134.77,340.99,345.83,8.74;7,134.77,352.95,121.12,8.74">Moreover, besides these features strictly related to interpretability, yet according to <ref type="bibr" coords="7,183.21,340.99,12.18,8.74" target="#b4">[5,</ref><ref type="bibr" coords="7,195.39,340.99,12.18,8.74" target="#b24">24,</ref><ref type="bibr" coords="7,207.57,340.99,12.18,8.74" target="#b27">27,</ref><ref type="bibr" coords="7,219.74,340.99,12.18,8.74" target="#b38">38]</ref> a data mining and machine learning model should have other important desiderata.</s><s coords="7,259.63,352.95,220.97,8.74;7,134.77,364.90,154.46,8.74">Some of these desiderata are related to ethical aspects such as fairness and privacy.</s><s coords="7,293.05,364.90,187.54,8.74;7,134.76,376.86,345.83,8.74;7,134.76,388.82,345.82,8.74;7,134.76,400.77,107.88,8.74">The first principle requires that the model guarantees the protection of groups against (implicit or explicit) discrimination <ref type="bibr" coords="7,134.76,388.82,14.61,8.74">[85]</ref>; while the second one requires that the model does not reveal sensitive information about people <ref type="bibr" coords="7,229.36,400.77,9.96,8.74" target="#b3">[4]</ref>.</s><s coords="7,246.32,400.77,234.27,8.74;7,134.76,412.73,345.83,8.74;7,134.76,424.68,211.25,8.74">The level of interpretability of a model together with the standards of privacy and non-discrimination which are guaranteed may impact on how much human users trust that model.</s><s coords="7,348.71,424.68,131.88,8.74;7,134.76,436.64,345.83,8.74;7,134.76,448.59,85.11,8.74">The degree of trust on a model increases if the model is built by respecting constraints of monotonicity given by the users <ref type="bibr" coords="7,176.15,448.59,15.90,8.74" target="#b66">[66,</ref><ref type="bibr" coords="7,192.05,448.59,11.93,8.74" target="#b75">75,</ref><ref type="bibr" coords="7,203.98,448.59,11.93,8.74" target="#b99">99]</ref>.</s><s coords="7,222.80,448.59,257.79,8.74;7,134.76,460.55,345.82,8.74;7,134.76,472.50,345.83,8.74;7,134.76,484.46,129.83,8.74">A predictor respecting the monotonicity principle is, for example, a predictor where the increase of the values of a numerical attribute tends to either increase or decrease in a monotonic way the probability of a record of being member of a class <ref type="bibr" coords="7,246.33,484.46,14.61,8.74" target="#b27">[27]</ref>.</s><s coords="7,268.61,484.46,211.98,8.74;7,134.76,496.41,345.83,8.74;7,134.76,508.37,231.55,8.74">Another property that influences the trust level of a model is usability: people tend to trust more models providing information that assist them to accomplish a task with awareness.</s><s coords="7,369.23,508.37,111.36,8.74;7,134.76,520.32,345.82,8.74;7,134.76,532.28,53.41,8.74">In this line, an interactive and queryable explanation results to be more usable than a textual and fixed explanation.</s></p><p><s coords="7,149.71,549.52,330.89,8.74;7,134.76,561.47,345.83,8.74">Data mining and machine learning models should have other important desiderata such as reliability, robustness, causality, scalability and generality.</s><s coords="7,134.76,573.43,345.83,8.74;7,134.76,585.38,345.83,8.74;7,134.76,597.34,345.83,8.74;7,134.76,609.29,166.06,8.74">This means that a model should have the ability to maintain certain levels of performance independently from the parameters or from the input data (reliability/robustness) and that controlled changes in the input due to a perturbation affect the model behavior (causality).</s><s coords="7,304.87,609.29,175.72,8.74;7,134.76,621.25,345.83,8.74;7,134.76,633.20,56.13,8.74">Moreover, since we are in the Big Data era, it is opportune to have models able to scale to large input data with large input spaces.</s><s coords="7,193.79,633.20,286.80,8.74;7,134.76,645.16,345.82,8.74;7,134.76,657.11,288.37,8.74">Finally, since often in different application scenarios one might use the same model with different data, it is preferable to have portable models that do not require special training regimes or restrictions (generality).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" coords="8,134.77,237.79,192.60,8.77">Recognized Interpretable Models</head><p><s coords="8,134.77,256.86,345.82,8.74;8,134.77,268.81,191.20,8.74">In the state of the art a small set of existing interpretable models is recognized: decision tree, rules, linear models <ref type="bibr" coords="8,282.24,268.81,15.90,8.74" target="#b27">[27,</ref><ref type="bibr" coords="8,298.14,268.81,11.93,8.74" target="#b36">36,</ref><ref type="bibr" coords="8,310.06,268.81,11.93,8.74" target="#b84">84]</ref>.</s><s coords="8,329.16,268.81,151.43,8.74;8,134.77,280.77,199.75,8.74">These models are considered easily understandable and interpretable for humans.</s></p><p><s coords="8,149.71,292.72,330.88,8.74;8,134.77,304.68,345.83,8.74;8,134.77,316.63,345.83,8.74;8,134.77,328.59,287.83,8.74">A decision system based on a decision tree exploits a graph structured like a tree and composed of internal nodes representing tests on features or attributes (e.g., whether a variable has a value lower than, equals to or grater than a threshold, see Figure <ref type="figure" coords="8,229.29,328.59,3.87,8.74" target="#fig_0">1</ref>), and leaf nodes representing a class label.</s><s coords="8,426.05,328.59,54.54,8.74;8,134.77,340.54,150.38,8.74">Each branch represents a possible outcome <ref type="bibr" coords="8,266.88,340.54,14.61,8.74" target="#b79">[79]</ref>.</s><s coords="8,288.36,340.54,192.22,8.74;8,134.77,352.50,120.56,8.74">The paths from the root to the leaves represent the classification rules.</s><s coords="8,258.99,352.50,221.60,8.74;8,134.77,364.45,212.39,8.74">Indeed, a decision tree can be linearized into a set of decision rules with the if-then form <ref type="bibr" coords="8,303.43,364.45,15.90,8.74" target="#b77">[77,</ref><ref type="bibr" coords="8,319.33,364.45,11.93,8.74" target="#b78">78,</ref><ref type="bibr" coords="8,331.25,364.45,11.93,8.74" target="#b26">26]</ref>:</s></p><formula xml:id="formula_0" coords="8,190.25,385.25,234.85,9.65">if condition 1 ∧ condition 2 ∧ condition 3 then outcome.</formula><p><s coords="8,134.77,406.06,345.82,8.74;8,134.77,418.01,345.82,8.74;8,134.77,429.97,178.28,8.74">Here, the outcome corresponds to the class label of a leaf node while the conjunctions of conditions in the if clause correspond to the different conditions in the path from the root to that leaf node.</s></p><p><s coords="8,149.71,441.92,330.88,8.74;8,134.77,453.88,99.48,8.74">More generally, a decision rule is a function which maps an observation to an appropriate action.</s><s coords="8,238.85,453.88,241.74,8.74;8,134.77,465.83,345.83,8.74;8,134.77,477.79,78.20,8.74">Decision rules can be extracted by generating the socalled classification rules, i.e., association rules that in the consequence have the class label <ref type="bibr" coords="8,199.68,477.79,9.96,8.74" target="#b2">[3]</ref>.</s><s coords="8,216.39,477.79,264.20,8.74;8,134.77,489.74,236.76,8.74">The most common rules are if-then rules where the if clause is a combination of conditions on the input variables.</s><s coords="8,375.39,489.74,105.19,8.74;8,134.77,501.70,227.00,8.74">In particular, it may be formed by conjunctions, negations and disjunctions.</s><s coords="8,365.21,501.70,115.38,8.74;8,134.77,513.65,316.96,8.74">However, methods for rule extraction typically take into consideration only rules with conjunctions.</s><s coords="8,455.11,513.65,25.49,8.74;8,134.77,525.61,345.83,8.74;8,134.77,537.56,345.83,8.74;8,134.77,549.52,345.82,8.74;8,134.77,561.47,345.82,8.74;8,134.77,573.43,345.83,8.74;8,134.77,585.38,345.83,8.74;8,134.77,597.34,345.83,8.74;8,134.77,609.29,345.82,8.74;8,134.77,621.25,170.39,8.74">Other types of rules are: m-of-n rules where given a set of n conditions if m of them are verified then the consequence of the rule is considered true <ref type="bibr" coords="8,384.17,537.56,14.61,8.74" target="#b68">[68]</ref>; list of rules where given an ordered set of rules is considered true the consequent of the first rule which is verified <ref type="bibr" coords="8,208.42,561.47,19.37,8.74" target="#b109">[109]</ref>; falling rule lists consists of a list of if-then rules ordered with respect to the probability of a specific outcome and the order identifies the example to be classified by that rule <ref type="bibr" coords="8,294.98,585.38,19.37,8.74" target="#b102">[102]</ref>; decision sets where an unordered set of classification rules is provided such that the rules are not connected by else statements, but each rule is an independent classifier that can assign its label without regard for any other rules <ref type="bibr" coords="8,286.89,621.25,14.61,8.74" target="#b53">[53]</ref>.</s></p><p><s coords="8,149.71,633.20,330.88,8.74;8,134.77,645.16,92.47,8.74">The interpretation of rules and decision trees is different with respect to different aspects <ref type="bibr" coords="8,208.97,645.16,14.61,8.74" target="#b27">[27]</ref>.</s><s coords="8,230.66,645.16,249.94,8.74;8,134.77,657.11,241.04,8.74">Decision trees are widely adopted for their graphical representation, while rules have a textual representation.</s><s coords="8,380.33,657.11,100.26,8.74;9,134.77,222.80,345.83,8.74;9,134.77,234.76,148.53,8.74">The main difference is that textual representation does not provide immediately information about the more relevant attributes of a rule.</s><s coords="9,286.80,234.76,193.80,8.74;9,134.77,246.71,203.34,8.74">On the other hand, the hierarchical position of the features in a tree gives this kind of clue.</s></p><p><s coords="9,149.71,260.63,330.88,8.74;9,134.77,272.58,81.33,8.74">Attributes' relative importance could be added to rules by means of positional information.</s><s coords="9,219.07,272.58,261.52,8.74;9,134.77,284.54,284.77,8.74">Specifically, rule conditions are shown by following the order in which the rule extraction algorithm added them to the rule.</s><s coords="9,423.98,284.54,56.61,8.74;9,134.77,296.50,345.82,8.74;9,134.77,308.45,345.82,8.74;9,134.77,320.41,226.38,8.74">Even though the representation of rules causes some difficulties in understanding the whole model, it enables the study of single rules representing partial parts of the whole knowledge ("local patterns") which are composable.</s><s coords="9,364.38,320.41,116.21,8.74;9,134.77,332.36,345.83,8.74;9,134.77,344.32,123.45,8.74">Also in a decision tree, the analysis of each path separately from the leaf node to the root, enables users to focus on such local patterns.</s><s coords="9,261.35,344.32,219.24,8.74;9,134.77,356.27,109.70,8.74">However, if the tree is very deep in this case it is a much more complex task.</s><s coords="9,247.25,356.27,233.35,8.74;9,134.77,368.23,345.83,8.74;9,134.77,380.18,345.83,8.74;9,134.77,392.14,256.57,8.74">A further crucial difference between rules and decision trees is that in a decision tree each record is classified by only one leaf node, i.e., the class predicted are represented in a mutually exclusive and exhaustive way by the set of leaves and their paths to the root node.</s><s coords="9,395.10,392.14,85.49,8.74;9,134.77,404.09,345.83,8.74;9,134.77,416.05,118.85,8.74">On the other hand, a certain record can satisfy the antecedent of rules having as consequent a different class for that record.</s><s coords="9,257.05,416.05,223.54,8.74;9,134.77,428.00,345.83,8.74;9,134.77,439.96,63.48,8.74">Indeed, rule based classifiers have the disadvantage of requiring an additional approach for resolving such situations of conflicting outcome <ref type="bibr" coords="9,175.00,439.96,19.37,8.74" target="#b107">[107]</ref>.</s><s coords="9,201.96,439.96,278.63,8.74;9,134.77,451.91,216.56,8.74">Many rule based classifiers deal with this issue by returning an ordered rule list, instead of an unordered rule set.</s><s coords="9,354.58,451.91,126.01,8.74;9,134.77,463.87,345.83,8.74;9,134.77,475.82,115.39,8.74">In this way it is returned the outcome corresponding to the first rule matching the test record and ignoring the other rules in the list.</s><s coords="9,254.17,475.82,226.42,8.74;9,134.77,487.78,126.60,8.74">We notice that ordered rule lists may be harder to interpret than classical rules.</s><s coords="9,264.84,487.78,215.75,8.74;9,134.77,499.73,275.73,8.74">In fact, in this model a given rule cannot be considered independently from the precedent rules in the list <ref type="bibr" coords="9,387.25,499.73,19.37,8.74" target="#b107">[107]</ref>.</s><s coords="9,413.71,499.73,66.89,8.74;9,134.77,511.69,345.83,8.74;9,134.77,523.64,223.85,8.74">Another widely used approach consists in considering the top-k rules satisfying the test record where the ordering is given by a certain weight (e.g.</s><s coords="9,361.57,523.64,119.02,8.74;9,134.77,535.60,22.14,8.74">accuracy, Laplace accuracy, etc.).</s><s coords="9,159.82,535.60,320.77,8.74;9,134.77,547.55,179.36,8.74">Then, the outcome of the rules with the average highest weight among the top-k is returned as predicted class <ref type="bibr" coords="9,290.87,547.55,19.37,8.74" target="#b109">[109]</ref>.</s></p><p><s coords="9,149.71,561.47,308.96,8.74">Finally, explanations can also be provided through linear models <ref type="bibr" coords="9,427.68,561.47,15.50,8.74" target="#b47">[47,</ref><ref type="bibr" coords="9,443.17,561.47,11.62,8.74" target="#b84">84]</ref>.</s><s coords="9,461.16,561.47,19.43,8.74;9,134.77,573.43,345.82,8.74;9,134.77,585.38,317.08,8.74">This can be done by considering and visualizing both the sign and the magnitude of the contribution of the attributes for a given prediction (see Figure <ref type="figure" coords="9,440.22,585.38,3.87,8.74" target="#fig_1">2</ref>).</s><s coords="9,455.98,585.38,24.61,8.74;9,134.77,597.34,345.82,8.74;9,134.77,609.29,85.88,8.74">If the contribution of an attribute-value is positive, then it contributes by increasing the model's output.</s><s coords="9,224.12,609.29,256.47,8.74;9,134.77,621.25,138.49,8.74">Instead, if the sign is negative then the attribute-value decreases the output of the model.</s><s coords="9,276.13,621.25,204.46,8.74;9,134.77,633.20,345.83,8.74;9,134.77,645.16,59.76,8.74">If an attribute-value has an higher contribution than another, then it means that it has an higher influence on the prediction of the model.</s><s coords="9,198.95,645.16,281.65,8.74;9,134.77,657.11,345.83,8.74;10,134.77,119.99,345.82,8.74;10,134.77,131.95,137.79,8.74">The produced contributions summarize the performance of the model, thus the difference between the predictions of the model and expected predictions, providing the opportunity of quantifying the changes of the model prediction for each test record.</s><s coords="10,276.80,131.95,203.79,8.74;10,134.77,143.90,345.83,8.74;10,134.77,155.86,61.71,8.74">In particular, it is possible to identify the attributes leading to this change and for each attribute how much it contributed to the change.</s></p><p><s coords="10,149.71,167.81,330.88,8.74;10,134.77,179.77,345.82,8.74;10,134.77,191.72,345.83,8.74;10,134.77,203.68,345.82,8.74;10,134.77,215.63,287.00,8.74">As last remark we point out that in general, when an explanation for a prediction is provided, it is often useful to analyze besides the explanation (satisfied rules, branch of the tree, set of weights, etc.), also instances which are exceptions with respect to the "boundaries" provided by the explanation, or with very few differences with respect to the prototypes returned as explanation.</s><s coords="10,424.67,215.63,55.93,8.74;10,134.77,227.59,345.82,8.74;10,134.77,239.54,138.00,8.74">For example, instances covered by the rule body but with an outcome label different from the class of the outcome predicted.</s><s coords="10,276.86,239.54,203.73,8.74;10,134.77,251.50,345.83,8.74;10,134.77,263.45,258.89,8.74">Even though this sort of exception analysis is hardly performed, it can be more informative than the direct explanation, and it can also provide clues about the application domain <ref type="bibr" coords="10,375.39,263.45,14.61,8.74" target="#b73">[73]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" coords="10,134.77,291.88,284.53,8.77">Explanations and Interpretable Models Complexity</head><p><s coords="10,134.77,310.41,345.83,8.74;10,134.77,322.37,49.26,8.74">In the literature, very little space is dedicated to a crucial aspect: the model complexity.</s><s coords="10,186.36,322.37,294.23,8.74;10,134.77,334.32,260.99,8.74">The evaluation of the model complexity is generally tied to the model comprehensibility, and this is a very hard task to address.</s><s coords="10,400.01,334.32,80.58,8.74;10,134.77,346.28,345.82,8.74;10,134.77,358.24,79.58,8.74">As a consequence, this evaluation is generally estimated with a rough approximation related to the size of the model.</s><s coords="10,218.72,358.24,261.87,8.74;10,134.77,370.19,67.58,8.74">Moreover, complexity is often used as an opposed term to interpretability.</s></p><p><s coords="10,149.71,382.15,330.89,8.74;10,134.77,394.10,231.23,8.74">In <ref type="bibr" coords="10,162.54,382.15,15.50,8.74" target="#b32">[32]</ref> the complexity is identified by the number of regions, i.e., the parts of the model, for which the boundaries are defined.</s><s coords="10,370.36,394.10,110.23,8.74;10,134.77,406.06,345.82,8.74;10,134.77,418.01,123.30,8.74">In <ref type="bibr" coords="10,383.85,394.10,15.50,8.74" target="#b84">[84]</ref> as complexity for linear models is adopted the number of non-zero weights, while for decision trees the depth of the tree.</s><s coords="10,262.62,418.01,217.97,8.74;10,134.77,429.97,345.83,8.74;10,134.77,441.92,215.66,8.74">In <ref type="bibr" coords="10,276.30,418.01,15.50,8.74" target="#b22">[22]</ref> the complexity of a rule (and thus of an explanation) is measured by the length of the rule condition, defined as the number of attribute-value pairs in the condition.</s><s coords="10,354.39,441.92,126.20,8.74;10,134.77,453.88,345.83,8.74;10,134.77,465.83,83.36,8.74">Given two rules with similar frequency and accuracy, the rule with a smaller length may be preferred as it is more interpretable.</s><s coords="10,221.43,465.83,259.16,8.74;10,134.77,477.79,345.83,8.74;10,134.77,489.74,23.33,8.74">Similarly, in case of lists of rules the complexity is typically measured considering the total number of attribute-value pairs in the whole set of rules.</s><s coords="10,160.75,489.74,319.84,8.74;10,134.77,501.70,345.82,8.74;10,134.77,513.65,91.87,8.74">However, this could be a suitable way for measuring the model complexity, since in an ordered rule list different test records need distinct numbers of rules to be evaluated <ref type="bibr" coords="10,208.37,513.65,14.61,8.74" target="#b27">[27]</ref>.</s><s coords="10,231.06,513.65,249.53,8.74;10,134.77,525.61,345.82,8.74">In this kind of model, a more honest measure could be the average number of conditions evaluated to classify a set of test records <ref type="bibr" coords="10,462.32,525.61,14.61,8.74" target="#b72">[72]</ref>.</s><s coords="10,134.77,537.56,307.85,8.74">However, this is more a "measure of the explanation" of a list of rules.</s></p><p><s coords="10,149.71,549.52,330.88,8.74;10,134.77,561.47,345.82,8.74;10,134.77,573.43,196.36,8.74">Differently from the not flexible representation of decision tree where the prediction of a single record is mutually exhaustive and exclusive, rules characterization contains only significant clauses.</s><s coords="10,334.70,573.43,145.89,8.74;10,134.77,585.38,345.82,8.74;10,134.77,597.34,345.82,8.74;10,134.77,609.29,274.82,8.74">As a consequence, an optimal set of rules does not contain any duplicated information, given the fact that an outcome label can appear only one time in the consequent of a set of rules, while in a decision tree it typically comes out more than once.</s><s coords="10,413.19,609.29,67.40,8.74;10,134.77,621.25,345.83,8.74;10,134.77,633.20,66.02,8.74">Moreover, rules do not capture insignificant clauses, while decision trees can also have insignificant branches.</s><s coords="10,204.73,633.20,275.85,8.74;10,134.77,645.16,345.83,8.74;10,134.77,657.11,217.46,8.74">This happens because rule based classifier generally select one attribute-value while expanding a rule, whereas decision tree algorithms usually select one attribute while expanding the tree <ref type="bibr" coords="10,333.96,657.11,14.61,8.74" target="#b27">[27]</ref>.</s><s coords="10,355.83,657.11,124.77,8.74;11,134.77,119.99,181.67,8.74">Considering these aspects to estimate the complexity is very difficult.</s><s coords="11,320.98,119.99,159.61,8.74;11,134.77,131.95,345.83,8.74;11,134.77,143.90,345.83,8.74;11,134.77,155.86,323.45,8.74">Consequently, even though a model equivalence exists, the estimation of the fact that a different representation for the same model (or explanation) is more complex than another when using decision trees or rules can be very subjective with respect to the interpreter.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5" coords="11,134.77,185.96,247.89,8.77">Interpretable Data for Interpretable Models</head><p><s coords="11,134.77,206.16,279.23,8.74">The types of data used for classification may have diverse nature.</s><s coords="11,416.74,206.16,63.85,8.74;11,134.77,218.12,270.73,8.74">Different types of data present a different level of interpretability for a human.</s><s coords="11,408.36,218.12,72.23,8.74;11,134.77,230.07,225.02,8.74">The most understandable data format for humans is the table <ref type="bibr" coords="11,341.52,230.07,14.61,8.74" target="#b36">[36]</ref>.</s><s coords="11,363.52,230.07,117.08,8.74;11,134.77,242.03,345.82,8.74;11,134.77,253.98,345.83,8.74;11,134.77,265.94,185.42,8.74">Since matrices and vectors are the typical data representation used by the vast majority of data mining and machine learning techniques, tables are also easily managed by these algorithms without requiring specific transformations.</s></p><p><s coords="11,149.71,277.94,330.88,8.74;11,134.77,289.90,42.48,8.74">Other forms of data which are very common in human daily life are images and texts.</s><s coords="11,181.01,289.90,299.58,8.74;11,134.77,301.85,50.55,8.74">They are perhaps for human brain even more easily understandable than tables.</s><s coords="11,187.67,301.85,292.93,8.74;11,134.77,313.81,345.83,8.74;11,134.77,325.76,198.19,8.74">On the other hand, the processing of these data for predictive models requires their transformation into vectors that make them easier to process by algorithms but less interpretable for humans.</s><s coords="11,336.60,325.76,143.99,8.74;11,134.77,337.72,345.83,8.74;11,134.77,349.67,345.82,8.74;11,134.77,361.63,50.70,8.74">Indeed, on images and texts, the state of art techniques typically apply predictive models based on super vector machine, neural networks or deep neural networks that are usually hard to be interpreted.</s><s coords="11,187.93,361.63,292.65,8.74;11,134.77,373.58,345.82,8.74;11,134.77,385.54,182.09,8.74">As a consequence, certain recognized interpretable models cannot be directly employed for this type of data in order to obtain an interpretable model or a human understandable explanation.</s><s coords="11,321.53,385.54,159.06,8.74;11,134.77,397.49,345.83,8.74;11,134.77,409.45,345.82,8.74;11,134.77,421.40,232.02,8.74">Transformations using equivalences, approximations or heuristics are required in such a way that images and texts can be employed by prediction systems and used for providing the interpretation of the model and/or the prediction at the same time.</s></p><p><s coords="11,149.71,433.41,330.89,8.74;11,134.77,445.36,345.83,8.74;11,134.77,457.32,88.15,8.74">Finally, there exist other forms of data such as sequence data, spatio-temporal data and complex network data that may be used by data mining and machine learning algorithms.</s><s coords="11,227.39,457.32,253.20,8.74;11,134.77,469.27,345.83,8.74;11,134.77,481.23,139.34,8.74">However in the literature, to the best of our knowledge, there is no work addressing the interpretability of models for data different from images, texts, and tabular data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="11,134.77,511.96,203.44,10.52">Open The Black Box Problems</head><p><s coords="11,134.77,537.51,345.83,8.74;11,134.77,549.47,138.84,8.74">An accurate analysis and review of the literature lead to the identification of different categories of problems.</s></p><p><s coords="11,149.71,561.47,330.88,8.74;11,134.77,573.43,99.67,8.74">At a very high level, we can distinguish between reverse engineering and design of explanations.</s><s coords="11,237.70,573.43,242.89,8.74;11,134.77,585.38,345.82,8.74;11,134.77,597.34,24.61,8.74">In the first case, given the decision records produced by a black box decision maker the problem consists in reconstructing an explanation for it.</s><s coords="11,162.65,597.34,317.94,8.74;11,134.77,609.29,77.24,8.74">The original dataset upon which the black box is trained is generally not known in real life.</s><s coords="11,214.95,609.29,265.64,8.74;11,134.77,621.25,99.24,8.74">Details about reverse engineering approaches are discussed at the end of this section.</s><s coords="11,237.26,621.25,243.33,8.74;11,134.77,633.20,259.72,8.74">On the other hand, it is used and exploited to build the explanations by most of the works presented in this survey.</s><s coords="11,397.92,633.20,82.67,8.74;11,134.77,645.16,345.83,8.74;11,134.77,657.11,264.54,8.74">In the second case, given a dataset of training decision records the task consists in developing an interpretable predictor model together with its explanations.</s><s coords="12,149.71,225.50,330.88,8.74;12,134.77,237.46,232.12,8.74">Through a deep analysis of the state of the art we are able to further refine the first category obtaining three different problems.</s><s coords="12,370.70,237.46,109.89,8.74;12,134.77,249.41,345.83,8.74;12,134.77,261.37,102.79,8.74">We name them black box model explanation problem, black box outcome explanation problem, and black box inspection problem.</s><s coords="12,242.15,261.37,238.44,8.74;12,134.77,273.32,36.50,8.74">We name the second category transparent box design problem.</s><s coords="12,175.40,273.32,305.19,8.74;12,134.77,285.28,345.83,8.74;12,134.77,297.23,134.91,8.74">All these problems can be formalized as specific cases of the general classification problems with the common target of providing an interpretable and accurate predictive model.</s><s coords="12,273.21,297.23,207.38,8.74;12,134.77,309.19,78.49,8.74">Details of the formalization are provided in the following sections.</s><s coords="12,216.36,309.19,264.22,8.74;12,134.77,321.14,345.82,8.74;12,134.77,333.10,345.82,8.74;12,134.77,345.05,233.42,8.74">Other important variants are generally not treated in the literature making the problem of discovering an explanation increasingly difficult: (i) Is it allowed to query the black box at will to obtain new decision examples, or only a fixed dataset of decision records is available?</s><s coords="12,371.09,345.05,109.50,8.74;12,134.77,357.01,345.83,8.74;12,134.77,368.96,42.57,8.74">(ii) Is the complete set of features used by the decision model known, or instead only part of these features is known?</s><s coords="12,180.27,368.96,300.32,8.74;12,134.77,380.92,108.34,8.74">In this survey we do not address these issues as in the literature there is not sufficient material.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" coords="12,134.77,410.39,133.32,8.77">Problem Formulation</head><p><s coords="12,134.77,429.97,312.28,8.74">In the following, we generalize the classification problem (see Figure <ref type="figure" coords="12,435.42,429.97,3.87,8.74" target="#fig_2">3</ref>).</s></p><p><s coords="12,149.71,441.92,269.75,8.74;12,420.92,440.35,7.07,6.12;12,431.38,441.92,49.22,8.74;12,134.77,453.88,7.11,8.74;12,143.33,452.30,7.07,6.12;12,154.84,453.88,325.75,8.74;12,134.77,465.83,95.33,8.74">A predictor, also named model or classifier, is a function b : X m → Y where X m is the feature space with m corresponding to the number of features, and Y is the target space.</s><s coords="12,234.45,465.83,246.14,8.74;12,134.77,477.79,148.18,9.30;12,282.95,476.21,7.07,6.12;12,290.51,477.79,58.94,9.30;12,349.45,476.21,7.07,6.12;12,357.02,477.79,93.78,8.74;12,450.80,476.21,7.07,6.12;12,458.36,477.79,22.23,8.74;12,134.77,489.74,61.18,8.74;12,196.52,488.17,7.07,6.12;12,204.08,489.74,63.84,8.74;12,268.50,488.17,4.08,6.12;12,276.63,489.74,85.72,8.74">The feature space X can correspond to any basic data type like the set of integers X = I m , reals X = R m , booleans X = {0, 1} m , and strings X = S m , where S = Σ * and Σ = {a, b, c, . . .</s><s coords="12,364.01,489.74,116.59,8.74;12,134.77,501.70,116.13,8.74">, } is the alphabet (a finite non-empty set of symbols).</s><s coords="12,253.58,501.70,227.01,8.74;12,134.77,513.65,164.68,8.74">The feature space X can also be a complex data type composed of different basic data type.</s><s coords="12,302.53,513.65,103.29,9.30;12,405.82,512.08,3.97,6.12;12,412.01,513.65,68.58,8.74;12,134.77,525.61,238.37,8.74">For example, X = I × R 2 × S contains an integer feature, two real features and a string feature.</s><s coords="12,377.02,525.61,103.58,8.74;12,134.77,537.56,345.82,8.74;12,134.77,549.52,345.82,8.74;12,134.77,561.47,124.56,8.74">On the other hand, the target space Y (with dimensionality equals to one) contains the different labels (classes or outcomes) and identifies a semantic concept where Y can be a set of booleans, integers or strings.</s></p><p><s coords="12,149.71,573.43,207.58,8.74;13,149.71,143.90,330.88,8.74;13,134.77,155.86,285.37,8.74">A predictor b is the output of a learner function In the following we indicate with b a black box predictor belonging to the set of uninterpretable data mining and machine learning models.</s><s coords="13,423.92,155.86,56.67,8.74;13,134.77,167.81,345.83,8.74;13,134.77,179.77,345.82,8.74;13,134.77,191.72,58.22,8.74">According to Section 3, b is a black box because the reasoning behind the function is not understandable by humans and the outcome returned does not provide any clue for its choice.</s><s coords="13,196.31,191.72,284.28,8.74;13,134.77,203.68,59.19,9.65">In real-world applications, b is an opaque classifier resulting from a learning L b .</s><s coords="13,196.80,203.68,283.80,8.74;13,134.77,215.63,186.06,8.74">Similarly, we indicate with c a comprehensible predictor for which is available a global or a local explanation.</s></p><formula xml:id="formula_1" coords="12,134.77,571.85,345.83,22.27">L b such that L b : (X n×m × Y n ) → (X m → Y).</formula><p><s coords="13,149.71,227.59,330.88,8.74;13,134.77,239.54,60.37,8.74">The performance of the comprehensible predictor c is generally evaluated by two measures.</s><s coords="13,197.81,239.54,282.78,8.74;13,134.77,251.50,292.86,8.74">The accuracy is used to evaluate how good are the performance of both the black box predictor b and the comprehensible predictor c.</s><s coords="13,431.01,251.50,49.58,8.74;13,134.77,263.45,345.82,8.74;13,134.77,275.41,111.31,8.74">The fidelity is employed to evaluate how good is the comprehensible predictor c in mimicking the black box predictor b.</s><s coords="13,249.22,275.41,231.37,8.74;13,134.77,287.36,345.83,8.74;13,134.77,296.80,69.30,11.26;13,215.14,304.35,16.42,6.12;13,234.26,299.32,246.33,8.74;13,134.77,308.75,113.57,11.26;13,259.41,316.30,16.42,6.12;13,278.53,311.27,20.52,8.74">Indeed, given a data set D = {X, Y } we can apply to each record x ∈ X both the predictors: (i) for the black box b we get the set of predictions Ŷ = x∈X b(x), while (ii) for the comprehensible predictor c we get the set of predictions Ȳ = x∈X c(x).</s></p><p><s coords="13,149.71,323.23,330.88,8.74;13,134.77,335.18,345.83,8.74;13,134.77,344.62,345.83,11.26">Thus, we can evaluate the accuracy of the black box b and of the comprehensible predictor c by comparing the real target values Y against the predicted target values Ŷ , and Ȳ with accuracy( Ŷ , Y ) and accuracy( Ȳ , Y ), respectively.</s><s coords="13,134.77,359.09,345.82,8.74;13,134.77,368.53,280.71,11.26">Moreover, we can evaluate the behavior of the predictor c with respect to b evaluating the fidelity of c by means of the function fidelity( Ŷ , Ȳ ).</s><s coords="13,419.56,371.05,61.04,8.74;13,134.77,383.00,344.95,8.74;13,134.77,392.44,345.83,11.26;13,134.77,406.91,88.34,8.74">Note that the fidelity score can be calculated by applying the same calculus of the accuracy function where as target value is used the prediction Ȳ of the black box b instead of the real values Y .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,134.77,434.81,150.86,8.77">Black Box Model Explanation</head><p><s coords="13,134.77,452.80,345.83,8.74;13,134.77,464.76,345.83,8.74;13,134.77,476.71,345.82,8.74;13,134.77,488.67,113.73,8.74">Given a black box model solving a classification problem, the black box explanation problem consists in providing an interpretable and transparent model which is able to mimic the behavior of the black box and which is also understandable by humans (see Figure <ref type="figure" coords="13,236.87,488.67,3.87,8.74" target="#fig_4">4</ref>).</s><s coords="13,251.82,488.67,228.78,8.74;13,134.77,500.62,212.09,8.74">In other words, the interpretable model approximating the black box must be globally interpretable.</s><s coords="13,350.06,500.62,130.53,8.74;13,134.77,512.58,211.84,8.74">As consequence, we define the black box model explanation problem as follows:</s></p><p><s coords="13,134.77,531.02,225.74,8.77">Definition 1 (Black Box Model Explanation).</s><s coords="13,363.86,531.05,116.73,8.74;13,134.77,543.00,345.83,8.74;13,134.77,554.96,119.62,8.74;13,255.84,553.38,7.07,6.12;13,266.46,554.96,47.75,8.74;13,315.67,553.38,18.22,6.12;13,336.72,554.96,16.74,8.74;13,354.27,553.38,4.92,6.12;13,359.70,554.96,30.92,8.74;13,392.08,553.38,7.07,6.12;13,402.70,554.96,77.90,8.74;13,134.77,566.91,345.83,8.74;13,134.77,578.87,345.83,9.65;13,134.77,590.82,197.58,9.65;13,333.80,589.25,7.07,6.12;13,344.14,590.82,136.45,8.74;13,134.77,602.78,345.83,9.65;13,134.77,614.74,140.15,9.65">Given a black box predictor b and a dataset D = {X, Y }, the black box model explanation problem consists in finding a function f : (X m → Y) × (X n×m × Y n ) → (X m → Y) which takes as input a black box b and a dataset D, and returns a comprehensible global predictor c g , i.e., f (b, D) = c g , such that c g is able to mimic the behavior of b, and exists a global explanator function ε g : (X m → Y) → E that can derive from c g a set of explanations E ∈ E modeling in a human understandable way the logic behind c g , i.e., ε g (c g ) = E.</s></p><p><s coords="13,149.71,633.20,330.88,8.74;13,134.77,645.16,257.75,8.74">A large set of the papers reviewed in this survey describe various designs for the function f to solve the black box explanation problem.</s><s coords="13,395.98,645.16,84.61,8.74;13,134.77,657.11,345.82,8.74;14,134.77,321.95,345.83,9.65;14,134.77,333.91,220.96,9.65">The set of explanations E can be modeled for example by a decision tree or by a set of rules <ref type="bibr" coords="13,462.32,657.11,14.61,8.74" target="#b36">[36]</ref>,  while the comprehensible global predictor c g is the predictor returning as global explanation ε g the decision tree or the set of rules.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,134.77,364.92,164.19,8.77">Black Box Outcome Explanation</head><p><s coords="14,134.77,386.04,345.82,8.74;14,134.77,397.99,345.83,8.74;14,134.77,409.95,307.58,8.74">Given a black box model solving a classification problem, the black box outcome explanation problem consists in providing an interpretable outcome, that is a method for providing an explanation for the outcome of the black box.</s><s coords="14,445.54,409.95,35.05,8.74;14,134.77,421.90,345.83,8.74;14,134.77,433.86,345.83,8.74;14,134.77,445.81,58.45,8.74">In other words, the interpretable model must return the prediction together with an explanation about the reasons for that prediction, i.e., the prediction is only locally interpretable.</s><s coords="14,196.47,445.81,284.12,8.74;14,134.77,457.77,265.91,8.74">It is not required to explain the whole logic behind the black box but only the reasons for the choice of a particular instance.</s><s coords="14,404.77,457.77,75.82,8.74;14,134.77,469.72,235.31,8.74;14,134.77,491.77,242.38,8.77">Consequently, we define the black box outcome explanation problem as: Definition 2 (Black Box Outcome Explanation).</s><s coords="14,381.59,491.80,99.00,8.74;14,134.77,503.76,345.82,8.74;14,134.77,515.71,138.85,8.74">Given a black box predictor b and a dataset D = {X, Y }, the black box outcome explanation problem consists in finding a function f :</s></p><formula xml:id="formula_2" coords="14,276.39,514.14,176.74,10.31">(X m → Y) × (X n×m × Y n ) → (X m → Y)</formula><formula xml:id="formula_3" coords="14,282.99,550.01,144.13,11.23">l : ((X m → Y) × (X m → Y) × X m</formula><p><s coords="14,427.62,551.58,52.97,8.74;14,134.77,563.54,345.83,9.65;14,134.77,575.49,116.58,8.74;14,252.80,573.92,7.07,6.12;14,260.37,575.49,220.22,8.74;14,134.77,587.45,205.73,9.65">) → E which takes as input the black box b, the comprehensible local predictor c l , and a data record x with features in X m , and returns a human understandable explanation e ∈ E for the data record x, i.e., ε l (b, c l , x) = e.</s></p><p><s coords="14,149.71,609.29,330.88,8.74;14,134.77,621.25,345.82,8.74;14,134.77,633.20,138.56,8.74">We report in this survey recent works describing very diversified approaches to implement function f , overcoming the limitations of explaining the whole model (illustrated in Section 6).</s><s coords="14,276.46,633.20,204.13,8.74;14,134.77,645.16,345.83,9.65;14,134.77,657.11,102.81,8.74">As an example, in this view of the problem, we can consider that the explanation e l may be either a path of a decision tree or an association rule <ref type="bibr" coords="14,219.31,657.11,14.61,8.74" target="#b27">[27]</ref>.</s><s coords="15,149.71,372.62,330.88,8.74;15,134.77,384.58,345.83,8.74;15,134.77,396.53,345.83,8.74;15,134.77,408.49,345.82,8.74;15,134.77,420.44,67.60,8.74">For example, the function f may be a technique based on sensitivity analysis that, by observing the changing occurring in the predictions when varying the input of b, returns a set of visualizations (e.g, partial dependence plots <ref type="bibr" coords="15,449.85,396.53,14.61,8.74" target="#b48">[48]</ref>, or variable effect characteristic curve <ref type="bibr" coords="15,287.01,408.49,15.50,8.74" target="#b17">[17]</ref>) highlighting the feature importance for the predictions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="15,134.77,450.71,168.11,8.77">Transparent Box Design Problem</head><p><s coords="15,134.77,471.07,345.83,8.74;15,134.77,483.03,308.79,8.74">Given a classification problem the transparent box design problem consists in providing a model which is locally or globally interpretable on its own.</s><s coords="15,134.77,504.33,246.97,8.77">Definition 4 (Transparent Box Design Problem).</s><s coords="15,386.41,504.36,94.18,8.74;15,134.77,516.31,345.82,8.74;15,134.77,528.10,51.72,9.82;15,187.94,526.69,18.22,6.12;15,206.81,528.27,45.73,8.74;15,254.00,526.69,7.07,6.12;15,264.34,528.27,216.26,8.74;15,134.77,540.06,345.83,9.82">Given a dataset D = {X, Y }, the transparent box design problem consists in finding a learning function L c : (X n×m ×Y) → (X m → Y) which takes as input the dataset D = {X, Y } and returns a (locally or globally) comprehensible predictor c, i.e., L c (D) = c.</s><s coords="15,134.77,552.18,345.83,9.65;15,134.77,564.13,345.83,9.65;15,134.77,576.09,345.82,8.74;15,134.77,588.04,10.98,8.74">This implies that there exists a local explanator function ε l or a global explanator function ε g (defined as before) that takes as input the comprehensible predictor c and returns a human understandable explanation e ∈ E or a set of explanations E.</s></p><p><s coords="15,149.71,609.12,330.89,9.82;15,134.77,621.25,345.83,9.65;15,134.77,633.20,345.83,8.74;15,134.77,645.16,345.83,9.65;15,134.77,657.11,155.75,8.74">For example, the functions L c and c may be the decision tree learner and predictor respectively, while the global explanator ε g may return as explanation a system for following the choices taken along the various branches of the tree, and ε l may return a textual representation of the path followed according to the decision suggested by the predictor.</s><s coords="16,149.71,246.06,330.89,8.74;16,134.77,258.01,345.82,8.74;16,134.77,269.97,345.83,8.74;16,134.77,281.92,310.93,8.74">Thus, according to our problem definitions, in this survey, when we say that a method is able to open the black box, we are referring to one of the following statements: (i) it explains the model, (ii) it explains the outcome, (iii) it can inspect the black box internally, (iv) it provides a transparent solution.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="16,134.77,313.50,290.80,10.52">Problem And Explanator Based Classification</head><p><s coords="16,134.77,339.89,345.83,8.74;16,134.77,351.85,249.35,8.74">In this survey, we propose a classification based on the type of problem faced and on the explanator adopted to open the black box.</s><s coords="16,388.91,351.85,91.68,8.74;16,134.77,363.81,247.60,8.74">In particular, in our classification we take into account the following features:</s></p><p><s coords="16,140.99,384.57,309.58,8.77;16,140.99,396.75,247.41,8.77;16,140.99,408.92,288.30,8.77;16,140.99,421.09,251.01,8.77">the type of problem faced (according to the definitions in Section 4); the type of explanator adopted to open the black box; the type of black box model that the explanator is able to open; the type of data used as input by the black box model.</s></p><p><s coords="16,134.77,441.70,345.82,8.74;16,134.77,453.66,345.83,8.74;16,134.77,465.61,106.69,8.74">In each section we group together all the papers that share the same problem definition, while the subsections correspond to the different solutions adopted to develop the explanators.</s><s coords="16,245.09,465.61,235.50,8.74;16,134.77,477.57,179.92,8.74">In turn, in each subsection, we group the papers that try to explain the same type of black box.</s><s coords="16,317.54,477.57,163.05,8.74;16,134.77,489.52,311.58,8.74">Finally, we keep the type of data used by the black box as a feature which is specified for each work analyzed.</s></p><p><s coords="16,149.71,501.70,298.06,8.74">We organize the sections discussing the different problems as follows.</s><s coords="16,450.76,501.70,29.83,8.74;16,134.77,513.65,345.83,8.74;16,134.77,525.61,90.05,8.74">In Section 6 we analyze the papers presenting approaches to solve the black box model explanation problem.</s><s coords="16,228.47,525.61,252.12,8.74;16,134.77,537.56,183.17,8.74">These approaches provide a globally interpretable predictor which is able to mimic the black box.</s><s coords="16,321.72,537.56,158.87,8.74;16,134.77,549.52,345.82,8.74;16,134.77,561.47,345.83,8.74;16,134.77,573.43,64.72,8.74">On the other hand, in Section 7 are reviewed the methods solving the black box outcome explanation problem: the predictor returned is locally interpretable and provides an explanation only for a given record.</s><s coords="16,203.03,573.43,277.56,8.74;16,134.77,585.38,345.82,8.74;16,134.77,597.34,345.83,8.74;16,134.77,609.29,185.92,8.74">In Section 8 we discuss the papers proposing methodologies for inspecting black boxes, i.e., not providing a comprehensible predictor but a visualization tool for studying how the black box work internally, and what can happen when a certain input is provided.</s><s coords="16,325.03,609.29,155.56,8.74;16,134.77,621.25,345.83,8.74;16,134.77,633.20,65.11,8.74">Finally, in Section 9 we report the papers designing a transparent predictor to overcome the "obscure" limitation of black boxes.</s><s coords="16,203.62,633.20,276.98,8.74;16,134.77,645.16,345.83,8.74;16,134.77,657.11,20.81,8.74">These approaches try to provide a global or local interpretable model without sacrificing the accuracy of a black box learned to solve the same task.</s></p><p><s coords="17,149.71,119.99,330.88,8.74;17,134.77,131.95,189.64,8.74">For each of the sections above, we propose a further categorization with respect to the type of explanator adopted.</s><s coords="17,328.60,131.95,151.99,8.74;17,134.77,143.90,193.53,8.74">This categorization reflects on the papers grouped into the various subsections:</s></p><p><s coords="17,140.99,163.59,163.43,8.77">-Decision Tree (DT) or Single Tree.</s><s coords="17,307.65,163.62,172.94,8.74;17,151.70,175.58,328.89,8.74;17,151.70,187.53,213.37,8.74">It is commonly recognized that decision tree is one of the more interpretable and easily understandable models, primarily for global, but also for local, explanations.</s><s coords="17,368.05,187.53,112.55,8.74;17,151.70,199.49,328.89,8.74;17,151.70,211.44,24.91,8.74;17,140.99,223.33,5.73,8.77">Indeed, a very widespread technique for opening the black box is the so-called "single tree approximation".</s><s coords="17,151.70,223.36,200.66,8.74">-Decion Rules (DR) or Rule Based Explanator.</s><s coords="17,355.42,223.36,125.17,8.74;17,151.70,235.31,177.54,8.74">Decision rules are among the more human understandable techniques.</s><s coords="17,333.10,235.31,147.49,8.74;17,151.70,247.27,115.68,8.74">There exist various types of rules (illustrated in Section 3.3).</s><s coords="17,270.12,247.27,210.47,8.74;17,151.70,259.22,156.47,8.74">They are used to explain the model, the outcome and also for the transparent design.</s><s coords="17,311.75,259.22,168.84,8.74;17,151.70,271.18,181.30,8.74;17,140.99,283.06,5.73,8.77">We remark the existence of techniques for transforming a tree into a set of rules.</s><s coords="17,151.70,283.09,115.77,8.74">-Features Importance (FI).</s><s coords="17,272.40,283.09,208.20,8.74;17,151.70,295.05,328.89,8.74;17,151.70,307.00,264.48,8.74;17,140.99,318.89,5.73,8.77">A very simple but effective solution acting as either global or local explanation consists in returning as explanation the set of features used by the black box together with their weight.</s><s coords="17,151.70,318.92,88.00,8.74">-Salient Mask (SM).</s><s coords="17,244.62,318.92,235.97,8.74;17,151.70,330.87,328.89,8.74;17,151.70,342.83,328.89,8.74">An efficient way of pointing out what causes a certain outcome, especially when images or texts are treated, consists in using "masks" visually highlighting the determining aspects of the record analyzed.</s><s coords="17,151.70,354.78,249.29,8.74;17,140.99,366.67,5.73,8.77">They are generally used to explain deep neural networks.</s><s coords="17,151.70,366.70,114.56,8.74">-Sensitivity Analysis (SA).</s><s coords="17,270.69,366.70,209.91,8.74;17,151.70,378.65,328.89,8.74;17,151.70,390.61,29.94,8.74">It consists of evaluating the uncertainty in the outcome of a black box with respect to different sources of uncertainty in its inputs.</s><s coords="17,184.96,390.61,293.96,8.74;17,140.99,402.49,5.73,8.77">It is generally used to develop visual tools for black box inspection.</s><s coords="17,151.70,402.52,141.56,8.74">-Partial Dependence Plot (PDP).</s><s coords="17,296.71,402.52,183.88,8.74;17,151.70,414.48,328.89,8.74;17,151.70,426.43,116.62,8.74;17,140.99,438.32,5.73,8.77">These plots help in visualizing and understanding the relationship between the outcome of a black box and the input in a reduced feature space.</s><s coords="17,151.70,438.35,112.99,8.74">-Prototype Selection (PS).</s><s coords="17,269.58,438.35,211.01,8.74;17,151.70,450.30,328.89,8.74;17,151.70,462.26,258.52,8.74">This explanator consists in returning, together with the outcome, an example very similar to the classified record, in order to make clear which criteria the prediction was returned.</s><s coords="17,414.83,462.26,65.75,8.74;17,151.70,474.21,328.89,8.74;17,151.70,486.17,328.89,8.74;17,151.70,498.12,97.39,8.74;17,140.99,510.01,5.73,8.77">A prototype is an object that is representative of a set of similar instances and is part of the observed points, or it is an artifact summarizing a subset of them with similar characteristics.</s><s coords="17,151.70,510.04,111.55,8.74">-Neurons Activation (NA).</s><s coords="17,265.66,510.04,214.93,8.74;17,151.70,521.99,328.89,8.74;17,151.70,533.95,255.48,8.74">The inspection of neural networks and deep neural network can be carried out also by observing which are the fundamental neurons activated with respect to particular input records.</s></p><p><s coords="17,149.71,553.71,330.88,8.74">In the following, we list all the black boxes opened in the reviewed papers.</s><s coords="17,134.77,565.66,345.82,8.74;17,134.77,577.62,99.16,8.74">These black boxes are all supervised learning algorithm designed to solve a classification problem <ref type="bibr" coords="17,215.66,577.62,14.61,8.74" target="#b95">[95]</ref>.</s></p><p><s coords="17,140.99,597.31,107.68,8.77">-Neural Network (NN).</s><s coords="17,251.80,597.34,228.80,8.74;17,151.70,609.29,237.28,8.74">Inspired by biological neural networks, artificial neural networks learn to do tasks by considering examples.</s><s coords="17,391.78,609.29,88.81,8.74;17,151.70,621.25,110.20,8.74">A NN is formed by a set of connected neurons.</s><s coords="17,265.29,621.25,215.30,8.74">Each link between neurons can transmit a signal.</s><s coords="17,151.70,633.20,328.89,8.74;17,151.70,645.16,139.53,8.74">The receiving neuron can process the signal and then transmit to downstream neurons connected to it.</s><s coords="17,294.95,645.16,185.64,8.74">Typically, neurons are organized in layers.</s></p><p><s coords="17,151.70,657.11,293.92,8.74">Different layers perform different transformations on their inputs.</s><s coords="17,450.10,657.11,30.50,8.74;18,151.70,119.99,328.89,8.74;18,151.70,131.95,157.68,8.74">Signals travel from the input layer, to the output layer, passing through the hidden layer(s) in the middle multiple times.</s><s coords="18,311.79,131.95,168.81,8.74;18,151.70,143.90,328.89,8.74;18,151.70,155.86,153.84,8.74;18,140.99,167.73,5.73,8.77">Neurons and connections may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends.</s><s coords="18,151.70,167.76,93.34,8.74">-Tree Ensemble (TE).</s><s coords="18,249.36,167.76,231.24,8.74;18,151.70,179.72,328.89,8.74;18,151.70,191.67,117.81,8.74">Ensemble methods combine more than one learning algorithm to improve the predictive power of any of the single learning algorithms that they combines.</s><s coords="18,272.76,191.67,207.83,8.74;18,151.70,203.63,89.46,8.74">Random forests, boosted trees and tree bagging are examples of TEs.</s><s coords="18,243.76,203.63,236.83,8.74;18,151.70,215.58,266.91,8.74">They combine the predictions of different decision trees each one trained on an independent subset of the input data.</s><s coords="18,149.71,378.69,330.88,8.74;18,134.77,390.64,45.66,8.74">Moreover, recently agnostic approaches for explaining black boxes are being developed.</s><s coords="18,183.60,390.64,297.00,8.74;18,134.77,402.60,307.22,8.74">An Agnostic Explanator (AGN) is a comprehensible predictor which is not tied to a particular type of black box, explanation or data type.</s><s coords="18,445.36,402.60,35.23,8.74;18,134.77,414.55,345.83,8.74;18,134.77,426.51,224.87,8.74">In other words, in theory, an agnostic predictor can explain indifferently a neural network or a tree ensemble using a single tree or a set of rules.</s><s coords="18,362.06,426.51,118.53,8.74;18,134.77,438.46,345.83,8.74;18,134.77,450.42,345.83,8.74;18,134.77,462.37,324.42,8.74">Since only a few approaches in the literature describe themselves to be fully agnostic, and since the principal task is to explain a black box predictor, in this paper, if not differently specified, we term agnostic the approaches defined to explain any type of black box.</s></p><p><s coords="18,149.71,474.33,330.88,8.74;18,134.77,486.28,58.66,8.74">The types of data used as input of black boxes analyzed in this survey are the following:</s></p><p><s coords="18,140.99,505.94,78.34,8.77">-Tabular (TAB).</s><s coords="18,221.72,505.97,258.87,8.74;18,151.70,517.92,328.89,8.74;18,151.70,529.88,136.24,8.74;18,140.99,541.76,5.73,8.77">With tabular data, we indicate any classical dataset in which every record shares the same set of features and each feature is either numerical, categorical or boolean.</s><s coords="18,151.70,541.79,61.96,8.74">-Image (IMG).</s><s coords="18,217.81,541.79,199.59,8.74">Many black boxes work with labeled images.</s><s coords="18,421.55,541.79,59.05,8.74;18,151.70,553.74,328.89,8.74;18,151.70,565.70,219.20,8.74;18,140.99,577.57,5.73,8.77">These images can be treated as they are by the black box or can be preprocessed (e.g, re-sized in order to have all the same dimensions).</s><s coords="18,151.70,577.60,54.91,8.74">-Text (TXT).</s><s coords="18,209.81,577.60,270.78,8.74;18,151.70,589.56,328.89,8.74;18,151.70,601.51,273.00,8.74">As language modeling is one of the tasks most widely assessed nowadays together with image recognition, labeled datasets of text are generally used for tasks like spam detection or topic classification.</s></p><p><s coords="18,134.77,621.25,345.82,8.74;18,134.77,633.20,222.13,8.74">In data mining and machine learning many other types of data are also used like sequences, networks, mobility trajectories, etc.</s><s coords="18,360.47,633.20,120.13,8.74;18,134.77,645.16,345.82,8.74;18,134.77,657.11,18.54,8.74">However, they are not used as input in the methods of the papers proposing a solution for opening the black box.</s></p><p><s coords="19,134.77,192.15,345.83,7.89;19,134.77,203.14,345.83,7.86;19,134.77,214.10,141.18,7.86">Fig. <ref type="figure" coords="19,154.40,192.15,4.13,7.89">8</ref>. Reverse Engineering approach: the learned black box predictor is queried with a test dataset to produce an oracle which associate to each record a label which is not real but assigned by the black box.</s></p><p><s coords="19,149.71,247.93,330.88,8.74;19,134.77,259.89,345.83,8.74;19,134.77,271.84,325.24,8.74">Table <ref type="table" coords="19,176.29,247.93,4.98,8.74" target="#tab_1">1</ref> lists the methods for opening and explaining black boxes and summarizes the various fundamental features and characteristics listed so far, together with additional information that we believe could be useful for the reader.</s><s coords="19,463.44,271.84,17.16,8.74;19,134.77,283.80,345.83,8.74;19,134.77,295.75,345.82,8.74;19,134.77,307.71,205.93,8.74">The columns Examples, Code and Dataset indicates if any kind of example of explanation is shown in the paper, and if the source code and the dataset used in the experiments are publicly available, respectively.</s><s coords="19,343.76,307.71,136.83,8.74;19,134.77,319.66,181.83,8.74">The columns General and Random are discussed in the following section.</s><s coords="19,319.24,319.66,161.35,8.74;19,134.77,331.62,345.83,8.74;19,134.77,343.57,46.45,8.74">We point out that Table <ref type="table" coords="19,425.68,319.66,4.98,8.74" target="#tab_1">1</ref> reports the main references only, while existing extensions or derived works are discussed in the survey.</s><s coords="19,183.86,343.57,296.73,8.74;19,134.77,355.53,168.01,8.74">Table <ref type="table" coords="19,210.58,343.57,4.98,8.74" target="#tab_2">2</ref> reports the legend of Table <ref type="table" coords="19,335.88,343.57,3.87,8.74" target="#tab_1">1</ref>, i.e., the expanded acronym and the meaning of the features in Table <ref type="table" coords="19,295.03,355.53,3.87,8.74" target="#tab_1">1</ref>.</s><s coords="19,305.86,355.53,174.73,8.74;19,134.77,367.48,345.83,8.74;19,134.77,379.44,345.82,8.74;19,134.77,391.39,345.83,8.74">Moreover, in order to provide the reader with a useful tool to find a particular set of papers with determined characteristics, Appendix A provides Tables 3, 4 and 5, in which are reported the list of the papers with respect to each problem, explanator and black box, respectively.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="19,134.77,421.58,342.17,8.77;19,134.77,433.53,51.14,8.77">Reverse Engineering: A Common Approach For Understanding The Black Box</head><p><s coords="19,134.77,453.81,345.83,8.74;19,134.77,465.77,345.82,8.74;19,134.77,477.72,345.82,8.74;19,134.77,489.68,214.41,8.74">Before proceeding in the detailed analysis and classification of papers proposing method f for understanding black boxes b, we present in this section the most largely used approach to solve the black box model and outcome explanation problems and the black box inspection problem.</s><s coords="19,353.24,489.68,127.35,8.74;19,134.77,501.63,345.83,8.74;19,134.77,513.59,345.83,8.74;19,134.77,525.54,214.55,8.74">We refer to this approach as reverse engineering because the black box predictor b is queried with a certain test dataset in order to create an oracle dataset that in turn will be used to train the comprehensible predictor (see Figure <ref type="figure" coords="19,337.69,525.54,3.87,8.74">8</ref>).</s><s coords="19,352.21,525.54,128.38,8.74;19,134.77,537.50,345.83,8.74;19,134.77,549.45,18.54,8.74">The name reverse engineering comes from the fact that we can only observe the input and output of the black box.</s></p><p><s coords="19,149.71,561.47,330.88,8.74;19,134.77,573.43,345.83,8.74;19,134.77,585.38,345.83,8.74;19,134.77,597.34,345.83,8.74;19,134.77,609.29,244.41,8.74">With respect to the black box model and outcome explanation problems, the possibility of action tied with this approach relies on the choice of adopting a particular type of comprehensible predictor, and in the possibility of querying the black box with input records created in a controlled way and/or by using random perturbations of the initial train or test dataset.</s><s coords="19,382.48,609.29,98.12,8.74;19,134.77,621.25,345.83,8.74;19,134.77,633.20,345.82,8.74;19,134.77,645.16,198.78,8.74">Regarding the random perturbations of the input used to feed the black box, it is important to recall that recent studies discovered that DNN built for classification problems on texts and images can be easily fooled (see Section 2).</s><s coords="19,335.81,645.16,144.78,8.74;19,134.77,657.11,273.71,8.74">Not human perceptible changes in an image can lead a DNN to label the record as something else.</s><s coords="19,411.42,657.11,69.17,8.74;21,134.77,359.64,345.82,8.74;21,134.77,371.60,284.61,8.74">Thus, according to these discoveries, the methods treating images or text, in theory, should not be enabled to use completely random perturbations of their input.</s><s coords="21,422.12,371.60,58.47,8.74;21,134.77,383.55,165.41,8.74">However, this is not always the case in practice <ref type="bibr" coords="21,281.91,383.55,14.61,8.74" target="#b84">[84]</ref>.</s></p><p><s coords="21,149.71,395.51,330.89,8.74;21,134.77,407.46,235.49,8.74">Such reverse engineering approach can be classified as generalizable or not (or pedagocial vs. decompsitional as described in <ref type="bibr" coords="21,348.12,407.46,14.76,8.74" target="#b65">[65]</ref>).</s><s coords="21,373.10,407.46,107.49,8.74;21,134.77,419.42,345.82,8.74;21,134.77,431.37,345.82,8.74;21,134.77,443.33,304.71,8.74">We say that an approach is generalizable when a purely reverse engineering procedure is followed, i.e., the black box is only queried with different input records to obtain an oracle used for learning the comprehensible predictor (see Figure <ref type="figure" coords="21,402.53,443.33,29.56,8.74" target="#fig_9">9-(left)</ref>).</s><s coords="21,444.11,443.33,36.48,8.74;21,134.77,455.28,345.83,8.74;21,134.77,467.24,112.10,8.74">In other words, internal peculiarities of the black box are not exploited to build the comprehensible predictor.</s><s coords="21,250.33,467.24,230.27,8.74;21,134.77,479.19,345.82,8.74;21,134.77,491.15,192.93,8.74">Thus, if an approach is generalizable, even though it is presented to explain a particular type of black box, in reality, it can be used to interpret any kind of black box predictor.</s><s coords="21,330.97,491.15,149.62,8.74;21,134.77,503.10,122.23,8.74">That is, it is an agnostic approach for interpreting black boxes.</s><s coords="21,260.15,503.10,220.44,8.74;21,134.77,515.06,345.83,8.74;21,134.77,527.01,221.59,8.74">On the other hand, we say that an approach is not generalizable if it can be used to open only that particular type of black box for which it was designed for (see Figure <ref type="figure" coords="21,312.65,527.01,3.97,8.74" target="#fig_9">9</ref>-(right)).</s><s coords="21,359.41,527.01,121.18,8.74;21,134.77,538.97,345.82,8.74;21,134.77,550.92,345.82,8.74;21,134.77,562.88,26.01,8.74">For example, if an approach is designed to interpret random forest and internally use a concept of distance between trees, then such approach can not be utilized to explain predictions of a NN.</s><s coords="21,164.10,562.88,263.57,8.74">A not generalizable approach can not be black box agnostic.</s></p><p><s coords="21,149.71,574.83,330.88,8.74;21,134.77,586.79,38.55,8.74">In Table <ref type="table" coords="21,189.28,574.83,4.98,8.74" target="#tab_1">1</ref> we keep track of these aspects with the two features General and Random.</s><s coords="21,176.71,586.79,303.88,8.74;21,134.77,598.75,345.82,8.74;21,134.77,610.70,345.83,8.74;21,134.77,622.66,42.37,8.74">With General we indicate if an explanatory approach can be generalized for every black box, while with Random we indicate if any kind of random perturbation or permutation of the original dataset is used by the explanatory approach.</s></p><p><s coords="21,149.71,645.16,330.88,8.74;21,134.77,657.11,323.12,8.74">In light of these concepts, as the reader will discover below, a further classification not explicitly indicated emerges from the analysis of these papers.</s><s coords="21,461.17,657.11,19.43,8.74;22,134.77,277.25,345.82,8.74;22,134.77,289.20,45.69,8.74">This fact can be at the same time a strong point or a weakness of the current state of the art.</s><s coords="22,184.61,289.20,295.99,8.74;22,134.77,301.16,95.21,8.74">Indeed, we highlight that the works for opening the black box are realized for two cases.</s><s coords="22,233.33,301.16,247.26,8.74;22,134.77,313.11,345.83,8.74;22,134.77,325.07,324.41,8.74">The first (larger) group contains approaches proposed to tackle a particular problem (e.g., medical cases) or to explain a particular type of black box, that is, the solutions are specific for the problem instance.</s><s coords="22,463.44,325.07,17.16,8.74;22,134.77,337.02,345.83,8.74;22,134.77,348.98,264.45,8.74">The second group contains general purpose solutions that try to be general as much as possible and propose agnostic and generalizable solutions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="22,134.77,380.52,322.43,10.52">Solving the Black Box Model Explanation Problem</head><p><s coords="22,134.77,406.88,345.83,8.74;22,134.77,418.83,215.76,8.74">In this section we review the methods for opening the black box facing the black box model explanation problem (see Section 4.1).</s><s coords="22,354.65,418.83,125.94,8.74;22,134.77,430.79,345.83,8.74;22,134.77,442.74,277.99,8.74">That is, the proposed methods provide globally interpretable models which are able to mimic the behavior of black boxes and which are also understandable by humans.</s><s coords="22,417.05,442.74,63.55,8.74;22,134.77,454.70,136.62,8.74">We recognized different groups of approaches.</s><s coords="22,275.72,454.70,204.88,8.74;22,134.77,466.65,292.93,8.74">In Section 6.1 we analyze the proposals using a decision tree as explanator, while in Section 6.2 they use rules.</s><s coords="22,432.00,466.65,48.59,8.74;22,134.77,478.61,345.83,8.74">Section 6.3 describes the methods which are designed to work with any type of black box.</s><s coords="22,134.77,490.56,211.16,8.74">Finally, Section 6.4 contains the remaining ones.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1" coords="22,134.77,521.48,244.44,8.77">Explanation via Single Tree Approximation</head><p><s coords="22,134.77,542.49,345.83,8.74;22,134.77,554.44,271.34,8.74">In this section we present a set of works addressing the black box model explanation problem by implementing in different ways the function f .</s><s coords="22,409.08,554.44,71.51,8.74;22,134.77,566.40,345.82,9.65;22,134.77,578.35,284.41,9.65">All the following works adopt a decision tree as comprehensible global predictor c g , and consequently represent the explanation ε g with the decision tree itself.</s><s coords="22,422.59,578.35,57.99,8.74;22,134.77,590.31,340.69,8.74">Moreover, we point out that all the methods presented in this section work on tabular data.</s></p><p><s coords="22,134.77,621.22,164.85,8.77">Explanation of Neural Networks.</s><s coords="22,304.40,621.25,176.19,8.74;22,134.77,633.20,345.83,8.74;22,134.77,645.16,345.83,9.65;22,134.77,657.11,94.64,8.74">The following papers describe the implementation of functions f which are able to interpret a black box b consisting in a Neural Network (NN) <ref type="bibr" coords="22,242.23,645.16,15.50,8.74" target="#b95">[95]</ref> with a comprehensible global predictor c g consisting in a decision tree.</s><s coords="22,232.99,657.11,247.60,8.74;23,134.77,119.99,345.83,8.74;23,134.77,131.95,130.82,8.74">In these works, the NNs are considered black-boxes, i.e., the only interface permitted is presenting an input x to the neural network b and obtaining the outcome ŷ.</s><s coords="23,269.44,131.95,211.15,8.74;23,134.77,143.90,345.82,8.74;23,134.77,155.86,89.77,8.74">The final goal is to comprehend how the neural networks behave by submitting to it a large set of instances and analyzing their different predictions.</s></p><p><s coords="23,149.71,177.31,330.89,8.74;23,134.77,189.27,32.04,8.74">Single tree approximations for NNs were first presented in 1996 by Craven et al. <ref type="bibr" coords="23,148.54,189.27,14.61,8.74" target="#b20">[20]</ref>.</s><s coords="23,170.07,189.27,310.51,8.74;23,134.77,201.22,231.62,8.74">The comprehensible representations of the neural network b is returned by Trepan which is the implementation of function f .</s><s coords="23,369.44,201.22,111.15,8.74;23,134.77,213.18,55.24,8.74;23,225.71,213.18,254.88,9.65;23,134.77,225.13,345.83,8.74;23,134.77,237.09,111.12,8.74">Trepan queries the neural network b to a decision tree c g approximating the concepts represented by the networks by maximizing the gain ratio <ref type="bibr" coords="23,332.34,225.13,15.50,8.74" target="#b95">[95]</ref> together with an estimation of the current model fidelity.</s><s coords="23,248.51,237.09,232.08,8.74;23,134.77,249.04,345.82,8.74;23,134.77,261.00,345.83,8.74;23,134.77,272.95,345.83,8.74">Another advantage of Trepan with respect to common tree classifiers like ID3 or C4.5 <ref type="bibr" coords="23,270.51,249.04,15.50,8.74" target="#b95">[95]</ref> is that, thanks to the black box b, it can use as many instances as desired for each split, so that also the node splits near to the bottom of the tree are realized using a considerable amount of training data.</s></p><p><s coords="23,149.71,288.16,228.47,8.74">In <ref type="bibr" coords="23,161.28,288.16,14.61,8.74" target="#b50">[50]</ref>, Krishnan et al. present a three step method f .</s><s coords="23,380.62,288.16,99.97,8.74;23,134.77,300.12,345.82,8.74;23,134.77,312.07,164.21,8.74">The first step generates a sort of "prototype" for each target class in Y by using genetic programming to query the trained neural network b.</s><s coords="23,301.85,312.07,178.74,8.74;23,134.77,324.03,137.90,8.74">The input features dataset X is exploited for constraining the prototypes.</s><s coords="23,275.95,324.03,204.65,8.74;23,134.77,335.98,258.04,9.65">The second step selects the best prototypes for inducing the learning of the decision tree c g in the third step.</s><s coords="23,395.17,335.98,85.42,8.74;23,134.77,347.94,345.82,8.74;23,134.77,359.89,18.93,8.74">This approach leads to get more understandable and smaller decision trees starting from smaller data sets.</s></p><p><s coords="23,149.71,375.10,330.88,8.74;23,134.77,387.06,179.48,8.74">In <ref type="bibr" coords="23,162.38,375.10,9.96,8.74" target="#b9">[9]</ref>, Boz describes DecText, another procedure that uses a decision tree c to explain neural network black boxes b.</s><s coords="23,318.20,387.06,162.39,8.74;23,134.77,399.01,345.82,8.74;23,134.77,410.97,201.64,8.74">The overall procedure recalls Trepan <ref type="bibr" coords="23,134.77,399.01,15.50,8.74" target="#b20">[20]</ref> with the innovation of four splitting methods aimed at finding the most relevant features during the tree construction.</s><s coords="23,339.98,410.97,140.62,8.74;23,134.77,422.92,345.83,8.74;23,134.77,434.88,268.47,8.74">Moreover, since one of the main purposes of the tree is to maximize the fidelity while keeping the model simple, a fidelity pruning strategy to reduce the tree size is defined.</s><s coords="23,407.33,434.88,73.27,8.74;23,134.77,446.83,106.04,8.74">A set of random instances are generated.</s><s coords="23,245.04,446.83,235.56,8.74;23,134.77,458.79,345.83,8.74;23,134.77,470.74,78.67,8.74">Then, starting from the bottom of the tree, for each internal node a leaf is created with the majority label using the labeling of the random instances.</s><s coords="23,216.69,470.74,263.90,8.74;23,134.77,482.70,191.28,8.74">If the fidelity of the new tree overtakes the old one, than the maximum fidelity and the tree are updated.</s></p><p><s coords="23,149.71,497.91,330.88,8.74;23,134.77,509.86,345.83,9.65;23,134.77,521.82,187.54,8.74">In <ref type="bibr" coords="23,162.63,497.91,15.50,8.74" target="#b39">[39]</ref> Johansson et al. use Genetic Programming to evolve Decision Trees (the comprehensible global predictor c g ), in order to mimic the behavior of a neural network ensemble (the black box b).</s><s coords="23,325.58,521.82,155.01,8.74;23,134.77,533.78,345.82,8.74;23,134.77,545.73,219.61,8.74">The dataset D used by genetic programming (implementing function f ) consists of a lot of different combinations of the original data and oracle data labeled by b.</s><s coords="23,358.43,545.73,122.16,8.74;23,134.77,557.69,345.83,8.74;23,134.77,569.64,345.83,8.74;23,134.77,581.60,288.98,9.65">The paper shows that trees based only on original training data have the worst performance in terms of accuracy in the test data, while the trees evolved using both the oracle guide and the original data produce significantly more accurate trees c g .</s><s coords="23,149.71,609.29,330.89,8.74;23,134.77,621.25,345.83,8.74;23,134.77,633.20,345.82,8.74;23,134.77,645.16,345.83,8.74;23,134.77,657.11,131.09,8.74">We underline that, even though these approaches are developed to explain neural networks, since peculiarities of the neural networks are not used by f , which uses b only as an oracle, these approaches can be potentially adopted as agnostic explanators, i.e., they can be used to open any kind of black box and represent it with a single tree.</s></p><p><s coords="24,134.77,119.96,160.33,8.77">Explanation of Tree Ensembles.</s><s coords="24,300.71,119.99,179.88,8.74;24,134.77,131.95,230.68,8.74">Richer collections of trees provide higher performance and less uncertainty in the prediction.</s><s coords="24,370.05,131.95,110.55,8.74;24,134.77,143.90,239.42,8.74">On the other hand, it is generally difficult to make sense of the resultant forests.</s><s coords="24,376.94,143.90,103.65,8.74;24,134.77,155.86,345.83,8.74;24,134.77,167.81,144.90,8.74">The papers presented in this section describe functions f for approximating a black box model b consisting in Tree Ensembles (TE) <ref type="bibr" coords="24,242.21,167.81,15.50,8.74" target="#b95">[95]</ref> (e.g.</s><s coords="24,282.82,167.81,197.78,8.74;24,134.77,179.77,345.83,9.65;24,134.77,191.72,61.54,8.74">random forests) with a global comprehensible predictor c g in the form of a decision tree, and explanation ε g as a the decision tree as before.</s></p><p><s coords="24,149.71,210.72,330.88,8.74;24,134.77,222.67,345.83,8.74;24,134.77,234.63,80.59,9.65">Unlike previous works, the tree ensembles are not only viewed as black boxes, but also some of their internal features are used to derive the global comprehensible model c g .</s><s coords="24,219.20,234.63,261.39,8.74;24,134.77,246.58,345.82,8.74;24,134.77,258.54,190.94,8.74">For example, Chipman et al., in <ref type="bibr" coords="24,364.96,234.63,15.50,8.74" target="#b14">[14]</ref> observe that although hundreds of distinct trees are identified by random forests, in practice, many of them generally differ only by few nodes.</s><s coords="24,329.72,258.54,150.87,8.74;24,134.77,270.50,325.39,8.74">In addition, some trees may differ only in the topology, but use the same partitioning of the feature space X .</s><s coords="24,463.43,270.50,17.16,8.74;24,134.77,282.45,258.89,8.74">The paper proposes several measures of dissimilarity for trees.</s><s coords="24,398.13,282.45,82.45,8.74;24,134.77,294.41,345.83,8.74;24,134.77,306.36,205.11,8.74">Such measures are used to summarize forest of trees through clustering, and finally use archetypes of the associated clusters as model explanation.</s><s coords="24,342.83,306.36,137.76,8.74;24,134.77,318.32,345.83,9.65;24,134.77,330.27,313.19,8.74">Here, f corresponds to the clustering procedure, and the global comprehensible predictor c g is the set of tree archetypes minimizing the distance among all the trees in each cluster.</s><s coords="24,451.65,330.27,28.94,8.74;24,134.77,342.23,299.41,8.74">In this approach, f does not extend the input dataset D with random data.</s></p><p><s coords="24,149.71,356.21,330.88,8.74;24,134.77,368.16,345.82,8.74">On the other hand, random data enrichment and model combination are the basis for the Combined Multiple Model (CCM) procedure f presented in <ref type="bibr" coords="24,462.32,368.16,14.61,8.74" target="#b23">[23]</ref>.</s><s coords="24,134.77,380.12,345.55,8.74;24,134.77,392.07,194.08,9.65">Given the tree ensemble black box b, it first modifies n times the input dataset D and learns a set of n black boxes b i ∀i = 1, . . .</s><s coords="24,330.51,392.07,150.08,8.74;24,134.77,404.03,244.73,8.74">, n, and then it randomly generates data record x which are labeled using a combination (e.g.</s><s coords="24,382.08,404.03,98.51,8.74;24,134.77,415.98,130.65,9.65">bagging) of the n black boxes b i , i.e., C b1,...,bn (x) = ŷ.</s><s coords="24,268.86,415.98,211.74,8.74;24,134.77,427.94,52.84,8.74">In this way, the training dataset D = D ∪ {x, ŷ} is increased.</s><s coords="24,191.06,427.94,289.54,9.65;24,134.77,439.89,345.83,8.74;24,134.77,451.85,345.82,8.74;24,134.77,463.80,84.23,8.74">Finally, it builds the global comprehensible model c g as a decision tree (C4.5 <ref type="bibr" coords="24,182.35,439.89,15.50,8.74" target="#b79">[79]</ref>) on the enriched dataset D. Since it is not exploiting particular features of the tree ensemble b, also this approach can be generalized with respect to the black box b.</s><s coords="24,223.00,463.80,257.60,8.74;24,134.77,475.76,345.83,8.74;24,134.77,487.72,345.82,9.65;24,134.77,499.67,131.38,8.74">In line with <ref type="bibr" coords="24,279.01,463.80,14.61,8.74" target="#b23">[23]</ref>, the authors of <ref type="bibr" coords="24,367.90,463.80,15.50,8.74" target="#b29">[29]</ref> generate a very large artificial dataset D using the prediction of the random forest b, then explain b by training a decision tree c g on this artificial dataset in order to mime the behavior of the random forest.</s><s coords="24,269.13,499.67,210.71,9.65;24,134.77,511.63,345.82,8.74;24,134.77,523.58,128.04,8.74">Finally, they improve the comprehensibility of c g by cutting the decision tree with respect to a human understandable depth (i.e., from 6 to 11 nodes of depth).</s><s coords="24,266.10,523.58,214.49,8.74;24,134.77,535.54,345.83,8.74;24,134.77,547.49,345.82,9.65;24,134.77,559.45,229.03,8.74"><ref type="bibr" coords="24,266.10,523.58,20.48,8.74" target="#b114">[114]</ref> proposes Single Tree Approximation (STA), an extension of <ref type="bibr" coords="24,205.08,535.54,15.50,8.74" target="#b29">[29]</ref> which empowers the construction of the final decision tree c g by using test hypothesis to understand which are the best splits observing the Gini indexes on the trees of the random forest b.</s></p><p><s coords="24,149.71,573.43,330.89,8.74;24,134.77,585.38,345.82,8.74;24,134.77,597.34,279.41,9.65">Schetinin et al. in <ref type="bibr" coords="24,226.78,573.43,15.50,8.74" target="#b87">[87]</ref> present an approach for the probabilistic interpretation of the black box b Bayesian decision trees ensembles <ref type="bibr" coords="24,364.26,585.38,15.50,8.74" target="#b10">[10]</ref> through a quantitative evaluation of uncertainty of a Confident Decision Tree (CDT) c g .</s><s coords="24,416.80,597.34,63.79,8.74;24,134.77,609.29,345.82,8.74;24,134.77,621.25,345.83,8.74;24,134.77,633.20,345.83,9.65;24,134.77,645.16,345.82,8.74;24,134.77,657.11,345.82,8.74;25,134.77,119.99,345.83,8.74;25,134.77,131.95,345.83,8.74;25,134.77,143.90,167.81,8.74">The methodology f for interpreting b is summarized as follows: (i) the classification confidence for each tree in the ensemble is calculated using the training data D, (ii) the decision tree c g that covers the maximal number of correct training examples is selected, keeping minimal the amount of misclassifications on the remaining examples by sub-sequentially refining the training dataset D. Similarly to <ref type="bibr" coords="24,462.32,657.11,14.61,8.74" target="#b14">[14]</ref>, also this explanation method f does not extend the input dataset D with random data and cannot be generalized to other black boxes but can be used only with Bayesian decision tree ensembles.</s></p><p><s coords="25,149.71,155.86,330.88,8.74;25,134.77,167.81,272.13,8.74">In <ref type="bibr" coords="25,161.64,155.86,14.61,8.74" target="#b32">[32]</ref>, Hara et al. reinterpret Additive Tree Models (ATM) (the black box b) using a probabilistic generative model interpretable by humans.</s><s coords="25,409.40,167.81,71.20,8.74;25,134.77,179.77,204.75,8.74">An interpretable ATM has a sufficiently small number of regions.</s><s coords="25,342.20,179.77,138.38,8.74;25,134.77,191.72,299.81,8.74">Therefore, their aim is to reduce the number of regions in an ATM while minimizing the model error.</s><s coords="25,438.02,191.72,42.57,8.74;25,134.77,203.68,345.82,8.74;25,134.77,215.63,32.16,8.74">To satisfy these requirements, they propose a post processing method f that works as follows.</s><s coords="25,169.26,215.63,241.72,8.74">First, it learns an ATM b generating a number of regions.</s><s coords="25,413.32,215.63,67.27,8.74;25,134.77,227.59,345.83,9.65;25,134.77,239.54,191.44,8.74">Then, it mimics b using a simpler model (the comprehensible global predictor c g ) where the number of regions is fixed as small, e.g., ten.</s><s coords="25,329.18,239.54,151.41,8.74;25,134.77,251.50,345.82,8.74;25,134.77,263.45,214.32,8.74">In particular, to obtain the simpler model an Expectation Maximization algorithm is adopted <ref type="bibr" coords="25,395.24,251.50,15.50,8.74" target="#b95">[95]</ref> minimizing the Kullback-Leibler divergence from the ensemble b.</s></p><p><s coords="25,149.71,275.41,330.88,8.74;25,134.77,287.36,345.82,8.74;25,134.77,299.32,227.52,9.65">The authors of <ref type="bibr" coords="25,217.85,275.41,15.50,8.74" target="#b94">[94]</ref> propose Tree Space Prototype (TSP), an approach f for interpretating tree ensembles (the black box b) by finding tree prototypes (the comprehensible global predictor c g ) in the tree space.</s><s coords="25,364.84,299.32,115.75,8.74;25,134.77,311.27,345.83,8.74;25,134.77,323.23,343.17,8.74">The main contributions for f are: (i) the definition of the random forest proximity between trees, and (ii) the design of the procedure to extract the tree prototypes used for classification.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" coords="25,134.77,352.07,189.24,8.77">Explanation via Rule Extraction</head><p><s coords="25,134.77,371.00,345.83,8.74;25,134.77,382.96,111.63,8.74">Another commonly used state of the art interpretable and easily understandable model is the set of rules.</s><s coords="25,250.56,382.96,230.03,8.74;25,134.77,394.91,345.83,8.74">When a set of rules describing the logic behind the black box model is returned the interpretability is provided at a global level.</s><s coords="25,134.77,406.87,345.82,8.74;25,134.77,418.82,345.83,8.74;25,134.77,430.78,296.58,9.65">In the following, we present a set of reference works solving the black box model explanation problem by implementing in different ways function f , and by adopting any kind of decision rules as comprehensible global predictor c g .</s><s coords="25,434.53,430.78,46.06,8.74;25,134.77,442.73,345.83,9.65">Hence, the global explanation ε g change accordingly to the type of rules extracted by c g .</s><s coords="25,134.77,454.69,345.83,8.74;25,134.77,466.64,95.25,8.74">Similarly to the previous section, also all the methods presented in this section work on tabular data.</s></p><p><s coords="25,134.77,495.48,164.85,8.77">Explanation of Neural Networks.</s><s coords="25,304.40,495.51,176.19,8.74;25,134.77,507.47,345.83,8.74;25,134.77,519.42,123.41,8.74">The following papers describe the implementation of functions f which are able to interpret a black box b consisting in a Neural Network (NN) <ref type="bibr" coords="25,239.91,519.42,14.61,8.74" target="#b95">[95]</ref>.</s><s coords="25,261.13,519.42,219.46,8.74;25,134.77,531.38,241.62,8.74">In the literature already exists a survey specialized on techniques extracting rules from neural networks <ref type="bibr" coords="25,363.10,531.38,9.96,8.74" target="#b4">[5]</ref>.</s><s coords="25,379.51,531.38,101.08,8.74;25,134.77,543.33,345.82,8.74;25,134.77,555.29,345.83,8.74;25,134.77,567.24,201.45,8.74">It provides an overview of mechanisms designed to (i) insert knowledge into neural networks (knowledge initialization), (ii) extract rules from trained NNs (rule extraction), and (iii) use NNs to refine existing rules (rule refinement).</s><s coords="25,340.04,567.24,140.55,8.74;25,134.77,579.20,345.82,8.74;25,134.77,591.15,34.93,9.65">The approaches presented in <ref type="bibr" coords="25,470.08,567.24,10.52,8.74" target="#b4">[5]</ref> are strongly dependent on the black box b and on the specific type of decision rules c g .</s><s coords="25,172.35,591.15,308.25,8.74;25,134.77,603.11,239.41,8.74">Thus, they are not generalizable and can not be employed to solve other instances of the black box model explanation problem.</s><s coords="25,377.68,603.11,102.91,8.74;25,134.77,615.06,207.42,8.74">The survey <ref type="bibr" coords="25,429.87,603.11,10.52,8.74" target="#b4">[5]</ref> classifies the methods according to the following criteria:</s></p><p><s coords="25,140.99,633.68,184.92,8.77">-Expressive power of the extracted rules.</s></p><p><s coords="25,140.99,645.38,336.18,8.77">-Translucency: that is decompositional, pedagogical and eclectic properties.</s></p><p><s coords="25,140.99,657.08,201.05,8.77">-Portability of the rule extraction technique.</s></p><p><s coords="26,140.99,119.96,138.16,8.77">-Quality of the rules extracted.</s><s coords="26,281.37,119.99,199.22,8.74;26,151.70,131.95,328.89,8.74;26,151.70,143.90,144.68,8.74;26,140.99,155.51,5.73,8.77">Quality includes accuracy, fidelity, consistency, i.e., different training of the NN extract the rules that lead to the same classification of unseen examples.</s><s coords="26,151.70,155.54,104.36,8.74">-Algorithmic complexity.</s></p><p><s coords="26,149.71,174.20,251.72,8.74">A typical paper analyzed in <ref type="bibr" coords="26,273.88,174.20,10.52,8.74" target="#b4">[5]</ref> is <ref type="bibr" coords="26,297.69,174.20,15.50,8.74" target="#b19">[19]</ref> where <ref type="bibr" coords="26,345.27,174.20,56.17,8.74">Craven et al.</ref></s><s coords="26,134.77,222.02,345.82,8.74;26,134.77,233.97,345.82,8.74;26,134.77,245.93,53.18,8.74">If the input x ∈ D with outcome ŷ is not covered by the set of rules, then a conjunctive (or m-of-n) rule is formed from {x, ŷ} considering all the possible antecedents.</s><s coords="26,190.47,245.93,290.12,8.74">The procedure ends when all the target classes have been processed.</s></p><p><s coords="26,149.71,257.88,330.89,8.74;26,134.77,269.84,214.17,8.74">In <ref type="bibr" coords="26,162.96,257.88,15.50,8.74" target="#b40">[40]</ref> Johansson et al. exploit G-REX <ref type="bibr" coords="26,330.39,257.88,14.61,8.74" target="#b37">[37]</ref>, an algorithm for rule extraction, as function f to explain a neural network b.</s><s coords="26,352.24,269.84,128.35,8.74;26,134.77,281.79,345.83,8.74;26,134.77,293.75,345.82,8.74;26,134.77,305.70,88.66,9.65">They use the classical reverse engineering schema where random permutations of the original dataset D are annotated by b, and such dataset is used as input by G-REX, which corresponds with c g in this case.</s><s coords="26,227.27,305.70,253.33,8.74;26,134.77,317.66,135.54,8.74">In particular, G-REX extracts rules by exploiting genetic programming as a key concept.</s><s coords="26,273.50,317.66,207.09,8.74;26,134.77,329.61,327.27,8.74">In subsequent works, the authors show that the proposed methodology f can be also employed to interpret trees ensembles.</s><s coords="26,465.09,329.61,15.50,8.74;26,134.77,341.57,345.82,8.74;26,134.77,353.52,232.21,8.74"><ref type="bibr" coords="26,465.09,329.61,15.50,8.74" target="#b38">[38]</ref> extends G-REX for handling regression problems by generating regression trees, and classification problems by generating fuzzy rules.</s></p><p><s coords="26,149.71,365.48,330.89,8.74;26,134.77,377.44,76.76,8.74">In <ref type="bibr" coords="26,162.63,365.48,20.48,8.74" target="#b115">[115]</ref> the authors present REFNE, an approach f to explain neural network ensembles b.</s><s coords="26,214.35,377.44,266.23,8.74;26,134.77,389.39,197.49,9.65">REFNE uses ensembles for generating instances and then, extracts symbolic rules c g from those instances.</s><s coords="26,335.58,389.39,145.01,8.74;26,134.77,401.35,345.82,8.74;26,134.77,413.30,298.87,8.74">REFNE avoids useless discretizations of continuous attributes, by applying a particular discretization leading to discretize different continuous attributes using different intervals.</s><s coords="26,437.37,413.30,43.22,8.74;26,134.77,425.26,345.83,8.74;26,134.77,437.21,195.08,8.74">Moreover, REFNE can also be used as a rule learning approach, i.e., it solves the transparent box design problem (see Section 4.1).</s><s coords="26,333.36,437.21,147.23,8.74;26,134.77,449.17,345.82,9.65;26,134.77,461.12,26.07,8.74">Also in <ref type="bibr" coords="26,367.86,437.21,10.52,8.74" target="#b5">[6]</ref> Augasta et al. propose RxREN a rule extraction algorithm c g which returns the explanation of a trained NN b.</s><s coords="26,164.92,461.12,142.50,8.74">The method f works as follows.</s><s coords="26,311.49,461.12,169.10,8.74;26,134.77,473.08,345.82,8.74;26,134.77,485.03,189.35,8.74">First, it prunes the insignificant input neurons from trained NNs and identifies the data range necessary to classify the given test instance with a specific class.</s><s coords="26,327.36,485.03,153.22,8.74;26,134.77,496.99,345.82,8.74;26,134.77,508.94,345.82,8.74;26,134.77,520.90,231.80,8.74">Second, using a reverse engineering technique, through RxREN generates the classification rules for each class label exploiting the data ranges previously identified, and improve the initial set of rules by a process that prunes and updates the rules.</s></p><p><s coords="26,134.77,549.49,207.96,8.77">Explanation of Support Vector Machines.</s><s coords="26,347.60,549.52,132.99,8.74;26,134.77,561.47,345.83,8.74;26,134.77,573.43,345.08,9.65;26,134.77,585.38,153.89,8.74">The following papers show implementations of functions f for explaining a black box b consisting in a Support Vector Machine (SVM) <ref type="bibr" coords="26,240.16,573.43,15.50,8.74" target="#b95">[95]</ref> still returning a comprehensible global predictor c g consisting in a set of decision rules.</s></p><p><s coords="26,149.71,597.34,330.88,8.74;26,134.77,609.29,224.53,9.65">The authors of <ref type="bibr" coords="26,216.39,597.34,15.50,8.74" target="#b70">[70]</ref> propose the SVM+Prototypes (SVM+P) procedure f for rule extraction c g from support vector machines b.</s><s coords="26,363.27,609.29,117.31,8.74;26,134.77,621.25,345.83,8.74;26,134.77,633.20,218.96,8.74">It works as follows: it first determines the decision function by means of a SVM, then a clustering algorithm is used to find out a prototype vector for each class.</s><s coords="26,356.24,633.20,124.35,8.74;26,134.77,645.16,345.82,8.74;26,134.77,657.11,211.12,8.74">By using geometric methods, these points are joined with the support vectors for defining ellipsoids in the input space that can be transformed into if-then rules.</s><s coords="27,149.71,293.75,330.89,8.74;27,134.77,305.71,345.82,8.74;27,134.77,317.66,292.59,9.65">Fung et al., in <ref type="bibr" coords="27,214.91,293.75,14.61,8.74" target="#b28">[28]</ref>, describe as function f an algorithm based on constraint programming for converting linear SVM b (and other hyperplane-based linear classifiers) into a set of non overlapping and interpretable rules c g .</s><s coords="27,430.94,317.66,49.65,8.74;27,134.77,329.62,250.96,8.74">These rules are asymptotically equivalent to the original linear SVM.</s><s coords="27,389.34,329.62,91.26,8.74;27,134.77,341.57,345.83,8.74;27,134.77,353.53,190.18,8.74">Each iteration of the algorithm for extracting the rules is designed to solve a constrained optimization problem having a low computational cost.</s><s coords="27,329.71,353.53,150.88,8.74;27,134.77,365.48,345.83,8.74;27,134.77,377.44,96.64,8.74">We underline that this black box explanation solution f is not generalizable and can be employed only for Linear SVM-like black boxes.</s></p><p><s coords="27,149.71,393.17,330.88,8.74;27,134.77,405.12,306.51,8.74">In <ref type="bibr" coords="27,162.25,393.17,15.50,8.74" target="#b65">[65]</ref> the authors propose a qualitative comparison of the explanations returned by techniques for extraction of rules from SVM black boxes (e.g.</s><s coords="27,443.92,405.12,36.67,8.74;27,134.77,417.08,345.82,8.74;27,134.77,429.03,278.71,8.74">SVM+P <ref type="bibr" coords="27,134.77,417.08,14.61,8.74" target="#b70">[70]</ref>, Fung method <ref type="bibr" coords="27,217.90,417.08,15.50,8.74" target="#b28">[28]</ref>) against the redefining of methods designed for explaining neural networks, i.e., C4.5 <ref type="bibr" coords="27,266.94,429.03,14.61,8.74" target="#b95">[95]</ref>, Trepan <ref type="bibr" coords="27,321.94,429.03,15.50,8.74" target="#b20">[20]</ref> and G-REX <ref type="bibr" coords="27,395.21,429.03,14.61,8.74" target="#b37">[37]</ref>.</s><s coords="27,416.47,429.03,64.12,8.74;27,134.77,440.99,345.82,8.74;27,134.77,452.94,266.40,8.74">How we anticipated in the previous section, the authors delineate the existence of two type of approaches to extract rules: pedagogical and decompositional.</s><s coords="27,404.56,452.94,76.04,8.74;27,134.77,464.90,345.83,8.74;27,134.77,476.85,34.70,8.74">Pedagogical techniques f directly extract rules which relate the inputs and outputs of the predictor (e.g.</s><s coords="27,172.59,476.85,308.01,8.74;27,134.77,488.81,172.95,8.74"><ref type="bibr" coords="27,172.59,476.85,15.50,8.74" target="#b20">[20,</ref><ref type="bibr" coords="27,188.09,476.85,11.62,8.74" target="#b37">37]</ref>), while decompositional approaches are closely intertwined with the internal structure of the SVM (e.g.</s><s coords="27,311.61,488.81,34.87,8.74"><ref type="bibr" coords="27,311.61,488.81,15.50,8.74" target="#b70">[70,</ref><ref type="bibr" coords="27,327.11,488.81,11.62,8.74" target="#b28">28]</ref>).</s><s coords="27,350.38,488.81,130.22,8.74;27,134.77,500.76,281.03,8.74">We recall that, in Table <ref type="table" coords="27,460.37,488.81,4.98,8.74" target="#tab_1">1</ref> we identify with the term generalizable the pedagogical approaches.</s></p><p><s coords="27,134.77,549.49,157.95,8.77">Explanation of Tree Ensembles.</s><s coords="27,297.51,549.52,183.09,8.74;27,134.77,561.47,345.82,8.74;27,134.77,573.43,118.71,9.65">Finally, in <ref type="bibr" coords="27,344.55,549.52,14.61,8.74" target="#b22">[22]</ref>, Deng proposes the inTrees framework f to explain black boxes b defined as Tree Ensembles (TE) by returning a set of decision rules c g .</s><s coords="27,255.74,573.43,224.85,8.74;27,134.77,585.38,345.82,8.74">In particular, InTrees extracts, measures, prunes and selects rules from tree ensembles, and calculates frequent variable interactions.</s><s coords="27,134.77,597.34,345.83,8.74;27,134.77,609.29,345.82,8.74">The set of black boxes b that inTrees can explain is represented by any kind of tree ensemble like random forests, regularized random forests and boosted trees.</s><s coords="27,134.77,621.25,345.82,8.74">InTrees framework can be used for both classification and regression problems.</s><s coords="27,134.77,633.20,345.82,8.74;27,134.77,645.16,345.82,8.74;27,134.77,657.11,65.73,8.74">The technique described by InTrees is also known as Simplified Tree Ensamble Learner (STEL): it extracts the most supported and simplest rules form the trees ensemble.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" coords="28,134.77,119.96,129.18,8.77">Agnostic Explanator</head><p><s coords="28,134.77,136.31,345.83,8.74;28,134.77,148.26,114.72,8.74">Recent approaches for interpretation are agnostic (AGN) with respect to the black box to be explained.</s><s coords="28,252.77,148.26,227.82,8.74;28,134.77,160.22,345.83,8.74;28,134.77,172.17,161.51,8.74">In this section, we present a set of works solving the black box model explanation problem by implementing function f such that any type of black box b can be explained.</s><s coords="28,299.48,172.17,181.11,8.74;28,134.77,184.13,345.83,9.65;28,134.77,196.08,86.65,9.65">These approaches do not return a specific comprehensible global predictor c g , thus the type of explanation ε g change with respect to f and c g .</s><s coords="28,224.74,196.08,225.88,8.74">By definition all these approaches are generalizable.</s></p><p><s coords="28,149.71,208.04,310.93,8.74">Probably the first attempt of an agnostic solution was proposed in <ref type="bibr" coords="28,442.37,208.04,14.61,8.74" target="#b60">[60]</ref>.</s><s coords="28,463.85,208.04,16.74,8.74;28,134.77,219.99,345.83,8.74;28,134.77,231.95,345.83,8.74;28,134.77,243.91,345.82,8.74;28,134.77,255.86,34.73,8.74">Lou et al. propose a method f which exploits Generalized Additive Models (GAMs) and it is able to interpret regression splines (linear and logistics), single trees and tree ensembles (bagged trees, boosted trees, boosted bagged trees and random forests).</s><s coords="28,173.18,255.86,307.41,8.74;28,134.77,267.82,141.67,8.74">GAMs are presented as the gold standard for intelligibility when only univariate terms are considered.</s><s coords="28,280.51,267.82,200.09,9.65;28,134.77,279.77,345.82,8.74;28,134.77,291.73,308.62,8.74">Indeed, the explanation ε c is returned as the importance of the contribution of the individual features in b together with their shape function, such that the impact of each predictor can be quantified.</s><s coords="28,445.91,291.73,34.68,8.74;28,134.77,303.68,345.82,8.74;28,134.77,315.64,106.39,8.74">A shape function is the plot of a function capturing the linearities and nonlinearities together with its shape.</s><s coords="28,245.48,315.64,114.02,8.74">It works on tabular data.</s><s coords="28,363.82,315.64,116.77,8.74;28,134.77,327.59,219.49,8.74">A refinement of the GAM approach is proposed by the same authors in <ref type="bibr" coords="28,335.99,327.59,14.61,8.74" target="#b61">[61]</ref>.</s><s coords="28,357.86,327.59,122.74,8.74;28,134.77,339.55,331.72,8.74">A case study on health care showing the application of the GAM the refinement is presented in <ref type="bibr" coords="28,448.22,339.55,14.61,8.74" target="#b13">[13]</ref>.</s><s coords="28,471.46,339.55,9.13,8.74;28,134.77,351.50,345.83,8.74;28,134.77,363.46,123.62,8.74">In particular, this approach is used for the prediction of the pneumonia risk and hospital 30-day readmission.</s></p><p><s coords="28,149.71,375.41,330.88,8.74;28,134.77,387.37,326.42,8.74">In <ref type="bibr" coords="28,162.47,375.41,15.50,8.74" target="#b33">[33]</ref> the authors present an iterative algorithm f that allows finding features and dependencies exploited by a classifier when producing a prediction.</s><s coords="28,463.43,387.37,17.16,8.74;28,134.77,399.32,345.82,8.74;28,134.77,411.28,65.58,9.65">The attributes and the dependencies among the grouped attributes depict the global explanation ε g .</s><s coords="28,203.24,411.28,277.35,8.74;28,134.77,423.23,345.83,8.74;28,134.77,435.19,345.82,8.74">The proposed approach f named GoldenEye is based on tabular data randomization (within class permutation, dataset permutation, etc.) and on grouping attributes with interactions have an impact on the predictive power.</s></p><p><s coords="28,149.71,447.14,330.89,8.74;28,134.77,459.10,8.72,8.74">In <ref type="bibr" coords="28,163.56,447.14,14.61,8.74" target="#b51">[51]</ref>, PALM is presented (Partition Aware Local Model ) to implement f .</s><s coords="28,147.74,459.10,332.85,8.74;28,134.77,471.05,314.20,8.74">In particular, PALM is a method that is able to learn and summarize the structure of the training dataset to help the machine learning debugging.</s><s coords="28,451.81,471.05,28.78,8.74;28,134.77,483.01,345.83,8.74;28,134.77,494.96,345.83,8.74;28,134.77,506.92,40.99,8.74">PALM mimes a black box b using a meta-model for partitioning the training dataset, and a set of sub-models for approximating and miming the patterns within each partition.</s><s coords="28,178.29,506.92,302.30,9.65;28,134.77,518.87,345.82,8.74;28,134.77,530.83,303.10,8.74">As meta-model it uses a decision tree (c g ) so that the user can examine its structure and determine if the rules detected follow the intuition or not, and link efficiently problematic test records to the responsible train data.</s><s coords="28,441.49,530.83,39.10,8.74;28,134.77,542.78,345.82,8.74;28,134.77,554.74,294.49,8.74">The submodels linked to the leaves of the tree can be a arbitrarily complex model able to catch elaborate local patterns, but yet interpretable by humans.</s><s coords="28,432.90,554.74,47.69,8.74;28,134.77,566.69,345.83,8.74;28,134.77,578.65,87.80,8.74">Thus, with respect to the final sub-models PALM is not only black box agnostic but also explanator agnostic.</s><s coords="28,225.87,578.65,254.73,8.74;28,134.77,590.61,74.72,8.74">Moreover, PALM is also data agnostic; i.e., it can work on any kind of data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4" coords="28,134.77,616.86,200.42,8.77">Explanation via Other Approaches</head><p><s coords="28,134.77,633.20,334.30,8.74">In <ref type="bibr" coords="28,147.95,633.20,15.50,8.74" target="#b97">[97]</ref> a solution for the black box model explanation problem is presented.</s><s coords="28,473.12,633.20,7.47,8.74;28,134.77,645.16,303.64,8.74">It adopts an approach that can not be classified as one of the previous.</s><s coords="28,442.05,645.16,38.53,8.74;28,134.77,657.11,345.83,8.74;29,134.77,119.99,345.83,8.74;29,134.77,131.95,112.83,8.74">The proposed approach f uses the internals of a random forest model b to produce recommendations on the transformation of true negative examples into positively predicted examples.</s><s coords="29,250.55,131.95,230.04,8.74;29,134.77,143.90,345.83,9.65">These recommendations, which are strictly related to the feature importance, corresponds to the comprehensible global predictor c g .</s><s coords="29,134.77,155.86,345.83,8.74;29,134.77,167.81,345.83,8.74;29,134.77,179.77,140.97,8.74">In particular, the function f aims at transforming a negative instance into a positive instance by analyzing the path on the trees in the forest predicting such instance as positive or negative.</s><s coords="29,279.59,179.77,201.00,8.74;29,134.77,191.72,345.83,8.74;29,134.77,203.68,151.19,8.74">The explanation of b is provided by means of the helpfulness of the features in the paths adopted for changing the instance outcome from negative to positive.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" coords="29,134.77,237.28,338.08,10.52">Solving the Black Box Outcome Explanation Problem</head><p><s coords="29,134.77,265.70,345.83,8.74;29,134.77,277.66,112.30,8.74">In this section we review the methods solving the black box outcome explanation problem (see Section 4.1).</s><s coords="29,250.47,277.66,230.12,8.74;29,134.77,289.61,345.82,8.74;29,134.77,301.57,51.07,8.74">These methods provide a locally interpretable model which is able to explain the prediction of the black box in understandable terms for humans.</s><s coords="29,188.67,301.57,291.92,8.74;29,134.77,313.52,292.63,8.74">This category of approaches using a local point of view with respect to the prediction is becoming the most studied in the last years.</s><s coords="29,431.85,313.52,48.74,8.74;29,134.77,325.48,345.82,8.74;29,134.77,337.43,345.83,8.74;29,134.77,349.39,345.83,8.74;29,134.77,361.34,55.90,8.74">Section 7.1 describes the methods providing the salient parts of the record for which a prediction is required using Deep Neural Networks (DNNs), while Section 7.2 analyzes the methods which are able to provide a local explanation for any type of black box.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1" coords="29,134.77,394.32,312.31,8.77">Explanation of Deep Neural Network via Saliency Masks</head><p><s coords="29,134.77,417.39,345.82,8.74;29,134.77,429.34,345.83,8.74;29,134.77,441.30,201.93,8.74">In the following works the opened black box b is a DNN and the explanation is provided by using a Saliency Mask (SM), i.e. a subset of the original record which is mainly responsible for the prediction.</s><s coords="29,340.05,441.30,140.54,8.74;29,134.77,453.25,265.09,8.74">For example, as salient mask we can consider the part of an image or a sentence in a text.</s><s coords="29,404.42,453.25,76.17,8.74;29,134.77,465.21,345.82,8.74">A saliency image summarizes where a DNN looks into an image for recognizing their predictions.</s><s coords="29,134.77,477.16,345.82,9.65;29,134.77,489.12,345.83,8.74;29,134.77,501.07,59.89,8.74">The function f to extract the local explanation ε l is always not generalizable and often strictly tied with the particular type of network, i.e., convolutional, recursive, etc.</s></p><p><s coords="29,149.71,513.65,330.88,8.74;29,134.77,525.61,156.85,8.74">The work <ref type="bibr" coords="29,195.61,513.65,20.48,8.74" target="#b108">[108]</ref> introduces an attention based model f which automatically identifies the contents of an image.</s><s coords="29,295.96,525.61,184.63,8.74;29,134.77,537.56,345.82,8.74;29,134.77,549.52,345.83,8.74;29,134.77,561.47,345.82,8.74;29,134.77,573.43,19.93,8.74">The black box is a neural network which consists of a combination of a Convolutional NN (CNN) for the features extraction and a Recursive NN (RNN) containing Long Short Term Memory (LSTM), nodes producing the image caption by generating a single word for each iteration.</s><s coords="29,157.90,573.43,322.69,9.65;29,134.77,585.38,345.83,8.74;29,134.77,597.34,19.93,8.74">The explanation ε l of the prediction is provided through a visualization of the attention (area of an image, see Figure <ref type="figure" coords="29,331.28,585.38,7.90,8.74" target="#fig_12">11</ref>-left) for each word in the caption.</s><s coords="29,157.83,597.34,214.17,8.74">A similar result is obtained by Fong et al. in <ref type="bibr" coords="29,353.73,597.34,14.61,8.74" target="#b25">[25]</ref>.</s><s coords="29,375.15,597.34,105.45,8.74;29,134.77,609.29,268.85,9.65">In this work the authors propose a framework f of explanations c l as meta-predictors.</s><s coords="29,407.22,609.29,73.37,8.74;29,134.77,621.25,345.83,9.65;29,134.77,633.20,147.31,8.74">In their view, an explanation ε l , and thus a meta-predictor, is a rule that predicts the response of a black box b to certain inputs.</s><s coords="29,285.35,633.20,195.24,8.74;29,134.77,645.16,345.83,8.74;29,134.77,657.11,72.55,8.74">Moreover, they propose to use saliency maps as explanations for black boxes to highlight the salient part of the images (see Figure <ref type="figure" coords="29,166.20,657.11,8.22,8.74" target="#fig_12">11</ref>-right).</s><s coords="30,149.71,234.58,330.88,8.74;30,134.77,246.54,154.99,8.74">Similarly, another set of works produce saliency masks incorporating network activations into their visualizations.</s><s coords="30,292.94,246.54,187.66,8.74;30,134.77,258.49,123.31,8.74">This kind of approaches f are named Class Activation Mapping (CAM).</s><s coords="30,261.27,258.49,219.32,8.74;30,134.77,270.45,170.99,8.74">In <ref type="bibr" coords="30,273.60,258.49,19.37,8.74" target="#b113">[113]</ref>, global average pooling in CNN (the black box b) is used for generating the CAM.</s><s coords="30,309.08,270.45,171.51,9.65;30,134.77,282.40,345.83,8.74;30,134.77,294.36,46.41,8.74">A CAM (the local explanation ε l ) for a particular outcome label indicates the discriminative active region that identifies that label.</s><s coords="30,185.79,294.36,294.80,8.74;30,134.77,306.31,345.83,8.74;30,134.77,318.27,104.43,8.74"><ref type="bibr" coords="30,185.79,294.36,15.50,8.74" target="#b89">[89]</ref> defines its relaxed generalization Grad-CAM which visualizes the linear combination of a late layer's activations and label-specific weights (or gradients for <ref type="bibr" coords="30,212.08,318.27,19.37,8.74" target="#b113">[113]</ref>).</s><s coords="30,243.52,318.27,237.07,8.74;30,134.77,330.22,345.83,8.74;30,134.77,342.18,134.47,8.74">All these approaches arbitrarily invoke different back propagation and/or activation, which results in aesthetically pleasing, heuristic explanations of image saliency.</s><s coords="30,272.70,342.18,207.89,8.74;30,134.77,354.13,345.83,8.74;30,134.77,366.09,104.16,8.74">Their solution is not black box agnostic limited to NN, but it requires specific architectural modifications <ref type="bibr" coords="30,401.29,354.13,20.48,8.74" target="#b113">[113]</ref> or access to intermediate layers <ref type="bibr" coords="30,220.66,366.09,14.61,8.74" target="#b89">[89]</ref>.</s></p><p><s coords="30,149.71,378.22,330.88,8.74;30,134.77,390.17,245.69,8.74">With respect to texts, in <ref type="bibr" coords="30,262.98,378.22,15.50,8.74" target="#b56">[56]</ref> the authors develop an approach f which incorporates rationales as part of the learning process of b.</s><s coords="30,383.46,390.17,97.13,8.74;30,134.77,402.13,345.83,8.74;30,134.77,414.08,296.65,8.74">A rationale is a simple subset of words representing a short and coherent piece of text (e.g., phrases), and alone must be sufficient for the prediction of the original text.</s><s coords="30,435.37,414.08,45.22,8.74;30,134.77,426.04,345.83,9.65;30,134.77,438.00,185.89,8.74">A rational is the local explanator ε l and provides the saliency of the text analyzed, i.e., indicates the reason for a certain outcome.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2" coords="30,134.77,468.73,129.18,8.77">Agnostic Explanator</head><p><s coords="30,134.77,489.57,345.82,8.74;30,134.77,501.52,345.83,8.74;30,134.77,513.48,124.44,8.74">In this section we present the agnostic solutions proposed for the black box outcome explanation problem implementing function f such that any type of black box b can be explained.</s><s coords="30,261.70,513.48,218.89,8.74;30,134.77,525.43,204.33,9.65">All these approaches are generalizable by definition and return a comprehensible local predictor c l .</s><s coords="30,342.41,525.43,138.19,8.74;30,134.77,537.39,95.62,8.74">Thus, they can be employed for diversified data types.</s></p><p><s coords="30,149.71,549.52,330.88,8.74;30,134.77,561.47,345.83,8.74;30,134.77,573.43,345.82,8.74;30,134.77,585.38,179.99,9.65">In <ref type="bibr" coords="30,162.59,549.52,14.61,8.74" target="#b84">[84]</ref>, Ribeiro et al. present the Local Interpretable Model-agnostic Explanations (LIME) approach f which does not depend on the type of data, nor on the type of black box b to be opened, nor on a particular type of comprehensible local predictor c l or explanation ε l .</s><s coords="30,318.69,585.38,161.90,8.74;30,134.77,597.34,324.95,8.74">In other words, LIME can return an understandable explanation for the prediction obtained by any black box.</s><s coords="30,463.43,597.34,17.16,8.74;30,134.77,609.29,345.82,8.74;30,134.77,621.25,345.82,8.74;30,134.77,633.20,202.39,8.74">The main intuition of LIME is that the explanation may be derived locally from the records generated randomly in the neighborhood of the record to be explained, and weighted according to their proximity to it.</s><s coords="30,339.57,633.20,141.02,8.74;30,134.77,645.16,345.83,9.65;30,134.77,657.11,164.66,9.65">In their experiments, the authors adopt linear models as comprehensible local predictor c l returning the importance of the features as explanation ε l .</s><s coords="30,301.99,657.11,178.60,8.74;31,134.77,119.99,345.82,8.74;31,134.77,131.95,26.93,8.74">As black box b the following classifiers are tested: decision trees, logistic regression, nearest neighbors, SVM and random forest.</s><s coords="31,164.76,131.95,315.83,8.74;31,134.77,143.90,307.95,8.74">A weak point of this approach is the required transformation of any type of data in a binary format which is claimed to be human interpretable.</s><s coords="31,445.88,143.90,34.71,8.74;31,134.77,155.86,345.82,8.74"><ref type="bibr" coords="31,445.88,143.90,15.50,8.74" target="#b83">[83]</ref> and <ref type="bibr" coords="31,134.77,155.86,15.50,8.74" target="#b82">[82]</ref> propose extensions of LIME with an analysis of particular aspects and cases.</s></p><p><s coords="31,149.71,167.81,330.88,8.74;31,134.77,179.77,345.83,8.74;31,134.77,191.72,192.58,8.74">A similar approach is presented in <ref type="bibr" coords="31,300.56,167.81,14.61,8.74" target="#b98">[98]</ref>, where Turner et al. design the Model Explanation System (MES) f that augments black box predictions with explanations by using a Monte Carlo algorithm.</s><s coords="31,331.74,191.72,148.85,8.74;31,134.77,203.68,345.83,8.74;31,134.77,215.63,345.83,9.65">In practice, they derive a scoring system for finding the best explanation based on formal requirements and consider that the explanations ε l are simple logical statements, i.e., decision rules.</s><s coords="31,134.77,227.59,269.03,8.74">The authors test logistic regression and SVMs as black box b.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8" coords="31,134.77,255.36,270.81,10.52">Solving the Black Box Inspection Problem</head><p><s coords="31,134.77,277.97,345.83,8.74;31,134.77,289.92,180.38,8.74">In this section we review the methods for opening the black box facing the black box inspection problem (see Section 4.1).</s><s coords="31,319.38,289.92,161.21,8.74;31,134.77,301.88,345.82,8.74;31,134.77,313.83,345.82,8.74;31,134.77,325.79,223.01,8.74">Given a black box solving a classification problem, the inspection problem consists in providing a representation for understanding either how the black box model works or why the black box returns certain predictions more likely than others.</s><s coords="31,361.18,325.79,119.42,8.74;31,134.77,337.74,345.83,8.74;31,134.77,349.70,145.19,8.74">In <ref type="bibr" coords="31,373.72,325.79,14.60,8.74" target="#b88">[88]</ref>, Seifet et al. provide a survey of visualizations of DNNs by defining a classification scheme describing visualization goals and methods.</s><s coords="31,284.39,349.70,196.20,8.74;31,134.77,361.65,147.57,8.74">They found that most papers use pixel displays to show neuron activations.</s><s coords="31,286.19,361.65,194.40,8.74;31,134.77,373.61,345.82,8.74;31,134.77,385.56,206.46,8.74">As in the previous sections, in the following we propose a classification based on the type of technique f used to provide the visual explanation of how the black box works.</s><s coords="31,344.81,385.56,135.79,8.74;31,134.77,397.52,117.09,8.74">Most papers in this section try to inspect NNs and DNNs.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1" coords="31,134.77,424.67,199.12,8.77">Inspection via Sensitivity Analysis</head><p><s coords="31,134.77,441.92,345.82,8.74;31,134.77,453.88,256.73,8.74">In this section we review the works solving the black box inspection problem by implementing function f using Sensitivity Analysis (SA).</s><s coords="31,396.11,453.88,84.49,8.74;31,134.77,465.83,345.82,8.74;31,134.77,477.79,114.12,8.74">Sensitivity analysis studies the correlation between the uncertainty in the output of a predictor and that one in its inputs <ref type="bibr" coords="31,230.62,477.79,14.61,8.74" target="#b86">[86]</ref>.</s><s coords="31,252.22,477.79,227.15,8.74">All the following methods work on tabular datasets.</s></p><p><s coords="31,149.71,489.74,330.88,8.74;31,134.77,501.70,345.82,8.74;31,134.77,513.65,53.94,8.74">Sensitivity analysis for "illuminating" the black box was first proposed by Olden in <ref type="bibr" coords="31,177.18,501.70,15.50,8.74" target="#b71">[71]</ref> where a visual method for understanding the mechanism of NN is described.</s><s coords="31,192.41,513.65,288.18,8.74;31,134.77,525.61,345.83,8.74;31,134.77,537.56,345.82,8.74;31,134.77,549.52,179.99,8.74">In particular, they propose to assess the importance of axon connections and the contribution of input variables by means of sensitivity analysis and Neural Interpretation Diagram (NID) to remove not significant connections and improve the network interpretability.</s></p><p><s coords="31,149.71,561.47,330.89,8.74;31,134.77,573.43,345.83,8.74;31,134.77,585.38,135.80,8.74">In <ref type="bibr" coords="31,162.55,561.47,10.52,8.74" target="#b6">[7]</ref> the authors propose a procedure based on Gaussian Process Classification (GDP) which allows explaining the decisions of any classification method through an explanation vector.</s><s coords="31,274.22,585.38,206.37,8.74">That is, the procedure f is black box agnostic.</s><s coords="31,134.77,597.34,345.83,8.74;31,134.77,609.29,218.02,8.74">The explanation vectors are visualized to highlight the features that were most influential for the decision of a particular instance.</s><s coords="31,355.72,609.29,124.88,8.74;31,134.77,621.25,166.91,9.65">Thus, we are dealing with an inspection for outcome explanation ε l .</s></p><p><s coords="31,149.71,633.20,330.89,8.74;31,134.77,645.16,345.83,8.74;31,134.77,657.11,32.19,8.74">In <ref type="bibr" coords="31,163.22,633.20,14.61,8.74" target="#b21">[21]</ref>, Datta et al. introduce a set of Quantitative Input Influence (QII) measures f capturing how much inputs influence the outputs of black box predictors.</s><s coords="31,170.46,657.11,310.14,8.74;32,134.77,367.21,66.18,8.74">These measures provide a foundation for transparency reports of black box predictors.</s><s coords="32,205.31,367.21,275.29,8.74;32,134.77,379.17,90.30,8.74">In practice, the output consists in the feature importance for outcome predictions.</s></p><p><s coords="32,149.71,391.20,330.88,8.74;32,134.77,403.15,120.97,8.74">[92] studies the problem of attributing the prediction of a DNN (the black box b) to its input features.</s><s coords="32,259.23,403.15,221.36,8.74;32,134.77,415.11,136.23,8.74">Two fundamental axioms are identified: sensitivity and implementation invariance.</s><s coords="32,273.93,415.11,206.67,8.74;32,134.77,427.06,345.82,8.74;32,134.77,439.02,73.38,8.74">These axioms guide the design of an attribution method f , called Integrated Gradients (IG), that requires no modification to the original network.</s><s coords="32,211.46,439.02,269.13,8.74;32,134.77,450.97,99.15,8.74">Differently from the previous work, this approach is tested on different types of data.</s></p><p><s coords="32,149.71,463.01,330.88,8.74;32,134.77,474.96,171.80,8.74">Finally, Cortez in <ref type="bibr" coords="32,233.31,463.01,16.13,8.74" target="#b16">[16,</ref><ref type="bibr" coords="32,249.44,463.01,12.10,8.74" target="#b17">17]</ref> uses sensitivity analysis based and visualization techniques f to explain black boxes b.</s><s coords="32,310.97,474.96,169.62,8.74;32,134.77,486.92,266.43,8.74">The sensitivity measures are variables calculated as the range, gradient, variance of the prediction.</s><s coords="32,405.04,486.92,75.55,8.74;32,134.77,498.87,345.82,8.74;32,134.77,510.83,345.83,8.74;32,134.77,522.78,227.52,8.74">Then, the visualizations realized are barplots for the features importance, and Variable Effect Characteristic curve (VEC) <ref type="bibr" coords="32,262.38,510.83,15.50,8.74" target="#b18">[18]</ref> plotting the input values (x-axis) versus the (average) outcome responses (see Figure <ref type="figure" coords="32,313.70,522.78,41.42,8.74" target="#fig_13">12 -(left)</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2" coords="32,134.77,553.02,199.19,8.77">Inspection via Partial Dependence</head><p><s coords="32,134.77,573.35,345.82,8.74;32,134.77,585.31,345.83,8.74;32,134.77,597.26,31.95,8.74">In this section we report a set of approaches solving the black box inspection problem by implementing a function f which returns a Partial Dependence Plot (PDP).</s><s coords="32,169.51,597.26,311.09,8.74;32,134.77,609.22,312.75,8.74">Partial dependence plot is a tool for visualizing the relationship between the response variable and predictor variables in a reduced feature space.</s><s coords="32,450.64,609.22,29.95,8.74;32,134.77,621.17,345.82,8.74;32,134.77,633.13,73.20,8.74">All the approaches presented in this section are black box agnostic and are tested on tabular datasets.</s></p><p><s coords="32,149.71,645.16,330.88,8.74;32,134.77,657.11,268.57,8.74">In <ref type="bibr" coords="32,162.77,645.16,14.61,8.74" target="#b35">[35]</ref>, the authors present an approach f aimed at evaluating the importance of non-additive interactions between any set of features.</s><s coords="32,406.40,657.11,74.19,8.74;33,134.77,119.99,345.83,8.74;33,134.77,131.95,345.82,8.74;33,134.77,143.90,70.82,8.74">The implementa-tion uses the Variable Interaction Network (VIN) visualization generated from the use of ANOVA statistical methodology (a technique to calculate partial dependence plots).</s><s coords="33,208.53,143.90,272.06,8.74;33,134.77,155.86,122.71,8.74">VIN allows to visualize the importance of the features together with their interdependences.</s><s coords="33,260.39,155.86,220.20,8.74;33,134.77,167.81,345.82,8.74;33,134.77,179.77,345.83,8.74;33,134.77,191.72,300.19,8.74">Goldstein et al. provide in <ref type="bibr" coords="33,375.83,155.86,15.50,8.74" target="#b30">[30]</ref> a technique f which extends classical PDP named Individual Conditional Expectation (ICE) to visualize the model approximated by a black box b that help in visualizing the average partial relationship between the outcome and some features.</s><s coords="33,438.44,191.72,42.15,8.74;33,134.77,203.68,345.83,8.74;33,134.77,215.63,56.44,8.74">ICE plots improves PDP by highlighting the variation in the fitted values across the range of covariates.</s><s coords="33,194.14,215.63,286.45,8.74;33,134.77,227.59,345.83,8.74;33,134.77,239.54,220.62,8.74">In <ref type="bibr" coords="33,206.21,215.63,14.61,8.74" target="#b48">[48]</ref>, Krause et al. introduce random perturbations on the black box b input values to understand to which extent every feature impact the prediction through a visual inspection using the PDPs f .</s><s coords="33,358.44,239.54,122.15,8.74;33,134.77,251.50,345.82,8.74;33,134.77,263.45,30.03,8.74">The main idea of Prospector is to observe how the output varies by varying the input changing one variable at a time.</s><s coords="33,167.70,263.45,312.89,8.74;33,134.77,275.41,345.83,9.65;33,134.77,287.36,172.26,8.74">It provides an effective way to understand which are the most important features for a certain prediction ε l so that it can help in providing a valuable interpretation (see Figure <ref type="figure" coords="33,250.87,287.36,48.29,8.74" target="#fig_13">12 -(right)</ref>).</s><s coords="33,310.76,287.36,168.76,8.74;33,134.77,299.32,345.83,8.74;33,134.77,311.27,243.66,8.74">In <ref type="bibr" coords="33,323.63,287.36,10.52,8.74" target="#b1">[2]</ref> the authors propose a method f for auditing (i.e., inspecting) black box predictors b, studying to which extent existing models benefit of specific features in the data.</s><s coords="33,382.49,311.27,98.10,8.74;33,134.77,323.23,206.67,8.74">This method does not assume any knowledge on the models behavior.</s><s coords="33,344.86,323.23,135.74,8.74;33,134.77,335.18,345.82,9.65;33,134.77,347.14,324.83,8.74">In particular, the method f focuses on indirect influence and visualizes the global inspection ε g through an obscurity vs. accuracy plot (the features are obscured one after the other).</s><s coords="33,462.88,347.14,17.71,8.74;33,134.77,359.09,345.83,8.74;33,134.77,371.05,345.83,8.74;33,134.77,383.00,345.83,8.74;33,134.77,394.96,188.63,8.74">Yet, the dependence of a black box b on its input features is relatively quantified by the procedure f proposed in <ref type="bibr" coords="33,263.90,371.05,9.96,8.74" target="#b0">[1]</ref>, where the authors present an iterative procedure based on Orthogonal Projection of Input Attributes (OPIA), for enabling the interpretability of black box predictors.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3" coords="33,134.77,428.61,192.05,8.77">Inspection via Other Approaches</head><p><s coords="33,134.77,452.36,345.83,8.74;33,134.77,464.31,345.83,8.74">In the following, we present solutions for the black box inspection problem that adopt an approach f which can be categorized as none of the previous ones.</s><s coords="33,134.77,476.27,281.93,8.74">They all refer to DNNs as black box b and are not generalizable.</s><s coords="33,149.71,488.98,330.88,8.74;33,134.77,500.94,345.82,8.74;33,134.77,512.89,73.93,8.74"><ref type="bibr" coords="33,149.71,488.98,20.48,8.74" target="#b110">[110]</ref> proposes two tools for visualizing and interpreting DNNs and for understanding what computations DNNs perform at intermediate layers and which neurons activate.</s><s coords="33,211.98,512.89,268.61,8.74;33,134.77,524.85,198.10,8.74">These tools visualize the activations of each layer of a trained CNN during the process of images or videos.</s><s coords="33,336.65,524.85,143.94,8.74;33,134.77,536.80,307.01,8.74">Moreover, they visualize the features of the different layers by regularized optimization in image space.</s><s coords="33,444.78,536.80,35.81,8.74;33,134.77,548.76,345.83,8.74;33,134.77,560.71,326.65,8.74">Yosinski et al. found that by analyzing the live activations, changing in correspondence of different inputs, helps to generate an explanation on the DNNs behave.</s><s coords="33,465.09,560.71,15.50,8.74;33,134.77,572.67,345.83,8.74"><ref type="bibr" coords="33,465.09,560.71,15.50,8.74" target="#b96">[96]</ref> shows the extraction of a visual interpretation of a DNN using a decision tree.</s><s coords="33,134.77,584.62,185.27,8.74">The method TreeView f works as follows.</s><s coords="33,323.66,584.62,156.93,8.74;33,134.77,596.58,345.83,8.74;33,134.77,608.54,31.91,8.74">Given the black box b as a DNN, it first decomposes the feature space into K (user defined) potentially overlapping factors.</s><s coords="33,170.09,608.54,310.50,8.74;33,134.77,620.49,160.46,8.74">Then, it builds a meta feature for each of the K clusters and a random forest that predicts the cluster labels.</s><s coords="33,297.92,620.49,182.67,8.74;33,134.77,632.45,291.41,8.74">Finally, it generates and shows a surrogate decision tree from the forest as an approximation of the black box.</s></p><p><s coords="33,149.71,645.16,330.88,8.74;33,134.77,657.11,345.83,8.74;34,134.77,119.99,345.83,8.74;34,134.77,131.95,98.44,8.74">Shwartz-Ziv et al. in <ref type="bibr" coords="33,243.46,645.16,15.50,8.74" target="#b90">[90]</ref> showed the effectiveness of the Information Plane f visualization of DNNs highlighting that the empirical error minimization of each stochastic gradient descent phase epoch is always followed by a slow representation compression.</s></p><p><s coords="34,149.71,144.42,330.88,8.74;34,134.77,156.38,345.82,8.74;34,134.77,168.33,345.83,8.74;34,134.77,180.29,22.19,8.74">Finally, it is worth mentioning that <ref type="bibr" coords="34,307.93,144.42,15.50,8.74" target="#b81">[81]</ref> presents the discovery that a single neuron unit of a DNN can perform alone a sentiment analysis task after the training of the network reaching the same level of performance of strong baselines.</s><s coords="34,160.18,180.29,320.42,8.74;34,134.77,192.24,289.55,8.74">Also in <ref type="bibr" coords="34,194.08,180.29,19.37,8.74" target="#b111">[111]</ref>, Zeiler et al. backtrack the network computations to identify which image patches are responsible for certain neural activations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9" coords="34,134.77,225.35,288.06,10.52">Solving the Transparent Box Design Problem</head><p><s coords="34,134.77,253.27,345.83,8.74;34,134.77,265.23,345.82,8.74;34,134.77,277.18,332.91,8.74">In this section we review the approaches designed to solve the classification problem using a transparent method which is locally or globally interpretable on its own, i.e., solving the transparent box design problem (see Section 4.1).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1" coords="34,134.77,309.66,189.24,8.77">Explanation via Rule Extraction</head><p><s coords="34,134.77,332.23,345.83,8.74;34,134.77,344.19,345.83,8.74;34,134.77,356.14,38.09,8.74">In this section we present the most relevant state of the art works solving the transparent box design problem by means of comprehensible predictors c based on rules.</s><s coords="34,176.91,356.14,303.68,9.65;34,134.77,368.10,345.83,9.65;34,134.77,380.05,66.22,9.65">In these cases, c g is a comprehensible global predictor providing the whole set of rules leading to any possible decision: a global explanator ε g is made available by c g .</s><s coords="34,204.16,380.05,276.43,8.74">All the methods presented in this section work on tabular data.</s></p><p><s coords="34,149.71,392.53,330.88,8.74;34,134.77,404.49,345.83,8.74;34,134.77,416.44,279.52,8.74">In <ref type="bibr" coords="34,163.16,392.53,20.48,8.74" target="#b109">[109]</ref> the authors propose the approach f named CPAR (Classification based on Predictive Association Rules) combining the positive aspects of both associative classification and traditional rule-based classification.</s><s coords="34,417.14,416.44,63.45,8.74;34,134.77,428.40,345.82,8.74;34,134.77,440.35,345.83,8.74;34,134.77,452.31,131.90,9.65">Indeed, following the basic idea of FOIL <ref type="bibr" coords="34,248.63,428.40,14.61,8.74" target="#b80">[80]</ref>, CPAR does not generate a large set of candidates as in associative classification, and applies a greedy approach for generating rules c g directly from training data.</s></p><p><s coords="34,149.71,464.78,330.88,8.74;34,134.77,476.74,201.92,9.65">Wang and Rudin, in <ref type="bibr" coords="34,245.03,464.78,20.48,8.74" target="#b102">[102]</ref> propose a method f to extract falling rule lists c g (see Section 3.3) instead of classical rules.</s><s coords="34,341.08,476.74,139.52,8.74;34,134.77,488.69,183.40,8.74">The falling rule lists extraction method f relies on a Bayesian framework.</s></p><p><s coords="34,149.71,501.17,330.88,8.74;34,134.77,513.13,268.49,8.74">In <ref type="bibr" coords="34,162.75,501.17,14.61,8.74" target="#b57">[57]</ref>, the authors tackle the problem to build a system for medical scoring which is interpretable and characterized by high accuracy.</s><s coords="34,406.39,513.13,74.20,8.74;34,134.77,525.08,345.82,8.74;34,134.77,537.04,111.83,9.65">To this end, they propose Bayesian Rule Lists (BRL) f to extract the comprehensible global predictor c g as a decision list.</s><s coords="34,249.15,537.04,231.44,8.74;34,134.77,548.99,345.83,8.74;34,134.77,560.95,87.34,8.74">A decision list consists of a series of if-then statements discretizing the whole feature space into a set of simple and directly interpretable decision statements.</s></p><p><s coords="34,149.71,573.43,190.17,8.74">A Bayesian approach is followed also in <ref type="bibr" coords="34,321.62,573.43,14.61,8.74" target="#b91">[91]</ref>.</s><s coords="34,342.68,573.43,137.91,8.74;34,134.77,585.38,345.82,8.74;34,134.77,597.34,140.44,9.65">The authors propose algorithms f for learning Two-Level Boolean Rules (TLBR) in Conjunctive Normal Form or Disjunctive Normal Form c g .</s><s coords="34,279.12,597.34,140.37,8.74">Two formulations are proposed.</s><s coords="34,423.40,597.34,57.18,8.74;34,134.77,609.29,345.83,8.74;34,134.77,621.25,240.68,8.74">The first one is an integer program whose objective function combines the total number of errors and the total number of features used in the rule.</s><s coords="34,378.36,621.25,102.24,8.74;34,134.77,633.20,345.83,8.74;34,134.77,645.16,294.84,8.74">The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample.</s><s coords="34,433.78,645.16,46.81,8.74;34,134.77,657.11,345.83,8.74;35,134.77,119.99,345.83,8.74;35,134.77,131.95,254.49,8.74">In <ref type="bibr" coords="34,447.09,645.16,15.50,8.74" target="#b54">[54]</ref> the authors propose a method f exploiting a two-level boolean rule predictor to solve the black box model explanation, i.e., the transparent approach is used in the reverse engineering approach to explain the black box.</s></p><p><s coords="35,149.71,143.90,188.53,8.74">Yet another type of rule is exploited in <ref type="bibr" coords="35,319.97,143.90,14.61,8.74" target="#b53">[53]</ref>.</s><s coords="35,341.21,143.90,139.38,8.74;35,134.77,155.86,345.83,8.74;35,134.77,167.81,345.83,9.65;35,134.77,179.77,54.27,8.74">Here, Lakkaraju et al. propose a framework f for generating prediction models, which are both interpretable and accuratem, by extracting Interpretable Decision Sets (IDS) c g , i.e., independent if-then rules.</s><s coords="35,191.48,179.77,289.11,8.74;35,134.77,191.72,170.99,8.74">Since each rule is independently applicable, decision sets are simple, succinct, and easily to be interpreted.</s><s coords="35,310.43,191.72,170.16,8.74;35,134.77,203.68,331.04,8.74">In particular, this approach can learn accurate, short, and non-overlapping rules covering the whole feature space.</s></p><p><s coords="35,149.71,215.63,309.50,9.65">Rule Sets are adopted in <ref type="bibr" coords="35,265.13,215.63,20.48,8.74" target="#b104">[104]</ref> as comprehensible global predictor c g .</s><s coords="35,463.44,215.63,17.16,8.74;35,134.77,227.59,274.61,8.74">The authors present a Bayesian framework f for learning Rule Sets.</s><s coords="35,412.49,227.59,68.10,8.74;35,134.77,239.54,345.82,8.74;35,134.77,251.50,335.54,8.74">A set of parameters is provided to the user to encourage the model to have a desired size and shape in order to conform with a domain-specific definition of interpretability.</s><s coords="35,473.12,251.50,7.47,8.74;35,134.77,263.45,345.83,8.74;35,134.77,275.41,213.79,8.74">A Rule Set consists of a small number of short rules where an instance is classified as positive if it satisfies at least one of the rules.</s><s coords="35,352.21,275.41,128.38,8.74;35,134.77,287.36,251.75,8.74">The rule set provides reasons for predictions, and also descriptions of a particular class.</s></p><p><s coords="35,149.71,299.32,330.88,8.74;35,134.77,311.27,345.83,8.74;35,134.77,323.23,37.14,8.74">Finally, in <ref type="bibr" coords="35,198.59,299.32,15.50,8.74" target="#b64">[64]</ref> an approach f is designed to learn both sparse conjunctive and disjunctive clause rules from training data through a linear programming solution.</s><s coords="35,174.74,323.23,305.85,8.74;35,134.77,335.18,316.66,9.65">The optimization formulation leads the resulting rule-based global predictor c g (1Rule) to automatically balance accuracy and interpretability.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2" coords="35,134.77,363.62,208.16,8.77">Explanation via Prototype Selection</head><p><s coords="35,134.77,382.15,345.82,8.74;35,134.77,394.10,345.83,9.65;35,134.77,406.06,265.73,9.65">In this section we present the design of a set of approaches f for solving the transparent box design problem returning a comprehensible predictor c g equipped with a human understandable global explanator function ε g .</s><s coords="35,404.10,406.06,76.49,8.74;35,134.77,418.01,345.82,8.74;35,134.77,429.97,117.28,8.74">A prototype, also referred to with the name artifact or archetype, is an object that is representative of a set of similar instances.</s><s coords="35,254.54,429.97,226.06,8.74;35,134.77,441.92,214.98,8.74;35,351.20,440.35,7.07,6.12;35,358.78,441.92,121.81,8.74;35,134.77,453.88,345.82,8.74;35,134.77,465.83,191.34,8.74">A prototype can be an instance x part of the training set D = {X, Y }, or can lie anywhere in the space X m ×Y of the dataset D. Having only prototypes among the observed points is desirable for interpretability, but it can also improve the classification error.</s><s coords="35,330.41,465.83,150.18,8.74;35,134.77,477.79,345.83,8.74;35,134.77,489.74,345.82,8.74;35,134.77,501.70,237.68,8.74">As an example of a prototype we can consider the record minimizing the sum of the distances with all the other points of a set (like in K-Medoids) or the record generated averaging the value of the features of a set of points (like in K-Means) <ref type="bibr" coords="35,354.18,501.70,14.61,8.74" target="#b95">[95]</ref>.</s><s coords="35,375.48,501.70,105.10,8.74;35,134.77,513.65,345.82,8.74;35,134.77,525.61,108.48,8.74">Different definitions and requirements to find a prototype are specified in each work using the prototypes to explain the black box.</s></p><p><s coords="35,149.71,537.56,330.88,8.74;35,134.77,549.52,345.83,8.74;35,134.77,561.47,295.73,8.74">In <ref type="bibr" coords="35,162.30,537.56,9.96,8.74" target="#b7">[8]</ref>, Bien et al. design the transparent Prototype Selection (PS) approach f that first seeks for the best prototype (two strategies are proposed), and then assigns the points in D to the label corresponding to the prototype.</s><s coords="35,433.80,561.47,46.78,8.74;35,134.77,573.43,267.72,8.74">In particular, they face the problem of recognizing hand written digits.</s><s coords="35,405.98,573.43,74.61,8.74;35,134.77,585.38,345.83,8.74;35,134.77,597.34,345.82,8.74;35,134.77,609.29,212.25,8.74">In this approach, every instance can be described by more than one prototype, and more than a prototype can refer to the same label (e.g., there can be more than one prototype for digit zero, more than one for digit one, etc.).</s><s coords="35,350.53,609.29,130.06,8.74;35,134.77,621.25,345.82,9.65;35,134.77,633.20,345.82,8.74;35,134.77,645.16,345.82,8.74;35,134.77,657.11,98.76,8.74">The comprehensible predictor c g provides a global explanation in which every instance must have a prototype corresponding to its label in its neighborhood; no instances should have a prototype with a different label in its neighborhood, and there should be as few prototypes as possible.</s><s coords="36,149.71,119.99,330.88,8.74;36,134.77,131.95,345.83,9.65;36,134.77,143.90,45.00,8.74"><ref type="bibr" coords="36,149.71,119.99,79.26,8.74">Kim et al. in [44,</ref><ref type="bibr" coords="36,228.96,119.99,12.10,8.74" target="#b45">45]</ref> design the Bayesian Case Model (BCM) comprehensible predictor c l able to learn prototypes by clustering the data and to learn subspaces.</s><s coords="36,183.02,143.90,297.57,8.74;36,134.77,155.86,345.82,8.74;36,134.77,167.81,45.14,8.74">Each prototype is the representative sample of a given cluster, while the subspaces are set of features which are important in identifying the cluster prototype.</s><s coords="36,182.87,167.81,297.72,9.65;36,134.77,179.77,144.18,8.74">That is, the global explanator ε g returns a set of prototypes together with their fundamental features.</s><s coords="36,283.34,179.77,197.25,8.74;36,134.77,191.72,345.83,8.74;36,134.77,203.68,327.00,8.74">Possible drawbacks of this approach are the high number of parameters (e.g., number of clusters) and various types of probability distributions which are assumed to be correct for each type of data.</s><s coords="36,465.09,203.68,15.50,8.74;36,134.77,215.63,345.83,8.74;36,134.77,227.59,65.97,8.74"><ref type="bibr" coords="36,465.09,203.68,15.50,8.74" target="#b42">[42]</ref> proposes an extension of BCM which exploits humans interaction to improve the prototypes.</s><s coords="36,203.82,227.59,276.77,8.74;36,134.77,239.54,345.82,8.74;36,134.77,251.50,248.95,8.74">Finally, in <ref type="bibr" coords="36,250.78,227.59,15.50,8.74" target="#b43">[43]</ref> the approach is further expanded to include criticisms, where a criticism is an instance that does not fit the model very well, i.e., a counter-example part of the cluster of a prototype.</s></p><p><s coords="36,149.71,263.45,330.88,8.74;36,134.77,275.41,345.83,8.74;36,134.77,287.36,230.60,8.74">With respect to prototypes and DNN, <ref type="bibr" coords="36,314.98,263.45,15.50,8.74" target="#b63">[63]</ref> proposes a method b to change the image representations in order to use only information from the original image representation and from a generic natural image prior.</s><s coords="36,367.67,287.36,112.92,8.74;36,134.77,299.32,345.83,8.74;36,134.77,311.27,345.83,8.74;36,134.77,323.23,345.83,8.74">This task is mainly related to image reconstruction rather than black box explanation, but it is realized with the aim of understanding the example to which the DNN b is related to producing a certain prediction by realizing a sort of artificial image prototype.</s><s coords="36,134.77,335.18,345.82,8.74;36,134.77,347.14,223.60,8.74">There is a significant amount of work in understanding the representation of DNN by means of artifact images, <ref type="bibr" coords="36,286.97,347.14,16.80,8.74" target="#b41">[41,</ref><ref type="bibr" coords="36,303.77,347.14,16.80,8.74" target="#b100">100,</ref><ref type="bibr" coords="36,320.57,347.14,16.80,8.74" target="#b105">105,</ref><ref type="bibr" coords="36,337.37,347.14,16.80,8.74" target="#b111">111]</ref>.</s></p><p><s coords="36,149.71,359.09,330.89,8.74">We conclude this section presenting how <ref type="bibr" coords="36,331.91,359.09,15.50,8.74" target="#b25">[25]</ref> deals with artifacts in DNNs.</s><s coords="36,134.77,371.05,345.83,8.74;36,134.77,383.00,345.83,8.74">Finding a single representative prototype by perturbation, deletion, preservation, and similar approaches has the risk of triggering artifacts of the black box.</s><s coords="36,134.77,394.96,345.82,8.74;36,134.77,406.91,38.55,8.74">As discussed in Section 8.3, NN and DNN are known to be affected by surprising artifacts.</s><s coords="36,176.21,406.91,304.38,8.74;36,134.77,418.87,345.82,8.74;36,134.77,430.82,345.83,8.74;36,134.77,442.78,345.82,8.74;36,134.77,454.74,345.82,8.74;36,134.77,466.69,172.72,8.74">For example, <ref type="bibr" coords="36,235.04,406.91,15.50,8.74" target="#b52">[52]</ref> shows that a nearly-invisible image perturbation can lead a NN to classify an object for another; <ref type="bibr" coords="36,333.77,418.87,15.50,8.74" target="#b69">[69]</ref> constructs abstract synthetic images that are classified arbitrarily; <ref type="bibr" coords="36,297.50,430.82,15.50,8.74" target="#b63">[63]</ref> finds deconstructed versions of an image which are indistinguishable from the viewpoint of the DNN from the original image, and also with respect to texts <ref type="bibr" coords="36,297.22,454.74,15.50,8.74" target="#b58">[58]</ref> inserts typos and random sentences in real texts that are classified arbitrarily.</s><s coords="36,311.25,466.69,169.34,8.74;36,134.77,478.65,345.82,8.74;36,134.77,490.60,114.52,8.74">These examples demonstrate that it is possible to find particular inputs that can drive the DNN to generate nonsensical or unexpected outputs.</s><s coords="36,252.01,490.60,228.58,8.74;36,134.77,502.56,345.83,8.74;36,134.77,514.51,148.43,8.74">While not all artifacts look "unnatural", nevertheless they form a subset of images that are sampled with negligible probability when the network is normally operated.</s><s coords="36,286.73,514.51,193.87,8.74;36,134.77,526.47,345.83,8.74;36,134.77,538.42,89.08,8.74">In our opinion, two guidelines should be followed to avoid such artifacts in generating explanations for DNNs, and for every black box in general.</s><s coords="36,226.68,538.42,253.91,8.74;36,134.77,550.38,201.20,8.74">The first one is that powerful explanations should, just like any predictor, generalize as much as possible.</s><s coords="36,339.79,550.38,140.81,8.74;36,134.77,562.33,185.66,8.74">Second, the artifacts should not be representative of natural perturbations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3" coords="36,134.77,590.76,200.42,8.77">Explanation via Other Approaches</head><p><s coords="36,134.77,609.29,345.82,8.74;36,134.77,621.25,326.08,8.74">In the following we present solutions for the transparent box design problem adopting approaches f that can not be categorized as the previous ones.</s><s coords="36,465.09,621.25,15.50,8.74;36,134.77,633.20,345.82,8.74;36,134.77,645.16,103.13,9.65"><ref type="bibr" coords="36,465.09,621.25,15.50,8.74" target="#b47">[47]</ref> describes a method f based on Naive Bayes aimed to explain individual predictions ε l of black boxes b.</s><s coords="36,240.38,645.16,240.21,8.74;36,134.77,657.11,345.82,8.74;37,134.77,119.99,200.11,9.65">The proposed approach exploits notions from coalitional game theory, and explains predictions utilizing the contribution of the value of different individual features ε l (see Figure <ref type="figure" coords="37,323.25,119.99,3.87,8.74" target="#fig_1">2</ref>).</s><s coords="37,338.61,119.99,141.98,8.74;37,134.77,131.95,274.55,8.74">The method is agnostic with respect to the black box used and is tested only on tabular data.</s><s coords="37,412.64,131.95,67.95,8.74;37,134.77,143.90,345.83,8.74;37,134.77,155.86,345.82,9.65;37,134.77,167.81,19.40,8.74">Finally, in <ref type="bibr" coords="37,460.11,131.95,20.48,8.74" target="#b103">[103]</ref> Wang et al. propose a method f named OT-SpAMs based on oblique tree sparse additive models for obtaining a global interpretable predictor c g as a decision tree.</s><s coords="37,156.57,167.81,324.02,8.74;37,134.77,179.77,345.82,8.74;37,134.77,191.72,33.29,8.74">OT-SpAMs divides the feature space into regions using a sparse oblique tree splitting and assigns local sparse additive experts (leaf of the tree) to individual regions.</s><s coords="37,172.23,191.72,308.36,8.74;37,134.77,203.68,98.14,9.65">Basically, OT-SpAMs passes from complicated trees/linear models to an explainable tree ε g .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10" coords="37,134.77,235.46,91.23,10.52">Conclusion</head><p><s coords="37,134.77,262.07,345.83,8.74;37,134.77,274.03,345.82,8.74;37,134.77,285.98,111.18,8.74">In this paper we have presented a comprehensive overview of methods proposed in the literature for explaining decision systems based on opaque and obscure machine learning models.</s><s coords="37,249.75,285.98,230.84,8.74;37,134.77,297.94,169.79,8.74">First, we have identified the different components of the family of the explanation problems.</s><s coords="37,307.39,297.94,173.20,8.74;37,134.77,309.89,345.82,8.74;37,134.77,321.85,79.93,8.74">In particular, we have provided a formal definition of each problem belonging to that family capturing for each one the proper peculiarity.</s><s coords="37,217.89,321.85,262.70,8.74;37,134.77,333.80,345.82,8.74;37,134.77,345.76,161.55,8.74">We have named these problems: black box model explanation problem, black box outcome explanation problem, black box inspection problem and transparent box design problem.</s><s coords="37,301.12,345.76,179.47,8.74;37,134.77,357.71,345.83,8.74;37,134.77,369.67,345.83,8.74;37,134.77,381.62,345.83,8.74;37,134.77,393.58,74.17,8.74">Then, we have proposed a classification of methods studied in the literature which take into account the following dimensions: the specific explanation problem addressed, the type of explanator adopted, the black box model opened, and the type of data used as input by the black box model.</s></p><p><s coords="37,149.71,405.80,330.88,8.74;37,134.77,417.75,345.83,8.74;37,134.77,429.71,131.18,8.74">As shown in this paper, a considerable amount of work has already been done in different scientific communities and especially in the machine learning and data mining communities.</s><s coords="37,268.78,429.71,211.80,8.74;37,134.77,441.66,345.83,8.74;37,134.77,453.62,345.83,8.74;37,134.77,465.57,106.76,8.74">The first one is mostly focused on describing how the black boxes work, while the second one is more interested into explaining the decisions even without understanding the details on how the opaque decision systems work in general.</s></p><p><s coords="37,149.71,477.79,330.88,8.74;37,134.77,489.74,345.82,8.74;37,134.77,501.70,268.12,8.74">The analysis of the literature conducted in this paper has led to the conclusion that despite many approaches have been proposed to explain black boxes, some important scientific questions still remain unanswered.</s><s coords="37,407.15,501.70,73.44,8.74;37,134.77,513.65,345.82,8.74;37,134.77,525.61,63.29,8.74">One of the most important open problems is that, until now, there is no agreement on what an explanation is.</s><s coords="37,201.44,525.61,279.16,8.74;37,134.77,537.56,323.17,8.74">Indeed, some works provide as explanation a set of rules, others a decision tree, others a prototype (especially in the context of images).</s><s coords="37,462.17,537.56,18.41,8.74;37,134.77,549.52,345.83,8.74;37,134.77,561.47,345.82,8.74;37,134.77,573.43,345.82,8.74;37,134.77,585.38,271.70,8.74">It is evident that the research activity in this field completely ignored the importance of studying a general and common formalism for defining an explanation, identifying which are the properties that an explanation should guarantee, e.g., soundness, completeness, compactness and comprehensibility.</s><s coords="37,410.83,585.38,69.76,8.74;37,134.77,597.34,345.83,8.74;37,134.77,609.29,345.82,8.74;37,134.77,621.25,121.36,8.74">Concerning this last property, there is no work that seriously addresses the problem of quantifying the grade of comprehensibility of an explanation for humans, although it is of fundamental importance.</s><s coords="37,259.78,621.25,220.81,8.74;37,134.77,633.20,345.83,8.74;37,134.77,645.16,280.06,8.74">The study of measures able to capture this aspect is challenging because it also consider also aspects like the expertise of the user or the amount of time available to understand the explanation.</s><s coords="37,418.65,645.16,61.94,8.74;37,134.77,657.11,345.83,8.74;38,134.77,119.99,345.82,8.74;38,134.77,131.95,275.66,8.74">The definition of a (mathematical) formalism for explanations and of tools for measuring how much an explanation is comprehensible for humans would improve the practical applicability of most of the approaches presented in this paper.</s><s coords="38,149.71,159.18,330.88,8.74;38,134.77,171.14,345.83,8.74;38,134.77,183.09,180.05,8.74">Moreover, there are other open research questions related to black boxes and explanations that are starting to be treated by the scientific community and that deserve attention and more investigation.</s></p><p><s coords="38,149.71,210.33,330.88,8.74;38,134.77,222.28,345.82,8.74">A common assumption of all categories of works presented in this paper is that the features used by the black box decision system are completely known.</s><s coords="38,134.77,234.24,345.83,8.74;38,134.77,246.19,74.76,8.74">However, a black box might use additional information besides that explicitly asked to the user.</s><s coords="38,212.08,246.19,268.51,8.74;38,134.77,258.15,313.71,8.74">For example, it might link the user's information with different data sources for augmenting the data to be exploited for the prediction.</s><s coords="38,451.78,258.15,28.82,8.74;38,134.77,270.10,345.83,8.74;38,134.77,282.06,345.83,8.74;38,134.77,294.01,98.81,8.74">Therefore, an important aspect to be investigated is to understand how an explanation might also be derived in cases where black box systems make decisions in presence of latent features.</s><s coords="38,237.36,294.01,243.24,8.74;38,134.77,305.97,345.83,8.74;38,134.77,317.93,345.82,8.74;38,134.77,329.88,345.82,8.74;38,134.77,341.84,36.89,8.74">An interesting starting point for this research direction is the framework proposed in <ref type="bibr" coords="38,266.64,305.97,15.50,8.74" target="#b55">[55]</ref> by Lakkaraju et al. for the evaluation of the prediction models performances on labeled data where the decision of decisionmakers (either humans or black-boxes) is taken in the presence of unobserved features.</s></p><p><s coords="38,149.71,369.07,330.88,8.74;38,134.77,381.03,133.28,8.74">Another open research question is related to providing explanations in the field of recommender systems.</s><s coords="38,272.61,381.03,207.98,8.74;38,134.77,392.98,295.70,8.74">When a suggestion is provided to a customer, it should come together with the reasons for this recommendation.</s><s coords="38,434.35,392.98,46.24,8.74;38,134.77,404.94,345.83,8.74;38,134.77,416.89,345.83,8.74;38,134.77,428.85,204.76,8.74">In <ref type="bibr" coords="38,447.37,392.98,15.50,8.74" target="#b67">[67]</ref> the authors define a case-based reasoning approach to generate recommendations with the opportunity of obtaining both the explanation of the recommendation process and of the produced recommendations.</s></p><p><s coords="38,149.71,456.08,330.89,8.74;38,134.77,468.04,333.53,8.74">Lastly, a further interesting point is the fact that explanations are important on their own and predictors might be learned directly from explanations.</s><s coords="38,473.12,468.04,7.47,8.74;38,134.77,479.99,345.83,8.74;38,134.77,491.95,345.83,8.74;38,134.77,503.91,75.97,8.74">A starting study of this aspect is <ref type="bibr" coords="38,276.78,479.99,15.50,8.74" target="#b49">[49]</ref> that presents a software agent learned to simulate the Mario Bros game only utilizing explanations rather than the logs of previous plays.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,241.41,204.99,132.55,7.89;8,238.51,116.83,138.34,73.39"><head>Fig. 1 .</head><label>1</label><figDesc><div><p><s coords="8,241.41,204.99,27.89,7.89">Fig. 1.</s><s coords="8,272.37,205.01,101.59,7.86">Example of decision tree.</s></p></div></figDesc><graphic coords="8,238.51,116.83,138.34,73.39" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,190.97,185.14,233.42,7.89;9,238.51,116.83,138.33,53.54"><head>Fig. 2 .</head><label>2</label><figDesc><div><p><s coords="9,190.97,185.14,233.42,7.89">Fig. 2. Example of feature importance for a linear model.</s></p></div></figDesc><graphic coords="9,238.51,116.83,138.33,53.54" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,245.74,192.15,123.88,7.89;12,224.68,116.83,166.00,60.55"><head>Fig. 3 .</head><label>3</label><figDesc><div><p><s coords="12,245.74,192.15,123.88,7.89">Fig. 3. Classification Problem.</s></p></div></figDesc><graphic coords="12,224.68,116.83,166.00,60.55" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,223.02,585.21,257.57,9.82;12,134.77,597.34,105.36,8.74;12,241.58,595.76,18.22,6.12;12,263.89,597.34,47.39,8.74;12,312.11,595.76,4.92,6.12;12,321.12,597.34,159.47,8.74;12,134.77,609.29,173.67,8.74;12,309.89,607.72,7.07,6.12;12,317.46,609.29,163.14,8.74;12,134.77,621.25,175.21,8.74;12,149.71,633.20,330.88,9.65;12,134.77,644.99,345.83,9.82;12,134.77,657.11,345.83,9.65;13,134.77,119.99,345.83,8.74;13,134.77,131.95,330.17,9.65"><head/><label/><figDesc><div><p><s coords="12,223.02,585.21,257.57,9.82;12,134.77,597.34,105.36,8.74;12,241.58,595.76,18.22,6.12;12,263.89,597.34,47.39,8.74;12,312.11,595.76,4.92,6.12;12,321.12,597.34,121.77,8.74">The learner L b takes as input a dataset D = {X, Y } with n samples where X ∈ X n×m and Y ∈ Y n and returns the predictor b.</s><s coords="12,446.49,597.34,34.10,8.74;12,134.77,609.29,173.67,8.74;12,309.89,607.72,7.07,6.12;12,317.46,609.29,163.14,8.74;12,134.77,621.25,175.21,8.74">Given a data record in the feature space x ∈ X m , the predictor b can be employed to predict the target value ŷ, i.e., b(x) = ŷ.</s><s coords="12,149.71,633.20,330.88,9.65;12,134.77,644.99,345.83,9.82;12,134.77,657.11,221.48,9.65">Typically, in supervised learning [95], a training dataset D train is used for training the learner L b (D train ) which builds the predictor b, and a test dataset D test is used for evaluating the performance of b.</s><s coords="12,360.51,657.11,120.08,9.65;13,134.77,119.99,345.83,8.74;13,134.77,131.95,330.17,9.65">Given D test = {X, Y }, the evaluation is performed by observing for each couple of data record and target value (x, y) ∈ D test the number of correspondences between y and b(x) = ŷ.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="14,211.68,192.15,192.00,7.89;14,224.68,212.47,165.99,60.53"><head>Fig. 4 .</head><label>4</label><figDesc><div><p><s coords="14,211.68,192.15,192.00,7.89">Fig. 4. Black Box Model Explanation Problem.</s></p></div></figDesc><graphic coords="14,224.68,212.47,165.99,60.53" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,206.05,287.76,203.26,7.89"><head>Fig. 5 .</head><label>5</label><figDesc><div><p><s coords="14,206.05,287.76,203.26,7.89">Fig. 5. Black Box Outcome Explanation Problem.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="14,456.15,515.71,24.44,8.74;14,134.77,527.67,345.83,8.74;14,134.77,539.62,345.83,9.65;14,134.77,551.58,148.23,8.74"><head/><label/><figDesc><div><p><s coords="14,456.15,515.71,24.44,8.74;14,134.77,527.67,345.83,8.74;14,134.77,539.62,345.83,9.65;14,134.77,551.58,148.23,8.74">which takes as input a black box b and a dataset D, and returns a comprehensible local predictor c l , i.e., f (b, D) = c l , such that c l is able to mimic the behavior of b, and exists a local explanator function ε</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="15,229.51,192.13,156.34,7.89;15,224.68,116.83,165.99,60.53"><head>Fig. 6 .</head><label>6</label><figDesc><div><p><s coords="15,229.51,192.13,27.89,7.89">Fig. 6.</s><s coords="15,260.47,192.15,125.38,7.86">Black Box Inspection Problem.</s></p></div></figDesc><graphic coords="15,224.68,116.83,165.99,60.53" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="16,223.57,211.89,168.21,7.89;16,224.68,116.83,166.00,80.29"><head>Fig. 7 .</head><label>7</label><figDesc><div><p><s coords="16,223.57,211.89,168.21,7.89">Fig. 7. Transparent Box Design Problem.</s></p></div></figDesc><graphic coords="16,224.68,116.83,166.00,80.29" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="22,134.77,210.22,345.83,7.89;22,134.77,221.21,345.82,7.86;22,134.77,232.16,345.83,7.86;22,134.77,243.12,244.17,7.86;22,134.98,116.83,162.54,78.62"><head>Fig. 9 .</head><label>9</label><figDesc><div><p><s coords="22,134.77,210.22,345.83,7.89;22,134.77,221.21,265.70,7.86">Fig. 9. (Left) Generalizable reverse engineering approach: internal peculiarities of the black box are not exploited to build the comprehensible predictor.</s><s coords="22,403.35,221.21,77.24,7.86;22,134.77,232.16,345.83,7.86;22,134.77,243.12,244.17,7.86">(Right) Not Generalizable reverse engineering approach: the comprehensible predictor is the result of a procedure involving internal characteristics of the black box.</s></p></div></figDesc><graphic coords="22,134.98,116.83,162.54,78.62" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="26,404.73,174.20,75.86,8.74;26,134.77,186.15,345.83,8.74;26,134.77,198.11,345.83,8.74;26,134.77,210.06,345.83,8.74"><head/><label/><figDesc><div><p><s coords="26,404.73,174.20,75.86,8.74;26,134.77,186.15,345.83,8.74;26,134.77,198.11,228.84,8.74">present a method f to explain the behavior of a neural network b by transforming rule extraction (which is a search problem) into a learning problem.</s><s coords="26,366.96,198.11,113.63,8.74;26,134.77,210.06,345.83,8.74">The original training data D and a randomized extension of it are provided as input to the black box b.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="27,134.77,251.98,345.82,7.89;27,238.51,116.83,138.33,120.37"><head>Fig. 10 .</head><label>10</label><figDesc><div><p><s coords="27,134.77,251.98,33.19,7.89">Fig. 10.</s><s coords="27,170.23,252.00,310.36,7.86">From [65]: pedagogical (a) and decompositional (b) rule extraction techniques.</s></p></div></figDesc><graphic coords="27,238.51,116.83,138.33,120.37" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="30,134.77,178.58,345.83,7.89;30,134.77,189.57,345.82,7.86;30,134.77,200.53,257.51,7.86"><head>Fig. 11 .</head><label>11</label><figDesc><div><p><s coords="30,134.77,178.58,33.19,7.89">Fig. 11.</s><s coords="30,170.86,178.61,221.96,7.86">Saliency Masks for explanation of deep neural network.</s><s coords="30,395.73,178.61,84.87,7.86;30,134.77,189.57,135.40,7.86">(Left) From [108] the elements of the image highlighted.</s><s coords="30,272.58,189.57,208.00,7.86;30,134.77,200.53,257.51,7.86">(Right) From [25] the mask and the level of accuracy on the image considering and not considering the learned mask.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="32,134.77,299.96,345.83,7.89;32,134.77,310.94,345.83,7.86;32,134.77,321.90,345.83,7.86;32,134.77,332.86,330.72,7.86;32,134.98,167.03,162.53,118.15"><head>Fig. 12 .</head><label>12</label><figDesc><div><p><s coords="32,134.77,299.96,64.32,7.89">Fig. 12. (Left).</s><s coords="32,201.90,299.98,278.69,7.86;32,134.77,310.94,345.83,7.86;32,134.77,321.90,67.65,7.86">From [16] VEC curve and histogram for the pH input feature (x-axis) and the respective high quality wine probability outcome (left of y-axis) and frequency (right of y-axis).</s><s coords="32,206.26,321.90,31.38,7.86">(Right).</s><s coords="32,241.48,321.90,239.12,7.86;32,134.77,332.86,132.40,7.86">From [48] Age at enrollment shown as line plot (top) and partial dependence bar (middle).</s><s coords="32,270.23,332.86,195.25,7.86">Color denotes the predicted risk of the outcome.</s></p></div></figDesc><graphic coords="32,134.98,167.03,162.53,118.15" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="18,140.99,227.46,339.60,140.23"><head/><label/><figDesc><div><p><s coords="18,140.99,227.46,150.44,8.77">-Support Vector Machine (SVM).</s><s coords="18,294.07,227.49,186.52,8.74;18,151.70,239.45,328.89,8.74">Support Vector Machines utilize a subset of the training data, called support vectors, to represent the decision boundary.</s><s coords="18,151.70,251.40,328.89,8.74;18,151.70,263.36,113.57,8.74;18,140.99,275.23,5.73,8.77">A SVN is a classifier that searches for hyperplanes with the largest margin for the decision boundary.</s><s coords="18,151.70,275.26,128.54,8.74">-Deep Neural Network (DNN).</s><s coords="18,283.15,275.26,197.44,8.74;18,151.70,287.22,198.72,8.74">A DNN is a NN that can model complex nonlinear relationship with multiple hidden layers.</s><s coords="18,352.71,287.22,127.88,8.74;18,151.70,299.17,328.88,8.74">A DNN architecture is formed by a composition of models expressed as a layered combination of basic units.</s><s coords="18,151.70,311.13,328.89,8.74;18,151.70,323.09,57.67,8.74">In DNNs the data typically flows from the input to the output layer without looping back.</s><s coords="18,212.51,323.09,268.08,8.74">The most used DNN are Recurrent Neural Networks (RNNs).</s><s coords="18,151.70,335.04,328.89,8.74;18,151.70,347.00,232.94,8.74">A peculiar component of RNNs are Long Short-Term Memory (LSTM) nodes which are particularly effective for language modeling.</s><s coords="18,387.33,347.00,93.26,8.74;18,151.70,358.95,328.88,8.74">On the other hand, in image processing Convolutional Neural Networks (CNNs) are typically used.</s></p></div></figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="20,38.39,116.91,522.64,665.79"><head>Table 1 .</head><label>1</label><figDesc><div><p><s coords="20,201.64,116.93,249.23,7.86">Summary of methods for opening and explaining black boxes.</s></p></div></figDesc><table coords="20,38.39,148.95,522.64,633.75"><row><cell>N a m e</cell><cell>R e f .</cell><cell>A u t h o r s</cell><cell>Y e a r</cell><cell>P r o b l e m</cell><cell>E x p l a n a t o r</cell><cell>B l a c k B o x</cell><cell>D a t a T y p e</cell><cell>G e n e r a l</cell><cell>R a n d o m</cell><cell>E x a m p l e s</cell><cell>C o d e</cell><cell>D a t a s e t</cell></row><row><cell>Trepan</cell><cell>[20]</cell><cell>Craven et al.</cell><cell cols="2">1996 Model Expl.</cell><cell>DT</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[50]</cell><cell cols="3">Krishnan et al. 1999 Model Expl.</cell><cell>DT</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>DecText</cell><cell>[9]</cell><cell>Boz</cell><cell cols="2">2002 Model Expl.</cell><cell>DT</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>GPDT</cell><cell cols="4">[39] Johansson et al. 2009 Model Expl.</cell><cell>DT</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Tree Metrics [14]</cell><cell cols="3">Chipman et al. 1998 Model Expl.</cell><cell>DT</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>CCM</cell><cell cols="4">[23] Domingos et al. 1998 Model Expl.</cell><cell>DT</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[29]</cell><cell>Gibbons et al.</cell><cell cols="2">2013 Model Expl.</cell><cell>DT</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>STA</cell><cell>[114]</cell><cell>Zhou et al.</cell><cell cols="2">2016 Model Expl.</cell><cell>DT</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>CDT</cell><cell>[87]</cell><cell cols="3">Schetinin et al. 2007 Model Expl.</cell><cell>DT</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[32]</cell><cell>Hara et al.</cell><cell cols="2">2016 Model Expl.</cell><cell>DT</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>TSP</cell><cell>[94]</cell><cell>Tan et al.</cell><cell cols="2">2016 Model Expl.</cell><cell>DT</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Conj Rules [19]</cell><cell>Craven et al.</cell><cell cols="2">1994 Model Expl.</cell><cell>DR</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>G-REX</cell><cell cols="4">[37] Johansson et al. 2003 Model Expl.</cell><cell>DR</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>REFNE</cell><cell>[115]</cell><cell>Zhou et al.</cell><cell cols="2">2003 Model Expl.</cell><cell>DR</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>RxREN</cell><cell>[6]</cell><cell>Augasta et al.</cell><cell cols="2">2012 Model Expl.</cell><cell>DR</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>SVM+P</cell><cell>[70]</cell><cell>Nunez et al.</cell><cell cols="2">2002 Model Expl.</cell><cell>DR</cell><cell>SVM</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[28]</cell><cell>Fung et al.</cell><cell cols="2">2005 Model Expl.</cell><cell>DR</cell><cell>SVM</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>inTrees</cell><cell>[22]</cell><cell>Deng</cell><cell cols="2">2014 Model Expl.</cell><cell>DR</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[61]</cell><cell>Lou et al.</cell><cell cols="2">2013 Model Expl.</cell><cell>FI</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">GoldenEye [33]</cell><cell cols="3">Henelius et al. 2014 Model Expl.</cell><cell>FI</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>PALM</cell><cell>[51]</cell><cell cols="3">Krishnan et al. 2017 Model Expl.</cell><cell>DT</cell><cell>AGN</cell><cell>ANY</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[97]</cell><cell>Tolomei et al.</cell><cell cols="2">2017 Model Expl.</cell><cell>FI</cell><cell>TE</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[108]</cell><cell>Xu et al.</cell><cell cols="2">2015 Outcome Expl.</cell><cell>SM</cell><cell>DNN</cell><cell>IMG</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[25]</cell><cell>Fong et al.</cell><cell cols="2">2017 Outcome Expl.</cell><cell>SM</cell><cell>DNN</cell><cell>IMG</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>CAM</cell><cell>[113]</cell><cell>Zhou et al.</cell><cell cols="2">2016 Outcome Expl.</cell><cell>SM</cell><cell>DNN</cell><cell>IMG</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Grad-CAM [89]</cell><cell cols="3">Selvaraju et al. 2016 Outcome Expl.</cell><cell>SM</cell><cell>DNN</cell><cell>IMG</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[56]</cell><cell>Lei et al.</cell><cell cols="2">2016 Outcome Expl.</cell><cell>SM</cell><cell>DNN</cell><cell>TXT</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>LIME</cell><cell>[83]</cell><cell>Ribeiro et al.</cell><cell cols="2">2016 Outcome Expl.</cell><cell>FI</cell><cell>AGN</cell><cell>ANY</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>MES</cell><cell>[98]</cell><cell>Turner et al.</cell><cell cols="2">2016 Outcome Expl.</cell><cell>DR</cell><cell>AGN</cell><cell>ANY</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>NID</cell><cell>[71]</cell><cell>Olden et al.</cell><cell>2002</cell><cell>Inspection</cell><cell>SA</cell><cell>NN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>GDP</cell><cell>[7]</cell><cell>Baehrens</cell><cell>2010</cell><cell>Inspection</cell><cell>SA</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>IG</cell><cell>[92]</cell><cell>Sundararajan</cell><cell>2017</cell><cell>Inspection</cell><cell>SA</cell><cell>DNN</cell><cell>ANY</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>VEC</cell><cell>[16]</cell><cell>Cortez et al.</cell><cell>2011</cell><cell>Inspection</cell><cell>SA</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>VIN</cell><cell>[35]</cell><cell>Hooker</cell><cell>2004</cell><cell>Inspection</cell><cell>PDP</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>ICE</cell><cell cols="3">[30] Goldstein et al. 2015</cell><cell>Inspection</cell><cell>PDP</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">Prospector [48]</cell><cell>Krause et al.</cell><cell>2016</cell><cell>Inspection</cell><cell>PDP</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>Auditing</cell><cell>[2]</cell><cell>Adler et al.</cell><cell>2016</cell><cell>Inspection</cell><cell>PDP</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>OPIA</cell><cell>[1]</cell><cell cols="2">Adebayo et al. 2016</cell><cell>Inspection</cell><cell>PDP</cell><cell>AGN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[110]</cell><cell>Yosinski et al.</cell><cell>2015</cell><cell>Inspection</cell><cell>NA</cell><cell>DNN</cell><cell>IMG</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>TreeView</cell><cell cols="3">[96] Thiagarajan et al. 2016</cell><cell>Inspection</cell><cell>DT</cell><cell>DNN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>IP</cell><cell>[90]</cell><cell>Shwartz et al.</cell><cell>2017</cell><cell>Inspection</cell><cell>NA</cell><cell>DNN</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell>[81]</cell><cell>Radford</cell><cell>2017</cell><cell>Inspection</cell><cell>NA</cell><cell>DNN</cell><cell>TXT</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>CPAR</cell><cell>[109]</cell><cell>Yin et al.</cell><cell cols="2">2003 Transp. Design</cell><cell>DR</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>FRL</cell><cell>[102]</cell><cell>Wang et al.</cell><cell cols="2">2015 Transp. Design</cell><cell>DR</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>BRL</cell><cell>[57]</cell><cell>Letham et al.</cell><cell cols="2">2015 Transp. Design</cell><cell>DR</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>TLBR</cell><cell>[91]</cell><cell>Su et al.</cell><cell cols="2">2015 Transp. Design</cell><cell>DR</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>IDS</cell><cell cols="4">[53] Lakkaraju et al. 2016 Transp. Design</cell><cell>DR</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>Rule Set</cell><cell>[104]</cell><cell>Wang et al.</cell><cell cols="2">2016 Transp. Design</cell><cell>DR</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>1Rule</cell><cell cols="4">[64] Malioutov et al. 2017 Transp. Design</cell><cell>DR</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>PS</cell><cell>[8]</cell><cell>Bien et al.</cell><cell cols="2">2011 Transp. Design</cell><cell>PS</cell><cell>-</cell><cell>ANY</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>BCM</cell><cell>[44]</cell><cell>Kim et al.</cell><cell cols="2">2014 Transp. Design</cell><cell>PS</cell><cell>-</cell><cell>ANY</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell cols="4">[63] Mahendran et al. 2015 Transp. Design</cell><cell>PS</cell><cell>-</cell><cell>IMG</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>-</cell><cell cols="4">[47] Kononenko et al. 2010 Transp. Design</cell><cell>FI</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell cols="2">OT-SpAMs [103]</cell><cell>Wang et al.</cell><cell cols="2">2015 Transp. Design</cell><cell>DT</cell><cell>-</cell><cell>TAB</cell><cell/><cell/><cell/><cell/><cell/></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="21,134.77,116.91,365.06,215.36"><head>Table 2 .</head><label>2</label><figDesc><div><p><s coords="21,174.93,116.93,77.84,7.86">Legend of Table 1.</s><s coords="21,255.82,116.93,224.77,7.86;21,134.77,127.89,107.85,7.86">In the following are described the features reported and the abbreviations adopted.</s><s coords="21,138.87,236.73,341.62,7.86;21,143.89,247.69,355.93,7.86;21,188.01,258.65,311.81,7.86;21,188.01,269.61,15.87,7.86;21,143.09,280.57,356.74,7.86;21,188.01,291.53,154.54,7.86;21,140.72,302.49,287.88,7.86;21,149.75,313.45,19.75,7.86;21,188.01,313.45,157.11,7.86;21,144.31,324.41,292.59,7.86">Data Type TAB -TABular, IMG -IMaGe, TXT -TeXT, ANY -ANY type of data General Indicates if an explanatory approach can be generalized for every black box, i.e., it does not consider peculiarities of the black box to produce the explanation Random Indicates if any kind of random perturbation or permutation of the original dataset is required for the explanation Examples Indicates if example of explanations are shown in the paper Code Indicates if the source code is available Dataset Indicates if the datasets used in the experiments are available</s></p></div></figDesc><table coords="21,137.60,148.64,362.22,85.00"><row><cell>Feature Description</cell></row><row><cell>Problem Model Explanation, Outcome Explanation, Black Box Inspection, Transparent</cell></row><row><cell>Design</cell></row><row><cell>Explanator DT -Decision Tree, DR -Decision Rules, FI -Features Importance, SM -</cell></row><row><cell>Saliency Masks, SA -Sensitivity Analysis, PDP -Partial Dependence Plot,</cell></row><row><cell>NA -Neurons Activation, PS -Prototype Selection</cell></row><row><cell>Black Box NN -Neural Network, TE -Tree Ensemble, SVM -Support Vector Machines,</cell></row><row><cell>DNN -Deep Neural Network, AGN -AGNostic black box</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="38,134.77,565.96,345.83,39.67"><head>Table 3 .</head><label>3</label><figDesc><div><p><s coords="38,176.12,565.99,304.47,7.86;38,134.77,576.95,86.30,7.86">Summary of methods for opening and explaining black boxes with respect to the problem faced.</s></p></div></figDesc><table coords="38,164.29,597.74,39.48,7.89"><row><cell>Problem</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="4,144.73,647.48,407.82,7.47"><p><s coords="4,144.73,647.48,407.82,7.47">http://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,144.73,658.44,290.13,7.47"><p><s coords="4,144.73,658.44,290.13,7.47">http://www.techinsider.io/how-algorithms-can-be-racist-2016-4</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="5,144.73,658.44,151.13,7.47"><p><s coords="5,144.73,658.44,151.13,7.47">https://www.merriam-webster.com/</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head coords="39,134.77,144.68,106.27,10.52">Acknowledgement</head><p>This work is partially supported by the <rs type="funder">European Community</rs>'s <rs type="programName">H2020 Program</rs> under the funding scheme "INFRAIA-1-2014-2015: <rs type="programName">Research Infrastructures</rs>", grant agreement <rs type="grantNumber">654024</rs>, SoBigData, http://www.sobigdata.eu.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KtQ9Zrd">
					<orgName type="program" subtype="full">H2020 Program</orgName>
				</org>
				<org type="funding" xml:id="_MgZbnzP">
					<idno type="grant-number">654024</idno>
					<orgName type="program" subtype="full">Research Infrastructures</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="39,134.77,118.59,172.13,10.52">A Supplementary Materials</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="39,147.56,249.93,333.03,7.86;39,156.13,260.89,264.23,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main">Iterative orthogonal feature projection for diagnosing bias in black-box models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04967</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">J. Adebayo and L. Kagal. Iterative orthogonal feature projection for diagnosing bias in black-box models. arXiv preprint arXiv:1611.04967, 2016.</note>
</biblStruct>

<biblStruct coords="39,147.56,271.88,333.03,7.86;39,156.13,282.84,324.47,7.86;39,156.13,293.80,324.46,7.86;39,156.13,304.76,48.76,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main">Auditing black-box models for indirect influence</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rybeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2016 IEEE 16th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10"/>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Adler, C. Falk, S. A. Friedler, G. Rybeck, C. Scheidegger, B. Smith, and S. Venkatasubramanian. Auditing black-box models for indirect influence. In Data Mining (ICDM), 2016 IEEE 16th International Conference on, pages 1-10. IEEE, 2016.</note>
</biblStruct>

<biblStruct coords="39,147.56,315.75,333.03,7.86;39,156.13,326.71,324.46,7.86;39,156.13,337.67,20.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th int. conf. very large data bases, VLDB</title>
		<meeting>20th int. conf. very large data bases, VLDB</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1215</biblScope>
			<biblScope unit="page" from="487" to="499"/>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Agrawal, R. Srikant, et al. Fast algorithms for mining association rules. In Proc. 20th int. conf. very large data bases, VLDB, volume 1215, pages 487-499, 1994.</note>
</biblStruct>

<biblStruct coords="39,147.56,348.67,333.03,7.86;39,156.13,359.63,248.37,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive review on privacy preserving data mining</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">A A S</forename><surname>Aldeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Salleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Razzaque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SpringerPlus</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">694</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. A. A. S. Aldeen, M. Salleh, and M. A. Razzaque. A comprehensive review on privacy preserving data mining. SpringerPlus, 4(1):694, 2015.</note>
</biblStruct>

<biblStruct coords="39,147.56,370.62,333.03,7.86;39,156.13,381.58,324.46,7.86;39,156.13,392.54,77.82,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main">Survey and critique of techniques for extracting rules from trained artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Diederich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Tickle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-based systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="373" to="389"/>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Andrews, J. Diederich, and A. B. Tickle. Survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-based systems, 8(6):373-389, 1995.</note>
</biblStruct>

<biblStruct coords="39,147.56,403.54,333.02,7.86;39,156.13,414.50,324.46,7.86;39,156.13,425.45,82.42,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main">Reverse engineering the neural networks for rule extraction in classification problems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Augasta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kathirvalavakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural processing letters</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="150"/>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. G. Augasta and T. Kathirvalavakumar. Reverse engineering the neural net- works for rule extraction in classification problems. Neural processing letters, 35(2):131-150, 2012.</note>
</biblStruct>

<biblStruct coords="39,147.56,436.45,333.03,7.86;39,156.13,445.14,324.46,10.13;39,156.13,458.37,181.05,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-R</forename><forename type="middle">M</forename><surname>Ãžller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831"/>
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and K.-R. M Ãžller. How to explain individual classification decisions. Journal of Machine Learning Research, 11(Jun):1803-1831, 2010.</note>
</biblStruct>

<biblStruct coords="39,147.56,469.36,333.02,7.86;39,156.13,480.32,229.22,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main">Prototype selection for interpretable classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="2403" to="2424"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Bien and R. Tibshirani. Prototype selection for interpretable classification. The Annals of Applied Statistics, pages 2403-2424, 2011.</note>
</biblStruct>

<biblStruct coords="39,134.77,511.02,345.83,7.89;39,134.77,522.00,107.80,7.86;39,178.84,542.80,51.42,7.89;39,280.00,542.75,49.84,7.89;39,165.15,559.71,78.47,7.86;39,280.00,554.18,156.66,7.86;39,280.00,565.14,118.78,7.86;39,162.50,581.63,83.78,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main">Summary of methods for opening and explaining black boxes with respect to the explanator adopted</title>
		<imprint/>
	</monogr>
	<note>Explanator References Decition Tree (DT) [20], [50], [9], [39], [14], [23], [29], [114], [87], [32], [94], [51], [96], [103] Decision Rules (DR</note>
	<note type="raw_reference">Table 4. Summary of methods for opening and explaining black boxes with respect to the explanator adopted. Explanator References Decition Tree (DT) [20], [50], [9], [39], [14], [23], [29], [114], [87], [32], [94], [51], [96], [103] Decision Rules (DR)</note>
</biblStruct>

<biblStruct coords="40,147.56,120.67,333.03,7.86;40,156.13,131.63,324.47,7.86;40,156.13,142.59,165.26,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting decision trees from trained neural networks</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Boz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="456" to="461"/>
		</imprint>
	</monogr>
	<note type="raw_reference">O. Boz. Extracting decision trees from trained neural networks. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 456-461. ACM, 2002.</note>
</biblStruct>

<biblStruct coords="40,147.22,154.15,333.37,7.86;40,156.13,165.11,137.08,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classification and regression trees. CRC press, 1984.</note>
</biblStruct>

<biblStruct coords="40,147.22,176.68,333.36,7.86;40,156.13,187.64,324.46,7.86;40,156.13,198.59,97.10,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantics derived automatically from language corpora necessarily contain human biases</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Caliskan-Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07187</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Caliskan-Islam, J. J. Bryson, and A. Narayanan. Semantics derived automat- ically from language corpora necessarily contain human biases. arXiv preprint arXiv:1608.07187, 2016.</note>
</biblStruct>

<biblStruct coords="40,147.22,210.16,333.37,7.86;40,156.13,221.12,244.31,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main">The credit card market and regulation: In need of repair</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Renuart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NC Banking Inst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">23</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Carter, E. Renuart, M. Saunders, and C. C. Wu. The credit card market and regulation: In need of repair. NC Banking Inst., 10:23, 2006.</note>
</biblStruct>

<biblStruct coords="40,147.22,232.68,333.37,7.86;40,156.13,243.64,324.47,7.86;40,156.13,254.60,324.47,7.86;40,156.13,265.56,283.56,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1721" to="1730"/>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmis- sion. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1721-1730. ACM, 2015.</note>
</biblStruct>

<biblStruct coords="40,147.22,277.13,333.37,7.86;40,156.13,288.08,214.75,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main">Making sense of a forest of trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mcculloh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Science and Statistics</title>
		<imprint>
			<biblScope unit="page" from="84" to="92"/>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Chipman, E. George, and R. McCulloh. Making sense of a forest of trees. Computing Science and Statistics, pages 84-92, 1998.</note>
</biblStruct>

<biblStruct coords="40,147.22,299.65,333.37,7.86;40,156.13,310.61,324.47,7.86;40,156.13,321.57,174.41,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main">Regulating algorithms regulation? first ethico-legal principles, problems, and opportunities of algorithms</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Comandè</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transparent Data Mining for Big and Small Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="169" to="206"/>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Comandè. Regulating algorithms regulation? first ethico-legal principles, prob- lems, and opportunities of algorithms. In Transparent Data Mining for Big and Small Data, pages 169-206. Springer, 2017.</note>
</biblStruct>

<biblStruct coords="40,147.22,333.13,333.37,7.86;40,156.13,344.09,324.46,7.86;40,156.13,355.05,223.44,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main">Opening black box data mining models using sensitivity analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Embrechts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Data Mining (CIDM), 2011 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="341" to="348"/>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Cortez and M. J. Embrechts. Opening black box data mining models using sensitivity analysis. In Computational Intelligence and Data Mining (CIDM), 2011 IEEE Symposium on, pages 341-348. IEEE, 2011.</note>
</biblStruct>

<biblStruct coords="40,147.22,366.62,333.37,7.86;40,156.13,377.57,324.46,7.86;40,156.13,388.53,20.99,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main">Using sensitivity analysis and visualization techniques to open black box data mining models</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Embrechts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="page" from="1" to="17"/>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Cortez and M. J. Embrechts. Using sensitivity analysis and visualization tech- niques to open black box data mining models. Information Sciences, 225:1-17, 2013.</note>
</biblStruct>

<biblStruct coords="40,147.22,400.10,333.37,7.86;40,156.13,411.06,324.46,7.86;40,156.13,422.02,114.26,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main">Using data mining for wine quality assessment</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cerdeira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Reis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discovery Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5808</biblScope>
			<biblScope unit="page" from="66" to="79"/>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Cortez, J. Teixeira, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. Using data mining for wine quality assessment. In Discovery Science, volume 5808, pages 66-79. Springer, 2009.</note>
</biblStruct>

<biblStruct coords="40,147.22,433.58,333.37,7.86;40,156.13,444.54,218.37,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main">Using sampling and queries to extract rules from trained neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="37" to="45"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Craven and J. W. Shavlik. Using sampling and queries to extract rules from trained neural networks. In ICML, pages 37-45, 1994.</note>
</biblStruct>

<biblStruct coords="40,147.22,456.10,333.37,7.86;40,156.13,467.06,324.46,7.86;40,156.13,478.02,49.66,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting tree-structured representations of trained networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="24" to="30"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Craven and J. W. Shavlik. Extracting tree-structured representations of trained networks. In Advances in neural information processing systems, pages 24-30, 1996.</note>
</biblStruct>

<biblStruct coords="40,147.22,489.59,333.37,7.86;40,156.13,500.55,324.46,7.86;40,156.13,511.51,248.50,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="598" to="617"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Datta, S. Sen, and Y. Zick. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In Security and Privacy (SP), 2016 IEEE Symposium on, pages 598-617. IEEE, 2016.</note>
</biblStruct>

<biblStruct coords="40,147.22,523.07,49.92,7.86;40,216.29,523.07,182.76,7.86;40,418.18,523.07,62.41,7.86;40,156.13,534.03,92.38,7.86" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5456</idno>
		<title level="m">Interpreting tree ensembles with intrees</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">H. Deng. Interpreting tree ensembles with intrees. arXiv preprint arXiv:1408.5456, 2014.</note>
</biblStruct>

<biblStruct coords="41,147.22,120.67,333.37,7.86;41,156.13,131.63,85.50,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge discovery via multiple models</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="187" to="202"/>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Domingos. Knowledge discovery via multiple models. Intelligent Data Analysis, 2(1-4):187-202, 1998.</note>
</biblStruct>

<biblStruct coords="41,147.22,142.60,333.37,7.86;41,156.13,153.55,59.91,7.86" xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards a rigorous science of interpretable machine learning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable machine learning. 2017.</note>
</biblStruct>

<biblStruct coords="41,147.22,164.52,333.37,7.86;41,156.13,175.48,215.79,7.86" xml:id="b25">
	<monogr>
		<title level="m" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03296</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">R. Fong and A. Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. arXiv preprint arXiv:1704.03296, 2017.</note>
</biblStruct>

<biblStruct coords="41,147.22,186.45,333.37,7.86;41,156.13,197.41,62.45,7.86" xml:id="b26">
	<monogr>
		<title level="m" type="main">Generating accurate rule sets without global optimization</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. Frank and I. H. Witten. Generating accurate rule sets without global opti- mization. 1998.</note>
</biblStruct>

<biblStruct coords="41,147.22,208.37,333.37,7.86;41,156.13,219.33,205.63,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main">Comprehensible classification models: a position paper</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. A. Freitas. Comprehensible classification models: a position paper. ACM SIGKDD explorations newsletter, 15(1):1-10, 2014.</note>
</biblStruct>

<biblStruct coords="41,147.22,230.30,333.37,7.86;41,156.13,241.26,324.46,7.86;41,156.13,252.22,266.51,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main">Rule extraction from linear support vector machines</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sandilya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="32" to="40"/>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Fung, S. Sandilya, and R. B. Rao. Rule extraction from linear support vector machines. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 32-40. ACM, 2005.</note>
</biblStruct>

<biblStruct coords="41,147.22,263.18,333.37,7.86;41,156.13,274.14,324.46,7.86;41,156.13,285.10,324.46,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main">The cad-mdd: a computerized adaptive diagnostic screening tool for depression</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Finkelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Pilkonis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Kupfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of clinical psychiatry</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">669</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. D. Gibbons, G. Hooker, M. D. Finkelman, D. J. Weiss, P. A. Pilkonis, E. Frank, T. Moore, and D. J. Kupfer. The cad-mdd: a computerized adaptive diagnostic screening tool for depression. The Journal of clinical psychiatry, 74(7):669, 2013.</note>
</biblStruct>

<biblStruct coords="41,147.22,296.07,333.37,7.86;41,156.13,307.03,324.46,7.86;41,156.13,317.98,281.80,7.86" xml:id="b30">
	<analytic>
		<title level="a" type="main">Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kapelner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bleich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pitkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="65"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin. Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics, 24(1):44-65, 2015.</note>
</biblStruct>

<biblStruct coords="41,147.22,328.95,333.37,7.86;41,156.13,339.91,324.46,7.86;41,156.13,350.87,324.46,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main">Eu regulations on algorithmic decision-making and a right to explanation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606"/>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on human interpretability in machine learning (WHI 2016)</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">08813</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Goodman and S. Flaxman. Eu regulations on algorithmic decision-making and a right to explanation. In ICML workshop on human interpretability in machine learning (WHI 2016), New York, NY. http://arxiv. org/abs/1606.08813 v1, 2016.</note>
</biblStruct>

<biblStruct coords="41,147.22,361.84,333.37,7.86;41,156.13,372.79,97.10,7.86" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05390</idno>
		<title level="m">Making tree ensembles interpretable</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">S. Hara and K. Hayashi. Making tree ensembles interpretable. arXiv preprint arXiv:1606.05390, 2016.</note>
</biblStruct>

<biblStruct coords="41,147.22,383.76,333.37,7.86;41,156.13,394.72,324.46,7.86;41,156.13,405.68,141.53,7.86" xml:id="b33">
	<analytic>
		<title level="a" type="main">A peek into the black box: exploring classifiers by randomization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Henelius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Puolamäki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Boström</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Asker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Papapetrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="1503" to="1529"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Henelius, K. Puolamäki, H. Boström, L. Asker, and P. Papapetrou. A peek into the black box: exploring classifiers by randomization. Data mining and knowledge discovery, 28(5-6):1503-1529, 2014.</note>
</biblStruct>

<biblStruct coords="41,147.22,416.64,333.37,7.86;41,156.13,427.60,173.91,7.86" xml:id="b34">
	<analytic>
		<title level="a" type="main">Prediction and explanation in social systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6324</biblScope>
			<biblScope unit="page" from="486" to="488"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. M. Hofman, A. Sharma, and D. J. Watts. Prediction and explanation in social systems. Science, 355(6324):486-488, 2017.</note>
</biblStruct>

<biblStruct coords="41,147.22,438.57,333.37,7.86;41,156.13,449.53,324.47,7.86;41,156.13,460.49,165.26,7.86" xml:id="b35">
	<analytic>
		<title level="a" type="main">Discovering additive structure in black box functions</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="575" to="580"/>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Hooker. Discovering additive structure in black box functions. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 575-580. ACM, 2004.</note>
</biblStruct>

<biblStruct coords="41,147.22,471.45,333.36,7.86;41,156.13,482.41,324.46,7.86;41,156.13,493.37,226.17,7.86" xml:id="b36">
	<analytic>
		<title level="a" type="main">An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huysmans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dejaeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="154"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, and B. Baesens. An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models. Decision Support Systems, 51(1):141-154, 2011.</note>
</biblStruct>

<biblStruct coords="41,147.22,504.34,333.37,7.86;41,156.13,515.30,324.47,7.86;41,156.13,526.26,185.99,7.86" xml:id="b37">
	<analytic>
		<title level="a" type="main">Rule extraction from trained neural networks using genetic programming</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="13" to="16"/>
		</imprint>
	</monogr>
	<note type="raw_reference">U. Johansson, R. König, and L. Niklasson. Rule extraction from trained neu- ral networks using genetic programming. In 13th International Conference on Artificial Neural Networks, pages 13-16, 2003.</note>
</biblStruct>

<biblStruct coords="41,147.22,537.22,333.37,7.86;41,156.13,548.18,324.46,7.86;41,156.13,559.14,58.87,7.86" xml:id="b38">
	<analytic>
		<title level="a" type="main">The truth is in there-rule extraction from opaque models using genetic programming</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="658" to="663"/>
		</imprint>
	</monogr>
	<note type="raw_reference">U. Johansson, R. König, and L. Niklasson. The truth is in there-rule extraction from opaque models using genetic programming. In FLAIRS Conference, pages 658-663, 2004.</note>
</biblStruct>

<biblStruct coords="41,147.22,570.11,333.37,7.86;41,156.13,581.07,324.46,7.86;41,156.13,592.03,127.30,7.86" xml:id="b39">
	<analytic>
		<title level="a" type="main">Evolving decision trees using oracle guides</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="238" to="244"/>
		</imprint>
	</monogr>
	<note>CIDM'09. IEEE Symposium on</note>
	<note type="raw_reference">U. Johansson and L. Niklasson. Evolving decision trees using oracle guides. In Computational Intelligence and Data Mining, 2009. CIDM'09. IEEE Symposium on, pages 238-244. IEEE, 2009.</note>
</biblStruct>

<biblStruct coords="41,147.22,602.99,333.37,7.86;41,156.13,613.95,324.47,7.86;41,156.13,624.91,206.55,7.86" xml:id="b40">
	<analytic>
		<title level="a" type="main">Accuracy vs. comprehensibility in data mining models</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>König</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international conference on information fusion</title>
		<meeting>the seventh international conference on information fusion</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="295" to="300"/>
		</imprint>
	</monogr>
	<note type="raw_reference">U. Johansson, L. Niklasson, and R. König. Accuracy vs. comprehensibility in data mining models. In Proceedings of the seventh international conference on information fusion, volume 1, pages 295-300, 2004.</note>
</biblStruct>

<biblStruct coords="41,147.22,635.88,333.37,7.86;41,156.13,646.84,324.46,7.86;41,156.13,657.79,58.87,7.86" xml:id="b41">
	<analytic>
		<title level="a" type="main">Image reconstruction from bag-of-visual-words</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="955" to="962"/>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Kato and T. Harada. Image reconstruction from bag-of-visual-words. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pages 955-962, 2014.</note>
</biblStruct>

<biblStruct coords="42,147.22,120.67,333.37,7.86;42,156.13,131.63,235.09,7.86" xml:id="b42">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">ibcm: Interactive bayesian case model empowering humans via intuitive interaction</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Kim, E. Glassman, B. Johnson, and J. Shah. ibcm: Interactive bayesian case model empowering humans via intuitive interaction. 2015.</note>
</biblStruct>

<biblStruct coords="42,147.22,142.60,333.37,7.86;42,156.13,153.55,324.46,7.86;42,156.13,164.51,130.86,7.86" xml:id="b43">
	<analytic>
		<title level="a" type="main">Examples are not enough, learn to criticize! criticism for interpretability</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">O</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2280" to="2288"/>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Kim, O. O. Koyejo, and R. Khanna. Examples are not enough, learn to criti- cize! criticism for interpretability. In Advances In Neural Information Processing Systems, pages 2280-2288, 2016.</note>
</biblStruct>

<biblStruct coords="42,147.22,175.48,333.37,7.86;42,156.13,186.44,324.46,7.86;42,156.13,197.40,256.99,7.86" xml:id="b44">
	<analytic>
		<title level="a" type="main">The bayesian case model: A generative approach for case-based reasoning and prototype classification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1952" to="1960"/>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Kim, C. Rudin, and J. A. Shah. The bayesian case model: A generative approach for case-based reasoning and prototype classification. In Advances in Neural Information Processing Systems, pages 1952-1960, 2014.</note>
</biblStruct>

<biblStruct coords="42,147.22,208.36,333.37,7.86;42,156.13,219.32,324.47,7.86;42,156.13,230.28,175.80,7.86" xml:id="b45">
	<analytic>
		<title level="a" type="main">Mind the gap: A generative approach to interpretable feature selection and extraction</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2260" to="2268"/>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Kim, J. A. Shah, and F. Doshi-Velez. Mind the gap: A generative approach to interpretable feature selection and extraction. In Advances in Neural Information Processing Systems, pages 2260-2268, 2015.</note>
</biblStruct>

<biblStruct coords="42,147.22,241.25,333.37,7.86;42,156.13,252.21,201.45,7.86" xml:id="b46">
	<monogr>
		<title level="m" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04730</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. arXiv preprint arXiv:1703.04730, 2017.</note>
</biblStruct>

<biblStruct coords="42,147.22,263.17,333.37,7.86;42,156.13,274.13,295.45,7.86" xml:id="b47">
	<analytic>
		<title level="a" type="main">An efficient explanation of individual classifications using game theory</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="18"/>
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Kononenko et al. An efficient explanation of individual classifications using game theory. Journal of Machine Learning Research, 11(Jan):1-18, 2010.</note>
</biblStruct>

<biblStruct coords="42,147.22,285.10,333.36,7.86;42,156.13,296.06,324.46,7.86;42,156.13,307.02,295.17,7.86" xml:id="b48">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5686" to="5697"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Krause, A. Perer, and K. Ng. Interacting with predictions: Visual inspection of black-box machine learning models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 5686-5697. ACM, 2016.</note>
</biblStruct>

<biblStruct coords="42,147.22,317.98,333.36,7.86;42,156.13,328.94,324.46,7.86;42,156.13,339.90,239.73,7.86" xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning from explanations using sentiment and advice in rl</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Krening</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Feigh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="55"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell, M. Riedl, and A. Thomaz. Learning from explanations using sentiment and advice in rl. IEEE Transactions on Cognitive and Developmental Systems, 9(1):44-55, 2017.</note>
</biblStruct>

<biblStruct coords="42,147.22,350.87,333.37,7.86;42,156.13,361.83,236.92,7.86" xml:id="b50">
	<analytic>
		<title level="a" type="main">Extracting decision trees from trained neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Krishnan, G. Sivakumar, and P. Bhattacharya. Extracting decision trees from trained neural networks. Pattern recognition, 32(12), 1999.</note>
</biblStruct>

<biblStruct coords="42,147.22,372.79,333.37,7.86;42,156.13,383.75,324.46,7.86;42,156.13,394.71,80.12,7.86" xml:id="b51">
	<analytic>
		<title level="a" type="main">Palm: Machine learning explanations for iterative debugging</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics</title>
		<meeting>the 2nd Workshop on Human-In-the-Loop Data Analytics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Krishnan and E. Wu. Palm: Machine learning explanations for iterative debug- ging. In Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics, page 4. ACM, 2017.</note>
</biblStruct>

<biblStruct coords="42,147.22,405.68,333.37,7.86;42,156.13,416.64,187.10,7.86" xml:id="b52">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<title level="m">Adversarial examples in the physical world</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.</note>
</biblStruct>

<biblStruct coords="42,147.22,427.60,333.36,7.86;42,156.13,438.56,324.46,7.86;42,156.13,449.52,324.46,7.86;42,156.13,460.48,120.62,7.86" xml:id="b53">
	<analytic>
		<title level="a" type="main">Interpretable decision sets: A joint framework for description and prediction</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1675" to="1684"/>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Lakkaraju, S. H. Bach, and J. Leskovec. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1675-1684. ACM, 2016.</note>
</biblStruct>

<biblStruct coords="42,147.22,471.45,333.37,7.86;42,156.13,482.41,310.07,7.86" xml:id="b54">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01154</idno>
		<title level="m">Interpretable &amp; explorable approximations of black box models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">H. Lakkaraju, E. Kamar, R. Caruana, and J. Leskovec. Interpretable &amp; explorable approximations of black box models. arXiv preprint arXiv:1707.01154, 2017.</note>
</biblStruct>

<biblStruct coords="42,147.22,493.37,333.37,7.86;42,156.13,504.33,324.46,7.86;42,156.13,515.29,324.46,7.86;42,156.13,526.25,287.52,7.86" xml:id="b55">
	<analytic>
		<title level="a" type="main">The selective labels problem: Evaluating algorithmic predictions in the presence of unobservables</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="275" to="284"/>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Lakkaraju, J. Kleinberg, J. Leskovec, J. Ludwig, and S. Mullainathan. The selective labels problem: Evaluating algorithmic predictions in the presence of un- observables. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 275-284. ACM, 2017.</note>
</biblStruct>

<biblStruct coords="42,147.22,537.22,333.37,7.86;42,156.13,548.17,132.38,7.86" xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04155</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Rationalizing neural predictions. arXiv preprint</note>
	<note type="raw_reference">T. Lei, R. Barzilay, and T. Jaakkola. Rationalizing neural predictions. arXiv preprint arXiv:1606.04155, 2016.</note>
</biblStruct>

<biblStruct coords="42,147.22,559.14,333.37,7.86;42,156.13,570.10,324.47,7.86;42,156.13,581.06,223.02,7.86" xml:id="b57">
	<analytic>
		<title level="a" type="main">Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1350" to="1371"/>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Letham, C. Rudin, T. H. McCormick, D. Madigan, et al. Interpretable classi- fiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3):1350-1371, 2015.</note>
</biblStruct>

<biblStruct coords="42,147.22,592.03,333.37,7.86;42,156.13,602.98,201.42,7.86" xml:id="b58">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08006</idno>
		<title level="m">Deep text classification can be fooled</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">B. Liang, H. Li, M. Su, P. Bian, X. Li, and W. Shi. Deep text classification can be fooled. arXiv preprint arXiv:1704.08006, 2017.</note>
</biblStruct>

<biblStruct coords="42,147.22,613.95,70.15,7.86;42,233.96,613.95,168.42,7.86;42,418.97,613.95,61.63,7.86;42,156.13,624.91,97.10,7.86" xml:id="b59">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03490</idno>
		<title level="m">The mythos of model interpretability</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Z. C. Lipton. The mythos of model interpretability. arXiv preprint arXiv:1606.03490, 2016.</note>
</biblStruct>

<biblStruct coords="42,147.22,635.88,333.37,7.86;42,156.13,646.84,324.46,7.86;42,156.13,657.79,269.13,7.86" xml:id="b60">
	<analytic>
		<title level="a" type="main">Intelligible models for classification and regression</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="150" to="158"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Lou, R. Caruana, and J. Gehrke. Intelligible models for classification and regression. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150-158. ACM, 2012.</note>
</biblStruct>

<biblStruct coords="43,147.22,120.67,333.37,7.86;43,156.13,131.63,324.46,7.86;43,156.13,142.59,324.46,7.86" xml:id="b61">
	<analytic>
		<title level="a" type="main">Accurate intelligible models with pairwise interactions</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="623" to="631"/>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Lou, R. Caruana, J. Gehrke, and G. Hooker. Accurate intelligible models with pairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 623-631. ACM, 2013.</note>
</biblStruct>

<biblStruct coords="43,147.22,153.55,333.36,7.86;43,156.13,164.51,176.27,7.86" xml:id="b62">
	<analytic>
		<title level="a" type="main">A blot on the profession</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Macpherson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British medical journal</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="issue">6623</biblScope>
			<biblScope unit="page">657</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>Clinical research</note>
	<note type="raw_reference">S. Lowry and G. Macpherson. A blot on the profession. British medical journal (Clinical research ed.), 296(6623):657, 1988.</note>
</biblStruct>

<biblStruct coords="43,147.22,175.48,333.37,7.86;43,156.13,186.44,324.46,7.86;43,156.13,197.40,178.51,7.86" xml:id="b63">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5188" to="5196"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5188-5196, 2015.</note>
</biblStruct>

<biblStruct coords="43,147.22,208.36,333.37,7.86;43,156.13,219.32,324.46,7.86;43,156.13,230.28,219.21,7.86" xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning interpretable classification rules with boolean compressed sensing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Malioutov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Emad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transparent Data Mining for Big and Small Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="95" to="121"/>
		</imprint>
	</monogr>
	<note type="raw_reference">D. M. Malioutov, K. R. Varshney, A. Emad, and S. Dash. Learning interpretable classification rules with boolean compressed sensing. In Transparent Data Mining for Big and Small Data, pages 95-121. Springer, 2017.</note>
</biblStruct>

<biblStruct coords="43,147.22,241.25,333.37,7.86;43,156.13,252.21,324.47,7.86;43,156.13,263.17,224.20,7.86" xml:id="b65">
	<analytic>
		<title level="a" type="main">Comprehensible credit scoring models using rule extraction from support vector machines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European journal of operational research</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1466" to="1476"/>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Martens, B. Baesens, T. Van Gestel, and J. Vanthienen. Comprehensible credit scoring models using rule extraction from support vector machines. European journal of operational research, 183(3):1466-1476, 2007.</note>
</biblStruct>

<biblStruct coords="43,147.22,274.13,333.37,7.86;43,156.13,285.09,324.46,7.86;43,156.13,296.05,20.99,7.86" xml:id="b66">
	<analytic>
		<title level="a" type="main">Performance of classification models from a user perspective</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Verbeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="782" to="793"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Martens, J. Vanthienen, W. Verbeke, and B. Baesens. Performance of classifi- cation models from a user perspective. Decision Support Systems, 51(4):782-793, 2011.</note>
</biblStruct>

<biblStruct coords="43,147.22,307.02,333.37,7.86;43,156.13,317.97,82.42,7.86" xml:id="b67">
	<analytic>
		<title level="a" type="main">Explanation in recommender systems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="197"/>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. McSherry. Explanation in recommender systems. Artificial Intelligence Review, 24(2):179-197, 2005.</note>
</biblStruct>

<biblStruct coords="43,147.22,328.94,333.37,7.86;43,156.13,339.90,324.46,7.86;43,156.13,350.86,211.74,7.86" xml:id="b68">
	<analytic>
		<title level="a" type="main">Id2-of-3: Constructive induction of m-of-n concepts for discriminators in decision trees</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth international workshop on machine learning</title>
		<meeting>the eighth international workshop on machine learning</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="183" to="187"/>
		</imprint>
	</monogr>
	<note type="raw_reference">P. M. Murphy and M. J. Pazzani. Id2-of-3: Constructive induction of m-of-n con- cepts for discriminators in decision trees. In Proceedings of the eighth international workshop on machine learning, pages 183-187, 1991.</note>
</biblStruct>

<biblStruct coords="43,147.22,361.82,333.37,7.86;43,156.13,372.78,324.46,7.86;43,156.13,383.74,320.33,7.86" xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436"/>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 427-436, 2015.</note>
</biblStruct>

<biblStruct coords="43,147.22,394.71,333.37,7.86;43,156.13,405.67,157.25,7.86" xml:id="b70">
	<analytic>
		<title level="a" type="main">Rule extraction from support vector machines</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Angulo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Català</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Esann</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="107" to="112"/>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Núñez, C. Angulo, and A. Català. Rule extraction from support vector ma- chines. In Esann, pages 107-112, 2002.</note>
</biblStruct>

<biblStruct coords="43,147.22,416.63,333.36,7.86;43,156.13,427.59,324.46,7.86;43,156.13,438.55,173.24,7.86" xml:id="b71">
	<analytic>
		<title level="a" type="main">Illuminating the black box: a randomization approach for understanding variable contributions in artificial neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Olden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological modelling</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="150"/>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. D. Olden and D. A. Jackson. Illuminating the black box: a randomization approach for understanding variable contributions in artificial neural networks. Ecological modelling, 154(1):135-150, 2002.</note>
</biblStruct>

<biblStruct coords="43,147.22,449.52,333.37,7.86;43,156.13,460.48,324.46,7.86;43,156.13,471.44,319.59,7.86" xml:id="b72">
	<analytic>
		<title level="a" type="main">Improving the interpretability of classification rules discovered by an ant colony algorithm</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">E</forename><surname>Otero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th annual conference on Genetic and evolutionary computation</title>
		<meeting>the 15th annual conference on Genetic and evolutionary computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="80"/>
		</imprint>
	</monogr>
	<note type="raw_reference">F. E. Otero and A. A. Freitas. Improving the interpretability of classification rules discovered by an ant colony algorithm. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 73-80. ACM, 2013.</note>
</biblStruct>

<biblStruct coords="43,147.22,482.40,333.37,7.86;43,156.13,493.36,295.90,7.86" xml:id="b73">
	<analytic>
		<title level="a" type="main">Predicting post-synaptic activity in proteins with data mining</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Pappa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>suppl 2):ii19-ii25</note>
	<note type="raw_reference">G. L. Pappa, A. J. Baines, and A. A. Freitas. Predicting post-synaptic activity in proteins with data mining. Bioinformatics, 21(suppl 2):ii19-ii25, 2005.</note>
</biblStruct>

<biblStruct coords="43,147.22,504.33,333.37,7.86;43,156.13,515.29,182.15,7.86" xml:id="b74">
	<monogr>
		<title level="m" type="main">The black box society: The secret algorithms that control money and information</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pasquale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Pasquale. The black box society: The secret algorithms that control money and information. Harvard University Press, 2015.</note>
</biblStruct>

<biblStruct coords="43,147.22,526.25,333.37,7.86;43,156.13,537.21,324.46,7.86;43,156.13,548.17,82.42,7.86" xml:id="b75">
	<analytic>
		<title level="a" type="main">Acceptance of rules generated by machine learning among medical experts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Shankle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods of information in medicine</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="380" to="385"/>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. J. Pazzani, S. Mani, W. R. Shankle, et al. Acceptance of rules generated by machine learning among medical experts. Methods of information in medicine, 40(5):380-385, 2001.</note>
</biblStruct>

<biblStruct coords="43,147.22,559.14,333.37,7.86;43,156.13,570.09,324.46,7.86;43,156.13,581.05,223.00,7.86" xml:id="b76">
	<analytic>
		<title level="a" type="main">Discrimination-aware data mining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pedreshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="560" to="568"/>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Pedreshi, S. Ruggieri, and F. Turini. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 560-568. ACM, 2008.</note>
</biblStruct>

<biblStruct coords="43,147.22,592.02,333.37,7.86;43,156.13,602.98,118.83,7.86" xml:id="b77">
	<analytic>
		<title level="a" type="main">Generating production rules from decision trees</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ijcai</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="304" to="307"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. R. Quinlan. Generating production rules from decision trees. In ijcai, vol- ume 87, pages 304-307, 1987.</note>
</biblStruct>

<biblStruct coords="43,147.22,613.94,333.37,7.86;43,156.13,624.90,115.35,7.86" xml:id="b78">
	<analytic>
		<title level="a" type="main">Simplifying decision trees</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of man-machine studies</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="234"/>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. R. Quinlan. Simplifying decision trees. International journal of man-machine studies, 27(3):221-234, 1987.</note>
</biblStruct>

<biblStruct coords="43,147.22,635.87,291.78,7.86" xml:id="b79">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4. 5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. R. Quinlan. C4. 5: Programs for Machine Learning. Elsevier, 1993.</note>
</biblStruct>

<biblStruct coords="43,147.22,646.84,333.37,7.86;43,156.13,657.79,242.08,7.86" xml:id="b80">
	<analytic>
		<title level="a" type="main">Foil: A midterm report</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Cameron-Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1" to="20"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. R. Quinlan and R. M. Cameron-Jones. Foil: A midterm report. In European conference on machine learning, pages 1-20. Springer, 1993.</note>
</biblStruct>

<biblStruct coords="44,147.22,120.67,333.37,7.86;44,156.13,131.63,251.20,7.86" xml:id="b81">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Radford, R. Jozefowicz, and I. Sutskever. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.</note>
</biblStruct>

<biblStruct coords="44,147.22,141.99,333.37,7.86;44,156.13,152.95,233.18,7.86" xml:id="b82">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05386</idno>
		<title level="m">Model-agnostic interpretability of machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">M. T. Ribeiro, S. Singh, and C. Guestrin. Model-agnostic interpretability of machine learning. arXiv preprint arXiv:1606.05386, 2016.</note>
</biblStruct>

<biblStruct coords="44,147.22,163.30,333.37,7.86;44,156.13,174.26,324.46,7.86;44,156.13,185.22,97.10,7.86" xml:id="b83">
	<monogr>
		<title level="m" type="main">Nothing else matters: Modelagnostic explanations by identifying prediction invariance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05817</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">M. T. Ribeiro, S. Singh, and C. Guestrin. Nothing else matters: Model- agnostic explanations by identifying prediction invariance. arXiv preprint arXiv:1611.05817, 2016.</note>
</biblStruct>

<biblStruct coords="44,147.22,195.58,333.37,7.86;44,156.13,206.54,324.46,7.86;44,156.13,217.50,324.46,7.86;44,156.13,228.46,48.38,7.86" xml:id="b84">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. T. Ribeiro, S. Singh, and C. Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining, pages 1135-1144. ACM, 2016.</note>
</biblStruct>

<biblStruct coords="44,156.13,238.81,324.45,7.86;44,156.13,249.77,231.94,7.86" xml:id="b85">
	<analytic>
		<title level="a" type="main">A multidisciplinary survey on discrimination analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Romei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="582" to="638"/>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Romei and S. Ruggieri. A multidisciplinary survey on discrimination analysis. The Knowledge Engineering Review, 29(5):582-638, 2014.</note>
</biblStruct>

<biblStruct coords="44,147.22,260.13,333.37,7.86;44,156.13,271.09,82.42,7.86" xml:id="b86">
	<analytic>
		<title level="a" type="main">Sensitivity analysis for importance assessment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Saltelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Risk analysis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="579" to="590"/>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Saltelli. Sensitivity analysis for importance assessment. Risk analysis, 22(3):579-590, 2002.</note>
</biblStruct>

<biblStruct coords="44,147.22,281.45,333.37,7.86;44,156.13,292.41,324.46,7.86;44,156.13,303.36,324.46,7.86;44,156.13,314.32,195.69,7.86" xml:id="b87">
	<analytic>
		<title level="a" type="main">Confident interpretation of bayesian decision tree ensembles for clinical applications</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Schetinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Fieldsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Coats</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Krzanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Everson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="319"/>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">V. Schetinin, J. E. Fieldsend, D. Partridge, T. J. Coats, W. J. Krzanowski, R. M. Everson, T. C. Bailey, and A. Hernandez. Confident interpretation of bayesian de- cision tree ensembles for clinical applications. IEEE Transactions on Information Technology in Biomedicine, 11(3):312-319, 2007.</note>
</biblStruct>

<biblStruct coords="44,147.22,324.68,333.37,7.86;44,156.13,335.64,324.46,7.86;44,156.13,346.60,324.46,7.86;44,156.13,357.56,20.99,7.86" xml:id="b88">
	<analytic>
		<title level="a" type="main">Visualizations of deep neural networks in computer vision: A survey</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aamir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balagopalan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Grottel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transparent Data Mining for Big and Small Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="123" to="144"/>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Seifert, A. Aamir, A. Balagopalan, D. Jain, A. Sharma, S. Grottel, and S. Gumhold. Visualizations of deep neural networks in computer vision: A survey. In Transparent Data Mining for Big and Small Data, pages 123-144. Springer, 2017.</note>
</biblStruct>

<biblStruct coords="44,147.22,367.92,333.37,7.86;44,156.13,378.87,324.47,7.86;44,156.13,389.83,272.40,7.86" xml:id="b89">
	<monogr>
		<title level="m" type="main">Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02391</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, and D. Batra. Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization. arXiv preprint arXiv:1610.02391, 2016.</note>
</biblStruct>

<biblStruct coords="44,147.22,400.19,333.37,7.86;44,156.13,411.15,226.77,7.86" xml:id="b90">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">R. Shwartz-Ziv and N. Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.</note>
</biblStruct>

<biblStruct coords="44,147.22,421.51,333.37,7.86;44,156.13,432.47,317.79,7.86" xml:id="b91">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Malioutov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07361</idno>
		<title level="m">Interpretable two-level boolean rule learning for classification</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">G. Su, D. Wei, K. R. Varshney, and D. M. Malioutov. Interpretable two-level boolean rule learning for classification. arXiv preprint arXiv:1511.07361, 2015.</note>
</biblStruct>

<biblStruct coords="44,147.22,442.82,333.37,7.86;44,156.13,453.78,158.15,7.86" xml:id="b92">
	<monogr>
		<title level="m" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01365</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365, 2017.</note>
</biblStruct>

<biblStruct coords="44,147.22,464.14,333.37,7.86;44,156.13,475.10,324.47,7.86;44,156.13,486.06,92.38,7.86" xml:id="b93">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.</note>
</biblStruct>

<biblStruct coords="44,147.22,496.42,333.37,7.86;44,156.13,507.38,310.14,7.86" xml:id="b94">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Wells</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07115</idno>
		<title level="m">Tree space prototypes: Another look at making tree ensembles interpretable</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">H. F. Tan, G. Hooker, and M. T. Wells. Tree space prototypes: Another look at making tree ensembles interpretable. arXiv preprint arXiv:1611.07115, 2016.</note>
</biblStruct>

<biblStruct coords="44,147.22,517.73,320.39,7.86" xml:id="b95">
	<monogr>
		<title level="m" type="main">Introduction to data mining</title>
		<author>
			<persName coords=""><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">P.-N. Tan et al. Introduction to data mining. Pearson Education India, 2006.</note>
</biblStruct>

<biblStruct coords="44,147.22,528.09,333.37,7.86;44,156.13,539.05,324.46,7.86;44,156.13,550.01,97.10,7.86" xml:id="b96">
	<monogr>
		<title level="m" type="main">Treeview: Peeking into deep neural networks via feature-space partitioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">N</forename><surname>Ramamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07429</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">J. J. Thiagarajan, B. Kailkhura, P. Sattigeri, and K. N. Ramamurthy. Treeview: Peeking into deep neural networks via feature-space partitioning. arXiv preprint arXiv:1611.07429, 2016.</note>
</biblStruct>

<biblStruct coords="44,147.22,560.37,333.37,7.86;44,156.13,571.33,324.46,7.86;44,156.13,582.28,324.46,7.86;44,156.13,593.24,145.52,7.86" xml:id="b97">
	<analytic>
		<title level="a" type="main">Interpretable predictions of tree-based ensembles via actionable feature tweaking</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tolomei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="465" to="474"/>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Tolomei, F. Silvestri, A. Haines, and M. Lalmas. Interpretable predictions of tree-based ensembles via actionable feature tweaking. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 465-474. ACM, 2017.</note>
</biblStruct>

<biblStruct coords="44,147.22,603.60,333.37,7.86;44,156.13,614.56,324.46,7.86;44,156.13,625.52,20.99,7.86" xml:id="b98">
	<analytic>
		<title level="a" type="main">A model explanation system</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Signal Processing (MLSP), 2016 IEEE 26th International Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6"/>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Turner. A model explanation system. In Machine Learning for Signal Pro- cessing (MLSP), 2016 IEEE 26th International Workshop on, pages 1-6. IEEE, 2016.</note>
</biblStruct>

<biblStruct coords="44,147.22,635.88,333.37,7.86;44,156.13,646.84,324.47,7.86;44,156.13,657.79,201.97,7.86" xml:id="b99">
	<analytic>
		<title level="a" type="main">Building comprehensible customer churn prediction models with advanced rule induction techniques</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Verbeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2354" to="2364"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Verbeke, D. Martens, C. Mues, and B. Baesens. Building comprehensible cus- tomer churn prediction models with advanced rule induction techniques. Expert Systems with Applications, 38(3):2354-2364, 2011.</note>
</biblStruct>

<biblStruct coords="45,147.05,120.67,333.54,7.86;45,156.13,131.63,324.46,7.86;45,156.13,142.59,153.45,7.86" xml:id="b100">
	<analytic>
		<title level="a" type="main">Hoggles: Visualizing object detection features</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8"/>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. Hoggles: Visualizing object detection features. In Proceedings of the IEEE International Conference on Computer Vision, pages 1-8, 2013.</note>
</biblStruct>

<biblStruct coords="45,147.05,153.55,333.54,7.86;45,156.13,164.51,324.46,7.86;45,156.13,175.46,202.84,7.86" xml:id="b101">
	<analytic>
		<title level="a" type="main">Why a right to explanation of automated decision-making does not exist in the general data protection regulation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Floridi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Data Privacy Law</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="76" to="99"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Wachter, B. Mittelstadt, and L. Floridi. Why a right to explanation of auto- mated decision-making does not exist in the general data protection regulation. International Data Privacy Law, 7(2):76-99, 2017.</note>
</biblStruct>

<biblStruct coords="45,147.05,186.42,333.54,7.86;45,156.13,197.38,93.23,7.86" xml:id="b102">
	<analytic>
		<title level="a" type="main">Falling rule lists</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1013" to="1022"/>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Wang and C. Rudin. Falling rule lists. In Artificial Intelligence and Statistics, pages 1013-1022, 2015.</note>
</biblStruct>

<biblStruct coords="45,147.05,208.34,333.53,7.86;45,156.13,219.30,324.46,7.86;45,156.13,230.26,324.46,7.86;45,156.13,241.22,72.44,7.86" xml:id="b103">
	<analytic>
		<title level="a" type="main">Trading interpretability for accuracy: Oblique treed sparse additive models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fujimaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Motohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1245" to="1254"/>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Wang, R. Fujimaki, and Y. Motohashi. Trading interpretability for accuracy: Oblique treed sparse additive models. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1245- 1254. ACM, 2015.</note>
</biblStruct>

<biblStruct coords="45,147.05,252.18,333.54,7.86;45,156.13,263.14,324.46,7.86;45,156.13,274.09,286.07,7.86" xml:id="b104">
	<analytic>
		<title level="a" type="main">Bayesian rule sets for interpretable classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Velez-Doshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Macneille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2016 IEEE 16th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1269" to="1274"/>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Wang, C. Rudin, F. Velez-Doshi, Y. Liu, E. Klampfl, and P. MacNeille. Bayesian rule sets for interpretable classification. In Data Mining (ICDM), 2016 IEEE 16th International Conference on, pages 1269-1274. IEEE, 2016.</note>
</biblStruct>

<biblStruct coords="45,147.05,285.05,333.54,7.86;45,156.13,296.01,324.46,7.86;45,156.13,306.97,174.93,7.86" xml:id="b105">
	<analytic>
		<title level="a" type="main">Reconstructing an image from its local descriptors</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="337" to="344"/>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Weinzaepfel, H. Jégou, and P. Pérez. Reconstructing an image from its local descriptors. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 337-344. IEEE, 2011.</note>
</biblStruct>

<biblStruct coords="45,147.05,317.93,330.22,7.86" xml:id="b106">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01870</idno>
		<title level="m">Challenges for transparency</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Weller. Challenges for transparency. arXiv preprint arXiv:1708.01870, 2017.</note>
</biblStruct>

<biblStruct coords="45,147.05,328.89,333.54,7.86;45,156.13,339.85,324.47,7.86;45,156.13,350.81,161.22,7.86" xml:id="b107">
	<analytic>
		<title level="a" type="main">A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wettschereck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lazy learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="273" to="314"/>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Wettschereck, D. W. Aha, and T. Mohri. A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms. In Lazy learning, pages 273-314. Springer, 1997.</note>
</biblStruct>

<biblStruct coords="45,147.05,361.77,333.53,7.86;45,156.13,372.73,324.46,7.86;45,156.13,383.68,324.46,7.86;45,156.13,394.64,20.99,7.86" xml:id="b108">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057"/>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048-2057, 2015.</note>
</biblStruct>

<biblStruct coords="45,147.05,405.60,333.54,7.86;45,156.13,416.56,324.46,7.86;45,156.13,427.52,88.31,7.86" xml:id="b109">
	<analytic>
		<title level="a" type="main">Cpar: Classification based on predictive association rules</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 SIAM International Conference on Data Mining</title>
		<meeting>the 2003 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="331" to="335"/>
		</imprint>
	</monogr>
	<note type="raw_reference">X. Yin and J. Han. Cpar: Classification based on predictive association rules. In Proceedings of the 2003 SIAM International Conference on Data Mining, pages 331-335. SIAM, 2003.</note>
</biblStruct>

<biblStruct coords="45,147.05,438.48,333.54,7.86;45,156.13,449.44,310.36,7.86" xml:id="b110">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.</note>
</biblStruct>

<biblStruct coords="45,147.05,460.40,333.54,7.86;45,156.13,471.36,324.46,7.86;45,156.13,482.31,20.99,7.86" xml:id="b111">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833"/>
		</imprint>
	</monogr>
	<note type="raw_reference">M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional net- works. In European conference on computer vision, pages 818-833. Springer, 2014.</note>
</biblStruct>

<biblStruct coords="45,147.05,493.27,333.54,7.86;45,156.13,504.23,324.46,7.86;45,156.13,515.19,20.99,7.86" xml:id="b112">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.</note>
</biblStruct>

<biblStruct coords="45,147.05,526.15,333.54,7.86;45,156.13,537.11,324.46,7.86;45,156.13,548.07,281.92,7.86" xml:id="b113">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929"/>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2921-2929, 2016.</note>
</biblStruct>

<biblStruct coords="45,147.05,559.03,333.54,7.86;45,156.13,569.99,132.38,7.86" xml:id="b114">
	<monogr>
		<title level="m" type="main">Interpreting models via single tree approximation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09036</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Y. Zhou and G. Hooker. Interpreting models via single tree approximation. arXiv preprint arXiv:1610.09036, 2016.</note>
</biblStruct>

<biblStruct coords="45,147.05,580.94,333.54,7.86;45,156.13,591.90,263.95,7.86" xml:id="b115">
	<analytic>
		<title level="a" type="main">Extracting symbolic rules from trained neural network ensembles</title>
		<author>
			<persName coords=""><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ai Communications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15"/>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z.-H. Zhou, Y. Jiang, and S.-F. Chen. Extracting symbolic rules from trained neural network ensembles. Ai Communications, 16(1):3-15, 2003.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>